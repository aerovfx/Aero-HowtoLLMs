import { Phase } from "./Walkthrough";
import { commentary, IWalkthroughArgs } from "./WalkthroughTools";

export function walkthrough10_MicroGPT({ state, layout, walkthrough: wt, tools }: IWalkthroughArgs) {
    let { c_str, c_blockRef, c_dimRef, atTime, afterTime, cleanup, breakAfter } = tools;

    // Only run when in MicroGPT phase
    if (wt.phase !== Phase.MicroGPT_Intro) {
        return;
    }


    // Introduction to MicroGPT
    commentary(wt)`
        ChÃ o má»«ng Ä‘áº¿n vá»›i **MicroGPT** - má»™t phiÃªn báº£n cá»±c ká»³ Ä‘Æ¡n giáº£n cá»§a GPT Ä‘Æ°á»£c thiáº¿t káº¿ bá»Ÿi Andrej Karpathy.
        
        MicroGPT chá»‰ cÃ³ **4.336 tham sá»‘**, nhá» hÆ¡n gáº§n 20 láº§n so vá»›i Nano-GPT (85K tham sá»‘).
        
        ÄÃ¢y lÃ  mÃ´ hÃ¬nh lÃ½ tÆ°á»Ÿng Ä‘á»ƒ **hiá»ƒu rÃµ báº£n cháº¥t** cá»§a Transformer mÃ  khÃ´ng bá»‹ phÃ¢n tÃ¢m bá»Ÿi cÃ¡c chi tiáº¿t phá»©c táº¡p.
    `;

    commentary(wt)`
        ### Táº¡i sao MicroGPT quan trá»ng?
        
        MicroGPT chá»©ng minh ráº±ng báº¡n **khÃ´ng cáº§n** má»™t mÃ´ hÃ¬nh khá»•ng lá»“ Ä‘á»ƒ hiá»ƒu Transformer:
        
        - âœ¨ **ÄÆ¡n giáº£n**: Chá»‰ 1 layer, dá»… debug vÃ  visualize
        - âš¡ **Nhanh**: Huáº¥n luyá»‡n trong vÃ i giÃ¢y trÃªn CPU
        - ğŸ“ **Há»c táº­p**: Má»—i component Ä‘á»u rÃµ rÃ ng vÃ  dá»… theo dÃµi
        - ğŸ”¬ **Thá»­ nghiá»‡m**: Thay Ä‘á»•i kiáº¿n trÃºc vÃ  tháº¥y káº¿t quáº£ ngay láº­p tá»©c
    `;

    breakAfter();

    // Architecture overview
    commentary(wt)`
        ### Kiáº¿n trÃºc tá»•ng quan
        
        MicroGPT sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t Ä‘Æ¡n giáº£n hÃ³a:
        
        1. **RMSNorm** thay vÃ¬ LayerNorm (khÃ´ng cáº§n tÃ­nh mean)
        2. **KhÃ´ng cÃ³ bias** trong cÃ¡c lá»›p tuyáº¿n tÃ­nh
        3. **Square ReLU** thay vÃ¬ GELU
        4. Chá»‰ **1 layer Transformer** thay vÃ¬ nhiá»u layers
        5. **Hidden dimension nhá»**: 16 thay vÃ¬ 48
        6. **Context window**: 32 tokens
    `;

    // Architecture visualization
    let t0 = afterTime(null, 1.0);
    cleanup(t0);

    commentary(wt)`
        ### Quan sÃ¡t kiáº¿n trÃºc 3D
        
        HÃ£y quan sÃ¡t mÃ´ hÃ¬nh 3D bÃªn trÃ¡i. LÆ°u Ã½ ráº±ng MicroGPT chá»‰ cÃ³:
        
        - ğŸ“¦ **1 khá»‘i Transformer** (thay vÃ¬ 3 nhÆ° Nano-GPT)
        - ğŸ”µ **RMS Norm** xuáº¥t hiá»‡n 3 láº§n (mÃ u xanh cyan)
        - ğŸŸ¡ **Tá»± chÃº Ã½ Ä‘Æ¡n giáº£n** (4 heads, má»—i head chá»‰ 4 dims)
        - ğŸŸ  **MLP vá»›i Square ReLU** (mÃ u cam)
        - âšª **KhÃ´ng cÃ³ bias blocks** (so sÃ¡nh vá»›i Nano-GPT)
        
        Kiáº¿n trÃºc tá»‘i giáº£n nÃ y giÃºp báº¡n táº­p trung vÃ o **luá»“ng dá»¯ liá»‡u** thay vÃ¬ chi tiáº¿t phá»©c táº¡p.
    `;

    breakAfter();

    // RMSNorm explanation
    commentary(wt)`
        ### RMSNorm - ÄÆ¡n giáº£n hÃ³a Normalization
        
        **RMSNorm** (Root Mean Square Normalization) loáº¡i bá» bÆ°á»›c tÃ­nh mean:
        
        **LayerNorm** (phá»©c táº¡p):
        \`\`\`
        mean = sum(x) / n
        variance = sum((x - mean)Â²) / n
        output = (x - mean) / sqrt(variance + Îµ)
        \`\`\`
        
        **RMSNorm** (Ä‘Æ¡n giáº£n):
        \`\`\`
        rms = sqrt(sum(xÂ²) / n)
        output = x / (rms + Îµ)
        \`\`\`
        
        ğŸ’¡ **Lá»£i Ã­ch**:
        - Giáº£m 50% phÃ©p tÃ­nh
        - KhÃ´ng cáº§n lÆ°u mean trong backward pass
        - Váº«n á»•n Ä‘á»‹nh training tá»‘t!
    `;

    breakAfter();

    // No bias explanation
    commentary(wt)`
        ### Loáº¡i bá» Bias - Giáº£m tham sá»‘
        
        MicroGPT **loáº¡i bá» táº¥t cáº£ bias** trong cÃ¡c lá»›p Linear:
        
        **Vá»›i bias** (Nano-GPT):
        \`\`\`python
        y = x @ W + b  # b lÃ  bias vector
        \`\`\`
        
        **KhÃ´ng bias** (MicroGPT):
        \`\`\`python
        y = x @ W  # Chá»‰ cÃ³ weight matrix
        \`\`\`
        
        ğŸ“Š **TÃ¡c Ä‘á»™ng**:
        - Giáº£m ~5-10% tá»•ng sá»‘ tham sá»‘
        - ÄÆ¡n giáº£n hÃ³a tÃ­nh toÃ¡n
        - Váº«n Ä‘á»§ kháº£ nÄƒng biá»ƒu diá»…n cho character-level tasks
        
        âš ï¸ **LÆ°u Ã½**: Vá»›i cÃ¡c tÃ¡c vá»¥ phá»©c táº¡p, bias váº«n quan trá»ng!
    `;

    breakAfter();

    // Square ReLU
    commentary(wt)`
        ### Square ReLU - Activation Ä‘Æ¡n giáº£n
        
        **Square ReLU** thay tháº¿ GELU vá»›i cÃ´ng thá»©c cá»±c ká»³ Ä‘Æ¡n giáº£n:
        
        **GELU** (phá»©c táº¡p):
        \`\`\`python
        GELU(x) = x * Î¦(x)  # Î¦ lÃ  CDF cá»§a Gaussian
        # Hoáº·c xáº¥p xá»‰: x * Ïƒ(1.702 * x)
        \`\`\`
        
        **Square ReLU** (Ä‘Æ¡n giáº£n):
        \`\`\`python
        SquareReLU(x) = (max(0, x))Â²
        # Hoáº·c: ReLU(x) * ReLU(x)
        \`\`\`
        
        ğŸ“ˆ **Äáº·c Ä‘iá»ƒm**:
        - Cá»±c ká»³ nhanh (chá»‰ 1 phÃ©p so sÃ¡nh + 1 phÃ©p nhÃ¢n)
        - Gradient Ä‘Æ¡n giáº£n: 2x náº¿u x > 0, 0 náº¿u x â‰¤ 0
        - Hoáº¡t Ä‘á»™ng tá»‘t cho cÃ¡c mÃ´ hÃ¬nh nhá»
    `;

    breakAfter();

    // Comparison with Nano-GPT
    commentary(wt)`
        ### So sÃ¡nh MicroGPT vs Nano-GPT
        
        | Äáº·c Ä‘iá»ƒm | MicroGPT | Nano-GPT | Tá»· lá»‡ |
        |----------|----------|----------|-------|
        | **Tham sá»‘** | 4.336 | 85.584 | 1:20 |
        | **Layers** | 1 | 3 | 1:3 |
        | **Hidden dim** | 16 | 48 | 1:3 |
        | **Heads** | 4 | 4 | 1:1 |
        | **Head dim** | 4 | 12 | 1:3 |
        | **Context** | 32 tokens | 11 tokens | 3:1 |
        | **Norm** | RMSNorm | LayerNorm | - |
        | **Bias** | âŒ KhÃ´ng | âœ… CÃ³ | - |
        | **Activation** | Square ReLU | GELU | - |
        
        ğŸ¯ **Káº¿t luáº­n**: MicroGPT nhá» hÆ¡n 20 láº§n nhÆ°ng váº«n giá»¯ Ä‘Æ°á»£c cáº¥u trÃºc Transformer cÆ¡ báº£n!
    `;

    breakAfter();

    // Training characteristics
    commentary(wt)`
        ### Äáº·c Ä‘iá»ƒm huáº¥n luyá»‡n
        
        MicroGPT huáº¥n luyá»‡n **cá»±c ká»³ nhanh**:
        
        âš¡ **Tá»‘c Ä‘á»™**:
        - Má»—i epoch: ~2-5 giÃ¢y trÃªn CPU
        - Forward pass: ~0.1ms
        - Backward pass: ~0.2ms
        
        ğŸ’¾ **Bá»™ nhá»›**:
        - Model size: ~17KB (4.336 params Ã— 4 bytes)
        - Activations: ~2KB per batch
        - CÃ³ thá»ƒ cháº¡y trÃªn báº¥t ká»³ thiáº¿t bá»‹ nÃ o!
        
        ğŸ“š **Dá»¯ liá»‡u**:
        - PhÃ¹ há»£p vá»›i datasets nhá» (vÃ i MB)
        - Character-level tokenization
        - CÃ³ thá»ƒ overfit nhanh (cáº§n regularization)
    `;

    breakAfter();

    // Use cases
    commentary(wt)`
        ### Khi nÃ o nÃªn dÃ¹ng MicroGPT?
        
        âœ… **NÃªn dÃ¹ng khi**:
        - ğŸ“ Há»c cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a Transformer
        - ğŸ”¬ Thá»­ nghiá»‡m kiáº¿n trÃºc má»›i (attention variants, normalization, etc.)
        - ğŸ› Debug vÃ  phÃ¢n tÃ­ch chi tiáº¿t tá»«ng component
        - ğŸ“ TÃ¡c vá»¥ character-level Ä‘Æ¡n giáº£n (tÃªn, mÃ£ code ngáº¯n)
        - âš¡ Cáº§n káº¿t quáº£ nhanh Ä‘á»ƒ iterate
        - ğŸ’» KhÃ´ng cÃ³ GPU máº¡nh
        
        âŒ **KhÃ´ng nÃªn dÃ¹ng khi**:
        - ğŸ­ Production applications
        - ğŸ“– Xá»­ lÃ½ ngá»¯ cáº£nh dÃ i vÃ  phá»©c táº¡p
        - ğŸŒ Multi-lingual tasks
        - ğŸ¯ Cáº§n Ä‘á»™ chÃ­nh xÃ¡c cao
        - ğŸ“Š Datasets lá»›n (>100MB)
    `;

    breakAfter();

    // Code example
    commentary(wt)`
        ### VÃ­ dá»¥ code MicroGPT
        
        \`\`\`python
        # Äá»‹nh nghÄ©a MicroGPT config
        config = {
            'vocab_size': 32,      # Character-level
            'n_layer': 1,          # Chá»‰ 1 transformer block
            'n_head': 4,           # 4 attention heads
            'n_embd': 16,          # Hidden dimension
            'block_size': 32,      # Context length
            'bias': False,         # KhÃ´ng cÃ³ bias
            'norm_type': 'rmsnorm', # RMSNorm
            'activation': 'squared_relu',
        }
        
        # Khá»Ÿi táº¡o model
        model = MicroGPT(config)
        print(f"Parameters: {sum(p.numel() for p in model.parameters()):,}")
        # Output: Parameters: 4,336
        \`\`\`
    `;

    breakAfter();

    // Practical tips
    commentary(wt)`
        ### Máº¹o thá»±c hÃ nh vá»›i MicroGPT
        
        ğŸ¯ **Äá»ƒ há»c tá»‘t nháº¥t**:
        
        1. **Báº¯t Ä‘áº§u Ä‘Æ¡n giáº£n**: Train trÃªn Shakespeare hoáº·c tÃªn ngÆ°á»i
        2. **Visualize**: In ra attention weights vÃ  activations
        3. **Thá»­ nghiá»‡m**: Thay Ä‘á»•i tá»«ng component má»™t
        4. **So sÃ¡nh**: Äo lÆ°á»ng tÃ¡c Ä‘á»™ng cá»§a má»—i thay Ä‘á»•i
        5. **Scale up**: Khi hiá»ƒu rÃµ, chuyá»ƒn sang Nano-GPT
        
        ğŸ”§ **Modifications hay**:
        - Thá»­ LayerNorm vs RMSNorm
        - So sÃ¡nh ReLU, GELU, Square ReLU
        - ThÃªm/bá»›t attention heads
        - Thay Ä‘á»•i hidden dimension
    `;

    breakAfter();

    // Conclusion
    commentary(wt)`
        ### Káº¿t luáº­n
        
        MicroGPT lÃ  má»™t **cÃ´ng cá»¥ há»c táº­p tuyá»‡t vá»i**!
        
        ğŸ“ **BÃ i há»c quan trá»ng**:
        - Transformer cÃ³ thá»ƒ cá»±c ká»³ Ä‘Æ¡n giáº£n
        - Nhiá»u ká»¹ thuáº­t phá»©c táº¡p lÃ  khÃ´ng cáº§n thiáº¿t cho cÃ¡c tÃ¡c vá»¥ nhá»
        - ÄÆ¡n giáº£n hÃ³a giÃºp hiá»ƒu rÃµ báº£n cháº¥t cá»§a mÃ´ hÃ¬nh
        - "Simple is better than complex" - Zen of Python
        
        ğŸš€ **BÆ°á»›c tiáº¿p theo**:
        1. Thá»­ nghiá»‡m vá»›i MicroGPT
        2. Hiá»ƒu rÃµ tá»«ng component
        3. Chuyá»ƒn sang Nano-GPT (3 layers)
        4. Cuá»‘i cÃ¹ng, GPT-2 vÃ  cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n
        
        ğŸ’¡ **Nhá»› ráº±ng**: Má»i mÃ´ hÃ¬nh lá»›n Ä‘á»u báº¯t Ä‘áº§u tá»« nhá»¯ng Ã½ tÆ°á»Ÿng Ä‘Æ¡n giáº£n nhÆ° MicroGPT!
    `;

    breakAfter();
}

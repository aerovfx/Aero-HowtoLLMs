# Chapter 1: Understanding Large Language Models

DÆ°á»›i Ä‘Ã¢y lÃ  quy trÃ¬nh chi tiáº¿t Ä‘á»ƒ huáº¥n luyá»‡n má»™t **MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM)** tá»« Ä‘áº§u, dá»±a trÃªn 5 giai Ä‘oáº¡n chÃ­nh

---

### **1. Thu tháº­p vÃ  xá»­ lÃ½ dá»¯ liá»‡u**  
**Má»¥c tiÃªu**: Táº¡o bá»™ dá»¯ liá»‡u Ä‘a dáº¡ng, cháº¥t lÆ°á»£ng, sáºµn sÃ ng cho huáº¥n luyá»‡n.  
- **Nguá»“n dá»¯ liá»‡u**:  
  - VÄƒn báº£n Ä‘a dáº¡ng tá»« sÃ¡ch, bÃ i bÃ¡o, trang web (Common Crawl), code (GitHub), Wikipedia, diá»…n Ä‘Ã n...  
  - VÃ­ dá»¥: GPT-3 dÃ¹ng 570GB vÄƒn báº£n, BERT dÃ¹ng BookCorpus + Wikipedia.  
- **Tiá»n xá»­ lÃ½**:  
  - Lá»c ná»™i dung Ä‘á»™c háº¡i, spam, trÃ¹ng láº·p (sá»­ dá»¥ng **deduplication**).  
  - Chia vÄƒn báº£n thÃ nh cÃ¡c Ä‘oáº¡n/phÃ¢n Ä‘oáº¡n (chunking).  
  - Chuáº©n hÃ³a vÄƒn báº£n: XÃ³a HTML, chuyá»ƒn vá» lowercase (náº¿u cáº§n), xá»­ lÃ½ emoji/biá»ƒu tÆ°á»£ng.  
- **Token hÃ³a**:  
  - Sá»­ dá»¥ng tokenizer nhÆ° **BPE** (Byte-Pair Encoding) hoáº·c **SentencePiece** Ä‘á»ƒ chia vÄƒn báº£n thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ con (subwords).  
  - VÃ­ dá»¥: GPT dÃ¹ng BPE, BERT dÃ¹ng WordPiece.  
- **Xá»­ lÃ½ thiÃªn kiáº¿n (bias)**:  
  - CÃ¢n báº±ng dá»¯ liá»‡u theo giá»›i tÃ­nh, vÃ¹ng miá»n, ngÃ´n ngá»¯.  
  - DÃ¹ng cÃ´ng cá»¥ nhÆ° **DebiasWe** hoáº·c loáº¡i bá» tá»« ngá»¯ gÃ¢y ká»³ thá»‹.  

---

### **2. Chuáº©n bá»‹ kiáº¿n trÃºc mÃ´ hÃ¬nh**  
**Má»¥c tiÃªu**: Thiáº¿t káº¿ kiáº¿n trÃºc phÃ¹ há»£p vá»›i quy mÃ´ vÃ  tÃ i nguyÃªn tÃ­nh toÃ¡n.  
- **Chá»n kiáº¿n trÃºc ná»n**:  
  - **Transformer** (phá»• biáº¿n nháº¥t) vá»›i cÃ¡c biáº¿n thá»ƒ nhÆ° GPT (autoregressive), BERT (bidirectional), T5 (encoder-decoder).  
- **XÃ¡c Ä‘á»‹nh quy mÃ´**:  
  - Sá»‘ lá»›p (layers), sá»‘ head trong cÆ¡ cháº¿ **self-attention**, kÃ­ch thÆ°á»›c embedding.  
  - VÃ­ dá»¥: GPT-3 cÃ³ 96 lá»›p, 175 tá»· tham sá»‘.  
- **Khá»Ÿi táº¡o tham sá»‘**:  
  - DÃ¹ng ká»¹ thuáº­t khá»Ÿi táº¡o trá»ng sá»‘ nhÆ° **He initialization** hoáº·c **Xavier initialization**.  
- **Chuáº©n bá»‹ há»‡ thá»‘ng phÃ¢n tÃ¡n**:  
  - Cáº¥u hÃ¬nh **data parallelism** (chia dá»¯ liá»‡u) hoáº·c **model parallelism** (chia mÃ´ hÃ¬nh) trÃªn nhiá»u GPU/TPU.  
  - Sá»­ dá»¥ng framework nhÆ° **PyTorch Distributed** hoáº·c **TensorFlow Mesh**.  

---

### **3. Huáº¥n luyá»‡n mÃ´ hÃ¬nh (Pretraining)**  
**Má»¥c tiÃªu**: GiÃºp mÃ´ hÃ¬nh há»c biá»ƒu diá»…n ngÃ´n ngá»¯ tá»•ng quÃ¡t tá»« dá»¯ liá»‡u thÃ´.  
- **Tá»‘i Æ°u hÃ³a**:  
  - DÃ¹ng optimizer **AdamW** vá»›i learning rate warmup (vÃ­ dá»¥: tá»« 1e-6 Ä‘áº¿n 3e-4).  
  - Gradient clipping Ä‘á»ƒ trÃ¡nh exploding gradients.  
- **Quáº£n lÃ½ bá»™ nhá»›**:  
  - **Mixed Precision Training** (káº¿t há»£p FP16/FP32) Ä‘á»ƒ tiáº¿t kiá»‡m bá»™ nhá»›.  
  - **Gradient Checkpointing** (tÃ­nh láº¡i activation thay vÃ¬ lÆ°u trá»¯).  
- **Huáº¥n luyá»‡n phÃ¢n tÃ¡n**:  
  - Batch size lá»›n (hÃ ng triá»‡u token), chia trÃªn hÃ ng trÄƒm GPU.  
  - VÃ­ dá»¥: GPT-3 huáº¥n luyá»‡n trÃªn 1.024 GPU V100 trong vÃ i tuáº§n.  
- **Theo dÃµi**:  
  - GiÃ¡m sÃ¡t loss, perplexity, gradient norms.  
  - Kiá»ƒm tra Ä‘á»™ há»™i tá»¥ qua cÃ¡c bÃ i benchmark nhÆ° **LAMBADA** (Ä‘oÃ¡n tá»« tiáº¿p theo trong vÄƒn cáº£nh dÃ i).  

---

### **4. Tinh chá»‰nh mÃ´ hÃ¬nh (Fine-tuning & RLHF)**  
**Má»¥c tiÃªu**: Äiá»u chá»‰nh mÃ´ hÃ¬nh cho cÃ¡c tÃ¡c vá»¥ cá»¥ thá»ƒ hoáº·c cáº£i thiá»‡n hÃ nh vi.  
- **Fine-tuning**:  
  - Huáº¥n luyá»‡n láº¡i trÃªn táº­p dá»¯ liá»‡u nhá», chuyÃªn biá»‡t (vÃ­ dá»¥: há»i Ä‘Ã¡p y táº¿, dá»‹ch mÃ¡y).  
  - DÃ¹ng learning rate nhá» hÆ¡n pretraining (vÃ­ dá»¥: 1e-5) Ä‘á»ƒ trÃ¡nh overfitting.  
- **Reinforcement Learning from Human Feedback (RLHF)**:  
  - **BÆ°á»›c 1**: Thu tháº­p pháº£n há»“i tá»« con ngÆ°á»i (vÃ­ dá»¥: xáº¿p háº¡ng cÃ¡c cÃ¢u tráº£ lá»i).  
  - **BÆ°á»›c 2**: Huáº¥n luyá»‡n **reward model** Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¢u tráº£ lá»i.  
  - **BÆ°á»›c 3**: Tá»‘i Æ°u mÃ´ hÃ¬nh báº±ng PPO (Proximal Policy Optimization) Ä‘á»ƒ tá»‘i Ä‘a pháº§n thÆ°á»Ÿng.  
  - VÃ­ dá»¥: ChatGPT Ä‘Æ°á»£c tinh chá»‰nh báº±ng RLHF Ä‘á»ƒ tÄƒng tÃ­nh há»¯u Ã­ch vÃ  an toÃ n.  

---

### **5. Triá»ƒn khai vÃ  Ä‘Ã¡nh giÃ¡**  
**Má»¥c tiÃªu**: ÄÆ°a mÃ´ hÃ¬nh vÃ o thá»±c táº¿ vÃ  Ä‘o lÆ°á»ng hiá»‡u suáº¥t.  
- **Tá»‘i Æ°u hÃ³a triá»ƒn khai**:  
  - **Quantization**: Giáº£m Ä‘á»™ chÃ­nh xÃ¡c trá»ng sá»‘ (32-bit â†’ 8-bit) Ä‘á»ƒ tÄƒng tá»‘c suy luáº­n.  
  - **Pruning**: Loáº¡i bá» cÃ¡c neuron khÃ´ng quan trá»ng.  
  - **Distillation**: NÃ©n mÃ´ hÃ¬nh lá»›n thÃ nh mÃ´ hÃ¬nh nhá» (vÃ­ dá»¥: DistilBERT).  
- **ÄÃ¡nh giÃ¡**:  
  - **Äo lÆ°á»ng Ä‘á»‹nh lÆ°á»£ng**: Perplexity, Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº­p test (vÃ­ dá»¥: GLUE, SuperGLUE).  
  - **ÄÃ¡nh giÃ¡ Ä‘á»‹nh tÃ­nh**: Kiá»ƒm tra kháº£ nÄƒng táº¡o vÄƒn báº£n máº¡ch láº¡c, trÃ¡nh toxic content.  
  - **Kiá»ƒm tra tÃ­nh cÃ´ng báº±ng**: PhÃ¡t hiá»‡n bias qua cÃ´ng cá»¥ nhÆ° **Fairness Indicators**.  
- **GiÃ¡m sÃ¡t sau triá»ƒn khai**:  
  - Thu tháº­p pháº£n há»“i ngÆ°á»i dÃ¹ng, cáº­p nháº­t mÃ´ hÃ¬nh Ä‘á»‹nh ká»³.  

---

### **VÃ­ dá»¥ vá» quy trÃ¬nh hoÃ n chá»‰nh**  
1. **GPT-4**:  
   - Pretraining trÃªn hÃ ng nghÃ¬n tá»· token tá»« internet â†’ Fine-tuning cho cÃ¡c tÃ¡c vá»¥ cá»¥ thá»ƒ â†’ RLHF Ä‘á»ƒ chá»‰nh hÃ nh vi.  
2. **BLOOM**:  
   - Huáº¥n luyá»‡n trÃªn 46 ngÃ´n ngá»¯, xá»­ lÃ½ bias qua cá»™ng Ä‘á»“ng má»Ÿ â†’ Triá»ƒn khai dÆ°á»›i dáº¡ng mÃ£ nguá»“n má»Ÿ.  

---

### **ThÃ¡ch thá»©c chÃ­nh**  
- **Chi phÃ­**: Huáº¥n luyá»‡n GPT-3 tá»‘n ~$4.6 triá»‡u.  
- **Äáº¡o Ä‘á»©c**: Nguy cÆ¡ lan truyá»n thÃ´ng tin sai lá»‡ch, bias.  
- **MÃ´i trÆ°á»ng**: TiÃªu thá»¥ nÄƒng lÆ°á»£ng lá»›n (vÃ­ dá»¥: huáº¥n luyá»‡n BERT ~1,500 lbs COâ‚‚).  

NÃ o cÃ¹ng báº¯t Ä‘áº§u nhÃ© ğŸš€


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [13 investigating layers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Tháº¥u KÃ­nh Logit (The Logit Lens): Soi SÃ¡ng TÆ° Duy Táº§ng Trung Gian Cá»§a MÃ´ HÃ¬nh NgÃ´n Ngá»¯

## TÃ³m táº¯t (Abstract)
PhÆ°Æ¡ng phÃ¡p Tháº¥u kÃ­nh Logit (Logit Lens) cung cáº¥p má»™t giáº£i phÃ¡p Ä‘á»™t phÃ¡ Ä‘á»ƒ giáº£i mÃ£ "há»™p Ä‘en" cá»§a Large Language Models (LLMs) mÃ  khÃ´ng can thiá»‡p thay Ä‘á»•i kiáº¿n trÃºc mÃ´ hÃ¬nh. Thay vÃ¬ Ä‘á»£i Ä‘áº¿n khi Transformer hoÃ n táº¥t quy trÃ¬nh á»Ÿ lá»›p cuá»‘i cÃ¹ng, ká»¹ thuáº­t nÃ y káº¿t ná»‘i trá»±c tiáº¿p cÃ¡c táº§ng áº©n trung gian (Intermediate Hidden States) vá»›i Ma tráº­n giáº£i mÃ£ nhÃºng (Unembedding Matrix/Language Model Head). Báº±ng viá»‡c sá»­ dá»¥ng GPT-2 Small, bÃ i bÃ¡o cÃ¡o nÃ y minh chá»©ng sá»± kháº£ thi cá»§a viá»‡c giáº£i mÃ£ vÃ  dá»± Ä‘oÃ¡n trá»±c tiáº¿p Token tÆ°Æ¡ng lai á»Ÿ má»i giao Ä‘iá»ƒm ná»™i bá»™, phÃ¡c há»a báº£n Ä‘á»“ tÆ° duy tiáº¿n hÃ³a cá»§a mÃ´ hÃ¬nh qua tá»«ng block.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Kiáº¿n trÃºc tiÃªu chuáº©n cá»§a má»™t mÃ´ hÃ¬nh Transformer (nhÆ° GPT) bao gá»“m:
1. Táº§ng Embedding $Token + Positional$.
2. Chuá»—i cÃ¡c khá»‘i Transformer Blocks (nÆ¡i thá»±c hiá»‡n self-attention vÃ  feed-forward).
3. Táº§ng Unembedding (ThÆ°á»ng lÃ  má»™t ma tráº­n tuyáº¿n tÃ­nh - LM Head) Ä‘á»ƒ phÃ³ng chiáº¿u vector Ä‘áº§u ra vÃ o khÃ´ng gian Tá»« vá»±ng (Vocabulary space).

Má»™t cÃ¢u há»i mang tÃ­nh khiÃªu khÃ­ch Ä‘Æ°á»£c Ä‘áº·t ra vá» máº·t giáº£i thÃ­ch AI (Explainable AI): *"Äiá»u gÃ¬ sáº½ xáº£y ra náº¿u ta cáº¯t ngang mÃ´ hÃ¬nh á»Ÿ Layer thá»© 3 vÃ  Ã©p nÃ³ xuáº¥t dá»± Ä‘oÃ¡n ngay láº­p tá»©c?"* 
PhÆ°Æ¡ng phÃ¡p **Logit Lens** Ä‘Æ°á»£c phÃ¡t minh Ä‘á»™c láº­p trÃªn cá»™ng Ä‘á»“ng LessWrong tráº£ lá»i Ä‘Ãºng trá»ng tÃ¢m cÃ¢u há»i trÃªn: Thay vÃ¬ di chuyá»ƒn tuáº§n tá»± qua máº¡ng, ta giáº£ láº­p rÃºt ngáº¯n quy trÃ¬nh báº±ng cÃ¡ch Ä‘Æ°a tráº¡ng thÃ¡i áº©n táº§ng $L$ táº¡t ngang vÃ o trá»±c tiáº¿p bá»™ phÃ¢n giáº£i tá»« vá»±ng cuá»‘i cÃ¹ng.

---

## 2. NguyÃªn LÃ½ CÆ¡ Sá»± (Methodology)

### 2.1. Báº£n Cháº¥t HÃ¬nh Há»c Cá»§a Transformer
Äáº§u ra cá»§a báº¥t ká»³ má»™t Transformer Block nÃ o cÅ©ng báº£o toÃ n kÃ­ch thÆ°á»›c khÃ´ng gian vector cá»§a Embeddings gá»‘c (VD GPT-2 Small lÃ  768 dimensions). Nhá» sá»± giá»›i háº¡n Ä‘á»“ng nháº¥t Ä‘a chiá»u nÃ y, Ma tráº­n giáº£i mÃ£ cuá»‘i cÃ¹ng (`model.lm_head.weight`) dá»… dÃ ng xem báº¥t ká»³ táº§ng trung gian nÃ o lÃ  "váº­t liá»‡u kháº£ thi" Ä‘á»ƒ nhÃ¢n ma tráº­n. 

### 2.2. TrÃ­ch Xuáº¥t Dá»¯ Liá»‡u
Quy trÃ¬nh giáº£ láº­p Ä‘Æ°á»£c khá»Ÿi táº¡o:
- HÃ m Forward Pass láº¥y máº«u vá»›i tuá»³ chá»n `output_hidden_states=True`. 
- Bá» qua Layer 0 (Embedding ban Ä‘áº§u) vÃ¬ chÆ°a chá»‹u tÃ¡c Ä‘á»™ng há»c táº­p (transformation) nÃ o.
- 12 ma tráº­n Layers Ä‘Æ°á»£c trÃ­ch xuáº¥t (GPT-2 Small) cÃ³ dáº¡ng `[1, seq_length, 768]`.

### 2.3. Giáº£i MÃ£ Sá»›m (Early Decoding)
Sá»­ dá»¥ng cÃ´ng thá»©c chiáº¿u vector:

$$

\text{Logits}_{L} = \text{Hidden\_States}_{L} \times \text{LM\_Head}^T

$$

Tá»« Ä‘Ã³, ta Ã¡p dá»¥ng hÃ m $\text{argmax}$ phÃ¢n bá»• qua $\text{Softmax}$ cho ma tráº­n tá»« vá»±ng 50,000 chiáº¿u, tÃ¬m ra tá»« cÃ³ xÃ¡c suáº¥t cao nháº¥t táº¡i chÃ­nh Layer lÆ¡ lá»­ng Ä‘Ã³.

---

## 3. Kháº£o SÃ¡t ÄÃ¡nh GiÃ¡ (Analysis & Visualizations)

TrÃ­ch dáº«n máº«u dÃ¹ng cho thÃ­ nghiá»‡m: 
*â€œThe way you do anything is the way you do everythingâ€*.

Kháº£o sÃ¡t Ä‘Æ°á»£c thá»±c hiá»‡n song song Ä‘á»ƒ dá»± bÃ¡o Token "káº¿ tiáº¿p":
- **á» Layer Ä‘áº§u tiÃªn (Layer 1):** MÃ´ hÃ¬nh xuáº¥t ra toÃ n vÄƒn báº£n nhiá»…u (garbage) vÃ  hÆ° tá»« ("the", "else"). Äiá»u nÃ y minh chá»©ng táº¡i cháº·ng Ä‘áº§u, mÃ´ hÃ¬nh chá»‰ má»›i loay hoay tá»•ng há»£p ngá»¯ phÃ¡p cá»¥c bá»™ mÃ  chÆ°a mÆ°á»ng tÆ°á»£ng ná»•i chuá»—i nghÄ©a dÃ i.
- **á» Layer thá»© 3:** Khi Ä‘áº¿n Ä‘oáº¡n tÃ­nh toÃ¡n tá»« â€œdoâ€, model dá»± bÃ¡o tá»« tiáº¿p theo sáº½ lÃ  â€œnotâ€ (máº·c dÃ¹ cÃ¢u gá»‘c lÃ  â€œanythingâ€). â€œdo notâ€ lÃ  má»™t chuá»—i n-gram vÃ´ cÃ¹ng há»£p lá»‡ vá» máº·t ngá»¯ nghÄ©a ngá»¯ phÃ¡p ná»™i bá»™, chá»©ng minh kháº£ nÄƒng nhÃºng chÃ©o khá»‘i tá»« báº¯t Ä‘áº§u xuáº¥t hiá»‡n.
- **Nhiá»‡t Äá»“ Chuyá»ƒn MÃ u (Heatmap Visualization):** TÃ­nh toÃ¡n Softmax táº¡i Ä‘á»‹nh dáº¡ng Ma tráº­n tá»•ng $12 \text{ layers} \times \text{seq\_len}$ váº½ lÃªn bá»©c rÃ¨m chuyá»ƒn pha. Logit Lens chá»‰ ra: KhÃ´ng cáº§n Ä‘á»£i Ä‘áº¿n layer 12, báº£n ngÃ£ cá»¥m tá»« Ä‘Ã´i khi "chÃ­n" sá»›m vÃ  Ä‘Æ°á»£c chá»‘t háº¡ Ä‘Ãºng tá»« Layer 7-8 á»Ÿ nhá»¯ng tá»« khÃ³a quan trá»ng.

---

## 4. Káº¿t Luáº­n
Logit Lens khÃ´ng pháº£i thuáº­t toÃ¡n "Ä‘á»c tÃ¢m trÃ­" (Mind-reading) nhÆ° cÃ¡ch giá»›i truyá»n thÃ´ng thá»•i phá»“ng. ÄÃ¢y lÃ  ká»¹ thuáº­t chiáº¿u áº£nh vector nghá»‹ch Ä‘áº£o (Inverse Projection) cho tháº¥y máº¡ng lÆ°á»›i tÃ­nh toÃ¡n Token thay Ä‘á»•i liÃªn tá»¥c. Sá»± chuyá»ƒn mÃ¬nh tá»« dá»± Ä‘oÃ¡n "ngá»¯ phÃ¡p sÆ¡ cáº¥p" á»Ÿ táº§ng nÃ´ng Ä‘áº¿n "ngá»¯ nghÄ©a trá»«u tÆ°á»£ng" á»Ÿ táº§ng sÃ¢u má»Ÿ ra má»™t kho tÃ ng khá»•ng lá»“ cho viá»‡c sá»­a chá»¯a sai lá»‡ch mÃ´ hÃ¬nh (hallucination fixes) tá»« táº­n lÃµi kiáº¿n trÃºc.

---

## TÃ i Liá»‡u Tham Kháº£o (Citations)
1. Thá»±c nghiá»‡m mÃ´ phá»ng dá»±a theo lÃ½ thuyáº¿t tá»« bÃ i bÃ¡o "Logit Lens" trÃªn diá»…n Ä‘Ã n LessWrong, á»©ng dá»¥ng vÃ o GPT-2 báº±ng PyTorch rÃºt trÃ­ch táº¡i `aero_LLM_15_The Logit Lens.md`.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [PhÃ¢n TÃ­ch Sá»± TÆ°Æ¡ng Äá»“ng Tokens Trong vÃ  Giá»¯a CÃ¡c Ma Tráº­n Q, K, V (Pháº§n 1)](aero_llm_01_token_related_similarities_within_and_across_q_k_v_matrices_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_token_related_similarities_within_and_across_q_k_v_matrices_part_1_.md) |
| [PhÃ¢n TÃ­ch Sá»± TÆ°Æ¡ng Äá»“ng Tokens Trong vÃ  Giá»¯a CÃ¡c Ma Tráº­n Q, K, V (Pháº§n 2)](aero_llm_02_token_related_similarities_within_and_across_q_k_v_matrices_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_token_related_similarities_within_and_across_q_k_v_matrices_part_2_.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): PhÃ¢n TÃ­ch Äá»™ TÆ°Æ¡ng Äá»“ng Cá»§a Token XuyÃªn Suá»‘t CÃ¡c Táº§ng áº¨n](aero_llm_03_codechallenge_token_related_similarities_across_layers.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_token_related_similarities_across_layers.md) |
| [PhÃ¢n TÃ­ch Sá»± PhÃ¢n Cá»¥m vÃ  TÆ°Æ¡ng Äá»“ng Biá»ƒu Diá»…n (RSA) Trong Ma Tráº­n Q vÃ  K](aero_llm_04_grouping_and_rsa_in_q_and_k_matrices.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_grouping_and_rsa_in_q_and_k_matrices.md) |
| [Kháº£o SÃ¡t PhÃ¢n Táº§ng (Laminar Profile) Vá» RSA VÃ  Sá»± Chá»n Lá»c PhÃ¢n NhÃ³m](aero_llm_05_codechallenge_laminar_profile_of_rsa_and_category_selectivity.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_codechallenge_laminar_profile_of_rsa_and_category_selectivity.md) |
| [PhÃ¢n TÃ­ch Sá»‘ Chiá»u Hiá»‡u Quáº£ (Effective Dimensionality) ThÃ´ng Qua PCA](aero_llm_06_effective_dimensionality_analysis_with_pca.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_effective_dimensionality_analysis_with_pca.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): Kháº£o SÃ¡t Sá»‘ Chiá»u Hiá»‡u Quáº£ TrÃªn Pythia 2.8B](aero_llm_07_codechallenge_dimensionalities_in_pythia_2_3b.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_codechallenge_dimensionalities_in_pythia_2_3b.md) |
| [LÃ½ Thuyáº¿t ThÃ´ng Tin: Äo LÆ°á»ng Entropy VÃ  Mutual Information](aero_llm_08_mutual_information_theory_and_code.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_mutual_information_theory_and_code.md) |
| [PhÃ¢n TÃ­ch ThÃ´ng Tin TÆ°Æ¡ng Há»— Dá»c Theo CÃ¡c Táº§ng Cá»§a MÃ´ HÃ¬nh NgÃ´n Ngá»¯ (Pairwise Mutual Information Through LLMs)](aero_llm_09_pairwise_mutual_information_through_the_llm.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_pairwise_mutual_information_through_the_llm.md) |
| [PhÃ¢n TÃ­ch Äá»‘i Chiáº¿u Äo LÆ°á»ng TÆ°Æ¡ng Quan: Mutual Information vÃ  Covariance](aero_llm_10_mutual_information_vs_covariance.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_mutual_information_vs_covariance.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): MI VÃ  Khoáº£ng CÃ¡ch Token (Pháº§n 1)](aero_llm_11_codechallenge_attention_to_coffee_mi_and_token_distances_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_attention_to_coffee_mi_and_token_distances_part_1_.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): MI VÃ  Khoáº£ng CÃ¡ch Token (Pháº§n 2)](aero_llm_12_codechallenge_attention_to_coffee_mi_and_token_distances_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_attention_to_coffee_mi_and_token_distances_part_2_.md) |
| [PhÃ¢n Kháº£o Cáº¥u TrÃºc Cá»¥m (Clusters): Dáº¥u CÃ¢u Ná»™i Bá»™ vs Dáº¥u CÃ¢u Káº¿t ThÃºc Táº­p 1](aero_llm_13_codechallenge_clusters_in_internal_vs_terminal_punctuation_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_codechallenge_clusters_in_internal_vs_terminal_punctuation_part_1_.md) |
| [PhÃ¢n Kháº£o Cáº¥u TrÃºc Cá»¥m (Clusters): Dáº¥u CÃ¢u Ná»™i Bá»™ vs Dáº¥u CÃ¢u Káº¿t ThÃºc Táº­p 2](aero_llm_14_codechallenge_clusters_in_internal_vs_terminal_punctuation_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_clusters_in_internal_vs_terminal_punctuation_part_2_.md) |
| ğŸ“Œ **[Tháº¥u KÃ­nh Logit (The Logit Lens): Soi SÃ¡ng TÆ° Duy Táº§ng Trung Gian Cá»§a MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_15_the_logit_lens.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_15_the_logit_lens.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): á»¨ng Dá»¥ng Logit Lens Trong Máº¡ng BERT (Pháº§n 1)](aero_llm_16_codechallenge_logit_lens_in_bert_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_logit_lens_in_bert_part_1_.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): á»¨ng Dá»¥ng Logit Lens Trong Máº¡ng BERT (Pháº§n 2)](aero_llm_17_codechallenge_logit_lens_in_bert_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_codechallenge_logit_lens_in_bert_part_2_.md) |
| [PhÃ¢n TÃ­ch Sá»± TÆ°Æ¡ng Äá»“ng Tokens Trong vÃ  Giá»¯a CÃ¡c Ma Tráº­n Q, K, V (Pháº§n 1)](article_aero_llm_01_vn.md) | [Xem bÃ i viáº¿t â†’](article_aero_llm_01_vn.md) |
| [PhÃ¢n tÃ­ch ChuyÃªn SÃ¢u CÃ¡c Táº§ng áº¨n Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n (LLMs): Äo LÆ°á»ng, Biá»ƒu Diá»…n vÃ  Giáº£i MÃ£ Ná»™i Táº¡i](scientific_article_vn.md) | [Xem bÃ i viáº¿t â†’](scientific_article_vn.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

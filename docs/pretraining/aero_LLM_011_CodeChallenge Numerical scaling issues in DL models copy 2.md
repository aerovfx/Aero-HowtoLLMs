
# ğŸ“˜ CÃ¡c Váº¥n Äá» Tá»· Lá»‡ Sá»‘ Há»c Trong MÃ´ HÃ¬nh Há»c SÃ¢u: PhÃ¢n TÃ­ch Vai TrÃ² Cá»§a Scaling vÃ  Normalization Trong CÆ¡ Cháº¿ Attention

---

## **Abstract**

Trong cÃ¡c mÃ´ hÃ¬nh há»c sÃ¢u hiá»‡n Ä‘áº¡i, Ä‘áº·c biá»‡t lÃ  cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn Transformer, viá»‡c kiá»ƒm soÃ¡t Ä‘á»™ lá»›n cá»§a giÃ¡ trá»‹ sá»‘ há»c Ä‘Ã³ng vai trÃ² quan trá»ng trong Ä‘áº£m báº£o tÃ­nh á»•n Ä‘á»‹nh vÃ  hiá»‡u quáº£ huáº¥n luyá»‡n. BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch cÃ¡c váº¥n Ä‘á» liÃªn quan Ä‘áº¿n viá»‡c nhÃ¢n ma tráº­n, sá»± khuáº¿ch Ä‘áº¡i phÆ°Æ¡ng sai, vÃ  áº£nh hÆ°á»Ÿng cá»§a chÃºng Ä‘áº¿n hÃ m Softmax trong cÆ¡ cháº¿ attention. Dá»±a trÃªn tÃ i liá»‡u *CodeChallenge: Numerical Scaling Issues in DL Models*, nghiÃªn cá»©u lÃ m rÃµ lÃ½ do cáº§n chuáº©n hÃ³a tÃ­ch QKáµ€ báº±ng cÄƒn báº­c hai cá»§a chiá»u khÃ´ng gian, Ä‘á»“ng thá»i kháº£o sÃ¡t phÃ¢n phá»‘i tham sá»‘ Layer Normalization trong GPT-2. Káº¿t quáº£ cho tháº¥y scaling vÃ  normalization lÃ  cÃ¡c thÃ nh pháº§n thiáº¿t yáº¿u nháº±m duy trÃ¬ â€œvÃ¹ng Goldilocksâ€ cho logits trong quÃ¡ trÃ¬nh há»c. 

---

## **1. Introduction**

CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (Large Language Models â€“ LLMs) dá»±a trÃªn kiáº¿n trÃºc Transformer sá»­ dá»¥ng hÃ ng triá»‡u phÃ©p nhÃ¢n ma tráº­n trong má»—i bÆ°á»›c suy luáº­n. Máº·c dÃ¹ cÃ¡c phÃ©p toÃ¡n nÃ y giÃºp mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c biá»ƒu diá»…n phá»©c táº¡p, chÃºng cÅ©ng gÃ¢y ra hiá»‡n tÆ°á»£ng khuáº¿ch Ä‘áº¡i giÃ¡ trá»‹ sá»‘ há»c.

Theo tÃ i liá»‡u, Softmax lÃ  má»™t phÃ©p biáº¿n Ä‘á»•i máº¡nh nhÆ°ng ráº¥t nháº¡y cáº£m vá»›i Ä‘á»™ lá»›n cá»§a Ä‘áº§u vÃ o. Khi logits cÃ³ giÃ¡ trá»‹ quÃ¡ lá»›n, phÃ¢n phá»‘i xÃ¡c suáº¥t trá»Ÿ nÃªn cá»±c Ä‘oan, lÃ m suy giáº£m kháº£ nÄƒng há»c cá»§a mÃ´ hÃ¬nh. Do Ä‘Ã³, viá»‡c nghiÃªn cá»©u cÃ¡c váº¥n Ä‘á» scaling lÃ  cáº§n thiáº¿t Ä‘á»ƒ hiá»ƒu rÃµ cÆ¡ cháº¿ hoáº¡t Ä‘á»™ng cá»§a attention. 

BÃ i viáº¿t táº­p trung phÃ¢n tÃ­ch:

* áº¢nh hÆ°á»Ÿng cá»§a nhÃ¢n ma tráº­n Ä‘áº¿n phÆ°Æ¡ng sai,
* LÃ½ do cáº§n scaling trong attention,
* TÃ¡c Ä‘á»™ng Ä‘áº¿n Softmax,
* Vai trÃ² cá»§a Layer Normalization.

---

## **2. Theoretical Background**

### **2.1. Dot Product trong Attention**

Trong cÆ¡ cháº¿ self-attention, Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng giá»¯a Query vÃ  Key Ä‘Æ°á»£c tÃ­nh báº±ng:

[
A = QK^T
]

Má»—i pháº§n tá»­ cá»§a (A) lÃ  tÃ­ch vÃ´ hÆ°á»›ng cá»§a hai vector cÃ³ chiá»u (d).

Náº¿u cÃ¡c pháº§n tá»­ cá»§a (Q) vÃ  (K) cÃ³ phÃ¢n phá»‘i chuáº©n vá»›i phÆ°Æ¡ng sai báº±ng 1, thÃ¬ phÆ°Æ¡ng sai cá»§a tÃ­ch vÃ´ hÆ°á»›ng xáº¥p xá»‰:

[
Var(QK^T) \approx d
]

Do Ä‘Ã³, Ä‘á»™ lá»‡ch chuáº©n xáº¥p xá»‰:

[
\sigma \approx \sqrt{d}
]



---

### **2.2. Softmax vÃ  Äá»™ Nháº¡y Sá»‘ Há»c**

HÃ m Softmax Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a:

[
Softmax(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
]

Khi (z_i) lá»›n, hÃ m mÅ© lÃ m cho má»™t sá»‘ pháº§n tá»­ chiáº¿m Æ°u tháº¿ tuyá»‡t Ä‘á»‘i, dáº«n Ä‘áº¿n:

* Hiá»‡n tÆ°á»£ng bÃ£o hÃ²a,
* Gradient gáº§n báº±ng 0,
* Giáº£m kháº£ nÄƒng há»c.

Theo tÃ i liá»‡u, Ä‘Ã¢y lÃ  nguyÃªn nhÃ¢n chÃ­nh khiáº¿n logits cáº§n Ä‘Æ°á»£c kiá»ƒm soÃ¡t vá» máº·t sá»‘ há»c. 

---

### **2.3. Scaling trong Attention**

Äá»ƒ giáº£m phÆ°Æ¡ng sai cá»§a (QK^T), Transformer Ã¡p dá»¥ng phÃ©p chia:

[
A_{scaled} = \frac{QK^T}{\sqrt{d}}
]

PhÃ©p scaling nÃ y Ä‘Æ°a Ä‘á»™ lá»‡ch chuáº©n cá»§a ma tráº­n attention vá» xáº¥p xá»‰ 1, giÃºp Softmax hoáº¡t Ä‘á»™ng trong vÃ¹ng á»•n Ä‘á»‹nh. 

---

## **3. Methodology**

### **3.1. ThÃ­ Nghiá»‡m 1: Ma Tráº­n Ngáº«u NhiÃªn**

Hai ma tráº­n (Q, K \in \mathbb{R}^{50 \times 50}) Ä‘Æ°á»£c sinh tá»« phÃ¢n phá»‘i Gaussian chuáº©n.

CÃ¡c Ä‘áº¡i lÆ°á»£ng Ä‘Æ°á»£c tÃ­nh:

* (\sigma(Q)),
* (\sigma(K)),
* (\sigma(QK^T)),
* (\sqrt{50}).

Káº¿t quáº£ cho tháº¥y:

[
\sigma(QK^T) \approx \sqrt{50} \approx 7
]



---

### **3.2. ThÃ­ Nghiá»‡m 2: Thay Äá»•i Chiá»u KhÃ´ng Gian**

Ma tráº­n cÃ³ kÃ­ch thÆ°á»›c (50 \times n), vá»›i (n) tá»« 2 Ä‘áº¿n 100.

Má»—i láº§n láº·p, tÃ­nh:

* Äá»™ lá»‡ch chuáº©n cá»§a (QK^T),
* GiÃ¡ trá»‹ (\sqrt{n}).

Hai Ä‘áº¡i lÆ°á»£ng nÃ y Ä‘Æ°á»£c so sÃ¡nh báº±ng biá»ƒu Ä‘á»“.

Káº¿t quáº£ cho tháº¥y sá»± trÃ¹ng khá»›p gáº§n nhÆ° hoÃ n háº£o giá»¯a lÃ½ thuyáº¿t vÃ  thá»±c nghiá»‡m. 

---

### **3.3. ThÃ­ Nghiá»‡m 3: Softmax TrÆ°á»›c vÃ  Sau Scaling**

ThÃ­ nghiá»‡m nÃ y so sÃ¡nh:

1. Softmax cá»§a (QK^T),
2. Softmax cá»§a (\frac{QK^T}{\sqrt{d}}),
3. Negative log-softmax tÆ°Æ¡ng á»©ng.

CÃ¡c giÃ¡ trá»‹ Ä‘Æ°á»£c trá»±c quan hÃ³a báº±ng scatter plot.

Má»¥c tiÃªu lÃ  Ä‘Ã¡nh giÃ¡ áº£nh hÆ°á»Ÿng cá»§a scaling Ä‘áº¿n phÃ¢n phá»‘i xÃ¡c suáº¥t. 

---

### **3.4. ThÃ­ Nghiá»‡m 4: PhÃ¢n TÃ­ch Layer Norm Trong GPT-2**

Táº¥t cáº£ tham sá»‘ Layer Normalization cá»§a GPT-2 Ä‘Æ°á»£c trÃ­ch xuáº¥t:

* Weight (Î³ â€“ stretching),
* Bias (Î² â€“ shifting).

CÃ¡c giÃ¡ trá»‹ nÃ y Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng histogram vá»›i trá»¥c y á»Ÿ dáº¡ng log-scale. 

---

## **4. Experimental Results**

### **4.1. Khuáº¿ch Äáº¡i PhÆ°Æ¡ng Sai Khi NhÃ¢n Ma Tráº­n**

Káº¿t quáº£ cho tháº¥y:

* (\sigma(Q) \approx 1),
* (\sigma(K) \approx 1),
* (\sigma(QK^T) \approx \sqrt{d}).

Äiá»u nÃ y chá»©ng minh ráº±ng nhÃ¢n ma tráº­n lÃ m tÄƒng phÆ°Æ¡ng sai theo chiá»u khÃ´ng gian. 

---

### **4.2. áº¢nh HÆ°á»Ÿng Äáº¿n Softmax**

TrÆ°á»›c scaling:

* Chá»‰ má»™t vÃ i token cÃ³ xÃ¡c suáº¥t lá»›n,
* Pháº§n lá»›n xÃ¡c suáº¥t â‰ˆ 0.

Sau scaling:

* PhÃ¢n phá»‘i tráº£i Ä‘á»u hÆ¡n,
* Nhiá»u token cÃ³ cÆ¡ há»™i Ä‘Æ°á»£c chá»n.

Hiá»‡n tÆ°á»£ng nÃ y giÃºp mÃ´ hÃ¬nh há»c Ä‘a dáº¡ng hÆ¡n á»Ÿ giai Ä‘oáº¡n Ä‘áº§u. 

---

### **4.3. PhÃ¢n Phá»‘i Tham Sá»‘ Layer Norm**

PhÃ¢n tÃ­ch GPT-2 cho tháº¥y:

* Tham sá»‘ Î³ chá»§ yáº¿u náº±m trong khoáº£ng 0.2â€“0.4,
* Tham sá»‘ Î² táº­p trung quanh 0.

Äiá»u nÃ y cho tháº¥y Layer Norm chá»§ yáº¿u cÃ³ tÃ¡c dá»¥ng thu nhá» (shrink) activation. 

---

## **5. Discussion**

### **5.1. VÃ¹ng â€œGoldilocksâ€ Cá»§a Logits**

Theo tÃ i liá»‡u, logits cáº§n náº±m trong má»™t vÃ¹ng trung gian:

* KhÃ´ng quÃ¡ lá»›n â†’ trÃ¡nh bÃ£o hÃ²a,
* KhÃ´ng quÃ¡ nhá» â†’ trÃ¡nh máº¥t phÃ¢n biá»‡t.

Scaling vÃ  normalization giÃºp duy trÃ¬ vÃ¹ng nÃ y. 

---

### **5.2. Vai TrÃ² Cá»§a Normalization**

Layer Normalization giÃºp:

* á»”n Ä‘á»‹nh gradient,
* Giáº£m drift cá»§a activation,
* CÃ¢n báº±ng giá»¯a cÃ¡c táº§ng.

NÃ³ lÃ  thÃ nh pháº§n khÃ´ng thá»ƒ thiáº¿u trong Transformer.

---

### **5.3. LiÃªn Há»‡ Vá»›i Temperature Sampling**

Scaling trong attention cÃ³ vai trÃ² tÆ°Æ¡ng tá»± tham sá»‘ temperature (T):

[
P_i = \frac{e^{z_i/T}}{\sum_j e^{z_j/T}}
]

Cáº£ hai Ä‘á»u Ä‘iá»u chá»‰nh Ä‘á»™ â€œsáº¯c nÃ©tâ€ cá»§a phÃ¢n phá»‘i. 

---

## **6. Limitations**

NghiÃªn cá»©u cÃ²n tá»“n táº¡i má»™t sá»‘ háº¡n cháº¿:

* Chá»§ yáº¿u dá»±a trÃªn dá»¯ liá»‡u ngáº«u nhiÃªn,
* ChÆ°a Ä‘Ã¡nh giÃ¡ áº£nh hÆ°á»Ÿng Ä‘áº¿n downstream tasks,
* Chá»‰ kháº£o sÃ¡t GPT-2,
* KhÃ´ng so sÃ¡nh vá»›i cÃ¡c kiáº¿n trÃºc khÃ¡c.

Do Ä‘Ã³, káº¿t quáº£ mang tÃ­nh minh há»a nhiá»u hÆ¡n tá»•ng quÃ¡t.

---

## **7. Conclusion**

BÃ i viáº¿t Ä‘Ã£ phÃ¢n tÃ­ch cÃ¡c váº¥n Ä‘á» scaling sá»‘ há»c trong mÃ´ hÃ¬nh há»c sÃ¢u vÃ  cÆ¡ cháº¿ attention. CÃ¡c káº¿t luáº­n chÃ­nh gá»“m:

1. NhÃ¢n ma tráº­n lÃ m tÄƒng phÆ°Æ¡ng sai theo (\sqrt{d}).
2. Scaling lÃ  cáº§n thiáº¿t Ä‘á»ƒ á»•n Ä‘á»‹nh Softmax.
3. KhÃ´ng scaling dáº«n Ä‘áº¿n phÃ¢n phá»‘i xÃ¡c suáº¥t cá»±c Ä‘oan.
4. Layer Norm giÃºp kiá»ƒm soÃ¡t biÃªn Ä‘á»™ activation.
5. CÃ¡c cÆ¡ cháº¿ nÃ y phá»‘i há»£p Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh á»•n Ä‘á»‹nh sá»‘ há»c.

NghiÃªn cá»©u kháº³ng Ä‘á»‹nh ráº±ng kiá»ƒm soÃ¡t tá»· lá»‡ sá»‘ há»c lÃ  ná»n táº£ng cho viá»‡c huáº¥n luyá»‡n thÃ nh cÃ´ng cÃ¡c mÃ´ hÃ¬nh Transformer quy mÃ´ lá»›n.

---

## **References**

1. CodeChallenge: Numerical Scaling Issues in DL Models. Lecture Transcript.

2. Vaswani et al. (2017). *Attention Is All You Need*. NeurIPS.
3. Ba et al. (2016). Layer Normalization. *arXiv*.
4. Goodfellow et al. (2016). *Deep Learning*. MIT Press.

---
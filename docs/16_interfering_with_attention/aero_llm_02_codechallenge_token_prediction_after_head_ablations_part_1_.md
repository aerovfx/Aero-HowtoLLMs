
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [16 interfering with attention](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ thÃ¡ch Láº­p trÃ¬nh: Dá»± Ä‘oÃ¡n Token sau khi Cáº¯t bá» Head (Pháº§n 1)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y má»Ÿ rá»™ng nghiÃªn cá»©u vá» can thiá»‡p nhÃ¢n quáº£ lÃªn Attention Heads báº±ng cÃ¡ch thá»±c hiá»‡n quÃ©t toÃ n bá»™ cÃ¡c táº§ng vÃ  cÃ¡c Ä‘áº§u trong mÃ´ hÃ¬nh GPT-2 Small. Thá»­ thÃ¡ch táº­p trung vÃ o hai ká»‹ch báº£n thay tháº¿: (1) Thay tháº¿ hoáº¡t hÃ³a cá»§a head báº±ng sá»‘ khÃ´ng (Zeroing) vÃ  (2) Thay tháº¿ báº±ng giÃ¡ trá»‹ trung bÃ¬nh thá»±c nghiá»‡m cá»§a chÃ­nh head Ä‘Ã³ (Mean imputation). Sá»­ dá»¥ng cÆ¡ cháº¿ `Forward Pre-hook` linh hoáº¡t káº¿t há»£p vá»›i biáº¿n toÃ n cá»¥c Ä‘á»ƒ Ä‘iá»u khiá»ƒn thá»±c nghiá»‡m, nghiÃªn cá»©u phÃ¢n tÃ­ch sá»± biáº¿n thiÃªn cá»§a xÃ¡c suáº¥t Softmax Ä‘á»‘i vá»›i cÃ¡c token má»¥c tiÃªu. Káº¿t quáº£ cho tháº¥y sá»± nháº¡y cáº£m cá»§a mÃ´ hÃ¬nh Ä‘á»‘i vá»›i can thiá»‡p táº¡i cÃ¡c head lÃ  ráº¥t khÃ¡c nhau vÃ  khÃ´ng cÃ³ quy luáº­t khÃ´ng gian rÃµ rá»‡t, Ä‘á»“ng thá»i xÃ¡c nháº­n tÃ­nh bá»n vá»¯ng cá»§a dá»± Ä‘oÃ¡n tá»« khÃ³a ngay cáº£ khi cáº¥u trÃºc attention bá»‹ xÃ¡o trá»™n cá»¥c bá»™.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Tiáº¿p ná»‘i cÃ¡c ká»¹ thuáº­t cÃ´ láº­p Attention Head, thá»­ thÃ¡ch nÃ y Ä‘áº·t ra ba má»¥c tiÃªu chÃ­nh: (1) XÃ¢y dá»±ng hÃ m Hook Ä‘á»™ng cÃ³ kháº£ nÄƒng can thiá»‡p vÃ o báº¥t ká»³ Head nÃ o táº¡i báº¥t ká»³ Layer nÃ o; (2) So sÃ¡nh tÃ¡c Ä‘á»™ng cá»§a viá»‡c triá»‡t tiÃªu tÃ­n hiá»‡u so vá»›i viá»‡c duy trÃ¬ má»©c nÄƒng lÆ°á»£ng trung bÃ¬nh; (3) Trá»±c quan hÃ³a báº£n Ä‘á»“ nháº¡y cáº£m (Sensitivity map) cá»§a toÃ n bá»™ mÃ´ hÃ¬nh thÃ´ng qua Heatmap.

---

## 2. PhÆ°Æ¡ng PhÃ¡p Thá»±c Nghiá»‡m (Methodology)

### 2.1. Cáº¥u trÃºc Hook Linh hoáº¡t vÃ  Biáº¿n ToÃ n cá»¥c
Äá»ƒ trÃ¡nh viá»‡c hard-code cho tá»«ng táº§ng, chÃºng ta sá»­ dá»¥ng má»™t hÃ m Hook duy nháº¥t Ä‘Æ°á»£c cÃ i vÃ o táº¥t cáº£ cÃ¡c Transformer Blocks. 
- **CÆ¡ cháº¿ Ä‘iá»u khiá»ƒn:** Sá»­ dá»¥ng cÃ¡c biáº¿n toÃ n cá»¥c (global variables) nhÆ° `layer_to_ablate`, `head_to_ablate`, vÃ  `replace_zero` (Boolean).
- **Pháº¡m vi cá»¥c bá»™:** `if current_layer == layer_to_ablate: ...`. Äiá»u nÃ y cho phÃ©p thá»±c hiá»‡n vÃ²ng láº·p kÃ©p (double for loop) qua 144 tá»• há»£p (12 layers $\times$ 12 heads) má»™t cÃ¡ch tá»± Ä‘á»™ng.

### 2.2. Ká»¹ thuáº­t Imputation (GÃ¡n giÃ¡ trá»‹ thay tháº¿)
NghiÃªn cá»©u so sÃ¡nh hai phÆ°Æ¡ng phÃ¡p:
- **Zero Imputation:** GÃ¡n toÃ n bá»™ tensor cá»§a head má»¥c tiÃªu báº±ng 0.
- **Mean Imputation:** TÃ­nh toÃ¡n trung bÃ¬nh cá»™ng cá»§a toÃ n bá»™ cÃ¡c giÃ¡ trá»‹ hoáº¡t hÃ³a trong head (`head.mean()`) vÃ  gÃ¡n háº±ng sá»‘ nÃ y cho má»i vá»‹ trÃ­ trong head Ä‘Ã³. PhÆ°Æ¡ng phÃ¡p nÃ y giÃºp duy trÃ¬ "má»©c Ä‘á»™ hoáº¡t Ä‘á»™ng" trung bÃ¬nh nhÆ°ng triá»‡t tiÃªu cÃ¡c biáº¿n thiÃªn thÃ´ng tin cá»¥ thá»ƒ.

---

## 3. Káº¿t Quáº£ VÃ  PhÃ¢n TÃ­ch (Results & Analysis)

### 3.1. Báº£n Ä‘á»“ Nháº¡y cáº£m Softmax (Exercise 3)
ThÃ´ng qua Heatmap so sÃ¡nh sai lá»‡ch xÃ¡c suáº¥t ($P_{manipulated} - P_{clean}$):
- **Token Má»¥c tiÃªu ("Germany"):** Quan sÃ¡t tháº¥y sá»± thay Ä‘á»•i phÃ¢n tÃ¡n. Má»™t sá»‘ head khi bá»‹ cáº¯t bá» lÃ m giáº£m máº¡nh xÃ¡c suáº¥t Ä‘Ãºng, trong khi má»™t sá»‘ khÃ¡c láº¡i lÃ m tÄƒng nháº¹. KhÃ´ng cÃ³ mÃ´ hÃ¬nh "vÃ¹ng chá»©c nÄƒng" táº­p trung rÃµ rá»‡t.
- **Token Äá»‘i chá»©ng ("France"):** TÃ¡c Ä‘á»™ng lÃ  tá»‘i thiá»ƒu vÃ  gáº§n nhÆ° báº±ng khÃ´ng trÃªn toÃ n bá»™ báº£n Ä‘á»“. Äiá»u nÃ y chá»©ng minh cÃ¡c head can thiá»‡p cÃ³ tÃ­nh Ä‘áº·c hiá»‡u cao Ä‘á»‘i vá»›i logic dáº«n Ä‘áº¿n Ä‘Ã¡p Ã¡n Ä‘Ãºng.

### 3.2. TÃ­nh Bá»n Vá»¯ng cá»§a Dá»± Ä‘oÃ¡n (Argmax Analysis)
Trong 144 láº§n can thiá»‡p, mÃ´ hÃ¬nh váº«n dá»± Ä‘oÃ¡n Ä‘Ãºng "Germany" trong 143 trÆ°á»ng há»£p. Chá»‰ cÃ³ má»™t trÆ°á»ng há»£p duy nháº¥t mÃ´ hÃ¬nh chuyá»ƒn sang dá»± Ä‘oÃ¡n tá»« "the". Äiá»u nÃ y cá»§ng cá»‘ quan Ä‘iá»ƒm ráº±ng LLM cÃ³ cÆ¡ cháº¿ bÃ¹ trá»« lá»—i cá»±c ká»³ máº¡nh máº½ dá»c theo residual stream.

### 3.3. PhÃ¢n tÃ­ch Drift cá»§a Hoáº¡t hÃ³a (Exercise 4)
Viá»‡c trá»±c quan hÃ³a giÃ¡ trá»‹ trung bÃ¬nh thá»±c nghiá»‡m (`observed_head_mean`) tiáº¿t lá»™:
- CÃ¡c giÃ¡ trá»‹ trung bÃ¬nh thÆ°á»ng ráº¥t nhá» vÃ  táº­p trung quanh 0.
- KhÃ´ng cÃ³ xu hÆ°á»›ng tÄƒng hay giáº£m (drift) rÃµ rá»‡t khi Ä‘i sÃ¢u vÃ o cÃ¡c táº§ng cuá»‘i.
- KhÃ¡c biá»‡t giá»¯a káº¿t quáº£ Zero Imputation vÃ  Mean Imputation lÃ  khÃ´ng Ä‘Ã¡ng ká»ƒ vá» máº·t Ä‘á»‹nh tÃ­nh Ä‘á»‘i vá»›i tÃ¡c vá»¥ nÃ y.

---

## 4. Káº¿t Luáº­n
Thá»­ thÃ¡ch nÃ y minh chá»©ng ráº±ng máº·c dÃ¹ Attention Heads lÃ  cÃ¡c Ä‘Æ¡n vá»‹ tÃ­nh toÃ¡n Ä‘á»™c láº­p, vai trÃ² cá»§a chÃºng trong viá»‡c lÆ°u trá»¯ tri thá»©c tháº¿ giá»›i Ä‘Æ°á»£c phÃ¢n bá»• theo máº¡ng lÆ°á»›i phá»©c táº¡p thay vÃ¬ khu trÃº táº¡i cÃ¡c táº§ng cá»¥ thá»ƒ. Sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a viá»‡c gÃ¡n báº±ng khÃ´ng vÃ  gÃ¡n trung bÃ¬nh gá»£i Ã½ ráº±ng thÃ´ng tin quan trá»ng náº±m á»Ÿ "cÃ¡c biáº¿n Ä‘á»™ng" (fluctuations) xung quanh má»©c ná»n hÆ¡n lÃ  á»Ÿ chÃ­nh má»©c nÄƒng lÆ°á»£ng tuyá»‡t Ä‘á»‘i cá»§a Head.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. Thá»­ thÃ¡ch Head Ablated QuÃ©t toÃ n bá»™ layers trÃªn GPT-2 Small dá»±a trÃªn `aero_LLM_02_CodeChallenge Token prediction after head ablations (part 1).md`. PhÃ¢n tÃ­ch 144 ká»‹ch báº£n can thiá»‡p nhÃ¢n quáº£.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Cáº¯t bá» Attention Head vÃ  Dá»± Ä‘oÃ¡n Token (Head Ablation and Token Prediction)](aero_llm_01_head_ablation_and_token_prediction.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_head_ablation_and_token_prediction.md) |
| ğŸ“Œ **[Thá»­ thÃ¡ch Láº­p trÃ¬nh: Dá»± Ä‘oÃ¡n Token sau khi Cáº¯t bá» Head (Pháº§n 1)](aero_llm_02_codechallenge_token_prediction_after_head_ablations_part_1_.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_02_codechallenge_token_prediction_after_head_ablations_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Dá»± Ä‘oÃ¡n Token sau khi Cáº¯t bá» Head (Pháº§n 2)](aero_llm_03_codechallenge_token_prediction_after_head_ablations_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_token_prediction_after_head_ablations_part_2_.md) |
| [TÃ¡c Ä‘á»™ng cá»§a viá»‡c "Táº¯t tiáº¿ng" Head lÃªn Äá»™ tÆ°Æ¡ng Ä‘á»“ng Cosine (Impact of Head-Silencing on Cosine Similarity)](aero_llm_04_impact_of_head_silencing_on_cosine_similarity.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_impact_of_head_silencing_on_cosine_similarity.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: GPT-2 cÃ³ thá»±c sá»± thÃ­ch Pizza Dá»©a? (Má»™t nghiÃªn cá»©u vá» can thiá»‡p Attention chÃ­nh xÃ¡c)](aero_llm_05_codechallenge_does_gpt2_like_pineapple_pizza.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_codechallenge_does_gpt2_like_pineapple_pizza.md) |
| [VÃ¡ lá»—i Attention Head trong tÃ¡c vá»¥ Nháº­n dáº¡ng TÃ¢n ngá»¯ GiÃ¡n tiáº¿p (Attention Head Patching in IOI)](aero_llm_06_attention_head_patching_in_ioi.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_attention_head_patching_in_ioi.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: VÃ¡ lá»—i Head vÃ  Token trong tÃ¡c vá»¥ IOI (Head and Token Patching in IOI)](aero_llm_07_codechallenge_head_and_token_patching_in_ioi.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_codechallenge_head_and_token_patching_in_ioi.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

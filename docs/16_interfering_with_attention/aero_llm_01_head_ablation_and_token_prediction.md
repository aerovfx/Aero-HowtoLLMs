
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [16 interfering with attention](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Cáº¯t bá» Attention Head vÃ  Dá»± Ä‘oÃ¡n Token (Head Ablation and Token Prediction)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y nghiÃªn cá»©u tÃ¡c Ä‘á»™ng cá»§a viá»‡c can thiá»‡p nhÃ¢n quáº£ lÃªn cÃ¡c Attention Head riÃªng láº» Ä‘á»‘i vá»›i kháº£ nÄƒng tÃ­ch há»£p ngá»¯ cáº£nh cá»§a mÃ´ hÃ¬nh GPT-2 Small. Báº±ng cÃ¡ch sá»­ dá»¥ng cÆ¡ cháº¿ `Forward Pre-hook` Ä‘á»ƒ can thiá»‡p vÃ o táº§ng `c_proj` trÆ°á»›c khi cÃ¡c Ä‘áº§u (heads) bá»‹ trá»™n láº«n tuyáº¿n tÃ­nh, nghiÃªn cá»©u thá»±c hiá»‡n phÆ°Æ¡ng phÃ¡p cáº¯t bá» (ablation) báº±ng cÃ¡ch gÃ¡n giÃ¡ trá»‹ khÃ´ng (zeroing out) cho tá»«ng Ä‘áº§u. ThÃ­ nghiá»‡m sá»­ dá»¥ng tÃ¡c vá»¥ dá»± Ä‘oÃ¡n thá»§ Ä‘Ã´ ("Berlin is the capital of...") Ä‘á»ƒ Ä‘o lÆ°á»ng sá»± thay Ä‘á»•i cá»§a chá»‰ sá»‘ Log Softmax. Káº¿t quáº£ cho tháº¥y viá»‡c cáº¯t bá» má»™t Ä‘áº§u gÃ¢y ra sá»± suy giáº£m nháº¹ nhÆ°ng nháº¥t quÃ¡n Ä‘á»‘i vá»›i xÃ¡c suáº¥t cá»§a token Ä‘Ãºng ("Germany"), trong khi vÃ´ tÃ¬nh lÃ m tÄƒng xÃ¡c suáº¥t cho cÃ¡c token liÃªn quan vá» máº·t ngá»¯ nghÄ©a nhÆ°ng sai vá» máº·t thá»±c táº¿ ("France").

---

## 1. Má»Ÿ Äáº§u (Introduction)
CÃ¡c cÆ¡ cháº¿ Attention chá»‹u trÃ¡ch nhiá»‡m kÃ©o cÃ¡c vector embeddings theo nhá»¯ng hÆ°á»›ng giÃºp dá»± bÃ¡o chÃ­nh xÃ¡c token káº¿ tiáº¿p dá»±a trÃªn ngá»¯ cáº£nh vÃ  tri thá»©c tháº¿ giá»›i. Má»™t trong nhá»¯ng cÃ¢u há»i cÆ¡ báº£n cá»§a Diá»…n giáº£i cÆ¡ há»c (Mechanistic Interpretability) lÃ : lÃ m tháº¿ nÃ o Ä‘á»ƒ cÃ´ láº­p vÃ  thao tÃ¡c má»™t Attention Head duy nháº¥t trong khi chÃºng thÆ°á»ng bá»‹ trá»™n láº«n ngay láº­p tá»©c sau khi tÃ­nh toÃ¡n? BÃ¡o cÃ¡o nÃ y trÃ¬nh bÃ y ká»¹ thuáº­t can thiá»‡p vÃ o táº§ng Ä‘áº§u ra cá»§a attention trÆ°á»›c khi thÃ´ng tin bá»‹ xÃ¡o trá»™n bá»Ÿi ma tráº­n trá»™n tuyáº¿n tÃ­nh $W_O$.

---

## 2. Thiáº¿t Láº­p ThÃ­ Nghiá»‡m (Methodology)

### 2.1. CÆ¡ cháº¿ Forward Pre-hook vÃ  Táº§ng c_proj
Trong kiáº¿n trÃºc GPT-2 cá»§a OpenAI/HuggingFace, 12 attention heads sau khi Ä‘Æ°á»£c tÃ­nh toÃ¡n xong sáº½ Ä‘Æ°á»£c ná»‘i tiáº¿p (concatenated) vÃ  Ä‘Æ°a vÃ o táº§ng `c_proj`. Táº§ng nÃ y thá»±c hiá»‡n phÃ©p nhÃ¢n vá»›i ma tráº­n trá»™n $W_O$ Ä‘á»ƒ tÃ­ch há»£p thÃ´ng tin tá»« 12 heads vÃ o residual stream.
- **Ká»¹ thuáº­t:** Sá»­ dá»¥ng `register_forward_pre_hook` vÃ o táº§ng `c_proj`. 
- **LÃ½ do:** á» giai Ä‘oáº¡n "Pre-input" nÃ y, dá»¯ liá»‡u váº«n á»Ÿ dáº¡ng 12 khá»‘i 64 chiá»u ná»‘i tiáº¿p nhau. ChÃºng ta cÃ³ thá»ƒ reshape tensor Ä‘á»ƒ tÃ¡ch riÃªng chiá»u `heads` vÃ  dá»… dÃ ng thá»±c hiá»‡n can thiá»‡p lÃªn má»™t Ä‘áº§u cá»¥ thá»ƒ (`head_to_ablate`).

### 2.2. Nhiá»‡m vá»¥ Kiá»ƒm thá»­ Ngá»¯ cáº£nh
CÃ¢u máº«u: "Berlin is the capital of..."
- **Token Ä‘Ãºng (Target):** " Germany"
- **Token Ä‘á»‘i chá»©ng (Contrast):** " France"
- **Má»¥c tiÃªu:** Äo lÆ°á»ng sá»± thay Ä‘á»•i xÃ¡c suáº¥t (delta log softmax) khi má»™t head bá»‹ "táº¯t tiáº¿ng".

---

## 3. Káº¿t Quáº£ VÃ  PhÃ¢n TÃ­ch (Results & Analysis)

### 3.1. Sá»± Suy Giáº£m TÃ­nh ChÃ­nh XÃ¡c (Probability Suppression)
Thá»±c hiá»‡n cáº¯t bá» láº§n lÆ°á»£t 12 heads táº¡i Transformer Block thá»© 5:
- **XÃ¡c suáº¥t Ä‘Ãºng:** Log Softmax cho "Germany" giáº£m xuá»‘ng á»Ÿ háº§u háº¿t cÃ¡c head. Äiá»u nÃ y chá»©ng tá» má»—i head Ä‘á»u Ä‘Ã³ng gÃ³p má»™t pháº§n nhá» nhÆ°ng quan trá»ng vÃ o viá»‡c duy trÃ¬ tri thá»©c thá»±c thá»ƒ.
- **Sá»± trá»—i dáº­y cá»§a nhiá»…u ngá»¯ nghÄ©a:** ThÃº vá»‹ lÃ  xÃ¡c suáº¥t cho "France" láº¡i tÄƒng lÃªn. Äiá»u nÃ y gá»£i Ã½ ráº±ng viá»‡c phÃ¡ há»§y má»™t head logic lÃ m suy yáº¿u kháº£ nÄƒng lá»c context, khiáº¿n mÃ´ hÃ¬nh dá»… bá»‹ nháº§m láº«n giá»¯a cÃ¡c khÃ¡i niá»‡m cÃ¹ng nhÃ³m (quá»‘c gia chÃ¢u Ã‚u) nhÆ°ng khÃ´ng chÃ­nh xÃ¡c hoÃ n toÃ n.

### 3.2. TÃ­nh Bá»n Vá»¯ng Cá»§a MÃ´ HÃ¬nh (Model Robustness)
Máº·c dÃ¹ Logits thay Ä‘á»•i, mÃ´ hÃ¬nh váº«n dá»± Ä‘oÃ¡n Ä‘Ãºng tá»« "Germany" lÃ  lá»±a chá»n hÃ ng Ä‘áº§u (top-1) trong cáº£ 12 láº§n thá»­ nghiá»‡m. Äiá»u nÃ y cho tháº¥y kiáº¿n trÃºc LLM cÃ³ tÃ­nh dÆ° thá»«a (redundancy) cao; viá»‡c máº¥t má»™t thÃ nh pháº§n Ä‘Æ¡n láº» hiáº¿m khi lÃ m sá»¥p Ä‘á»• hoÃ n toÃ n hÃ nh vi cá»§a mÃ´ hÃ¬nh trong cÃ¡c tÃ¡c vá»¥ Ä‘Æ¡n giáº£n.

### 3.3. Táº§m Quan Trá»ng Cá»§a Äo LÆ°á»ng LiÃªn Tá»¥c
- **AI Safety:** Trong an toÃ n AI, chÃºng ta quan tÃ¢m nháº¥t Ä‘áº¿n token thá»±c táº¿ mÃ  mÃ´ hÃ¬nh sinh ra (categorical output). Tuy nhiÃªn, chá»‰ nhÃ¬n vÃ o káº¿t quáº£ Ä‘Ãºng/sai lÃ  quÃ¡ thÃ´.
- **Insight:** CÃ¡c phÃ©p Ä‘o sá»‘ thá»±c (continuous measurements) nhÆ° logit difference tiáº¿t lá»™ nhá»¯ng chuyá»ƒn dá»‹ch nhá» bÃªn dÆ°á»›i bá» máº·t. Nhá»¯ng chuyá»ƒn dá»‹ch nÃ y cÃ³ thá»ƒ tÃ­ch tá»¥ vÃ  gÃ¢y ra lá»—i nghiÃªm trá»ng trong cÃ¡c há»‡ thá»‘ng quy mÃ´ lá»›n hoáº·c cÃ¡c ngá»¯ cáº£nh phá»©c táº¡p hÆ¡n.

---

## 4. Káº¿t Luáº­n
Cáº¯t bá» Attention Head thÃ´ng qua Pre-hook lÃ  má»™t ká»¹ thuáº­t pháº«u thuáº­t chÃ­nh xÃ¡c hÆ¡n so vá»›i viá»‡c tÃ¡c Ä‘á»™ng vÃ o toÃ n bá»™ Hidden State. ThÃ­ nghiá»‡m xÃ¡c nháº­n ráº±ng cÃ¡c Attention Heads hoáº¡t Ä‘á»™ng nhÆ° cÃ¡c máº¡ch logic phÃ¢n tÃ¡n. CÃ¢u há»i quan trá»ng tiáº¿p theo lÃ : chÃºng ta nÃªn thay tháº¿ giÃ¡ trá»‹ bá»‹ cáº¯t bá» báº±ng sá»‘ khÃ´ng, giÃ¡ trá»‹ trung bÃ¬nh, hay má»™t háº±ng sá»‘ khÃ¡c? ÄÃ¢y sáº½ lÃ  trá»ng tÃ¢m cá»§a cÃ¡c thá»­ thÃ¡ch láº­p trÃ¬nh káº¿ tiáº¿p.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. ThÃ­ nghiá»‡m Head Ablation trÃªn GPT-2 Small dá»±a trÃªn `aero_LLM_01_Head ablation and token prediction.md`. PhÃ¢n tÃ­ch sá»± cÃ¢n báº±ng giá»¯a tri thá»©c thá»±c táº¿ vÃ  nhiá»…u ngá»¯ nghÄ©a.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| ğŸ“Œ **[Cáº¯t bá» Attention Head vÃ  Dá»± Ä‘oÃ¡n Token (Head Ablation and Token Prediction)](aero_llm_01_head_ablation_and_token_prediction.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_01_head_ablation_and_token_prediction.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Dá»± Ä‘oÃ¡n Token sau khi Cáº¯t bá» Head (Pháº§n 1)](aero_llm_02_codechallenge_token_prediction_after_head_ablations_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_codechallenge_token_prediction_after_head_ablations_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Dá»± Ä‘oÃ¡n Token sau khi Cáº¯t bá» Head (Pháº§n 2)](aero_llm_03_codechallenge_token_prediction_after_head_ablations_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_token_prediction_after_head_ablations_part_2_.md) |
| [TÃ¡c Ä‘á»™ng cá»§a viá»‡c "Táº¯t tiáº¿ng" Head lÃªn Äá»™ tÆ°Æ¡ng Ä‘á»“ng Cosine (Impact of Head-Silencing on Cosine Similarity)](aero_llm_04_impact_of_head_silencing_on_cosine_similarity.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_impact_of_head_silencing_on_cosine_similarity.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: GPT-2 cÃ³ thá»±c sá»± thÃ­ch Pizza Dá»©a? (Má»™t nghiÃªn cá»©u vá» can thiá»‡p Attention chÃ­nh xÃ¡c)](aero_llm_05_codechallenge_does_gpt2_like_pineapple_pizza.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_codechallenge_does_gpt2_like_pineapple_pizza.md) |
| [VÃ¡ lá»—i Attention Head trong tÃ¡c vá»¥ Nháº­n dáº¡ng TÃ¢n ngá»¯ GiÃ¡n tiáº¿p (Attention Head Patching in IOI)](aero_llm_06_attention_head_patching_in_ioi.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_attention_head_patching_in_ioi.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: VÃ¡ lá»—i Head vÃ  Token trong tÃ¡c vá»¥ IOI (Head and Token Patching in IOI)](aero_llm_07_codechallenge_head_and_token_patching_in_ioi.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_codechallenge_head_and_token_patching_in_ioi.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [20 Investigating token embeddings](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: PhÃ¢n RÃ£ Äá»™ DÃ i ÄÆ°á»ng Dáº«n Luá»“ng Sá»‘ DÆ° (Pháº§n 1)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y lÃ  bÆ°á»›c tá»•ng hÃ²a cÃ¡c khÃ¡i niá»‡m vá» Äá»™ dÃ i Ä‘Æ°á»ng dáº«n (Path length) vÃ  sá»± Ä‘Ã³ng gÃ³p cá»§a hai mÃ´ Ä‘un phá»¥ Attention/MLP vÃ o luá»“ng trung tÃ¢m (Residual Stream). Sá»­ dá»¥ng mÃ´ hÃ¬nh GPT-2 Large ($36$ Transformer blocks), thÃ­ nghiá»‡m gáº¯n mÃ£ theo dÃµi (Hooks) vÃ o Ä‘áº§u ra chiáº¿u (Projection layers - `c_proj`) cá»§a hai khá»‘i Attention vÃ  MLP Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u vá» lÆ°á»£ng "Ä‘iá»u chá»‰nh" (Adjustments) tá»« má»—i khá»‘i trÆ°á»›c khi ghi Ä‘Ã¨ trá»Ÿ láº¡i luá»“ng chÃ­nh. Thá»±c nghiá»‡m Ä‘o lÆ°á»ng Äá»™ tÆ°Æ¡ng Ä‘á»“ng Cosine (Cosine Similarity) giá»¯a táº­p vector Ä‘iá»u chá»‰nh cá»§a Attention vÃ  MLP chá»‰ ra ráº±ng: Táº¡i pháº§n lá»›n cÃ¡c táº§ng trung gian, hai cÆ¡ cháº¿ nÃ y hoáº¡t Ä‘á»™ng gáº§n nhÆ° trá»±c giao (Orthogonal) Ä‘á»™c láº­p hoÃ n toÃ n. Sá»± Ä‘an chÃ©o Ä‘á»“ng tuáº¿n chá»‰ bÃ¹ng lÃªn á»Ÿ nhá»¯ng cháº·ng má»Ÿ Ä‘áº§u vÃ  káº¿t thÃºc cá»§a kiáº¿n trÃºc.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Trong má»™t Transformer Block, vector ban Ä‘áº§u khÃ´ng bá»‹ biáº¿n Ä‘á»•i tháº³ng, mÃ  nÃ³ cháº£y dá»c theo dÃ²ng "Residual Stream". á» má»—i block, hai nhÃ¡nh ráº½ (Attention vÃ  MLP) sáº½ "Ä‘á»c" vector nÃ y, giáº£i quyáº¿t tÃ­nh toÃ¡n cá»¥c bá»™, vÃ  cá»™ng dá»“n láº¡i vÃ o dÃ²ng chÃ­nh dÃ²ng giÃ¡ trá»‹ "Ä‘Ã£ Ä‘iá»u chá»‰nh" (adjustment vectors). 
* CÃ¢u há»i Ä‘áº·t ra lÃ : Hai cÆ¡ cháº¿ nÃ y cÃ³ há»£p tÃ¡c hay dáº«m chÃ¢n lÃªn tÃ­nh toÃ¡n cá»§a nhau khÃ´ng? 
* Náº¿u Cosine Similarity $\approx 1$: Cáº£ Attention vÃ  MLP cÃ¹ng Ä‘áº©y Token Embedding vá» chung má»™t hÆ°á»›ng. NghÄ©a lÃ  chÃºng tÃ­nh toÃ¡n nhá»¯ng thÃ´ng sá»‘ dÆ° thá»«a y há»‡t nhau.
* Náº¿u Cosine Similarity $\approx 0$: ChÃºng Ä‘Ã³ng gÃ³p vÃ o luá»“ng chÃ­nh nhá»¯ng máº£ng kiáº¿n thá»©c vuÃ´ng gÃ³c (trá»±c giao) hoÃ n toÃ n tÃ¡ch biá»‡t.

BÃ i thá»±c hÃ nh nÃ y láº­p biá»ƒu Ä‘á»“ Cosine Similarity xuyÃªn suá»‘t 36 táº§ng cá»§a GPT-2 Large Ä‘á»ƒ kiá»ƒm chá»©ng má»©c Ä‘á»™ phÃ¢n chia cÃ´ng viá»‡c (Labor division) cá»§a mÃ´ hÃ¬nh.

---

## 2. Tiá»n Xá»­ LÃ½: Ká»¹ Thuáº­t Gáº¯n Hook KÃ­ch Hoáº¡t (Methodology)

### 2.1. CÃ i Cáº¯m Hooks VÃ o Module `c_proj`
Thay vÃ¬ Ä‘á»c `output_hidden_states` (lÃ  tá»•ng hÃ²a sau khi Ä‘Ã£ cá»™ng), ta cáº§n Ä‘o lÆ°á»ng *chÃ­nh xÃ¡c* thÃ´ng lÆ°á»£ng mÃ  mÃ´ Ä‘un xáº£ ra:
- Khá»‘i Self-Attention Projection: `attn.c_proj`
- Khá»‘i MLP Projection: `mlp.c_proj`
Dá»¯ liá»‡u há»©ng Ä‘Æ°á»£c khÃ´ng pháº£i Token Embeddings gá»‘c, mÃ  chÃ­nh lÃ  vector "Adjustments" quy mÃ´ 1280 dimensions cá»§a GPT-2 Large.

### 2.2. TrÃ­ch Xuáº¥t Dá»¯ Liá»‡u
Sá»­ dá»¥ng phÃ¢n Ä‘oáº¡n tÃ i liá»‡u vá» triáº¿t gia "Nietzsche" (gá»“m 342 tokens). Viá»‡c quÃ©t qua 36 Layers giÃºp táº¡o ra má»™t ma tráº­n Tensor ba chiá»u cho cáº£ Attention Adjustments vÃ  MLP Adjustments. Sá»‘ liá»‡u sau Ä‘Ã³ Ä‘Æ°á»£c Ä‘áº©y vÃ o cÃ´ng thá»©c PyTorch: `torch.nn.functional.cosine_similarity()`.

---

## 3. Kháº£o SÃ¡t ÄÃ¡nh GiÃ¡ Dá»¯ Liá»‡u: Thuyáº¿t Trá»±c Giao (Analysis)

### 3.1. Theo DÃµi Theo Tuyáº¿n Táº§ng (Layer-wise Cosine Similarity)
Trung bÃ¬nh hÃ³a káº¿t quáº£ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng theo táº§ng (kÃ¨m thanh Sai Sá»‘ Chuáº©n - Error Bars):
- **Táº§ng Ä‘áº§u vÃ  táº§ng cuá»‘i (Äoáº¡n 0-5 vÃ  30-35):** NgÃ³c Ä‘áº§u tÄƒng táº¡o thÃ nh bá» vai "Shoulder". Hai cÆ¡ cháº¿ nÃ y biá»ƒu hiá»‡n má»©c Ä‘á»™ Ä‘á»“ng thuáº­t nháº¹ (Cosine $\approx 0.1 \to 0.2$), phá»¥ trá»£ láº«n nhau Ä‘á»ƒ náº¡p hoáº·c Ä‘Ã³ng gÃ³i tá»« vá»±ng.
- **Táº§ng trung tÃ¢m (Äoáº¡n 6-29):** ÄÆ°á»ng phÃ¢n bá»• lÃºn sÃ¡t váº¡ch 0. Äiá»u nÃ y lÃ m sÃ¡ng tá» thuyáº¿t CÆ¡ cháº¿ Ä‘á»™c láº­p (Orthogonality Doctrine). á» há»‡ táº§ng sÃ¢u phÃ¢n tÃ­ch ngá»¯ cáº£nh, Attention (chÄƒm lo viá»‡c kÃ©o ngá»¯ cáº£nh xa xÃ´i) vÃ  MLP (hoáº¡t Ä‘á»™ng nhÆ° kho chá»©a tri thá»©c cá»¥c bá»™) gáº§n nhÆ° khÃ´ng cháº¡m máº·t nhau, thá»±c hiá»‡n Ä‘á»™c láº­p nhá»¯ng nhiá»‡m vá»¥ phÃ¢n rÃ£ vector riÃªng ráº½ cá»§a mÃ¬nh.

### 3.2. Biá»ƒu Äá»“ Táº§n Suáº¥t (Histogram Distribution)
Tráº£i pháº³ng toÃ n bá»™ ma tráº­n (Flattening $342 \text{ tokens} \times 36 \text{ layers}$), Ä‘á»‰nh thÃ¡p Histogram Ä‘Ã³ng Ä‘inh hoÃ n háº£o quanh má»‘c Cosine $= 0$. Má»™t pháº§n Ä‘uÃ´i lá»‡ch ngáº¯n (right-skewed tail) nghiÃªng vá» phÃ­a sá»‘ dÆ°Æ¡ng giáº£i thÃ­ch cho nhá»¯ng láº§n Ä‘á»“ng thuáº­n á»Ÿ Ä‘áº§u/cuá»‘i mÃ´ hÃ¬nh. NhÃ¬n chung, káº¿t cáº¥u Transformer khÃ´ng cÃ³ chá»— cho sá»± láº·p láº¡i thá»«a thÃ£i.

---

## 4. Káº¿t Luáº­n (BÃ n Luáº­n Ná»­a Cháº·ng)
Hooks lÃ  cÃ´ng cá»¥ giáº£i pháº«u sáº¯c bÃ©n giÃºp chÃºng ta má»• xáº» Residual Stream. Báº±ng cÃ¡ch can thiá»‡p vÃ o táº§ng `c_proj`, nghiÃªn cá»©u chá»©ng thá»±c báº£n thiáº¿t káº¿ chia Ä‘á»ƒ trá»‹ (Divide and Conquer) tá»‘i giáº£n nhÆ°ng phi thÆ°á»ng cá»§a Transformer. Viá»‡c chá»©ng minh 2 luá»“ng cÃ´ng viá»‡c Attention vÃ  MLP tháº³ng gÃ³c nhau táº¡o Ä‘Ã²n báº©y vá»¯ng cháº¯c Ä‘á»ƒ má»Ÿ khÃ³a pháº§n phÃ¢n tÃ­ch Äá»™ dÃ i Ä‘Æ°á»ng dáº«n sáº½ Ä‘Æ°á»£c diá»…n giáº£i á»Ÿ ná»­a sau cá»§a thá»­ thÃ¡ch nÃ y.

---

## TÃ i Liá»‡u Tham Kháº£o (Citations)
1. ThÃ­ nghiá»‡m cÃ i cáº¯m mÃ£ Ä‘o lÆ°á»ng táº¡i `aero_LLM_04_CodeChallenge Residual stream path length decomposition (part 1).md` (Thiáº¿t láº­p hÃ m Hook PyTorch cho `c_proj` cá»§a khá»‘i Transformer á»©ng dá»¥ng GPT-2 Large).
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [PhÃ¢n Kháº£o Token Embeddings: Äo LÆ°á»ng GÃ³c Quay Cá»§a Vector Biá»ƒu Diá»…n](aero_LLM_01_Calculating rotations of embeddings vectors.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Calculating rotations of embeddings vectors.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): Tiáº¿n HÃ³a Äa Táº§ng Cá»§a CÃ¡c Äiá»u Chá»‰nh GÃ³c Quay Tuáº§n Tá»±](aero_LLM_02_CodeChallenge Laminar evolution of sequential angular adjustments.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_CodeChallenge Laminar evolution of sequential angular adjustments.md) |
| [Äo LÆ°á»ng Äá»™ DÃ i ÄÆ°á»ng Dáº«n (Path Length) Sá»± TÆ°Æ¡ng Quan Vá»›i Dá»± ÄoÃ¡n Token](aero_LLM_03_Path length and logit token prediction.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_Path length and logit token prediction.md) |
| ğŸ“Œ **[Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: PhÃ¢n RÃ£ Äá»™ DÃ i ÄÆ°á»ng Dáº«n Luá»“ng Sá»‘ DÆ° (Pháº§n 1)](aero_LLM_04_CodeChallenge Residual stream path length decomposition (part 1).md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Residual stream path length decomposition (part 1).md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: PhÃ¢n RÃ£ Äá»™ DÃ i ÄÆ°á»ng Dáº«n Luá»“ng Sá»‘ DÆ° (Pháº§n 2)](aero_LLM_05_CodeChallenge Residual stream path length decomposition (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_CodeChallenge Residual stream path length decomposition (part 2).md) |
| [Quá»¹ Äáº¡o KhÃ´ng Gian Tráº¡ng ThÃ¡i (State-Space Trajectories) Cá»§a Há»‡ Vector NgÃ´n Ngá»¯](aero_LLM_06_State-space trajectories through embedding space.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_State-space trajectories through embedding space.md) |
| [PhÃ¢n Loáº¡i Tá»« Loáº¡i Báº±ng ThÆ° Viá»‡n SpaCy Trong PhÃ¢n TÃ­ch Mechanistic Interpretability](aero_LLM_07_Parts of speech with SpaCy library.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Parts of speech with SpaCy library.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: So SÃ¡nh Äá»™ DÃ i Quá»¹ Äáº¡o Cá»§a Danh Tá»« VÃ  TÃ­nh Tá»« (Pháº§n 1)](aero_LLM_08_CodeChallenge Do nouns or adjectives have longer trajectories (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge Do nouns or adjectives have longer trajectories (part 1).md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: So SÃ¡nh Äá»™ DÃ i Quá»¹ Äáº¡o Cá»§a Danh Tá»« VÃ  TÃ­nh Tá»« (Pháº§n 2)](aero_LLM_09_CodeChallenge Do nouns or adjectives have longer trajectories (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Do nouns or adjectives have longer trajectories (part 2).md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

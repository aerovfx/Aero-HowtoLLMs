
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [20 Investigating token embeddings](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: PhÃ¢n RÃ£ Äá»™ DÃ i ÄÆ°á»ng Dáº«n Luá»“ng Sá»‘ DÆ° (Pháº§n 2)

## TÃ³m táº¯t (Abstract)
Tiáº¿p tá»¥c cuá»™c kháº£o sÃ¡t trÃªn GPT-2 Large, á»Ÿ pháº§n hai, tiÃªu Ä‘iá»ƒm Ä‘Æ°á»£c dá»‹ch chuyá»ƒn tá»« gÃ³c quay sang phÃ¢n tÃ­ch tham sá»‘ Äá»™ dÃ i Ä‘Æ°á»ng dáº«n (Path Length) phÃ¡t sinh bá»Ÿi hai nhÃ¡nh Attention vÃ  MLP, so vá»›i luá»“ng sá»‘ dÆ° tá»•ng (Hidden States). Káº¿t ná»‘i biáº¿n thiÃªn Euclidean cho tháº¥y cáº¥u trÃºc Ä‘á»“ thá»‹ tÆ°Æ¡ng quan rÃµ rá»‡t: MLP thá»ƒ hiá»‡n áº£nh hÆ°á»Ÿng Ã¡p Ä‘áº£o hÆ¡n so vá»›i Attention trong viá»‡c táº¡o hÃ¬nh dáº¡ng vÃ³c vector chá»‘t xuáº¥t ra. Äá»“ng thá»i, nghiÃªn cá»©u cáº£nh bÃ¡o sá»± nguy hiá»ƒm khi xá»­ lÃ½ thá»‘ng kÃª Pearson do dá»¯ liá»‡u Token má»Ÿ mÃ n (First Token Outlier) gÃ£y ngoáº·t má»i phÃ¢n bá»‘, vÃ  giáº£i phÃ¡p thay tháº¿ thÃ´ng minh nháº¥t lÃ  lÆ°á»£c bá» nÃ³ hoáº·c chuyá»ƒn qua hÃ m háº¡ng Spearman.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Sau khi kháº³ng Ä‘á»‹nh ráº±ng hai bá»™ lá»c Attention vÃ  MLP thá»±c thi cÃ¡c thuáº­t toÃ¡n tÃ­nh toÃ¡n Ä‘á»‹nh hÆ°á»›ng trá»±c giao (Cosine Similarity $\approx 0$), cÃ¢u há»i quan thiáº¿t tiáº¿p theo lÃ : Váº­y cÆ¡ cáº¥u nÃ o máº¡nh hÆ¡n, cÆ¡ cáº¥u nÃ o Ä‘Ã³ng gÃ³p pháº§n xÃ¡c thá»‹t (magnitude) lÃ m giÃ£n vector nhiá»u hÆ¡n trÃªn Ä‘Æ°á»ng Ä‘i qua Transformer Layer?
Äá»ƒ theo váº¿t, phÃ¢n tÃ­ch Äá»™ dÃ i Ä‘Æ°á»ng dáº«n (Path Lengths $PL$) qua 3 bá»™ lá»c riÃªng ráº½ (PL Attn, PL MLP vÃ  PL Hidden States) Ä‘Æ°á»£c thiáº¿t láº­p. 

---

## 2. QuÃ¡ TrÃ¬nh Thi Thiáº¿t & PhÃ¢n TÃ­ch (Methodology & Analysis)

### 2.1. PhÃ¡c Äá»“ LÆ°á»›i ChÃªnh Lá»‡ch Euclide (Path Length Matrix)
Sá»­ dá»¥ng cÃ´ng thá»©c khoáº£ng cÃ¡ch Euclide chÃ©o lá»›p (Layer $i \to i-1$). 
Táº¡i cÃ¡c block Ä‘áº§u cá»§a mÃ´ hÃ¬nh, sá»± giÃ£n ná»Ÿ Path length váº«n Ã¢m á»‰, tuy nhiÃªn khi tiáº¿n sÃ¢u vÃ o nhá»¯ng cá»— mÃ¡y block á»Ÿ Ä‘oáº¡n cuá»‘i (cháº·ng Ã¡p chÃ³t trÆ°á»›c khi xuáº¥t ra Vocab Matrix), Ä‘á»™ lá»›n cá»§a vector Ä‘iá»u chá»‰nh tá»« cáº£ 3 luá»“ng (Attention, MLP, HS) tÄƒng sá»±c ná»©c. MÃ u biá»ƒu Ä‘á»“ tráº£i mÃ u bung sÃ¡ng máº¡nh á»Ÿ khÃºc Ä‘uÃ´i pháº£n Ã¡nh "CÃº Ä‘áº©y cuá»‘i cÃ¹ng" (Big Step Change) trÆ°á»›c rÃ o cáº£n chá»n tá»« tiáº¿p theo.

### 2.2. Khá»›p TÆ°Æ¡ng Quan Pearson R & Chá»‰ BÃ¡o Nhá»‰nh HÆ¡n Cá»§a MLP
Sá»­ dá»¥ng phÃ©p tÆ°Æ¡ng quan biáº¿n $PL_{attn} \leftrightarrow PL_{h\_states}$ vÃ  $PL_{mlp} \leftrightarrow PL_{h\_states}$:
- Háº§u háº¿t cÃ¡c Transformer Layers, cÆ°á»ng Ä‘á»™ bÆ¡m tÃ­n hiá»‡u cá»§a MLP cÃ³ quan há»‡ gáº¯n bÃ³ cao hÆ¡n nhiá»u (Correlation máº¡nh vÃ  Positive rÃµ rÃ ng) so vá»›i nhÃ¡nh váº¥t váº£ hÆ¡n cá»§a Attention.
- Sá»± tháº¯ng tháº¿ nÃ y há»£p lÃ½ theo logic cÆ¡ há»c: Nhiá»‡m vá»¥ cá»§a Khá»‘i Attention lÃ  phÃ³ng táº§m nhÃ¬n Ä‘i kháº¯p chuá»—i dÃ i tÃ³m báº¯t Context. Nhiá»‡m vá»¥ cá»§a MLP Ä‘Ã³ng vai trÃ² nhÃ o náº·n KhÃ´ng gian chiá»u (Expand Dimensionality), biáº¿n vector thÃ nh hÃ¬nh dÃ¡ng sáºµn sÃ ng cho quy trÃ¬nh phÃ¢n lá»›p.  
- HÆ¡n ná»¯a, vÃ¬ MLP náº±m Ä‘áº±ng sau nÃºt giao Attention, nÃ³ Ä‘Ã£ "Äƒn" theo pháº§n ná»™i tiáº¿t Ä‘iá»u chá»‰nh Ä‘Ã³, nghiá»…m nhiÃªn há»‡ quáº£ cá»§a MLP sáº½ lan tá»a Ä‘áº­m hÆ¡n khi tá»›i cháº·ng Ä‘Ã­ch káº¿t thÃºc Transform Block.

### 2.3. Hiá»‡u á»¨ng Báº» GÃ£y Outlier Tá»« Chá»¯ Äáº§u TiÃªn (The First Token Anomaly)
Thá»±c nghiá»‡m tiáº¿t lá»™ má»™t bÃ i há»c xÆ°Æ¡ng mÃ¡u: Khi Ä‘á»ƒ há»› hÃªnh **Token thá»© nháº¥t** dáº­p chung vÃ o máº» phÃ¢n tÃ­ch Correlation cá»§a Pearson, toÃ n bá»™ cáº¥u trÃºc biá»ƒu Ä‘á»“ láº­p tá»©c nÃ¡t vá»¥n (Tá»« máº¡nh máº½ $0.6$ tá»¥t tháº£m háº¡i Ã¢m $\to -0.2$). TÃ¡c nhÃ¢n cá»‘t lÃµi lÃ  First Token mang tráº¡ng thÃ¡i "khá»Ÿi Ä‘á»™ng mÃ¹" cá»±c Ä‘oan, xÃ© toáº¡c Ä‘á»“ thá»‹ vÄƒng xa khá»i tÃ¢m phÃ¢n phá»‘i cáº£ triá»‡u Ä‘á»™ lá»‡ch chuáº©n.
**CÃ¡ch xá»­ lÃ½:** 
1. Tá»‘t nháº¥t lÃ  loáº¡i bá» hoÃ n toÃ n Token 0.
2. Hoáº·c pháº£i sá»­ dá»¥ng cÆ¡ cháº¿ tÆ°Æ¡ng quan Spearman R (TÃ­nh báº±ng Rank) vá»‘n dÄ© triá»‡t tiÃªu má»i uy lá»±c cá»§a Outliers.

---

## 3. Káº¿t Luáº­n
Sá»± tÆ°Æ¡ng quan kháº­p khiá»…ng giá»¯a Path lengths minh há»a rÃµ mÃ´ hÃ¬nh "CÃ´ng xÆ°á»Ÿng hai khÃ¢u" cá»§a Transformer: Attention Ä‘Ã³ng vai ngÆ°á»i dÃ² tin (Information gatherer) vÃ  MLP gÃ¡nh vÃ¡c viá»‡c gia cÃ´ng Ä‘Ã³ng gÃ³i khá»‘i tÃ­nh (Information structurer), vá»›i sá»©c Ã©p cáº¥u hÃ¬nh kÃ­ch cá»¡ Vector pháº§n lá»›n bá»‹ MLP Ä‘á»‹nh hÃ¬nh. Bá»©c tranh nÃ y bá»“i Ä‘áº¯p kiáº¿n thá»©c vi mÃ´ vá» LLMs, Ä‘á»“ng thá»i giÆ°Æ¡ng cao lÃ¡ cá» Ä‘á» nháº¯c nhá»Ÿ vá» sá»± hiá»‡n diá»‡n rá»§i ro cá»§a Token khá»Ÿi Ä‘áº§u.

---

## TÃ i Liá»‡u Tham Kháº£o (Citations)
1. ThÃ­ nghiá»‡m cháº¡y hÃ m Correlation vÃ  tÃ­nh Cumulative sum trÃªn cÃ¡c luá»“ng trÃ­ch xuáº¥t Pytorch dá»±a theo mÃ£ nguá»“n táº¡i `aero_LLM_05_CodeChallenge Residual stream path length decomposition (part 2).md`.
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

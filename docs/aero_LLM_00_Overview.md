# CS229: XÃ¢y Dá»±ng MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n (LLMs) ðŸ§ 

> **Tá»•ng há»£p vÃ  biÃªn soáº¡n tá»« bÃ i giáº£ng CS229 - Machine Learning (Stanford).**
> TÃ i liá»‡u nÃ y tÃ³m táº¯t cÃ¡c nguyÃªn lÃ½ cá»‘t lÃµi, kiáº¿n trÃºc vÃ  quy trÃ¬nh huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (Large Language Models) hiá»‡n Ä‘áº¡i.

---

## ðŸ“š Má»¥c Lá»¥c

1. [ChÆ°Æ¡ng 1: Tá»•ng Quan Vá» LLMs](#chÆ°Æ¡ng-1-tá»•ng-quan-vá»-llms)
2. [ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t Cá»§a Viá»‡c Huáº¥n Luyá»‡n](#chÆ°Æ¡ng-2-5-trá»¥-cá»™t-cá»§a-viá»‡c-huáº¥n-luyá»‡n)
3. [ChÆ°Æ¡ng 3: Quy TrÃ¬nh: Tá»« Pre-training Äáº¿n Post-training](#chÆ°Æ¡ng-3-quy-trÃ¬nh-tá»«-pre-training-Ä‘áº¿n-post-training)
4. [ChÆ°Æ¡ng 4: CÆ¡ Cháº¿ Hoáº¡t Äá»™ng (Autoregressive & Tokenization)](#chÆ°Æ¡ng-4-cÆ¡-cháº¿-hoáº¡t-Ä‘á»™ng-autoregressive--tokenization)
5. [ChÆ°Æ¡ng 5: ÄÃ¡nh GiÃ¡ MÃ´ HÃ¬nh (Evaluation)](#chÆ°Æ¡ng-5-Ä‘Ã¡nh-giÃ¡-mÃ´-hÃ¬nh-evaluation)

---

## ChÆ°Æ¡ng 1: Tá»•ng Quan Vá» LLMs

**Äá»‹nh nghÄ©a:** LLM (Large Language Model) lÃ  cÃ¡c mÃ´ hÃ¬nh phÃ¢n phá»‘i xÃ¡c suáº¥t trÃªn cÃ¡c chuá»—i tá»« (sequences of tokens). NgÃ y nay, háº§u háº¿t cÃ¡c LLM Ä‘á»u dá»±a trÃªn kiáº¿n trÃºc **Transformer**.

**CÃ¡c vÃ­ dá»¥ tiÃªu biá»ƒu:**
- **OpenAI:** GPT-3, GPT-4 (ChatGPT)
- **Anthropic:** Claude
- **Google:** Gemini
- **Meta:** Llama

---

## ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t Cá»§a Viá»‡c Huáº¥n Luyá»‡n

Äá»ƒ xÃ¢y dá»±ng má»™t LLM thÃ nh cÃ´ng, khÃ´ng chá»‰ cáº§n Code mÃ  cáº§n sá»± phá»‘i há»£p cá»§a 5 yáº¿u tá»‘:

1.  **Architecture (Kiáº¿n trÃºc):** Thiáº¿t káº¿ máº¡ng Neural (vÃ­ dá»¥: Transformer, Attention mechanisms). *Giá»›i hÃ n lÃ¢m thÆ°á»ng táº­p trung vÃ o Ä‘Ã¢y.*
2.  **Training Loss & Algorithm:** HÃ m máº¥t mÃ¡t vÃ  thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a.
3.  **Data (Dá»¯ liá»‡u):** "NhiÃªn liá»‡u" cho mÃ´ hÃ¬nh. *Yáº¿u tá»‘ sá»‘ng cÃ²n trong thá»±c táº¿.*
4.  **Evaluation (ÄÃ¡nh giÃ¡):** ThÆ°á»›c Ä‘o sá»± thÃ´ng minh vÃ  hiá»‡u quáº£.
5.  **Systems (Há»‡ thá»‘ng):** Tá»‘i Æ°u hÃ³a pháº§n cá»©ng (GPU/TPU) Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh khá»•ng lá»“.

> ðŸ’¡ **LÆ°u Ã½:** Trong mÃ´i trÆ°á»ng cÃ´ng nghiá»‡p (Industry), trá»ng sá»‘ thÆ°á»ng dá»“n vÃ o **Data, Evaluation vÃ  Systems** nhiá»u hÆ¡n lÃ  viá»‡c sÃ¡ng táº¡o ra kiáº¿n trÃºc má»›i.

---

## ChÆ°Æ¡ng 3: Quy TrÃ¬nh: Tá»« Pre-training Äáº¿n Post-training

QuÃ¡ trÃ¬nh táº¡o ra má»™t AI Assistant nhÆ° ChatGPT tráº£i qua 2 giai Ä‘oáº¡n chÃ­nh:

### Giai Ä‘oáº¡n 1: Pre-training (Tiá»n huáº¥n luyá»‡n)
*   **Má»¥c tiÃªu:** Há»c cÃ¡ch mÃ´ phá»ng Internet.
*   **Nhiá»‡m vá»¥:** Dá»± Ä‘oÃ¡n tá»« tiáº¿p theo (Next token prediction).
*   **Káº¿t quáº£:** Má»™t mÃ´ hÃ¬nh cÃ³ kiáº¿n thá»©c rá»™ng nhÆ°ng chÆ°a biáº¿t cÃ¡ch "phá»¥c vá»¥" con ngÆ°á»i (Base model).
*   *VÃ­ dá»¥:* GPT-2, GPT-3.

### Giai Ä‘oáº¡n 2: Post-training (Háº­u huáº¥n luyá»‡n)
*   **Má»¥c tiÃªu:** Biáº¿n mÃ´ hÃ¬nh thÃ nh trá»£ lÃ½ (Assistant).
*   **PhÆ°Æ¡ng phÃ¡p:** Instruction tuning, RLHF (Reinforcement Learning from Human Feedback).
*   **Káº¿t quáº£:** Chatbot biáº¿t tráº£ lá»i cÃ¢u há»i, tÃ³m táº¯t, viáº¿t code theo lá»‡nh.
*   *VÃ­ dá»¥:* ChatGPT, Claude 3.5 Sonnet.

---

## ChÆ°Æ¡ng 4: CÆ¡ Cháº¿ Hoáº¡t Äá»™ng (Autoregressive & Tokenization)

### 1. Autoregressive Language Modeling (MÃ´ hÃ¬nh tá»± há»“i quy)
LLM sinh vÄƒn báº£n báº±ng cÃ¡ch dá»± Ä‘oÃ¡n tá»«ng tá»« má»™t dá»±a trÃªn ngá»¯ cáº£nh (context) phÃ­a trÆ°á»›c.

$$P(x) = \prod_{i=1}^{L} P(x_i | x_{1}, ..., x_{i-1})$$

*   **Háº¡n cháº¿:** Tá»‘c Ä‘á»™ suy luáº­n (Inference) cháº­m vÃ¬ pháº£i cháº¡y vÃ²ng láº·p (loop) Ä‘á»ƒ sinh tá»«ng tá»« má»™t.

### 2. Tokenization (MÃ£ hÃ³a vÄƒn báº£n)
MÃ¡y tÃ­nh khÃ´ng hiá»ƒu "tá»«" (word) hay "cÃ¢u", chÃºng hiá»ƒu sá»‘. Tokenizer lÃ  cáº§u ná»‘i chuyá»ƒn Ä‘á»•i Text $\leftrightarrow$ IDs.

*   **Táº¡i sao cáº§n Tokenizer?**
    *   Xá»­ lÃ½ ngÃ´n ngá»¯ khÃ´ng cÃ³ dáº¥u cÃ¡ch (Tiáº¿ng ThÃ¡i, Tiáº¿ng Trung).
    *   Xá»­ lÃ½ lá»—i chÃ­nh táº£ (Typos).
    *   Giáº£m Ä‘á»™ dÃ i chuá»—i Ä‘áº§u vÃ o (Sequence length) Ä‘á»ƒ tá»‘i Æ°u hiá»‡u nÄƒng tÃ­nh toÃ¡n.

*   **Thuáº­t toÃ¡n phá»• biáº¿n:** BPE (Byte Pair Encoding) - GhÃ©p cÃ¡c cáº·p kÃ½ tá»± xuáº¥t hiá»‡n thÆ°á»ng xuyÃªn thÃ nh má»™t token.

> âš ï¸ **CÃ¡c váº¥n Ä‘á» thÆ°á»ng gáº·p vá»›i Tokenizer:**
> *   **ToÃ¡n há»c:** CÃ¡c sá»‘ (vÃ­ dá»¥ `327`) cÃ³ thá»ƒ bá»‹ cáº¯t thÃ nh cÃ¡c token rá»i ráº¡c vÃ´ nghÄ©a, khiáº¿n LLM tÃ­nh toÃ¡n sai.
> *   **Láº­p trÃ¬nh:** TrÆ°á»›c Ä‘Ã¢y, khoáº£ng tráº¯ng (indentation) trong Python bá»‹ token hÃ³a kÃ©m, gÃ¢y khÃ³ khÄƒn cho viá»‡c viáº¿t code. (GPT-4 Ä‘Ã£ cáº£i thiá»‡n Ä‘iá»u nÃ y).

---

## ChÆ°Æ¡ng 5: ÄÃ¡nh GiÃ¡ MÃ´ HÃ¬nh (Evaluation)

LÃ m sao biáº¿t mÃ´ hÃ¬nh A thÃ´ng minh hÆ¡n mÃ´ hÃ¬nh B?

1.  **Perplexity (Äá»™ bá»‘i rá»‘i):**
    *   Äo lÆ°á»ng má»©c Ä‘á»™ "cháº¯c cháº¯n" cá»§a mÃ´ hÃ¬nh khi dá»± Ä‘oÃ¡n tá»« tiáº¿p theo.
    *   Chá»‰ sá»‘ cÃ ng **tháº¥p** cÃ ng tá»‘t.
    *   *Lá»‹ch sá»­:* Giáº£m tá»« >70 (2017) xuá»‘ng <10 (2023).

2.  **Benchmarks (Bá»™ Ä‘á» thi):**
    *   Sá»­ dá»¥ng cÃ¡c bÃ i kiá»ƒm tra tiÃªu chuáº©n hÃ³a Ä‘á»ƒ cháº¥m Ä‘iá»ƒm.
    *   *Phá»• biáº¿n:* **HELM** (Holistic Evaluation of Language Models), **Hugging Face Open Leaderboard**.

---
*BiÃªn soáº¡n bá»Ÿi Pixibot - Dá»±a trÃªn Stanford CS229.*

# Phân Tích So Sánh Hàm Kích Hoạt GELU và ReLU trong Mô Hình Ngôn Ngữ Lớn: Góc Nhìn Lý Thuyết và Thực Nghiệm

**Tác giả:** Phân tích học thuật  
**Ngày:** 14 tháng 2, 2026  
**Lĩnh vực:** Deep Learning, Natural Language Processing, Activation Functions

---

## Tóm tắt

Bài viết này trình bày phân tích toàn diện về hai hàm kích hoạt phi tuyến quan trọng trong deep learning: ReLU (Rectified Linear Unit) và GELU (Gaussian Error Linear Unit). Trong khi ReLU là lựa chọn tiêu chuẩn cho hầu hết các ứng dụng deep learning, GELU đã trở thành nền tảng phi tuyến cho các mô hình ngôn ngữ lớn (LLMs) hiện đại. Nghiên cứu kết hợp phân tích toán học, triển khai thực nghiệm trên CPU và GPU, và đánh giá hiệu suất tính toán để làm rõ ưu nhược điểm của từng phương pháp. Kết quả cho thấy GELU vượt trội về tính khả vi và độ mượt gradient, nhưng đi kèm với chi phí tính toán cao hơn, giải thích tại sao việc áp dụng nó vẫn chủ yếu giới hạn trong LLMs.

**Từ khóa:** GELU, ReLU, Activation Functions, Large Language Models, Gradient Descent, GPU Optimization, Deep Learning

---

## 1. Giới Thiệu

### 1.1 Vai Trò của Phi Tuyến Tính trong Deep Learning

Deep learning về bản chất dựa trên các phép toán tuyến tính—nhân ma trận và tổng hợp. Tuy nhiên, nếu chỉ có các phép toán tuyến tính, thì ngay cả mô hình deep learning phức tạp nhất cũng chỉ tương đương với hồi quy tuyến tính (linear regression) [1]. Đây là một hạn chế nghiêm trọng trong khả năng biểu diễn của mô hình.

**Định lý cơ bản:**
> Một neural network chỉ bao gồm các tầng tuyến tính, bất kể độ sâu, có thể được rút gọn thành một single linear transformation duy nhất.

**Chứng minh đơn giản:**
$$\mathbf{y} = \mathbf{W}_n \cdots \mathbf{W}_2 \mathbf{W}_1 \mathbf{x} = \mathbf{W}_{\text{combined}} \mathbf{x}$$

Trong đó $\mathbf{W}_{\text{combined}} = \prod_{i=1}^{n} \mathbf{W}_i$

Do đó, hàm kích hoạt phi tuyến là **absolutely essential** để neural networks có thể học các hàm phức tạp và phi tuyến.

### 1.2 Bối Cảnh Lịch Sử

**ReLU (Rectified Linear Unit):**
- Được giới thiệu rộng rãi bởi Krizhevsky et al. (2012) trong AlexNet [2]
- Trở thành tiêu chuẩn de facto cho computer vision và hầu hết deep learning applications
- Ưu điểm: Đơn giản, nhanh, hiệu quả, giải quyết vanishing gradient problem

**GELU (Gaussian Error Linear Unit):**
- Được đề xuất bởi Hendrycks & Gimpel (2016) [3]
- Ban đầu không được chú ý rộng rãi
- Chỉ thực sự phổ biến với sự bùng nổ của large language models (GPT, BERT, etc.)
- Đặc biệt quan trọng trong transformer architectures

### 1.3 Động Lực Nghiên Cứu

Câu hỏi trung tâm:
1. **Tại sao GELU được ưa chuộng trong LLMs trong khi ReLU vẫn thống trị các lĩnh vực khác?**
2. **Sự đánh đổi giữa hiệu suất tính toán và chất lượng gradient là gì?**
3. **Khi nào nên sử dụng GELU thay vì ReLU?**

---

## 2. Nền Tảng Toán Học

### 2.1 ReLU: Rectified Linear Unit

#### 2.1.1 Định Nghĩa

**Công thức toán học:**
$$\text{ReLU}(x) = \max(0, x) = \begin{cases} 
x & \text{if } x > 0 \\
0 & \text{if } x \leq 0
\end{cases}$$

**Triển khai NumPy:**
```python
def relu(x):
    """ReLU activation function using NumPy"""
    return x * (x > 0)
```

**Đặc điểm:**
- Piecewise linear function (hàm tuyến tính từng đoạn)
- Zeroes out tất cả giá trị âm
- Identity function cho giá trị dương
- Extremely simple và computationally cheap

#### 2.1.2 Đạo Hàm

**Công thức:**
$$\frac{d}{dx}\text{ReLU}(x) = \begin{cases} 
1 & \text{if } x > 0 \\
0 & \text{if } x < 0 \\
\text{undefined} & \text{if } x = 0
\end{cases}$$

**Vấn đề quan trọng:**
- **Discontinuous tại x = 0**: Đạo hàm có step function
- **Not differentiable at zero**: Formally awkward cho gradient descent
- **Dead neurons problem**: Neurons với activation âm có thể "chết" vĩnh viễn

### 2.2 GELU: Gaussian Error Linear Unit

#### 2.2.1 Định Nghĩa Chính Thức

**Công thức exact (sử dụng Error Function):**
$$\text{GELU}(x) = x \cdot \Phi(x) = \frac{x}{2}\left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]$$

Trong đó:
- $\Phi(x)$ là cumulative distribution function (CDF) của phân phối chuẩn
- $\text{erf}(x)$ là Gaussian error function

**Error Function:**
$$\text{erf}(x) = \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt$$

**Đặc điểm của erf:**
- Không thể biểu diễn bằng elementary functions (polynomials, trig functions)
- Phải tính bằng numerical integration hoặc series expansion
- Available trong NumPy, Math, SciPy libraries

**Triển khai Python (Exact):**
```python
from scipy.special import erf
import numpy as np

def gelu_exact(x):
    """GELU exact formula using error function"""
    return (x / 2) * (1 + erf(x / np.sqrt(2)))
```

#### 2.2.2 Công Thức Xấp Xỉ (Approximation)

Do chi phí tính toán của error function, các tác giả đề xuất approximation:

$$\text{GELU}(x) \approx 0.5x\left[1 + \tanh\left(\sqrt{\frac{2}{\pi}}\left(x + 0.044715x^3\right)\right)\right]$$

**Triển khai Python:**
```python
def gelu_approx(x):
    """GELU approximation using tanh"""
    return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))
```

**Độ chính xác:**
- Correlation coefficient giữa exact và approximation: **r ≈ 1.00** (gần như hoàn hảo)
- Visual inspection: Hai đường gần như trùng khớp hoàn toàn
- Practical usage: Approximation thường đủ chính xác

#### 2.2.3 Đạo Hàm GELU

**Đặc điểm:**
- **Smooth và continuous**: Không có discontinuities
- **Differentiable everywhere**: Có đạo hàm tại mọi điểm
- **Gradual transition**: Chuyển đổi mượt từ vùng âm sang vùng dương

**Numerical derivative:**
```python
dx = x[1] - x[0]  # Spacing giữa các điểm
dgelu_dx = torch.diff(gelu_output) / dx
```

### 2.3 So Sánh Trực Quan

#### 2.3.1 Hành Vi của Hàm

```
Đặc điểm              | ReLU           | GELU
---------------------|----------------|------------------
Giá trị âm           | Zeroed out     | Dampened (~10%)
Giá trị dương        | Identity       | Near-identity
Transition           | Sharp (tại 0)  | Smooth
Tính đối xứng        | Không          | Gần như đối xứng
```

**Quan sát thực nghiệm** (với x ∈ [-3, 3]):
- **ReLU**: Flat line ở x < 0, linear với slope=1 ở x > 0
- **GELU**: S-shaped curve, cho phép ~10% giá trị âm leak through
- **GELU**: Smoothed version của ReLU, không có sharp corners

#### 2.3.2 Đạo Hàm

```
Đặc điểm đạo hàm      | ReLU           | GELU
---------------------|----------------|------------------
Tính liên tục        | Discontinuous  | Continuous
Tại x = 0            | Undefined      | ≈ 0.5
Step function        | Có             | Không
Gradient flow        | Jagged         | Smooth
```

**Ý nghĩa cho training:**
- **ReLU**: Gradient có discontinuity → less variability trong loss landscape
- **GELU**: Smooth gradient → more nuanced parameter updates
- **GELU**: Better gradient flow through deep networks

---

## 3. Triển Khai Thực Nghiệm

### 3.1 Phương Pháp Luận

#### 3.1.1 Môi Trường Thử Nghiệm

**Hardware:**
- CPU: Standard multi-core processor
- GPU: CUDA-enabled GPU (via Google Colab)
- Framework: PyTorch 2.x

**Software libraries:**
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import erf
import time
```

#### 3.1.2 Thiết Kế Thí Nghiệm

**Exercise 1: NumPy Implementation**
- Mục tiêu: Hiểu mathematical foundations
- Triển khai 3 functions: ReLU, GELU exact, GELU approx
- Evaluation: x ∈ [-3, 3] với 101 points
- Visualization và correlation analysis

**Exercise 2: PyTorch Functions**
- Chuyển sang PyTorch's built-in functions
- `torch.nn.functional.relu()` và `torch.nn.functional.gelu()`
- Tính numerical derivatives với `torch.diff()`
- Phân tích gradient behavior

**Exercise 3: PyTorch Classes**
- Sử dụng class-based implementations
- `nn.ReLU()` và `nn.GELU()`
- So sánh function-based vs class-based approaches
- Verify equivalence

**Exercise 4: Performance Benchmarking**
- Scale: 1 million random inputs, 100 repetitions
- Platforms: CPU vs GPU
- Metrics: Computation time
- Functions tested: ReLU, GELU exact, GELU approx, `F.gelu()`

### 3.2 Kết Quả Thí Nghiệm

#### 3.2.1 Exercise 1: Visual Analysis

**Findings:**
1. **ReLU**: Perfect piecewise linear, sharp corner tại x=0
2. **GELU exact vs approx**: Visually indistinguishable
   - Correlation: r ≈ 1.00
   - Plotting every 3rd point của approx để thấy overlap
3. **GELU behavior**:
   - Negative values: Dampened to ~10-20% of input
   - Asymptotes to identity for large positive x
   - Smooth S-curve shape

**Code snippet:**
```python
x_vals = np.linspace(-3, 3, 101)
relu_out = relu(x_vals)
gelu_exact_out = gelu_exact(x_vals)
gelu_approx_out = gelu_approx(x_vals)

# Correlation
corr = np.corrcoef(gelu_exact_out, gelu_approx_out)[0,1]
print(f"Correlation: {corr:.6f}")  # ≈ 1.000000
```

#### 3.2.2 Exercise 2: Derivatives

**Numerical derivatives:**
```python
dx = x_vals[1] - x_vals[0]
drelu = torch.diff(F.relu(x_torch)) / dx
dgelu = torch.diff(F.gelu(x_torch)) / dx
```

**Key observations:**

**ReLU derivative:**
- Step function: 0 cho x < 0, 1 cho x > 0
- Discontinuity tại x = 0
- No gradient information trong negative region

**GELU derivative:**
- Smooth sigmoid-like curve
- Continuous across x = 0
- Gradient ≈ 0.5 tại x = 0
- Non-zero gradient cả negative region

**Implications:**
> Smooth gradient của GELU cho phép "less jagged" gradient flow trong backpropagation, potentially more variability có thể extract từ loss function để update parameters.

#### 3.2.3 Exercise 3: Function vs Class

**Verification:**
```python
# Function-based
out_func = F.relu(x)

# Class-based
relu_class = nn.ReLU()
out_class = relu_class(x)

# Check equivalence
assert torch.allclose(out_func, out_class)
```

**Kết luận:** Function và class implementations là equivalent về mathematical operations, chỉ khác về API và usage patterns.

#### 3.2.4 Exercise 4: Performance Benchmarking

**Experimental setup:**
```python
n_samples = 1_000_000
n_reps = 100
x = torch.randn(n_samples)  # Normal distribution
```

**CPU Results (PyTorch implementations):**

| Function | Time (relative) | Notes |
|----------|----------------|-------|
| ReLU | Baseline | Fastest |
| GELU exact | ~slower | Error function overhead |
| GELU approx | ~slower | Still slower than exact |
| F.gelu() | Similar to exact | PyTorch optimized |

**Quan sát CPU:**
- ReLU: Nhanh nhất (expected)
- GELU approx: Surprisingly slower than exact
- PyTorch functions: Đã được tối ưu hóa tốt

**GPU Results (CUDA):**

**Critical implementation detail:**
```python
device = torch.device('cuda')
x = x.to(device)

# IMPORTANT: Synchronize for accurate timing
torch.cuda.synchronize()
```

**Why synchronization matters:**
- GPU operations are **asynchronous**
- CPU continues executing while GPU computes
- `torch.cuda.synchronize()` forces CPU to wait
- Essential for accurate timing measurements
- Has overhead cost (not used in production)

**GPU Performance:**

| Function | Time (relative) | Speedup vs CPU |
|----------|----------------|----------------|
| ReLU | Fast | Significant |
| GELU exact | Moderate | High |
| GELU approx | **Faster than exact!** | High |
| F.gelu() | **Winner** | Highest |

**Key finding:**
> Trên GPU, approximation GELU nhanh hơn exact formula, ngược lại với CPU results. PyTorch's `F.gelu()` là fastest overall.

**Giải thích:**
- GPU optimizations for parallel operations
- Approximation formula có better parallelization
- PyTorch's implementation highly tuned cho GPU
- tanh operations well-optimized trên GPU

---

## 4. Phân Tích Chuyên Sâu

### 4.1 Tại Sao GELU Tốt Hơn cho LLMs?

#### 4.1.1 Gradient Flow Properties

**Smooth gradients:**
- Language models thường rất deep (dozens of layers)
- Gradient phải flow through many layers
- Discontinuous gradients (ReLU) có thể accumulate issues
- GELU's smooth derivative → better gradient propagation

**Non-zero gradients in negative region:**
- GELU cho phép ~10% negative values pass through
- Maintains gradient flow ngay cả khi activations slightly negative
- Prevents "dead neuron" problem

**Mathematical intuition:**
$$\frac{d}{dx}\text{GELU}(x) \neq 0 \text{ for } x < 0$$

Trong khi:
$$\frac{d}{dx}\text{ReLU}(x) = 0 \text{ for } x < 0$$

#### 4.1.2 Stochastic Regularization

**Probabilistic interpretation:**
GELU có thể được hiểu như stochastic regularizer:

$$\text{GELU}(x) = x \cdot \mathbb{1}_{X \sim \mathcal{N}(0,1)}(X < x)$$

Nghĩa là: "multiply input by Bernoulli variable dependent on input"

**Benefits:**
- Implicit regularization during training
- Reduces overfitting
- Particularly beneficial cho large models with many parameters

#### 4.1.3 Empirical Evidence from Literature

**GPT-2 (Radford et al., 2019):** [4]
- Sử dụng GELU exclusively
- Reported improvements over ReLU baselines

**BERT (Devlin et al., 2019):** [5]
- GELU trong all feed-forward layers
- Critical for achieving state-of-the-art results

**Transformer architectures:**
- Near-universal adoption của GELU
- Becomes standard component

### 4.2 Tại Sao ReLU Vẫn Phổ Biến?

#### 4.2.1 Computational Cost

**Chi phí tính toán:**
- GELU: ~2-3x slower than ReLU trên CPU
- Matters cho edge devices, mobile phones
- Power consumption considerations

**FLOPs (Floating Point Operations):**
```
ReLU:        ~1 operation (comparison + multiplication)
GELU exact:  ~10+ operations (erf calculation)
GELU approx: ~8 operations (tanh, polynomial)
```

#### 4.2.2 Legacy và Inertia

**Existing models:**
- Millions of pre-trained models use ReLU
- Retraining với GELU: expensive, time-consuming
- No guarantee of same/better performance
- "If it ain't broke, don't fix it" mentality

**Infrastructure:**
- Optimization toolchains built around ReLU
- Hardware accelerators tuned for ReLU
- Software libraries optimized

#### 4.2.3 Sparsity Promotion

**ReLU's zeroing property:**
```python
# ReLU creates sparse activations
x = torch.randn(1000)
relu_out = F.relu(x)
sparsity = (relu_out == 0).sum().item() / 1000
# sparsity ≈ 50% (vì normal distribution)
```

**Benefits cho Computer Vision:**
- Sparse filter kernels
- More interpretable features
- Memory efficiency
- Faster inference

**GELU không có property này:**
```python
gelu_out = F.gelu(x)
sparsity = (gelu_out == 0).sum().item() / 1000
# sparsity ≈ 0% (no exact zeros)
```

#### 4.2.4 Subtle Improvements

**Performance gains:**
- GELU's advantages: **subtle** cho small/medium models
- Only significant cho **extremely large** và **very deep** models
- LLMs: billions of parameters, dozens of layers
- Computer vision: millions of parameters, ~10-20 layers

**Cost-benefit analysis:**
- Small models: ReLU's speed advantage outweighs GELU's quality
- Large models: GELU's quality worth the computational cost

### 4.3 Domain-Specific Considerations

#### 4.3.1 Language Modeling

**Why GELU excels:**
- Sequential dependencies require smooth information flow
- Long-range dependencies benefit từ better gradients
- Context understanding needs nuanced activations
- Large model sizes amortize computational overhead

#### 4.3.2 Computer Vision

**Why ReLU persists:**
- Local features (convolutional kernels) benefit từ sparsity
- Shallower networks (relative to LLMs)
- Real-time requirements (object detection, segmentation)
- Edge deployment common

#### 4.3.3 Other Domains

**Recommendation systems:** ReLU
- Sparse user-item interactions
- Interpretability important

**Time series:** Mixed
- GELU cho very long sequences
- ReLU cho shorter sequences

**Reinforcement learning:** Mostly ReLU
- Sample efficiency critical
- Computational speed matters

---

## 5. Best Practices và Recommendations

### 5.1 Decision Framework

**Flowchart để chọn activation function:**

```
1. Are you building an LLM or transformer?
   → YES: Use GELU
   → NO: Continue to 2

2. Is your model very deep (>50 layers)?
   → YES: Consider GELU
   → NO: Continue to 3

3. Do you need edge/mobile deployment?
   → YES: Use ReLU
   → NO: Continue to 4

4. Is training speed critical?
   → YES: Use ReLU
   → NO: Consider GELU

5. Default: Use ReLU (safest choice)
```

### 5.2 Implementation Guidelines

#### 5.2.1 Sử Dụng GELU

**When:**
- Transformer architectures
- Models with >1B parameters
- NLP tasks with long sequences
- When you have GPU resources

**How:**
```python
import torch.nn as nn

class TransformerFFN(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.gelu = nn.GELU()  # Use class for clarity
        self.fc2 = nn.Linear(d_ff, d_model)
    
    def forward(self, x):
        return self.fc2(self.gelu(self.fc1(x)))
```

**Considerations:**
- Use PyTorch's built-in `nn.GELU()` (optimized)
- Không cần implement custom approximation
- Trust framework's optimizations

#### 5.2.2 Sử Dụng ReLU

**When:**
- Computer vision models (CNNs)
- Shallow networks (<20 layers)
- Edge/mobile deployment
- When speed is paramount

**How:**
```python
class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.relu = nn.ReLU(inplace=True)  # inplace for memory efficiency
    
    def forward(self, x):
        return self.relu(self.conv(x))
```

**Pro tip:**
- `inplace=True` saves memory
- Safe khi activation không cần cho gradient computation sau này

### 5.3 Hybrid Approaches

**Strategy:** Use different activations ở different parts của model

**Example:**
```python
class HybridModel(nn.Module):
    def __init__(self):
        super().__init__()
        # Early layers: ReLU (speed)
        self.early_layers = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
        )
        
        # Deep layers: GELU (quality)
        self.deep_layers = nn.Sequential(
            nn.Linear(512, 512),
            nn.GELU(),
            nn.Linear(512, 512),
            nn.GELU(),
        )
        
        self.output = nn.Linear(512, num_classes)
```

**Rationale:**
- Balance speed và quality
- Early layers: feature extraction (ReLU sufficient)
- Deep layers: complex reasoning (GELU beneficial)

---

## 6. Nghiên Cứu Liên Quan và Hướng Phát Triển

### 6.1 Các Activation Functions Khác

#### 6.1.1 Swish/SiLU

**Formula:**
$$\text{Swish}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}$$

**Properties:**
- Similar to GELU
- Slightly simpler computation
- Used trong EfficientNet [6]

#### 6.1.2 Mish

**Formula:**
$$\text{Mish}(x) = x \cdot \tanh(\text{softplus}(x)) = x \cdot \tanh(\ln(1 + e^x))$$

**Properties:**
- Smoother than Swish
- Better performance reported trong some tasks
- Higher computational cost

#### 6.1.3 Performance Comparison Table

| Activation | Smoothness | Speed | Gradient Quality | Best Domain |
|------------|-----------|-------|------------------|-------------|
| ReLU | Low | Fastest | Moderate | CV, General |
| GELU | High | Moderate | Excellent | NLP, LLMs |
| Swish | High | Moderate | Very Good | CV, Mixed |
| Mish | Highest | Slowest | Excellent | Research |

### 6.2 Hardware Optimization Trends

#### 6.2.1 GPU Developments

**Current:**
- CUDA kernels optimized cho common activations
- Tensor cores không specific cho activations
- Memory bandwidth often bottleneck

**Future:**
- Custom hardware units cho GELU
- Approximate computing in hardware
- Energy-efficient implementations

#### 6.2.2 Specialized Accelerators

**TPUs (Tensor Processing Units):**
- Google's custom chips
- Optimized for matrix operations
- Increasingly supporting complex activations

**NPUs (Neural Processing Units):**
- Edge devices
- Trade-off: simplicity vs capability
- ReLU remains dominant

### 6.3 Theoretical Advances

#### 6.3.1 Adaptive Activations

**Learnable parameters trong activation:**
```python
class LearnableGELU(nn.Module):
    def __init__(self):
        super().__init__()
        self.alpha = nn.Parameter(torch.ones(1))
    
    def forward(self, x):
        return F.gelu(self.alpha * x)
```

**Research direction:** Model learns optimal activation scaling

#### 6.3.2 Dynamic Activations

**Context-dependent activations:**
- Different activations for different inputs
- Mixture of experts approach
- Computational overhead questions

### 6.4 Future Predictions

**Short-term (2-5 years):**
- GELU solidifies dominance trong LLMs
- Hybrid approaches become more common
- Better GPU optimizations for GELU

**Medium-term (5-10 years):**
- Novel activation functions specifically designed cho transformers
- Hardware co-design (algorithms + chips)
- Adaptive/learnable activations

**Long-term (10+ years):**
- Biological inspiration: spiking neurons
- Quantum computing implications
- Fundamental rethinking của activation paradigm

---

## 7. Kết Luận

### 7.1 Tóm Tắt Findings

**Về toán học:**
1. **ReLU**: Simple, fast, piecewise linear, discontinuous derivative
2. **GELU**: Complex, slower, smooth, continuous derivative
3. **Approximation**: GELU approx highly accurate (r ≈ 1.00)

**Về hiệu suất:**
1. **CPU**: ReLU fastest, GELU exact < GELU approx
2. **GPU**: F.gelu() fastest, GELU approx > GELU exact
3. **Optimization**: Framework implementations beat custom code

**Về gradient flow:**
1. **ReLU**: Discontinuous, zero gradient cho x < 0
2. **GELU**: Smooth, non-zero gradient everywhere
3. **Implication**: Better cho very deep networks

**Về áp dụng:**
1. **LLMs**: GELU is standard choice
2. **Computer Vision**: ReLU remains dominant
3. **Trade-off**: Quality vs speed vs sparsity

### 7.2 Core Insights

**Insight 1: Context Matters**
> Không có "best" activation function universally. Choice phụ thuộc vào architecture, task, deployment constraints, và resources.

**Insight 2: Inertia is Real**
> Technical superiority không guarantee adoption. Existing infrastructure, trained models, và engineering practices create significant momentum.

**Insight 3: Specialization Emerging**
> Deep learning field đang move towards domain-specific optimizations rather than one-size-fits-all solutions.

**Insight 4: Co-evolution**
> Hardware và algorithms co-evolve. GELU's adoption drives hardware optimization, which enables further algorithm development.

### 7.3 Practical Takeaways

**For practitioners:**

1. **Building LLMs?** → Use GELU (no debate)
2. **Building CNNs?** → Use ReLU (unless research shows otherwise)
3. **Experimenting?** → Try both, measure impact
4. **Limited resources?** → ReLU is safer default
5. **Have GPUs?** → GELU cost is manageable

**For researchers:**

1. Study hybrid approaches
2. Investigate learned activations
3. Consider hardware co-design
4. Explore domain-specific functions
5. Benchmark thoroughly

### 7.4 Closing Thoughts

Câu hỏi "GELU vs ReLU" không có single correct answer. Như nhiều engineering decisions, nó là về trade-offs:

**ReLU offers:**
- Speed, simplicity, sparsity
- Proven track record
- Wide support

**GELU offers:**
- Smooth gradients, better flow
- State-of-the-art LLM performance
- Theoretical elegance

The choice depends on **your specific context, requirements, và constraints**.

Quan trọng nhất: **Understand the principles** behind each activation function. This knowledge empowers you to make informed decisions rather than blindly following trends.

---

## 8. Tài Liệu Tham Khảo

[1] Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. Chapter 6: Deep Feedforward Networks.

[2] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). "ImageNet Classification with Deep Convolutional Neural Networks." *Advances in Neural Information Processing Systems* (NeurIPS), 25, 1097-1105.

[3] Hendrycks, D., & Gimpel, K. (2016). "Gaussian Error Linear Units (GELUs)." *arXiv preprint arXiv:1606.08415*. https://arxiv.org/abs/1606.08415

[4] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). "Language Models are Unsupervised Multitask Learners." *OpenAI Blog*. https://openai.com/research/better-language-models

[5] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding." *Proceedings of NAACL-HLT*, 4171-4186. https://arxiv.org/abs/1810.04805

[6] Tan, M., & Le, Q. V. (2019). "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks." *Proceedings of the 36th International Conference on Machine Learning* (ICML), 6105-6114.

[7] Ramachandran, P., Zoph, B., & Le, Q. V. (2017). "Searching for Activation Functions." *arXiv preprint arXiv:1710.05941*. https://arxiv.org/abs/1710.05941

[8] Misra, D. (2019). "Mish: A Self Regularized Non-Monotonic Activation Function." *arXiv preprint arXiv:1908.08681*. https://arxiv.org/abs/1908.08681

[9] Clevert, D. A., Unterthiner, T., & Hochreiter, S. (2015). "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)." *arXiv preprint arXiv:1511.07289*. https://arxiv.org/abs/1511.07289

[10] He, K., Zhang, X., Ren, S., & Sun, J. (2015). "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification." *Proceedings of the IEEE International Conference on Computer Vision* (ICCV), 1026-1034.

[11] Vaswani, A., et al. (2017). "Attention is All You Need." *Advances in Neural Information Processing Systems* (NeurIPS), 30, 5998-6008.

[12] Paszke, A., et al. (2019). "PyTorch: An Imperative Style, High-Performance Deep Learning Library." *Advances in Neural Information Processing Systems* (NeurIPS), 32, 8024-8035.

---

## Phụ Lục A: Code Repository

### A.1 Complete Implementation

**File: activation_functions.py**
```python
"""
Complete implementation of ReLU and GELU activation functions
with NumPy and PyTorch
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from scipy.special import erf
import matplotlib.pyplot as plt
import time

# NumPy Implementations
def relu(x):
    """ReLU activation using NumPy"""
    return x * (x > 0)

def gelu_exact(x):
    """GELU exact formula using error function"""
    return (x / 2) * (1 + erf(x / np.sqrt(2)))

def gelu_approx(x):
    """GELU approximation using tanh"""
    return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))

# PyTorch Custom Implementations
def gelu_exact_torch(x):
    """GELU exact formula using PyTorch"""
    return (x / 2) * (1 + torch.erf(x / torch.sqrt(torch.tensor(2.0))))

def gelu_approx_torch(x):
    """GELU approximation using PyTorch"""
    return 0.5 * x * (1 + torch.tanh(
        torch.sqrt(torch.tensor(2.0/torch.pi)) * (x + 0.044715 * x**3)
    ))

# Visualization Function
def plot_activations():
    """Plot activation functions and their derivatives"""
    x = np.linspace(-3, 3, 101)
    x_torch = torch.linspace(-3, 3, 101)
    
    # Compute activations
    relu_out = relu(x)
    gelu_exact_out = gelu_exact(x)
    gelu_approx_out = gelu_approx(x)
    
    # Compute derivatives
    dx = x_torch[1] - x_torch[0]
    drelu = torch.diff(F.relu(x_torch)) / dx
    dgelu = torch.diff(F.gelu(x_torch)) / dx
    
    # Create figure
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # Plot activations
    ax1.plot(x, relu_out, label='ReLU', linewidth=2)
    ax1.plot(x, gelu_exact_out, label='GELU Exact', linewidth=2)
    ax1.plot(x[::3], gelu_approx_out[::3], 'o', 
             label='GELU Approx', markersize=4)
    ax1.axhline(y=0, color='k', linestyle='--', alpha=0.3)
    ax1.axvline(x=0, color='k', linestyle='--', alpha=0.3)
    ax1.set_xlabel('x', fontsize=12)
    ax1.set_ylabel('f(x)', fontsize=12)
    ax1.set_title('Activation Functions', fontsize=14)
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)
    
    # Plot derivatives
    x_deriv = x_torch[:-1].numpy()
    ax2.plot(x_deriv, drelu.numpy(), label='ReLU derivative', linewidth=2)
    ax2.plot(x_deriv, dgelu.numpy(), label='GELU derivative', linewidth=2)
    ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)
    ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)
    ax2.set_xlabel('x', fontsize=12)
    ax2.set_ylabel("f'(x)", fontsize=12)
    ax2.set_title('Derivatives', fontsize=14)
    ax2.legend(fontsize=10)
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('activation_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Compute correlation
    corr = np.corrcoef(gelu_exact_out, gelu_approx_out)[0, 1]
    print(f"Correlation between exact and approx: {corr:.6f}")

# Benchmarking Function
def benchmark_activations(device='cpu', n_samples=1000000, n_reps=100):
    """Benchmark activation function performance"""
    x = torch.randn(n_samples, device=device)
    
    results = {}
    
    # Test ReLU
    if device == 'cuda':
        torch.cuda.synchronize()
    start = time.time()
    for _ in range(n_reps):
        _ = F.relu(x)
    if device == 'cuda':
        torch.cuda.synchronize()
    results['ReLU'] = time.time() - start
    
    # Test GELU exact
    if device == 'cuda':
        torch.cuda.synchronize()
    start = time.time()
    for _ in range(n_reps):
        _ = gelu_exact_torch(x)
    if device == 'cuda':
        torch.cuda.synchronize()
    results['GELU Exact'] = time.time() - start
    
    # Test GELU approx
    if device == 'cuda':
        torch.cuda.synchronize()
    start = time.time()
    for _ in range(n_reps):
        _ = gelu_approx_torch(x)
    if device == 'cuda':
        torch.cuda.synchronize()
    results['GELU Approx'] = time.time() - start
    
    # Test F.gelu
    if device == 'cuda':
        torch.cuda.synchronize()
    start = time.time()
    for _ in range(n_reps):
        _ = F.gelu(x)
    if device == 'cuda':
        torch.cuda.synchronize()
    results['F.gelu()'] = time.time() - start
    
    return results

# Main execution
if __name__ == "__main__":
    print("Activation Function Analysis")
    print("=" * 50)
    
    # Visualize
    print("\n1. Generating visualizations...")
    plot_activations()
    
    # Benchmark CPU
    print("\n2. Benchmarking on CPU...")
    cpu_results = benchmark_activations(device='cpu')
    for func, time_taken in cpu_results.items():
        print(f"   {func}: {time_taken:.4f} seconds")
    
    # Benchmark GPU (if available)
    if torch.cuda.is_available():
        print("\n3. Benchmarking on GPU...")
        gpu_results = benchmark_activations(device='cuda')
        for func, time_taken in gpu_results.items():
            print(f"   {func}: {time_taken:.4f} seconds")
    else:
        print("\n3. GPU not available, skipping GPU benchmarks")
    
    print("\n" + "=" * 50)
    print("Analysis complete!")
```

### A.2 Usage Examples

**Example 1: Using in a neural network**
```python
import torch.nn as nn

class SimpleNet(nn.Module):
    def __init__(self, use_gelu=False):
        super().__init__()
        self.fc1 = nn.Linear(784, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 10)
        
        # Choose activation
        if use_gelu:
            self.activation = nn.GELU()
        else:
            self.activation = nn.ReLU()
    
    def forward(self, x):
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.fc3(x)
        return x

# Create models
model_relu = SimpleNet(use_gelu=False)
model_gelu = SimpleNet(use_gelu=True)
```

**Example 2: Custom activation module**
```python
class CustomGELU(nn.Module):
    """Custom GELU implementation with learnable parameter"""
    def __init__(self, approximate=False):
        super().__init__()
        self.approximate = approximate
    
    def forward(self, x):
        if self.approximate:
            return gelu_approx_torch(x)
        else:
            return F.gelu(x)

# Usage
custom_gelu = CustomGELU(approximate=True)
output = custom_gelu(torch.randn(10, 512))
```

---

## Phụ Lục B: Supplementary Mathematics

### B.1 Error Function Properties

**Definition:**
$$\text{erf}(x) = \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt$$

**Properties:**
1. $\text{erf}(-x) = -\text{erf}(x)$ (odd function)
2. $\text{erf}(0) = 0$
3. $\lim_{x \to \infty} \text{erf}(x) = 1$
4. $\lim_{x \to -\infty} \text{erf}(x) = -1$

**Series expansion:**
$$\text{erf}(x) = \frac{2}{\sqrt{\pi}} \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n+1}}{n!(2n+1)}$$

### B.2 GELU Derivation

**Starting point:** Stochastic regularization
$$\mathbb{E}[x \cdot \mathbb{1}_{X \sim \mathcal{N}(0,1)}(X < x)]$$

**CDF của standard normal:**
$$\Phi(x) = P(X \leq x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-t^2/2} dt$$

**Relationship với error function:**
$$\Phi(x) = \frac{1}{2}\left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]$$

**Therefore:**
$$\text{GELU}(x) = x \cdot \Phi(x) = \frac{x}{2}\left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]$$

### B.3 Approximation Derivation

**Goal:** Find simpler formula close to exact GELU

**Approach:** Use tanh approximation of erf

**Known relationship:**
$$\text{erf}(x) \approx \tanh\left(\sqrt{\frac{\pi}{2}} x + \alpha x^3\right)$$

**Optimal $\alpha$:** Through empirical fitting, $\alpha \approx 0.044715$

**Final approximation:**
$$\text{GELU}(x) \approx 0.5x\left[1 + \tanh\left(\sqrt{\frac{2}{\pi}}\left(x + 0.044715x^3\right)\right)\right]$$

---

## Phụ Lục C: Glossary

**Activation Function:** Non-linear function applied element-wise to neuron outputs

**Backpropagation:** Algorithm for computing gradients in neural networks

**CDF (Cumulative Distribution Function):** Probability that random variable ≤ x

**CUDA:** NVIDIA's parallel computing platform for GPUs

**Dead Neuron:** Neuron with always-zero activation during training

**Derivative:** Rate of change of function with respect to input

**Error Function (erf):** Special function arising in probability and statistics

**FLOPs:** Floating Point Operations, measure of computational cost

**Gradient Descent:** Optimization algorithm using gradients to minimize loss

**Inertia:** Resistance to change existing practices/systems

**Piecewise Linear:** Function that is linear within segments

**ReLU:** Rectified Linear Unit, max(0, x) activation

**Smoothness:** Property of function having continuous derivatives

**Sparsity:** Property of having many zero values

**Synchronization:** Coordinating timing between CPU and GPU

**Tensor:** Multi-dimensional array in deep learning frameworks

**Transformer:** Neural network architecture using attention mechanisms

---

**Ghi chú kết thúc:**

Bài viết này tổng hợp kiến thức từ lecture materials, hands-on experimentation, và academic literature để cung cấp comprehensive understanding về GELU vs ReLU activation functions. Emphasis đặt vào practical insights và decision-making frameworks thay vì chỉ theoretical analysis.

**Liên hệ và feedback:**
Nếu bạn có câu hỏi hoặc muốn thảo luận thêm về activation functions, vui lòng liên hệ qua các kênh academic discourse.

**Cập nhật:** 14/02/2026
**Version:** 1.0
**License:** Educational use permitted với proper attribution

---

*Tài liệu này được tạo cho mục đích giáo dục và nghiên cứu.*

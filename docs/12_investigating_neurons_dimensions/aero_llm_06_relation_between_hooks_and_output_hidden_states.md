
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [12 investigating neurons dimensions](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Má»‘i tÆ°Æ¡ng quan giá»¯a Hooks vÃ  Hidden States: Giáº£i cáº¥u trÃºc Khá»‘i Transformer (Reconstructing Transformer Blocks)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y lÃ m sÃ¡ng tá» má»‘i liÃªn há»‡ cÆ¡ há»c giá»¯a hai phÆ°Æ¡ng phÃ¡p trÃ­ch xuáº¥t dá»¯ liá»‡u: `output.hidden_states` (quan sÃ¡t dÃ²ng dÆ° - residual stream) vÃ  Hooks (quan sÃ¡t cÃ¡c Ä‘iá»u cháº¿ ná»™i bá»™). ThÃ´ng qua thá»±c nghiá»‡m tÃ¡i cáº¥u trÃºc hoáº¡t hÃ³a cá»§a Táº§ng 11 tá»« Táº§ng 10 trÃªn mÃ´ hÃ¬nh GPT-2, nghiÃªn cá»©u chá»©ng minh ráº±ng Ä‘áº§u ra cá»§a má»™t khá»‘i Transformer chÃ­nh báº±ng tá»•ng cá»§a Ä‘áº§u vÃ o vÃ  cÃ¡c "Ä‘á»™ lá»‡ch" (deltas) Ä‘Æ°á»£c tÃ­nh toÃ¡n bá»Ÿi phÃ¢n Ä‘oáº¡n Attention vÃ  MLP. BÃ¡o cÃ¡o cÅ©ng nháº¥n máº¡nh táº§m quan trá»ng cá»§a viá»‡c hiá»ƒu cÃ¡c cháº¿ Ä‘á»™ `eval` so vá»›i `train` trong PyTorch khi lÃ m viá»‡c vá»›i Hooks, Ä‘áº·c biá»‡t lÃ  vai trÃ² cá»§a hÃ m `detach()`.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Má»™t trong nhá»¯ng nguyÃªn lÃ½ cá»‘t lÃµi cá»§a kiáº¿n trÃºc Transformer lÃ  máº¡ng lÆ°á»›i cÃ¡c káº¿t ná»‘i dÆ° (residual connections). Thay vÃ¬ biáº¿n Ä‘á»•i hoÃ n toÃ n vector nhÃºng á»Ÿ má»—i táº§ng, mÃ´ hÃ¬nh chá»‰ tÃ­nh toÃ¡n cÃ¡c "Ä‘iá»u chá»‰nh" (adjustments) nhá» dá»±a trÃªn ngá»¯ cáº£nh vÃ  tri thá»©c tháº¿ giá»›i, sau Ä‘Ã³ cá»™ng dá»“n chÃºng vÃ o dÃ²ng cháº£y thÃ´ng tin. BÃ¡o cÃ¡o nÃ y sáº½ thá»±c chá»©ng nguyÃªn lÃ½ Ä‘Ã³ báº±ng cÃ¡ch káº¿t há»£p dá»¯ liá»‡u tá»« Hidden States vÃ  Hooks.

---

## 2. PhÆ°Æ¡ng PhÃ¡p Thá»±c Nghiá»‡m (Methodology)

### 2.1. Vá»‹ trÃ­ Cáº¥y Hook $C_proj$
Äá»ƒ láº¥y Ä‘Æ°á»£c "Ä‘á»™ lá»‡ch" cuá»‘i cÃ¹ng cá»§a má»—i phÃ¢n Ä‘oáº¡n, chÃºng ta cáº¥y Hook vÃ o lá»›p `c_proj` (output projection) cá»§a cáº£ Attention vÃ  MLP. ÄÃ¢y lÃ  Ä‘iá»ƒm cuá»‘i cÃ¹ng trÆ°á»›c khi cÃ¡c giÃ¡ trá»‹ Ä‘iá»u chá»‰nh Ä‘Æ°á»£c cá»™ng ngÆ°á»£c trá»Ÿ láº¡i vÃ o residual stream.

### 2.2. Quáº£n lÃ½ Äá»“ thá»‹ TÃ­nh toÃ¡n (Gradient Detachment)
Khi mÃ´ hÃ¬nh khÃ´ng á»Ÿ cháº¿ Ä‘á»™ `eval()`, cÃ¡c tensor trÃ­ch xuáº¥t qua Hook váº«n mang theo thÃ´ng tin vá» gradient vÃ  Ä‘á»“ thá»‹ tÃ­nh toÃ¡n. 
- **Ká»¹ thuáº­t:** Sá»­ dá»¥ng `.detach()` Ä‘á»ƒ tÃ¡ch cÃ¡c con sá»‘ thuáº§n tÃºy khá»i Ä‘á»“ thá»‹, giÃºp tiáº¿t kiá»‡m bá»™ nhá»› vÃ  trÃ¡nh cÃ¡c lá»—i tÃ­nh toÃ¡n khÃ´ng mong muá»‘n trong quÃ¡ trÃ¬nh phÃ¢n tÃ­ch háº­u ká»³ (post-processing).
- **LÆ°u Ã½:** Náº¿u sá»­ dá»¥ng `eval()`, tÃ­nh nÄƒng tÃ­nh gradient bá»‹ táº¯t hoÃ n toÃ n, giÃºp lÆ°á»£c bá» bÆ°á»›c detach nÃ y.

---

## 3. Káº¿t Quáº£ Thá»±c Nghiá»‡m: TÃ¡i cáº¥u trÃºc Hoáº¡t hÃ³a

### 3.1. Sá»± báº£o tá»“n TÃ­n hiá»‡u (Laminar Correlation)
Äá»“ thá»‹ phÃ¢n tÃ¡n giá»¯a Ä‘áº§u ra cá»§a Táº§ng 10 vÃ  Táº§ng 11 cho tháº¥y sá»± tÆ°Æ¡ng quan cá»±c máº¡nh ($r $\approx$ 1.0$). Äiá»u nÃ y kháº³ng Ä‘á»‹nh ráº±ng Hidden State khÃ´ng bá»‹ thay Ä‘á»•i hoÃ n toÃ n sau má»—i Transformer Block mÃ  chá»‰ bá»‹ biáº¿n Ä‘á»•i nháº¹.

### 3.2. CÃ´ng thá»©c TÃ¡i cáº¥u trÃºc (The Reconstruction Formula)
GiÃ¡ trá»‹ hoáº¡t hÃ³a cá»§a Táº§ng $L+1$ cÃ³ thá»ƒ Ä‘Æ°á»£c dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c tuyá»‡t Ä‘á»‘i báº±ng cÃ´ng thá»©c:

$$
\mathbf{H}_{L+1} = \mathbf{H}_L + \Delta_{Attention} + \Delta_{MLP}
$$

Thá»±c nghiá»‡m cho tháº¥y khi cá»™ng cÃ¡c giÃ¡ trá»‹ trÃ­ch xuáº¥t tá»« Hook ($\Delta$) vÃ o Hidden State hiá»‡n táº¡i, ta thu Ä‘Æ°á»£c káº¿t quáº£ khá»›p hoÃ n háº£o vá»›i Hidden State cá»§a táº§ng tiáº¿p theo trÃ­ch xuáº¥t tá»« `output.hidden_states`.

### 3.3. Hiá»‡n tÆ°á»£ng Ngoáº¡i lai (Outlier Handling)
Quan sÃ¡t thá»±c nghiá»‡m cho tháº¥y má»™t sá»‘ chiá»u (thÆ°á»ng á»Ÿ token Ä‘áº§u tiÃªn) cÃ³ giÃ¡ trá»‹ kÃ­ch hoáº¡t cá»±c lá»›n. Trong phÃ¢n tÃ­ch, viá»‡c sá»­ dá»¥ng cÃ¡c máº·t náº¡ (masks) Ä‘á»ƒ loáº¡i bá» cÃ¡c giÃ¡ trá»‹ ngoáº¡i lai nÃ y giÃºp viá»‡c quan sÃ¡t sá»± tÆ°Æ¡ng quan cá»§a 99% dá»¯ liá»‡u cÃ²n láº¡i trá»Ÿ nÃªn rÃµ rÃ ng hÆ¡n trÃªn Ä‘á»“ thá»‹.

---

## 4. Tháº£o Luáº­n: Vai trÃ² cá»§a Dropout trong PhÃ¢n tÃ­ch
Náº¿u giá»¯ mÃ´ hÃ¬nh á»Ÿ cháº¿ Ä‘á»™ `train`, cÃ¡c lá»›p Dropout váº«n hoáº¡t Ä‘á»™ng, gÃ¢y ra sá»± ngáº«u nhiÃªn trong cÃ¡c kÃ­ch hoáº¡t trÃ­ch xuáº¥t. Äiá»u nÃ y nháº¯c nhá»Ÿ cÃ¡c nhÃ  nghiÃªn cá»©u ráº±ng tráº¡ng thÃ¡i cá»§a mÃ´ hÃ¬nh (Mode) cÃ³ áº£nh hÆ°á»Ÿng quyáº¿t Ä‘á»‹nh Ä‘áº¿n tÃ­nh láº·p láº¡i cá»§a cÃ¡c phÃ©p Ä‘o cÆ¡ há»c.

---

## 5. Káº¿t Luáº­n
Sá»± khá»›p ná»‘i hoÃ n háº£o giá»¯a Hooks vÃ  Hidden States kháº³ng Ä‘á»‹nh tÃ­nh Ä‘Ãºng Ä‘áº¯n cá»§a mÃ´ thá»©c "dÃ²ng cháº£y dÆ°" (residual stream hypothesis). Viá»‡c hiá»ƒu rÃµ cÃ¡ch cÃ¡c thÃ nh pháº§n cá»™ng dá»“n vÃ o nhau cung cáº¥p cÃ´ng cá»¥ Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c can thiá»‡p sÃ¢u hÆ¡n, nhÆ° viá»‡c cÃ´ láº­p vÃ  chá»‰nh sá»­a má»™t thÃ nh pháº§n Ä‘Æ¡n láº» (vÃ­ dá»¥: chá»‰ chá»‰nh sá»­a MLP delta) Ä‘á»ƒ quan sÃ¡t tÃ¡c Ä‘á»™ng lan tá»a háº¡ nguá»“n.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. Giáº£i thuáº­t tÃ¡i cáº¥u trÃºc khá»‘i Transformer báº±ng Hooks dá»±a trÃªn `aero_LLM_06_Relation between hooks and output.hidden_states.md`. PhÃ¢n tÃ­ch sá»± Ä‘Ã³ng gÃ³p cá»§a Attention vÃ  MLP deltas vÃ o residual stream.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [ğŸ“‚ Module: 12_investigating_neurons_dimensions](README.md) | [Xem bÃ i viáº¿t â†’](README.md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Activation Maximization): CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Nhá»¯ng thÃ¡ch thá»©c trong LLM](aero_llm_01_activation_maximization_via_gradient_ascent_theory_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_activation_maximization_via_gradient_ascent_theory_.md) |
| [Triá»ƒn khai Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a: Tá»« Gradient Ascent Ä‘áº¿n Giáº£i mÃ£ Token (Activation Maximization Implementation)](aero_llm_02_activation_maximization_code_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_activation_maximization_code_.md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a qua Láº¥y máº«u Dá»¯ liá»‡u (Activation Maximization via Data Sampling)](aero_llm_03_activation_maximization_via_data_sampling.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_activation_maximization_via_data_sampling.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Kiá»ƒm chá»©ng TÃ­nh láº·p láº¡i cá»§a Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Reproducibility of Activation Maximization)](aero_llm_04_codechallenge_reproducibility_of_activation_maximization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_reproducibility_of_activation_maximization.md) |
| [Giáº£i pháº«u Ná»™i táº¡i MÃ´ hÃ¬nh báº±ng Hooks: Ká»¹ thuáº­t TrÃ­ch xuáº¥t Hoáº¡t hÃ³a (Extracting Activations via Hooks)](aero_llm_05_extracting_activations_using_hooks.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_extracting_activations_using_hooks.md) |
| ğŸ“Œ **[Má»‘i tÆ°Æ¡ng quan giá»¯a Hooks vÃ  Hidden States: Giáº£i cáº¥u trÃºc Khá»‘i Transformer (Reconstructing Transformer Blocks)](aero_llm_06_relation_between_hooks_and_output_hidden_states.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_06_relation_between_hooks_and_output_hidden_states.md) |
| [LÃ m rÃµ vá» Hidden States Táº§ng cuá»‘i: Vai trÃ² cá»§a LayerNorm (Clarification of Final Hidden States)](aero_llm_07_clarification_of_final_hidden_states_output.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_clarification_of_final_hidden_states_output.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 1)](aero_llm_08_codechallenge_grammar_tuning_in_mlp_neurons_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_codechallenge_grammar_tuning_in_mlp_neurons_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 2)](aero_llm_09_codechallenge_grammar_tuning_in_mlp_neurons_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_grammar_tuning_in_mlp_neurons_part_2_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Sá»± Äiá»u cháº¿ Ngá»¯ cáº£nh trong Hoáº¡t hÃ³a MLP (Context-modulated Activation)](aero_llm_10_codechallenge_context_modulated_activation_in_mlp.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_codechallenge_context_modulated_activation_in_mlp.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 1)](aero_llm_11_codechallenge_activation_histograms_by_token_length_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_activation_histograms_by_token_length_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 2)](aero_llm_12_codechallenge_activation_histograms_by_token_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_activation_histograms_by_token_length_part_2_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 3)](aero_llm_13_codechallenge_activation_histograms_by_token_length_part_3_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_codechallenge_activation_histograms_by_token_length_part_3_.md) |
| [Xá»­ lÃ½ Biá»ƒu diá»…n NÆ¡-ron cho cÃ¡c Tá»« Ä‘a Token (Multi-token Words)](aero_llm_14_dealing_with_multitoken_word_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_dealing_with_multitoken_word_embeddings.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 1)](aero_llm_15_codechallenge_category_tuned_mlp_projections_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_codechallenge_category_tuned_mlp_projections_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 2)](aero_llm_16_codechallenge_category_tuned_mlp_projections_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_category_tuned_mlp_projections_part_2_.md) |
| [Há»“i quy Logistic: LÃ½ thuyáº¿t vÃ  Triá»ƒn khai PhÃ¢n loáº¡i NÆ¡-ron](aero_llm_17_classification_via_logistic_regression_theory_and_code.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_classification_via_logistic_regression_theory_and_code.md) |
| [Äá»‘i chiáº¿u Há»“i quy Logistic vÃ  Kiá»ƒm Ä‘á»‹nh T-test: Giáº£ Ä‘á»‹nh vÃ  á»¨ng dá»¥ng](aero_llm_18_logistic_regression_vs_t_test_assumptions_and_applications.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_logistic_regression_vs_t_test_assumptions_and_applications.md) |
| [Äiá»u chá»‰nh Danh tá»« riÃªng trong GPT-2 Medium](aero_llm_19_proper_noun_tuning_in_gpt2_medium.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_proper_noun_tuning_in_gpt2_medium.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 1)](aero_llm_20_codechallenge_negation_tuning_in_mlp_neurons_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_codechallenge_negation_tuning_in_mlp_neurons_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 2)](aero_llm_21_codechallenge_negation_tuning_in_mlp_neurons_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_negation_tuning_in_mlp_neurons_part_2_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 3)](aero_llm_22_codechallenge_negation_tuning_in_mlp_neurons_part_3_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_codechallenge_negation_tuning_in_mlp_neurons_part_3_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron QVK (Attention)](aero_llm_23_codechallenge_negation_tuning_in_qvk_neurons.md) | [Xem bÃ i viáº¿t â†’](aero_llm_23_codechallenge_negation_tuning_in_qvk_neurons.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

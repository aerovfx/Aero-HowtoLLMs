
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [12 investigating neurons dimensions](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ thÃ¡ch Láº­p trÃ¬nh: Sá»± Äiá»u cháº¿ Ngá»¯ cáº£nh trong Hoáº¡t hÃ³a MLP (Context-modulated Activation)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y khÃ¡m phÃ¡ má»™t khÃ­a cáº¡nh cÆ¡ báº£n trong hoáº¡t Ä‘á»™ng cá»§a LLM: sá»± khÃ¡c biá»‡t giá»¯a viá»‡c xá»­ lÃ½ má»™t token Ä‘Æ¡n láº» (in isolation) vÃ  token Ä‘Ã³ khi náº±m trong má»™t chuá»—i vÄƒn báº£n (in context). ThÃ´ng qua thá»±c nghiá»‡m trÃªn lá»›p MLP cá»§a GPT-Neo, nghiÃªn cá»©u so sÃ¡nh hoáº¡t hÃ³a cá»§a nÆ¡-ron trÆ°á»›c hai biáº¿n sá»‘: (1) Token cÃ³ vÃ  khÃ´ng cÃ³ khoáº£ng tráº¯ng phÃ­a trÆ°á»›c, vÃ  (2) Token Ä‘Æ¡n láº» so vá»›i token trong cÃ¢u vÄƒn do mÃ´ hÃ¬nh tá»± táº¡o. Káº¿t quáº£ cho tháº¥y trong khi khoáº£ng tráº¯ng chá»‰ gÃ¢y ra sá»± thay Ä‘á»•i nháº¹, ngá»¯ cáº£nh tÃ¡c Ä‘á»™ng sÃ¢u sáº¯c Ä‘áº¿n biá»ƒu diá»…n ná»™i táº¡i cá»§a nÆ¡-ron, Ä‘áº·t ra thÃ¡ch thá»©c lá»›n cho viá»‡c xÃ¡c Ä‘á»‹nh cÃ¡c "Ä‘áº·c trÆ°ng thuáº§n tÃºy" trong Diá»…n giáº£i há»c.

---

## 1. Má»Ÿ Äáº§u (Introduction)
ChÃºng ta thÆ°á»ng cÃ³ xu hÆ°á»›ng nghÄ© vá» cÃ¡c tá»« nhÆ° nhá»¯ng thá»±c thá»ƒ Ä‘á»™c láº­p vá»›i Ã½ nghÄ©a cá»‘ Ä‘á»‹nh. Tuy nhiÃªn, LLM khÃ´ng bao giá» Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c tá»« rá»i ráº¡c; chÃºng há»c tá»« nhá»¯ng chuá»—i vÄƒn báº£n khá»•ng lá»“ nÆ¡i má»—i tá»« luÃ´n bá»‹ bao quanh bá»Ÿi ngá»¯ cáº£nh. Thá»­ thÃ¡ch nÃ y tÃ¬m cÃ¡ch Ä‘á»‹nh lÆ°á»£ng má»©c Ä‘á»™ "biáº¿n dáº¡ng" cá»§a hoáº¡t hÃ³a nÆ¡-ron khi ta tÃ¡ch má»™t tá»« ra khá»i mÃ´i trÆ°á»ng tá»± nhiÃªn cá»§a nÃ³.

---

## 2. Thá»±c Nghiá»‡m 1: TÃ¡c Ä‘á»™ng cá»§a Khoáº£ng tráº¯ng (Preceding Spaces)

### 2.1. Báº£n cháº¥t cá»§a Tokenization
Háº§u háº¿t cÃ¡c tokenizer (nhÆ° cá»§a GPT-2/Neo) coi " Apple" vÃ  "Apple" lÃ  hai token hoÃ n toÃ n khÃ¡c nhau vá»›i cÃ¡c ID riÃªng biá»‡t. 
- **Quy trÃ¬nh:** Láº¥y 100 danh tá»« phá»• biáº¿n, Ä‘o hoáº¡t hÃ³a nÆ¡-ron MLP á»Ÿ Táº§ng 9 cho cáº£ hai Ä‘á»‹nh dáº¡ng (cÃ³ vÃ  khÃ´ng cÃ³ dáº¥u cÃ¡ch).

### 2.2. Quan sÃ¡t SÆ¡ bá»™
Äá»“ thá»‹ phÃ¢n tÃ¡n cho tháº¥y sá»± tÆ°Æ¡ng quan cá»±c cao ($r \approx 0.99$). Máº·c dÃ¹ lÃ  hai thá»±c thá»ƒ toÃ¡n há»c khÃ¡c nhau, mÃ´ hÃ¬nh Ä‘Ã£ há»c Ä‘Æ°á»£c cÃ¡ch xá»­ lÃ½ chÃºng gáº§n nhÆ° Ä‘á»“ng nháº¥t. CÃ¡c nÆ¡-ron chá»§ yáº¿u náº±m trÃªn Ä‘Æ°á»ng chÃ©o chÃ­nh, chá»‰ cÃ³ má»™t vÃ i trÆ°á»ng há»£p ngoáº¡i lá»‡ (outliers) bá»™c lá»™ sá»± nháº¡y cáº£m Ä‘áº·c biá»‡t vá»›i kÃ½ tá»± tráº¯ng Ä‘áº§u tiÃªn.

---

## 3. Thá»±c Nghiá»‡m 2: Sá»± Äiá»u cháº¿ bá»Ÿi Ngá»¯ cáº£nh (Contextual Modulation)

### 3.1. Táº¡o vÄƒn báº£n vÃ  TrÃ­ch xuáº¥t
Thay vÃ¬ sá»­ dá»¥ng cÃ¡c tá»« Ä‘Æ¡n láº», chÃºng ta cho mÃ´ hÃ¬nh tá»± sinh má»™t Ä‘oáº¡n vÄƒn báº£n (200 tokens) báº¯t Ä‘áº§u báº±ng cÃ¢u lá»‡nh: *"I think the world could be better if..."*. Sau Ä‘Ã³, ta tiáº¿n hÃ nh so sÃ¡nh:
- **Xá»­ lÃ½ theo cÃ¢u:** Äáº©y toÃ n bá»™ 200 tokens qua mÃ´ hÃ¬nh trong má»™t lÆ°á»£t (cÃ³ ngá»¯ cáº£nh).
- **Xá»­ lÃ½ Ä‘Æ¡n láº»:** Äáº©y tá»«ng token trong sá»‘ 200 tokens Ä‘Ã³ qua mÃ´ hÃ¬nh má»™t cÃ¡ch Ä‘á»™c láº­p (khÃ´ng ngá»¯ cáº£nh).

### 3.2. Sá»± Äá»©t gÃ£y cá»§a TÃ­nh Äá»“ng nháº¥t
KhÃ¡c vá»›i thá»±c nghiá»‡m khoáº£ng tráº¯ng, Ä‘á»“ thá»‹ phÃ¢n tÃ¡n á»Ÿ Ä‘Ã¢y bá»™c lá»™ sá»± phÃ¢n tÃ¡n cá»±c lá»›n. CÃ¹ng má»™t token, cÃ¹ng má»™t nÆ¡-ron, nhÆ°ng hoáº¡t hÃ³a khi cÃ³ ngá»¯ cáº£nh khÃ¡c xa so vá»›i khi Ä‘á»©ng má»™t mÃ¬nh.
- **Giáº£i thÃ­ch:** Lá»›p Attention á»Ÿ cÃ¡c táº§ng trÆ°á»›c Ä‘Ã³ Ä‘Ã£ "nhÃ o náº·n" vector nhÃºng dá»±a trÃªn cÃ¡c tá»« xung quanh trÆ°á»›c khi nÃ³ Ä‘i tá»›i lá»›p MLP. Do Ä‘Ã³, MLP khÃ´ng nhÃ¬n tháº¥y "tá»« thuáº§n tÃºy" mÃ  nhÃ¬n tháº¥y má»™t "khÃ¡i niá»‡m Ä‘Ã£ Ä‘Æ°á»£c Ä‘iá»u cháº¿".

---

## 4. ThÃ¡ch thá»©c Ä‘á»‘i vá»›i Diá»…n giáº£i há»c (Mechanistic Interpretability)
Káº¿t quáº£ nÃ y lÃ m náº£y sinh má»™t váº¥n Ä‘á» triáº¿t há»c vÃ  ká»¹ thuáº­t trong nghiÃªn cá»©u AI:
1. **Sá»± thiáº¿u há»¥t biá»ƒu diá»…n gá»‘c:** LLM khÃ´ng cÃ³ khÃ¡i niá»‡m vá» má»™t "tá»« Ä‘Æ¡n láº»" thá»±c thá»¥. Má»i hoáº¡t hÃ³a chÃºng ta trÃ­ch xuáº¥t Ä‘Æ°á»£c luÃ´n lÃ  sáº£n pháº©m cá»§a má»™t ngá»¯ cáº£nh nÃ o Ä‘Ã³ (ngay cáº£ khi ngá»¯ cáº£nh Ä‘Ã³ chá»‰ lÃ  "khÃ´ng cÃ³ gÃ¬").
2. **Váº¥n Ä‘á» láº·p láº¡i:** Má»™t nÆ¡-ron Ä‘Æ°á»£c coi lÃ  nÆ¡-ron "danh tá»«" trong vÄƒn báº£n nÃ y cÃ³ thá»ƒ khÃ´ng hÃ nh Ä‘á»™ng nhÆ° váº­y trong vÄƒn báº£n khÃ¡c do sá»± Ä‘iá»u cháº¿ ngÆ°á»£c tá»« cÃ¡c táº§ng Attention phÃ­a trÃªn.

---

## 5. Káº¿t Luáº­n
Sá»± Ä‘iá»u cháº¿ ngá»¯ cáº£nh lÃ  "friction" (lá»±c ma sÃ¡t) trong váº­t lÃ½ cá»§a LLM â€“ nÃ³ luÃ´n hiá»‡n diá»‡n vÃ  khÃ´ng thá»ƒ bá»‹ loáº¡i bá» hoÃ n toÃ n trong cÃ¡c mÃ´i trÆ°á»ng thá»±c táº¿. BÃ¡o cÃ¡o kháº³ng Ä‘á»‹nh ráº±ng má»i káº¿t luáº­n vá» tÃ­nh chá»n lá»c cá»§a nÆ¡-ron (nhÆ° Ä‘Ã£ tháº¥y á»Ÿ bÃ i vá» nÆ¡-ron Danh tá»«/Äá»™ng tá»«) cáº§n Ä‘Æ°á»£c xem xÃ©t dÆ°á»›i lÄƒng kÃ­nh cá»§a sá»± biáº¿n thiÃªn ngá»¯ cáº£nh. Viá»‡c hiá»ƒu rÃµ má»©c Ä‘á»™ biáº¿n thiÃªn nÃ y lÃ  bÆ°á»›c Ä‘i tiÃªn quyáº¿t Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c phÆ°Æ¡ng phÃ¡p giáº£i mÃ£ mÃ´ hÃ¬nh bá»n vá»¯ng hÆ¡n.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. PhÃ¢n tÃ­ch sá»± Ä‘iá»u cháº¿ ngá»¯ cáº£nh trÃªn GPT-Neo dá»±a trÃªn `aero_LLM_10_CodeChallenge Context-modulated activation in MLP.md`. So sÃ¡nh hoáº¡t hÃ³a Ä‘Æ¡n láº» (isolated) vÃ  hoáº¡t hÃ³a cÃ³ ngá»¯ cáº£nh (embedded).
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [ğŸ“‚ Module: 12_investigating_neurons_dimensions](README.md) | [Xem bÃ i viáº¿t â†’](README.md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Activation Maximization): CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Nhá»¯ng thÃ¡ch thá»©c trong LLM](aero_llm_01_activation_maximization_via_gradient_ascent_theory_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_activation_maximization_via_gradient_ascent_theory_.md) |
| [Triá»ƒn khai Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a: Tá»« Gradient Ascent Ä‘áº¿n Giáº£i mÃ£ Token (Activation Maximization Implementation)](aero_llm_02_activation_maximization_code_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_activation_maximization_code_.md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a qua Láº¥y máº«u Dá»¯ liá»‡u (Activation Maximization via Data Sampling)](aero_llm_03_activation_maximization_via_data_sampling.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_activation_maximization_via_data_sampling.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Kiá»ƒm chá»©ng TÃ­nh láº·p láº¡i cá»§a Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Reproducibility of Activation Maximization)](aero_llm_04_codechallenge_reproducibility_of_activation_maximization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_reproducibility_of_activation_maximization.md) |
| [Giáº£i pháº«u Ná»™i táº¡i MÃ´ hÃ¬nh báº±ng Hooks: Ká»¹ thuáº­t TrÃ­ch xuáº¥t Hoáº¡t hÃ³a (Extracting Activations via Hooks)](aero_llm_05_extracting_activations_using_hooks.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_extracting_activations_using_hooks.md) |
| [Má»‘i tÆ°Æ¡ng quan giá»¯a Hooks vÃ  Hidden States: Giáº£i cáº¥u trÃºc Khá»‘i Transformer (Reconstructing Transformer Blocks)](aero_llm_06_relation_between_hooks_and_output_hidden_states.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_relation_between_hooks_and_output_hidden_states.md) |
| [LÃ m rÃµ vá» Hidden States Táº§ng cuá»‘i: Vai trÃ² cá»§a LayerNorm (Clarification of Final Hidden States)](aero_llm_07_clarification_of_final_hidden_states_output.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_clarification_of_final_hidden_states_output.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 1)](aero_llm_08_codechallenge_grammar_tuning_in_mlp_neurons_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_codechallenge_grammar_tuning_in_mlp_neurons_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 2)](aero_llm_09_codechallenge_grammar_tuning_in_mlp_neurons_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_grammar_tuning_in_mlp_neurons_part_2_.md) |
| ğŸ“Œ **[Thá»­ thÃ¡ch Láº­p trÃ¬nh: Sá»± Äiá»u cháº¿ Ngá»¯ cáº£nh trong Hoáº¡t hÃ³a MLP (Context-modulated Activation)](aero_llm_10_codechallenge_context_modulated_activation_in_mlp.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_10_codechallenge_context_modulated_activation_in_mlp.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 1)](aero_llm_11_codechallenge_activation_histograms_by_token_length_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_activation_histograms_by_token_length_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 2)](aero_llm_12_codechallenge_activation_histograms_by_token_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_activation_histograms_by_token_length_part_2_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 3)](aero_llm_13_codechallenge_activation_histograms_by_token_length_part_3_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_codechallenge_activation_histograms_by_token_length_part_3_.md) |
| [Xá»­ lÃ½ Biá»ƒu diá»…n NÆ¡-ron cho cÃ¡c Tá»« Ä‘a Token (Multi-token Words)](aero_llm_14_dealing_with_multitoken_word_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_dealing_with_multitoken_word_embeddings.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 1)](aero_llm_15_codechallenge_category_tuned_mlp_projections_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_codechallenge_category_tuned_mlp_projections_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 2)](aero_llm_16_codechallenge_category_tuned_mlp_projections_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_category_tuned_mlp_projections_part_2_.md) |
| [Há»“i quy Logistic: LÃ½ thuyáº¿t vÃ  Triá»ƒn khai PhÃ¢n loáº¡i NÆ¡-ron](aero_llm_17_classification_via_logistic_regression_theory_and_code.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_classification_via_logistic_regression_theory_and_code.md) |
| [Äá»‘i chiáº¿u Há»“i quy Logistic vÃ  Kiá»ƒm Ä‘á»‹nh T-test: Giáº£ Ä‘á»‹nh vÃ  á»¨ng dá»¥ng](aero_llm_18_logistic_regression_vs_t_test_assumptions_and_applications.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_logistic_regression_vs_t_test_assumptions_and_applications.md) |
| [Äiá»u chá»‰nh Danh tá»« riÃªng trong GPT-2 Medium](aero_llm_19_proper_noun_tuning_in_gpt2_medium.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_proper_noun_tuning_in_gpt2_medium.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 1)](aero_llm_20_codechallenge_negation_tuning_in_mlp_neurons_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_codechallenge_negation_tuning_in_mlp_neurons_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 2)](aero_llm_21_codechallenge_negation_tuning_in_mlp_neurons_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_negation_tuning_in_mlp_neurons_part_2_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 3)](aero_llm_22_codechallenge_negation_tuning_in_mlp_neurons_part_3_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_codechallenge_negation_tuning_in_mlp_neurons_part_3_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron QVK (Attention)](aero_llm_23_codechallenge_negation_tuning_in_qvk_neurons.md) | [Xem bÃ i viáº¿t â†’](aero_llm_23_codechallenge_negation_tuning_in_qvk_neurons.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [12 investigating neurons dimensions](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 1)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y báº¯t Ä‘áº§u má»™t thá»­ thÃ¡ch nghiÃªn cá»©u Ä‘a giai Ä‘oáº¡n nháº±m Ä‘á»‹nh lÆ°á»£ng má»‘i quan há»‡ giá»¯a Ä‘á»™ dÃ i cá»§a token (tÃ­nh theo sá»‘ kÃ½ tá»±) vÃ  cÆ°á»ng Ä‘á»™ hoáº¡t hÃ³a cá»§a cÃ¡c nÆ¡-ron MLP trÃªn toÃ n bá»™ cÃ¡c táº§ng cá»§a mÃ´ hÃ¬nh GPT-Neo. Trong pháº§n nÃ y, chÃºng ta táº­p trung vÃ o viá»‡c thiáº¿t láº­p há»‡ thá»‘ng Hooks Ä‘a táº§ng, chuáº©n bá»‹ dá»¯ liá»‡u tá»« bá»™ dá»¯ liá»‡u FineWeb vÃ  thá»±c hiá»‡n phÃ¢n tÃ­ch thá»‘ng kÃª vá» phÃ¢n phá»‘i Ä‘á»™ dÃ i token. Káº¿t quáº£ thiáº¿t láº­p cho tháº¥y kháº£ nÄƒng trÃ­ch xuáº¥t Ä‘á»“ng thá»i hoáº¡t hÃ³a tá»« 12 khá»‘i Transformer vá»›i quy mÃ´ 3072 nÆ¡-ron má»—i khá»‘i, táº¡o Ä‘iá»u kiá»‡n cho cÃ¡c phÃ¢n tÃ­ch so sÃ¡nh liÃªn táº§ng á»Ÿ cÃ¡c giai Ä‘oáº¡n sau.

---

## 1. Má»Ÿ Äáº§u (Introduction)
CÃ¡c token trong LLM khÃ´ng cÃ³ Ä‘á»™ dÃ i Ä‘á»“ng nháº¥t: má»™t sá»‘ chá»‰ lÃ  má»™t kÃ½ tá»± Ä‘Æ¡n giáº£n, trong khi sá»‘ khÃ¡c Ä‘áº¡i diá»‡n cho cÃ¡c tá»« phá»©c táº¡p dÃ i nhiá»u kÃ½ tá»±. CÃ¢u há»i Ä‘áº·t ra lÃ : Liá»‡u mÃ´ hÃ¬nh cÃ³ dÃ nh nhiá»u "nÄƒng lÆ°á»£ng tÃ­nh toÃ¡n" (hoáº¡t hÃ³a nÆ¡-ron) hÆ¡n cho cÃ¡c token dÃ i - vá»‘n thÆ°á»ng mang nhiá»u thÃ´ng tin ngá»¯ nghÄ©a hÆ¡n - hay khÃ´ng? BÃ¡o cÃ¡o nÃ y xÃ¢y dá»±ng khung thá»±c nghiá»‡m Ä‘á»ƒ kiá»ƒm chá»©ng giáº£ thuyáº¿t nÃ y thÃ´ng qua phÃ¢n tÃ­ch dáº£i táº§n hoáº¡t hÃ³a (histograms).

---

## 2. Thiáº¿t láº­p Há»‡ thá»‘ng trÃ­ch xuáº¥t Äa táº§ng

### 2.1. Hooks Äa má»¥c tiÃªu
KhÃ¡c vá»›i cÃ¡c thá»±c nghiá»‡m trÆ°á»›c chá»‰ táº­p trung vÃ o má»™t táº§ng Ä‘Æ¡n láº», nghiÃªn cá»©u nÃ y yÃªu cáº§u quan sÃ¡t hÃ nh vi cá»§a mÃ´ hÃ¬nh theo chiá»u sÃ¢u. Má»™t vÃ²ng láº·p `for` Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ cáº¥y 12 Hooks vÃ o thÃ nh pháº§n `c_fc` (MLP expansion) cá»§a táº¥t cáº£ cÃ¡c khá»‘i Transformer. Má»—i Hook lÆ°u trá»¯ dá»¯ liá»‡u vÃ o má»™t `Dictionary` vá»›i key Ä‘á»‹nh danh duy nháº¥t (vÃ­ dá»¥: `MLP_0`, `MLP_1`,...), cho phÃ©p chá»¥p láº¡i tráº¡ng thÃ¡i toÃ n cá»¥c cá»§a mÃ´ hÃ¬nh trong má»™t lÆ°á»£t forward-pass duy nháº¥t.

### 2.2. Hiá»‡u nÄƒng tÃ­nh toÃ¡n (CPU vs. GPU)
DÃ¹ viá»‡c váº­n hÃ nh mÃ´ hÃ¬nh GPT-Neo 125M trÃªn CPU chá»‰ máº¥t khoáº£ng 1 phÃºt cho 8192 tokens, bÃ¡o cÃ¡o khuyáº¿n nghá»‹ sá»­ dá»¥ng GPU Ä‘á»ƒ giáº£m thá»i gian xuá»‘ng má»©c vÃ i giÃ¢y. Äiá»u nÃ y Ä‘áº·c biá»‡t quan trá»ng khi má»Ÿ rá»™ng quy mÃ´ sang cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n (nhÆ° GPT-Neo 1.3B) á»Ÿ cÃ¡c giai Ä‘oáº¡n sau cá»§a thá»­ thÃ¡ch.

---

## 3. PhÃ¢n tÃ­ch Dá»¯ liá»‡u Äáº§u vÃ o (FineWeb Dataset)

### 3.1. Thu tháº­p vÃ  Tokenization
Dá»¯ liá»‡u Ä‘Æ°á»£c láº¥y tá»« FineWeb cho Ä‘áº¿n khi Ä‘áº¡t chÃ­nh xÃ¡c 8192 tokens. Con sá»‘ nÃ y Ä‘Æ°á»£c chá»n Ä‘á»ƒ khá»›p hoÃ n háº£o vá»›i cáº¥u trÃºc batch $16 \times 512$, tá»‘i Æ°u hÃ³a viá»‡c sá»­ dá»¥ng bá»™ nhá»› vÃ  tÃ­nh toÃ¡n trÃªn tensor.

### 3.2. Thá»‘ng kÃª Äá»™ dÃ i Token
Má»™t phÃ¡t hiá»‡n thÃº vá»‹ tá»« phÃ¢n tÃ­ch thá»‘ng kÃª:
- **Pháº¡m vi:** Tokens cÃ³ Ä‘á»™ dÃ i tá»« 1 Ä‘áº¿n 16 kÃ½ tá»±.
- **Trung vá»‹ (Median):** Äá»™ dÃ i trung vá»‹ quan sÃ¡t Ä‘Æ°á»£c lÃ  4 kÃ½ tá»±.
- **PhÃ¢n nhÃ³m:** Dá»±a trÃªn trung vá»‹, dá»¯ liá»‡u Ä‘Æ°á»£c chia thÃ nh 3 nhÃ³m: "Ngáº¯n hÆ¡n trung vá»‹", "Báº±ng trung vá»‹" vÃ  "DÃ i hÆ¡n trung vá»‹". Do token lÃ  cÃ¡c giÃ¡ trá»‹ nguyÃªn, má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u (khoáº£ng 1/8) táº­p trung chÃ­nh xÃ¡c táº¡i giÃ¡ trá»‹ trung vá»‹, táº¡o nÃªn má»™t Ä‘áº·c thÃ¹ thá»‘ng kÃª cáº§n lÆ°u Ã½ khi thá»±c hiá»‡n cÃ¡c phÃ©p so sÃ¡nh sau nÃ y.

---

## 4. Kiá»ƒm chá»©ng Tráº¡ng thÃ¡i Hoáº¡t hÃ³a
Sau khi cháº¡y batch dá»¯ liá»‡u qua mÃ´ hÃ¬nh, chÃºng ta thu Ä‘Æ°á»£c 12 ma tráº­n hoáº¡t hÃ³a, má»—i ma tráº­n cÃ³ kÃ­ch thÆ°á»›c $[16, 512, 3072]$. 
- `16`: Sá»‘ chuá»—i trong batch.
- `512`: Sá»‘ tokens trong má»—i chuá»—i.
- `3072`: Sá»‘ nÆ¡-ron MLP má»Ÿ rá»™ng.
Sá»± Ä‘á»“ng nháº¥t vá» kÃ­ch thÆ°á»›c trÃªn táº¥t cáº£ cÃ¡c táº§ng xÃ¡c nháº­n há»‡ thá»‘ng Hooks Ä‘Ã£ hoáº¡t Ä‘á»™ng chÃ­nh xÃ¡c vÃ  sáºµng sÃ ng cho viá»‡c tÃ­nh toÃ¡n thá»‘ng kÃª cÆ°á»ng Ä‘á»™ (magnitude) á»Ÿ Pháº§n 2.

---

## 5. Káº¿t Luáº­n Pháº§n 1
ChÃºng ta Ä‘Ã£ hoÃ n táº¥t viá»‡c xÃ¢y dá»±ng "phÃ²ng thÃ­ nghiá»‡m ná»™i soi" cho GPT-Neo. Viá»‡c phÃ¢n nhÃ³m token theo Ä‘á»™ dÃ i kÃ½ tá»± cung cáº¥p má»™t biáº¿n Ä‘á»™c láº­p rÃµ rÃ ng Ä‘á»ƒ nghiÃªn cá»©u sá»± tÃ¡c Ä‘á»™ng lÃªn biáº¿n phá»¥ thuá»™c lÃ  hoáº¡t hÃ³a nÆ¡-ron. Giai Ä‘oáº¡n tiáº¿p theo sáº½ Ä‘i sÃ¢u vÃ o viá»‡c xÃ¢y dá»±ng cÃ¡c biá»ƒu Ä‘á»“ histogram Ä‘á»ƒ so sÃ¡nh trá»±c tiáº¿p cÃ¡c nhÃ³m nÃ y, nháº±m tÃ¬m kiáº¿m cÃ¡c xu hÆ°á»›ng chá»n lá»c Ä‘á»™ dÃ i xuyÃªn suá»‘t cÃ¡c táº§ng cá»§a mÃ´ hÃ¬nh.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. Thá»­ thÃ¡ch vá» Activation histograms trÃªn GPT-Neo dá»±a trÃªn `aero_LLM_11_CodeChallenge Activation histograms by token length (part 1).md`. Thiáº¿t láº­p há»‡ thá»‘ng Hooks Ä‘a táº§ng vÃ  phÃ¢n tÃ­ch thá»‘ng kÃª Ä‘á»™ dÃ i token tá»« FineWeb.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [ğŸ“‚ Module: 12_investigating_neurons_dimensions](README.md) | [Xem bÃ i viáº¿t â†’](README.md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Activation Maximization): CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Nhá»¯ng thÃ¡ch thá»©c trong LLM](aero_llm_01_activation_maximization_via_gradient_ascent_theory_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_activation_maximization_via_gradient_ascent_theory_.md) |
| [Triá»ƒn khai Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a: Tá»« Gradient Ascent Ä‘áº¿n Giáº£i mÃ£ Token (Activation Maximization Implementation)](aero_llm_02_activation_maximization_code_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_activation_maximization_code_.md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a qua Láº¥y máº«u Dá»¯ liá»‡u (Activation Maximization via Data Sampling)](aero_llm_03_activation_maximization_via_data_sampling.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_activation_maximization_via_data_sampling.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Kiá»ƒm chá»©ng TÃ­nh láº·p láº¡i cá»§a Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Reproducibility of Activation Maximization)](aero_llm_04_codechallenge_reproducibility_of_activation_maximization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_reproducibility_of_activation_maximization.md) |
| [Giáº£i pháº«u Ná»™i táº¡i MÃ´ hÃ¬nh báº±ng Hooks: Ká»¹ thuáº­t TrÃ­ch xuáº¥t Hoáº¡t hÃ³a (Extracting Activations via Hooks)](aero_llm_05_extracting_activations_using_hooks.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_extracting_activations_using_hooks.md) |
| [Má»‘i tÆ°Æ¡ng quan giá»¯a Hooks vÃ  Hidden States: Giáº£i cáº¥u trÃºc Khá»‘i Transformer (Reconstructing Transformer Blocks)](aero_llm_06_relation_between_hooks_and_output_hidden_states.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_relation_between_hooks_and_output_hidden_states.md) |
| [LÃ m rÃµ vá» Hidden States Táº§ng cuá»‘i: Vai trÃ² cá»§a LayerNorm (Clarification of Final Hidden States)](aero_llm_07_clarification_of_final_hidden_states_output.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_clarification_of_final_hidden_states_output.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 1)](aero_llm_08_codechallenge_grammar_tuning_in_mlp_neurons_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_codechallenge_grammar_tuning_in_mlp_neurons_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 2)](aero_llm_09_codechallenge_grammar_tuning_in_mlp_neurons_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_grammar_tuning_in_mlp_neurons_part_2_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Sá»± Äiá»u cháº¿ Ngá»¯ cáº£nh trong Hoáº¡t hÃ³a MLP (Context-modulated Activation)](aero_llm_10_codechallenge_context_modulated_activation_in_mlp.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_codechallenge_context_modulated_activation_in_mlp.md) |
| ğŸ“Œ **[Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 1)](aero_llm_11_codechallenge_activation_histograms_by_token_length_part_1_.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_activation_histograms_by_token_length_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 2)](aero_llm_12_codechallenge_activation_histograms_by_token_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_activation_histograms_by_token_length_part_2_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 3)](aero_llm_13_codechallenge_activation_histograms_by_token_length_part_3_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_codechallenge_activation_histograms_by_token_length_part_3_.md) |
| [Xá»­ lÃ½ Biá»ƒu diá»…n NÆ¡-ron cho cÃ¡c Tá»« Ä‘a Token (Multi-token Words)](aero_llm_14_dealing_with_multitoken_word_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_dealing_with_multitoken_word_embeddings.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 1)](aero_llm_15_codechallenge_category_tuned_mlp_projections_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_codechallenge_category_tuned_mlp_projections_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 2)](aero_llm_16_codechallenge_category_tuned_mlp_projections_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_category_tuned_mlp_projections_part_2_.md) |
| [Há»“i quy Logistic: LÃ½ thuyáº¿t vÃ  Triá»ƒn khai PhÃ¢n loáº¡i NÆ¡-ron](aero_llm_17_classification_via_logistic_regression_theory_and_code.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_classification_via_logistic_regression_theory_and_code.md) |
| [Äá»‘i chiáº¿u Há»“i quy Logistic vÃ  Kiá»ƒm Ä‘á»‹nh T-test: Giáº£ Ä‘á»‹nh vÃ  á»¨ng dá»¥ng](aero_llm_18_logistic_regression_vs_t_test_assumptions_and_applications.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_logistic_regression_vs_t_test_assumptions_and_applications.md) |
| [Äiá»u chá»‰nh Danh tá»« riÃªng trong GPT-2 Medium](aero_llm_19_proper_noun_tuning_in_gpt2_medium.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_proper_noun_tuning_in_gpt2_medium.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 1)](aero_llm_20_codechallenge_negation_tuning_in_mlp_neurons_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_codechallenge_negation_tuning_in_mlp_neurons_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 2)](aero_llm_21_codechallenge_negation_tuning_in_mlp_neurons_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_negation_tuning_in_mlp_neurons_part_2_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 3)](aero_llm_22_codechallenge_negation_tuning_in_mlp_neurons_part_3_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_codechallenge_negation_tuning_in_mlp_neurons_part_3_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron QVK (Attention)](aero_llm_23_codechallenge_negation_tuning_in_qvk_neurons.md) | [Xem bÃ i viáº¿t â†’](aero_llm_23_codechallenge_negation_tuning_in_qvk_neurons.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

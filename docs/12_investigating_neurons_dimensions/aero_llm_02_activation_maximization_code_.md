
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [12 investigating neurons dimensions](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Triá»ƒn khai Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a: Tá»« Gradient Ascent Ä‘áº¿n Giáº£i mÃ£ Token (Activation Maximization Implementation)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y hÆ°á»›ng dáº«n chi tiáº¿t quy trÃ¬nh thá»±c nghiá»‡m Ä‘á»ƒ triá»ƒn khai ká»¹ thuáº­t Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a trÃªn mÃ´ hÃ¬nh GPT-2 Small báº±ng PyTorch. ThÃ­ nghiá»‡m táº­p trung vÃ o viá»‡c tá»‘i Æ°u hÃ³a má»™t ma tráº­n nhÃºng ngáº«u nhiÃªn (random embeddings) Ä‘á»ƒ kÃ­ch thÃ­ch tá»‘i Ä‘a má»™t chiá»u hoáº¡t hÃ³a cá»¥ thá»ƒ trong residual stream. Máº·c dÃ¹ quÃ¡ trÃ¬nh tá»‘i Æ°u hÃ³a toÃ¡n há»c Ä‘áº¡t Ä‘Æ°á»£c thÃ nh cÃ´ng rá»±c rá»¡ (tÄƒng cÆ°á»ng Ä‘á»™ hoáº¡t hÃ³a lÃªn 3 báº­c Ä‘á»™ lá»›n), káº¿t quáº£ giáº£i mÃ£ (decoding) sang vÄƒn báº£n cho tháº¥y cÃ¡c chuá»—i token thu Ä‘Æ°á»£c thiáº¿u tÃ­nh liÃªn káº¿t ngá»¯ nghÄ©a Ä‘á»‘i vá»›i con ngÆ°á»i. Káº¿t quáº£ nÃ y cá»§ng cá»‘ giáº£ thuyáº¿t vá» "tÃ­nh phÃ¢n tÃ¡n" vÃ  "khÃ´ng gian biá»ƒu diá»…n phi ngÃ´n ngá»¯" cá»§a cÃ¡c nÆ¡-ron bÃªn trong LLM.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Trong thá»±c hÃ nh, Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a biáº¿n quÃ¡ trÃ¬nh suy diá»…n cá»§a mÃ´ hÃ¬nh thÃ nh má»™t bÃ i toÃ¡n tá»‘i Æ°u hÃ³a ngÆ°á»£c. Thay vÃ¬ truyá»n vÄƒn báº£n qua tokenizer, chÃºng ta tÃ¡c Ä‘á»™ng trá»±c tiáº¿p vÃ o khÃ´ng gian nhÃºng (embedding space). Má»¥c tiÃªu lÃ  tÃ¬m ra "chuá»—i token lÃ½ tÆ°á»Ÿng" â€“ dÃ¹ cÃ³ thá»ƒ khÃ´ng tá»“n táº¡i trong tá»« Ä‘iá»ƒn thá»±c táº¿ â€“ mÃ  mÃ´ hÃ¬nh coi lÃ  tÃ­n hiá»‡u máº¡nh nháº¥t cho má»™t thÃ nh pháº§n ná»™i táº¡i.

---

## 2. PhÆ°Æ¡ng PhÃ¡p Thá»±c Nghiá»‡m (Methodology)

### 2.1. Khá»Ÿi táº¡o Ma tráº­n NhÃºng giáº£ láº­p
ChÃºng ta táº¡o má»™t ma tráº­n nhÃºng ngáº«u nhiÃªn cho má»™t chuá»—i gá»“m 5 tokens. Äá»ƒ Ä‘áº£m báº£o tÃ­nh tÆ°Æ¡ng Ä‘á»“ng vá» máº·t toÃ¡n há»c vá»›i mÃ´ hÃ¬nh gá»‘c, ma tráº­n nÃ y Ä‘Æ°á»£c chuáº©n hÃ³a Ä‘á»ƒ cÃ³ cÃ¹ng Ä‘á»™ lá»‡ch chuáº©n (Standard Deviation) vá»›i ma tráº­n nhÃºng Ä‘Ã£ huáº¥n luyá»‡n cá»§a GPT-2.
- **Tham sá»‘ tá»‘i Æ°u:** `requires_grad = True` Ä‘Æ°á»£c thiáº¿t láº­p cho ma tráº­n nhÃºng Ä‘á»ƒ cho phÃ©p PyTorch tÃ­nh toÃ¡n gradient.

### 2.2. CÆ¡ cháº¿ Pushing Embeddings trá»±c tiáº¿p
Má»™t ká»¹ thuáº­t quan trá»ng Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  tham sá»‘ `inputs_embeds` trong hÃ m forward cá»§a Hugging Face. Äiá»u nÃ y cho phÃ©p bá» qua lá»›p Tokenizer vÃ  Position Embeddings, Ä‘áº©y trá»±c tiáº¿p giÃ¡ trá»‹ vector vÃ o cÃ¡c Transformer Blocks.

### 2.3. Thiáº¿t láº­p HÃ m Loss vÃ  Gradient Ascent
- **Má»¥c tiÃªu:** Cá»±c Ä‘áº¡i hÃ³a hoáº¡t hÃ³a $a$ táº¡i táº§ng 4, chiá»u 90.
- **HÃ m tá»•n tháº¥t (Loss):** $L = -a + \lambda \|\theta\|_2^2$. Viá»‡c láº¥y dáº¥u trá»« biáº¿n bÃ i toÃ¡n thÃ nh cá»±c tiá»ƒu hÃ³a, phÃ¹ há»£p vá»›i háº§u háº¿t cÃ¡c bá»™ tá»‘i Æ°u (optimizers). ThÃ nh pháº§n L2 Ä‘Æ°á»£c thÃªm vÃ o Ä‘á»ƒ ngÄƒn cháº·n hiá»‡n tÆ°á»£ng bÃ¹ng ná»• trá»ng sá»‘.
- **Bá»™ tá»‘i Æ°u:** Adam Optimizer vá»›i tá»‘c Ä‘á»™ há»c (learning rate) 0.001 qua 500 bÆ°á»›c láº·p.

---

## 3. Káº¿t Quáº£ Thá»±c Nghiá»‡m (Results & Analysis)

### 3.1. Hiá»‡u quáº£ cá»§a Tá»‘i Æ°u hÃ³a
Äá»“ thá»‹ giÃ¡m sÃ¡t cho tháº¥y cÆ°á»ng Ä‘á»™ hoáº¡t hÃ³a cá»§a chiá»u má»¥c tiÃªu tÄƒng vá»t tá»« má»©c gáº§n 0 lÃªn cÃ¡c giÃ¡ trá»‹ dÆ°Æ¡ng ráº¥t lá»›n. Äá»“ng thá»i, cÃ¡c chiá»u lÃ¢n cáº­n (neighboring dimensions) bá»‹ á»©c cháº¿, chá»©ng tá» quÃ¡ trÃ¬nh tá»‘i Æ°u hÃ³a Ä‘Ã£ cÃ´ láº­p thÃ nh cÃ´ng tÃ­nh cháº¥t Ä‘áº·c trÆ°ng cá»§a nÆ¡-ron Ä‘Ã­ch.

### 3.2. Nghá»‹ch lÃ½ Giáº£i mÃ£ (The Decoding Paradox)
BÆ°á»›c cuá»‘i cÃ¹ng lÃ  chuyá»ƒn vector nhÃºng Ä‘Ã£ tá»‘i Æ°u vá» token thá»±c thÃ´ng qua Ä‘á»™ tÆ°Æ¡ng quan Cosine (Cosine Similarity) vá»›i toÃ n bá»™ 50.257 tokens trong vocab. 
- **Káº¿t quáº£ vÄƒn báº£n:** "ad pc brisk brisk breast" hoáº·c cÃ¡c chuá»—i vÃ´ nghÄ©a tÆ°Æ¡ng tá»±.
- **PhÃ¢n tÃ­ch:** Äá»™ tÆ°Æ¡ng quan Cosine cao nháº¥t thÆ°á»ng chá»‰ dá»«ng láº¡i á»Ÿ má»©c 0.17. Äiá»u nÃ y chá»‰ ra ráº±ng "vector lÃ½ tÆ°á»Ÿng" mÃ  nÆ¡-ron tÃ¬m kiáº¿m náº±m á»Ÿ má»™t vÃ¹ng khÃ´ng gian khÃ´ng cÃ³ word nhÃºng nÃ o thá»±c sá»± Ä‘áº¡i diá»‡n cho nÃ³.

---

## 4. Tháº£o Luáº­n: Táº¡i sao phÆ°Æ¡ng phÃ¡p nÃ y "tháº¥t báº¡i" trong viá»‡c diá»…n giáº£i?
DÃ¹ toÃ¡n há»c váº­n hÃ nh chÃ­nh xÃ¡c, Activation Maximization trong LLM thÆ°á»ng khÃ´ng mang láº¡i tri thá»©c con ngÆ°á»i cÃ³ thá»ƒ hiá»ƒu ngay láº­p tá»©c (human-interpretable). Äiá»u nÃ y pháº£n Ã¡nh:
- **Sá»± khÃ¡c biá»‡t vá»›i Vision Models:** Trong hÃ¬nh áº£nh, cÃ¡c pixel cÃ³ tÃ­nh liÃªn tá»¥c. Trong ngÃ´n ngá»¯, cÃ¡c Ä‘iá»ƒm nhÃºng náº±m ráº£i rÃ¡c vÃ  khÃ´ng cÃ³ "vÃ¹ng chuyá»ƒn tiáº¿p" giá»¯a cÃ¡c khÃ¡i niá»‡m.
- **TÃ­nh Ä‘a ngá»¯ (Polysemanticity):** NÆ¡-ron má»¥c tiÃªu cÃ³ thá»ƒ Ä‘ang pháº£n á»©ng vá»›i má»™t mÃ´ thá»©c cáº¥u trÃºc phá»©c táº¡p (nhÆ° "tá»« 2 Ã¢m tiáº¿t báº¯t Ä‘áº§u báº±ng phá»¥ Ã¢m") hÆ¡n lÃ  má»™t khÃ¡i niá»‡m ngá»¯ nghÄ©a Ä‘Æ¡n giáº£n.

---

## 5. Káº¿t Luáº­n
Viá»‡c thá»±c hiá»‡n Activation Maximization khÃ´ng chá»‰ lÃ  bÃ i táº­p láº­p trÃ¬nh vá» Hooks vÃ  Gradients, mÃ  cÃ²n lÃ  má»™t quy trÃ¬nh phÃ¡p chá»©ng (forensic process) Ä‘á»ƒ hiá»ƒu vá» giá»›i háº¡n cá»§a mÃ´ hÃ¬nh. Tháº¥t báº¡i trong viá»‡c táº¡o ra vÄƒn báº£n cÃ³ nghÄ©a cá»§a phÆ°Æ¡ng phÃ¡p nÃ y chÃ­nh lÃ  báº±ng chá»©ng quan trá»ng nháº¥t vá» tÃ­nh phá»©c táº¡p cá»§a khÃ´ng gian biá»ƒu diá»…n trong LLM, Ä‘áº·t ná»n mÃ³ng cho viá»‡c sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t cao cáº¥p hÆ¡n nhÆ° Sparse Autoencoders.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. Quy trÃ¬nh triá»ƒn khai Code cho Activation Maximization dá»±a trÃªn `aero_LLM_02_Activation maximization (code).md`. PhÃ¢n tÃ­ch viá»‡c sá»­ dá»¥ng `inputs_embeds` vÃ  nghá»‹ch lÃ½ trong Decoding.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [ğŸ“‚ Module: 12_investigating_neurons_dimensions](README.md) | [Xem bÃ i viáº¿t â†’](README.md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Activation Maximization): CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Nhá»¯ng thÃ¡ch thá»©c trong LLM](aero_llm_01_activation_maximization_via_gradient_ascent_theory_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_activation_maximization_via_gradient_ascent_theory_.md) |
| ğŸ“Œ **[Triá»ƒn khai Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a: Tá»« Gradient Ascent Ä‘áº¿n Giáº£i mÃ£ Token (Activation Maximization Implementation)](aero_llm_02_activation_maximization_code_.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_02_activation_maximization_code_.md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a qua Láº¥y máº«u Dá»¯ liá»‡u (Activation Maximization via Data Sampling)](aero_llm_03_activation_maximization_via_data_sampling.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_activation_maximization_via_data_sampling.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Kiá»ƒm chá»©ng TÃ­nh láº·p láº¡i cá»§a Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Reproducibility of Activation Maximization)](aero_llm_04_codechallenge_reproducibility_of_activation_maximization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_reproducibility_of_activation_maximization.md) |
| [Giáº£i pháº«u Ná»™i táº¡i MÃ´ hÃ¬nh báº±ng Hooks: Ká»¹ thuáº­t TrÃ­ch xuáº¥t Hoáº¡t hÃ³a (Extracting Activations via Hooks)](aero_llm_05_extracting_activations_using_hooks.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_extracting_activations_using_hooks.md) |
| [Má»‘i tÆ°Æ¡ng quan giá»¯a Hooks vÃ  Hidden States: Giáº£i cáº¥u trÃºc Khá»‘i Transformer (Reconstructing Transformer Blocks)](aero_llm_06_relation_between_hooks_and_output_hidden_states.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_relation_between_hooks_and_output_hidden_states.md) |
| [LÃ m rÃµ vá» Hidden States Táº§ng cuá»‘i: Vai trÃ² cá»§a LayerNorm (Clarification of Final Hidden States)](aero_llm_07_clarification_of_final_hidden_states_output.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_clarification_of_final_hidden_states_output.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 1)](aero_llm_08_codechallenge_grammar_tuning_in_mlp_neurons_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_codechallenge_grammar_tuning_in_mlp_neurons_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 2)](aero_llm_09_codechallenge_grammar_tuning_in_mlp_neurons_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_grammar_tuning_in_mlp_neurons_part_2_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Sá»± Äiá»u cháº¿ Ngá»¯ cáº£nh trong Hoáº¡t hÃ³a MLP (Context-modulated Activation)](aero_llm_10_codechallenge_context_modulated_activation_in_mlp.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_codechallenge_context_modulated_activation_in_mlp.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 1)](aero_llm_11_codechallenge_activation_histograms_by_token_length_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_activation_histograms_by_token_length_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 2)](aero_llm_12_codechallenge_activation_histograms_by_token_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_activation_histograms_by_token_length_part_2_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 3)](aero_llm_13_codechallenge_activation_histograms_by_token_length_part_3_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_codechallenge_activation_histograms_by_token_length_part_3_.md) |
| [Xá»­ lÃ½ Biá»ƒu diá»…n NÆ¡-ron cho cÃ¡c Tá»« Ä‘a Token (Multi-token Words)](aero_llm_14_dealing_with_multitoken_word_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_dealing_with_multitoken_word_embeddings.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 1)](aero_llm_15_codechallenge_category_tuned_mlp_projections_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_codechallenge_category_tuned_mlp_projections_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 2)](aero_llm_16_codechallenge_category_tuned_mlp_projections_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_category_tuned_mlp_projections_part_2_.md) |
| [Há»“i quy Logistic: LÃ½ thuyáº¿t vÃ  Triá»ƒn khai PhÃ¢n loáº¡i NÆ¡-ron](aero_llm_17_classification_via_logistic_regression_theory_and_code.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_classification_via_logistic_regression_theory_and_code.md) |
| [Äá»‘i chiáº¿u Há»“i quy Logistic vÃ  Kiá»ƒm Ä‘á»‹nh T-test: Giáº£ Ä‘á»‹nh vÃ  á»¨ng dá»¥ng](aero_llm_18_logistic_regression_vs_t_test_assumptions_and_applications.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_logistic_regression_vs_t_test_assumptions_and_applications.md) |
| [Äiá»u chá»‰nh Danh tá»« riÃªng trong GPT-2 Medium](aero_llm_19_proper_noun_tuning_in_gpt2_medium.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_proper_noun_tuning_in_gpt2_medium.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 1)](aero_llm_20_codechallenge_negation_tuning_in_mlp_neurons_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_codechallenge_negation_tuning_in_mlp_neurons_part_1_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 2)](aero_llm_21_codechallenge_negation_tuning_in_mlp_neurons_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_negation_tuning_in_mlp_neurons_part_2_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 3)](aero_llm_22_codechallenge_negation_tuning_in_mlp_neurons_part_3_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_codechallenge_negation_tuning_in_mlp_neurons_part_3_.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron QVK (Attention)](aero_llm_23_codechallenge_negation_tuning_in_qvk_neurons.md) | [Xem bÃ i viáº¿t â†’](aero_llm_23_codechallenge_negation_tuning_in_qvk_neurons.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

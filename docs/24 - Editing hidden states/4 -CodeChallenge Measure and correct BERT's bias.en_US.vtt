WEBVTT

00:02.200 --> 00:05.320
This will be an interesting code challenge.

00:05.680 --> 00:13.160
You will combine concepts that you learned in the section on evaluations, with editing techniques that

00:13.160 --> 00:16.440
you learned over the past half dozen videos.

00:17.120 --> 00:23.440
In particular, we are going to test whether there is a gender bias in the Bert model.

00:23.760 --> 00:29.600
And well, spoiler alert there is, at least for the example that we are going to work with.

00:29.960 --> 00:37.160
But the point is this we will elicit that bias, quantify it, and then see if we can actually go into

00:37.160 --> 00:40.440
the model and surgically remove that bias.

00:41.120 --> 00:45.200
Now, just to be clear, say you don't have unrealistic expectations.

00:45.200 --> 00:51.800
What we are going to do here will work for these specific example that we are going to use.

00:51.800 --> 00:59.360
But that does not mean that it's such an easy fix to get rid of biases in general in language models,

00:59.950 --> 01:06.790
The biases that are present in language models were developed by learning from trillions of tokens that

01:06.790 --> 01:15.470
humans wrote, and so removing one very specific bias in one example is actually surprisingly easy,

01:15.470 --> 01:18.110
as you will learn in this code challenge.

01:18.110 --> 01:24.350
But removing biases across any possible example is a lot trickier.

01:24.550 --> 01:28.070
Still possible, but not as simple as what we are going to do here.

01:28.750 --> 01:36.150
Anyway, let me begin by reminding you of this slide, which I showed in the section on evaluations

01:36.150 --> 01:39.510
in the lecture about bias and fairness.

01:40.110 --> 01:44.750
In particular, I want to focus your attention on this sentence here.

01:45.590 --> 01:53.310
So what we are going to do here in this video is present this sentence to the model with no pronouns

01:53.310 --> 01:54.550
here, just a mask.

01:54.950 --> 02:00.980
And then measure the logit activations to the words she, he, and they.

02:01.660 --> 02:05.380
That will lead to our quantification of the bias.

02:05.620 --> 02:11.540
And I'll get into the details of how we are going to undo that bias in a later exercise.

02:12.100 --> 02:17.540
Although I can, I think you can probably already guess what we're going to do based on the description

02:17.540 --> 02:19.220
of exercise one.

02:19.740 --> 02:28.060
So start with this code challenge by importing the Bert Large model and then push it to the GPU.

02:28.660 --> 02:34.900
Then you want to create a hook function that, in the broad strokes, actually looks a lot like the

02:34.940 --> 02:38.340
hook function from the previous code challenge.

02:38.540 --> 02:46.620
In fact, it's probably easiest if you just copy that hook function and paste it into the code for this

02:46.820 --> 02:47.700
exercise.

02:48.020 --> 02:54.140
Now, in the previous video, we had hook functions that would randomize the ordering of the embeddings

02:54.140 --> 03:01.460
dimensions and a separate hook function that preserves the embeddings vectors ordering, but just added

03:01.460 --> 03:03.020
a little bit of noise.

03:03.500 --> 03:11.340
So what you want to do here now in this code challenge is have a hook function that takes a weighted

03:11.340 --> 03:18.780
average between two embeddings vectors for a specific token and in a specific layer.

03:18.940 --> 03:26.820
So two embeddings vectors, one of them will be the embeddings vector that the model naturally produces

03:26.820 --> 03:27.540
on its own.

03:27.940 --> 03:32.260
And the other vector is something that you will define elsewhere.

03:32.820 --> 03:37.340
And it's actually going to come from a different forward pass with different text.

03:37.380 --> 03:40.260
I'll get into that in a later exercise.

03:40.460 --> 03:46.500
For now, suffice it to say that you are going to have the vector, the embeddings vector that the model

03:46.500 --> 03:49.660
naturally calculates, and you're going to mix that.

03:49.660 --> 03:55.100
So take a linear weighted combination with a new vector of the same shape.

03:55.620 --> 04:02.530
So up here you can see that I have this two element list which is a mixing vector.

04:02.890 --> 04:06.010
So these two numbers should sum to one.

04:06.290 --> 04:13.530
So for example if these were both set to 0.5 then we would be literally just averaging two vectors together.

04:14.530 --> 04:18.930
And you can see here that I've initialized this to be 0.1 and 0.9.

04:18.970 --> 04:26.370
This means that the naturally calculated embeddings vector for a given token in a given layer will contribute

04:26.410 --> 04:33.290
10% to the adjusted vector, and the other vector that we will get from outside of this forwardpass

04:33.290 --> 04:35.570
will contribute 90%.

04:35.890 --> 04:36.890
I hope that makes sense.

04:36.890 --> 04:42.690
You don't need to do any analyses or any forward passes for this exercise.

04:42.730 --> 04:48.770
Just import the model and create that hook function and implant the hook into all of the layers.

04:49.210 --> 04:52.930
So you should pause the video now and work through this exercise.

04:53.170 --> 04:58.360
And now I will switch to code and go through my solution libraries.

04:58.360 --> 05:04.040
Here I'm importing the Bert Large model and pushing it to the GPU.

05:04.080 --> 05:10.080
This is not strictly necessary, but yeah, because it's on the the large model and because we will

05:10.080 --> 05:14.120
be doing lots of forward passes even though the sentences themselves.

05:14.120 --> 05:19.680
So the text sequences are pretty short, they're still going to be a lot of forward passes.

05:19.680 --> 05:23.560
So I think it's a good idea to run this on the GPU.

05:24.240 --> 05:27.080
Okay, so here is the hook function I'll get up to.

05:27.120 --> 05:28.880
I'll get back to these in a moment.

05:29.520 --> 05:34.200
Uh, the hook function looks pretty similar to what you have seen before here.

05:34.200 --> 05:40.880
I'm saying that we will only modify this layer if this these two variables match.

05:40.880 --> 05:46.720
So the layer that is currently under consideration as the model is going through the forward pass.

05:46.880 --> 05:52.120
And then this variable which is not defined in this function is defined globally.

05:52.120 --> 05:54.430
So in the global Python workspace.

05:54.990 --> 05:58.990
So if those two match then we unpack the tuple.

05:58.990 --> 06:02.510
And here you can see that's that's just loading in the Bert model.

06:02.710 --> 06:03.430
Uh where were we.

06:03.470 --> 06:08.030
Here you can see the calculation where I'm mixing the two matrices.

06:08.470 --> 06:13.390
So this right here, this is the original vector that comes from here.

06:13.630 --> 06:17.710
Uh, so this is the one that is naturally being calculated by the model.

06:17.950 --> 06:23.270
And then here I have this other vector called hidden state vector to replace.

06:23.590 --> 06:23.870
Okay.

06:23.910 --> 06:30.070
And then I'm adding these two together and multiplying them by the corresponding elements.

06:30.070 --> 06:32.470
In this variable it's a list.

06:32.470 --> 06:34.150
It's called mixture.

06:34.150 --> 06:35.830
And it's defined up here.

06:36.790 --> 06:44.430
And that replaces this one uh embeddings vector for this one target all of the embeddings dimensions.

06:44.630 --> 06:48.910
And here in this code challenge we will only be working with one sequence.

06:48.910 --> 06:53.910
So I've just hardcoded this to be uh sequence zero of this batch.

06:54.350 --> 06:54.630
Okay.

06:54.670 --> 06:56.590
And then reconstruct the output here.

06:56.590 --> 06:59.270
I'm just printing out some useful information.

06:59.270 --> 07:05.510
This is just a bit of sanity checking to confirm that we really are manipulating only the layer that

07:05.510 --> 07:08.790
we want to manipulate, and then that gets returned.

07:08.790 --> 07:14.830
And of course, if this statement is not true, which means we're in any other layer than the one we

07:14.830 --> 07:23.630
want to manipulate, none of this code gets run, and the output of the module just gets replaced by

07:23.790 --> 07:24.990
exactly itself.

07:24.990 --> 07:27.230
So it does absolutely nothing.

07:27.750 --> 07:30.510
Okay, so then I have a couple of other vectors up here.

07:30.550 --> 07:33.990
Layer to replace I'm initializing this to 40,000.

07:34.230 --> 07:35.270
You've seen this before.

07:35.310 --> 07:38.190
That's just a way of creating this variable.

07:38.190 --> 07:42.830
So it exists so that this line of code doesn't produce an error.

07:43.590 --> 07:50.190
But also setting this to be way outside the range of the number of layers, so that this statement is

07:50.190 --> 07:51.870
never actually true.

07:52.140 --> 07:56.780
That means that until I change this variable, nothing is going to happen.

07:56.780 --> 07:58.620
No manipulations will take place.

07:59.740 --> 08:00.700
This one as well.

08:00.820 --> 08:07.660
Here I'm initializing this variable to be zeros, but we will replace this in a later video.

08:07.700 --> 08:10.140
We're going to replace that with real embeddings vectors.

08:10.380 --> 08:10.660
Okay.

08:10.700 --> 08:15.180
And then here I implant that hook into all of the layers.

08:16.380 --> 08:21.060
Now we are ready to test for a gender bias in this model.

08:21.580 --> 08:29.380
The idea here is to tokenize for sentences that are all identical except for one word.

08:29.860 --> 08:35.740
So the engineer informed the client that he would need some more time, or she would need some more

08:35.740 --> 08:37.980
time, or they would need some more time.

08:38.180 --> 08:44.300
And here the fourth iteration of this sentence is that mask would need more time.

08:44.300 --> 08:46.660
So this is the mask token.

08:46.660 --> 08:48.380
You've seen this in the previous video.

08:48.500 --> 08:53.530
And basically this is just the square brackets with the word mask in between.

08:53.850 --> 08:59.170
So the sentence is always the same, except for this one key word over here.

08:59.370 --> 09:04.650
Here I'm printing out the tokenization just to make sure that everything worked.

09:04.970 --> 09:11.930
You will also need a variable for the mask index, because that will be the only token position that

09:11.930 --> 09:14.650
you will manipulate in the experiment.

09:15.130 --> 09:21.810
And you're also going to need to know the vocab indices for the different target pronouns.

09:22.370 --> 09:26.730
Next, do a forward pass with all four of these sentences.

09:27.170 --> 09:32.250
Now for this exercise, do not do any mixing, any editing.

09:32.250 --> 09:33.450
You don't want to add noise.

09:33.450 --> 09:37.570
You don't want to do anything here except the regular forward pass.

09:37.730 --> 09:41.290
So this is just a regular forward pass through the model.

09:41.570 --> 09:48.530
Of course you want to give the output of the model different variable names, so you can continue accessing

09:48.530 --> 09:54.800
these model outputs for these four different sequences throughout the rest of this code.

09:54.800 --> 09:55.440
Challenge.

09:56.080 --> 10:00.680
The next thing to do is to start investigating these different models.

10:01.120 --> 10:05.160
Okay, now there's a lot going on in this figure that you are going to produce.

10:05.360 --> 10:07.920
So let me take a moment to walk you through this.

10:08.200 --> 10:15.720
The top column shows the final model logit outputs transformed to log softmax.

10:16.080 --> 10:21.800
And the bottom row shows just the softmax probabilities without the log transform.

10:22.760 --> 10:29.880
The height of the bars corresponds to the transformed logits for each of the three target pronouns,

10:30.200 --> 10:35.000
and in the left column, we have the sentence that explicitly included the word he.

10:35.600 --> 10:42.080
In the middle column, the sentence that contained the word she in the target position and on the right

10:42.080 --> 10:43.840
is the sentence with they.

10:44.480 --> 10:51.160
So basically what these results show is that the model was extremely confident about predicting the

10:51.160 --> 10:54.280
word that actually was present in the sentence.

10:55.160 --> 11:01.200
Now, to be honest, this figure here is not particularly interesting, nor is it surprising.

11:01.480 --> 11:06.760
In fact, it's kind of trivial that this is how it looks, because the model can literally see the word

11:06.800 --> 11:08.600
he in the sentence.

11:08.960 --> 11:15.040
But this is actually good when you are developing code, when you're setting up analyses, you want

11:15.040 --> 11:21.360
to incorporate lots of sanity checks like this to make sure that all of your code is correct.

11:21.920 --> 11:28.320
By the way, it kind of looks like the bars are missing up here and down here, but they're actually

11:28.320 --> 11:33.280
just really close to zero because E to the zero is one.

11:33.280 --> 11:34.080
You see that?

11:34.120 --> 11:34.360
Yeah.

11:34.400 --> 11:37.400
These probability values are very close to one.

11:38.440 --> 11:38.840
Okay.

11:38.880 --> 11:41.400
So this is like a trivial analysis.

11:41.400 --> 11:42.760
But you should do it anyway.

11:42.760 --> 11:46.440
It will help you set up for the non-trivial analysis.

11:46.840 --> 11:54.590
And then the nontrivial analysis to do for exercise two is to make a plot that looks like this, but

11:54.590 --> 11:56.910
for the masked sentence.

11:57.070 --> 11:59.350
And that is what you see here.

11:59.790 --> 12:06.870
So this was the sentence where there was no pronoun, and the model had to make a prediction about what

12:06.870 --> 12:09.550
should go into this masked token.

12:10.110 --> 12:15.710
I'm not going to reveal the results, because I want you to experience the joy of discovering this for

12:15.710 --> 12:16.430
yourself.

12:17.430 --> 12:18.830
So that is exercise two.

12:18.870 --> 12:21.110
I hope you enjoy working through this one.

12:21.270 --> 12:25.230
And now I will switch to code and discuss the findings in a bit more detail.

12:26.590 --> 12:28.230
Here are the target words.

12:28.230 --> 12:29.670
Here are the sentences.

12:29.710 --> 12:36.030
Again, they're all exactly the same except for one word difference he she they or mask.

12:36.470 --> 12:38.150
And also just a reminder.

12:38.150 --> 12:45.860
So these are the target words that we want to identify so that we can capture their log softmax Max

12:45.900 --> 12:53.780
logits in later analyses, and in Bert, we do not need preceding spaces because Bert ignores the preceding

12:53.780 --> 12:54.500
spaces.

12:54.820 --> 13:02.100
If this were a GPT model that we were testing, you would want to incorporate spaces before these target

13:02.100 --> 13:02.660
words.

13:02.780 --> 13:09.620
So just a little reminder about the differences between tokenization in Bert and GPT models.

13:10.140 --> 13:10.460
Okay.

13:10.460 --> 13:13.340
Let's see here I'm getting the target mask here.

13:13.340 --> 13:20.660
So that's just yeah the index location where this mask occurs and then all of the other target words

13:20.660 --> 13:21.820
and so on okay.

13:21.860 --> 13:28.220
So here we see the engineer inform the client that mask so blank would need more time.

13:28.220 --> 13:32.660
And we get all of these index positions here.

13:32.660 --> 13:39.300
Again I'm redefining completely redundantly redefining this variable here to be 40,000.

13:39.420 --> 13:41.460
That's just for like safety.

13:41.500 --> 13:49.330
It's for my own mental sanity basically just in case you need to rerun this code after running later

13:49.330 --> 13:53.850
code, let me just skip down to later in the figure you see okay.

13:53.890 --> 13:55.690
So for example an exercise four.

13:55.730 --> 13:59.770
We're going to be changing this variable layer to replace.

13:59.770 --> 14:02.490
Let's say you're exploring you're testing.

14:02.490 --> 14:05.450
You accidentally overwrite one of the variables.

14:05.490 --> 14:08.450
You want to go back and rerun this code.

14:08.450 --> 14:15.770
You will need to make sure that you really are resetting this variable layer to replace so that it doesn't

14:15.770 --> 14:20.010
actually continue modifying the model.

14:21.210 --> 14:27.250
Okay, so all that said, yeah, now I'm running four forward passes through the model with the different

14:27.290 --> 14:28.570
token sequences.

14:28.570 --> 14:36.570
And the final one is the mask that will allow us to measure the bias that the model has, at least in

14:36.570 --> 14:39.330
this specific example sentence.

14:39.810 --> 14:40.130
Okay.

14:40.170 --> 14:41.730
Then I'm let's see.

14:41.930 --> 14:50.410
Grabbing all of the logits for the masked token index for all of the vocab elements from the.

14:50.450 --> 14:57.290
He sentence log softmax and then showing from that log softmax the target.

14:57.330 --> 14:58.290
Indices.

14:58.410 --> 15:02.970
And then also here I'm undoing the logarithm to show the probability.

15:03.010 --> 15:03.690
Values.

15:04.090 --> 15:04.450
Okay.

15:04.490 --> 15:06.450
So this is the figure that I showed.

15:06.490 --> 15:09.490
Again this is kind of trivial that it looks like this.

15:09.490 --> 15:16.450
So we get basically really high probabilities for the target values in each of the corresponding sentences.

15:16.810 --> 15:21.410
It's trivial because the model literally sees that exact word in the sentence.

15:21.410 --> 15:23.850
So yeah, this is not super meaningful.

15:24.170 --> 15:30.210
On the other hand, if you did not see a pattern that looks like this, then you would be suspicious

15:30.210 --> 15:32.050
that something was wrong with your code.

15:32.050 --> 15:34.010
Maybe there's an indexing problem.

15:34.250 --> 15:42.050
So these kinds of visualizations are great sanity checks to incorporate into your analysis code.

15:42.370 --> 15:45.000
So this is for he she and they.

15:45.040 --> 15:52.640
And now the non-trivial analysis is what this plot will look like when we use the mask sentence.

15:52.640 --> 15:53.640
And that's what I'm doing here.

15:53.640 --> 15:59.440
So all of this code is the same as above but looking for the mask token.

15:59.720 --> 16:06.840
So we see if we look over at the softmax probabilities the probability value for heat is over 0.6.

16:06.840 --> 16:11.400
Maybe it's close to 70% for they it's around 20%.

16:11.400 --> 16:14.920
And for XI it's like I don't know 2% or whatever this is.

16:15.240 --> 16:22.040
And you see these differences also in the log of these values as well, that the log is closest to zero.

16:22.440 --> 16:25.880
So the largest for he and it's the smallest for XI.

16:26.320 --> 16:27.440
And what does this mean.

16:27.440 --> 16:34.560
This means that the model's activation for these different pronouns is highest for he and lower for

16:34.560 --> 16:35.880
xi compared to they.

16:36.440 --> 16:41.440
And by the way, there's something else I want to show you so they gets relatively low activation.

16:41.440 --> 16:48.950
What I want to show you now is what happens if I would change this sentence to say to make this plural

16:49.230 --> 16:56.630
so the engineers, instead of the engineer, the engineers plural informed the client that.

16:56.910 --> 16:57.870
And now let's see.

16:57.870 --> 16:59.110
I'll redo that.

16:59.190 --> 17:01.150
I don't need to redo that, but I'll do it anyway.

17:01.190 --> 17:02.950
Definitely need to rerun this.

17:03.670 --> 17:05.470
Uh, I don't care about this.

17:05.470 --> 17:06.390
And now I'm going.

17:06.430 --> 17:10.430
Ah, no, I well, I'll just rerun everything just for security.

17:10.470 --> 17:12.510
Okay, so here's what it looked like before.

17:12.670 --> 17:18.790
And now this is what it looks like when I just changed the word engineer to engineers.

17:19.230 --> 17:27.470
And just making it plural now dominates or allows the day token to dominate above the heat token.

17:29.110 --> 17:36.030
So again, to be clear, what we have now demonstrated unequivocally is that this particular language

17:36.030 --> 17:43.620
model has a bias for associating engineers with the pronoun he in this particular sentence.

17:43.980 --> 17:50.860
If you want to make strong claims about gender biases in language models in general, you would need

17:50.860 --> 17:57.900
to do this kind of analysis, but with a lot more examples and ideally in more models as well.

17:58.660 --> 18:07.140
Anyway, the goal of exercise three is to see if we can remove that gender bias through editing selective

18:07.140 --> 18:07.820
editing.

18:08.420 --> 18:14.260
Here we will focus entirely on layer ten, and you don't have to worry about any of the other layers

18:14.260 --> 18:14.620
for now.

18:14.620 --> 18:16.740
We'll get to that in the next exercise.

18:17.380 --> 18:26.940
So using the hook that you wrote in exercise one, replace the transformer output from block ten with

18:26.980 --> 18:35.900
10% of the vector that the model naturally calculates, and 90% from the exact same vector location.

18:35.940 --> 18:42.700
So the same token index, but coming from the model that actually processed the sentence with the word

18:42.740 --> 18:44.060
she in it.

18:44.500 --> 18:45.660
I hope that makes sense.

18:45.860 --> 18:55.060
So you need to find the embeddings vector for the word xi in transformer block ten from the output when

18:55.060 --> 18:57.700
the model actually saw the word xi.

18:58.260 --> 19:05.060
And that is what you mix together with the token for the word mask, which is being calculated in the

19:05.060 --> 19:06.020
forward pass.

19:06.100 --> 19:12.540
When you run the sentence with the word mask through the model, the figure you will create for this

19:12.540 --> 19:18.340
exercise is similar to the last figure that you created for exercise two.

19:18.820 --> 19:22.340
But you can include two sets of bar graphs.

19:22.620 --> 19:28.660
In fact, the blue lines here are identical to the bar graphs from exercise two.

19:29.180 --> 19:31.500
The orange bars here, those are new.

19:31.780 --> 19:35.980
And that comes from manipulating the model in layer ten.

19:36.340 --> 19:39.090
For that one token embeddings vector.

19:39.490 --> 19:43.330
So here I'm showing you what it would look like for the word they.

19:43.850 --> 19:46.890
And the question is what it looks like for he and she.

19:47.410 --> 19:51.330
And also again I'm showing the log softmax over here.

19:51.490 --> 19:55.410
And the softmax probability in these two different axes.

19:55.890 --> 20:01.290
Of course it's still the same data in the left and the right plot just visualized differently.

20:02.130 --> 20:07.530
The last thing to do in this exercise is to calculate a bias score.

20:08.130 --> 20:16.330
You can do this simply by subtracting the log softmax for the masked token for he minus she.

20:17.050 --> 20:24.610
A positive bias will mean more activation for he, and a negative bias would mean more activation for

20:24.650 --> 20:25.090
she.

20:25.610 --> 20:32.210
The ideal result, which would be no gender bias at all, would be a bias score of zero.

20:32.970 --> 20:38.200
In the next video, we're going to continue working with this metric over the different layers.

20:38.440 --> 20:45.320
But for now, here in exercise three, you can just calculate it for this one result with this one layer

20:45.320 --> 20:46.360
manipulation.

20:47.280 --> 20:52.240
I hope you find this exercise to be thought provoking and also fun to work through.

20:52.640 --> 20:54.360
And now I will switch to code.

20:55.640 --> 21:01.720
Here I define the variable layer to replace which was previously set to 40,000.

21:01.760 --> 21:04.200
Now I'm defining it to be ten.

21:04.600 --> 21:08.880
And I'm also redefining this hidden state vector to replace.

21:08.880 --> 21:15.080
Remember I initialized it with all zeros and now I'm replacing it with this layer.

21:15.080 --> 21:22.840
To replace I will ask you about this plus one in a moment and sequence one the mask target index, all

21:22.840 --> 21:28.880
of the embeddings vectors, and from the version of the model where the model actually saw the word

21:28.880 --> 21:29.400
she.

21:29.720 --> 21:34.720
So that means that this hidden states vector really corresponds to the token xi.

21:35.000 --> 21:41.870
And the idea is that we will mix together this token with whatever is the embeddings vector that the

21:41.870 --> 21:45.750
model is processing when it sees the word mask.

21:46.070 --> 21:49.270
So I'm not running through the tokens for he or she or they.

21:49.310 --> 21:56.510
I'm running through the tokens for the word mask, but I'm going to mix in the embeddings vector that

21:56.510 --> 22:05.510
gets elicited by the mask token with the embeddings vector for the token corresponding to xi in the

22:05.510 --> 22:08.710
same layer and in the same target position.

22:08.750 --> 22:08.990
Woof!

22:09.030 --> 22:10.750
Quite a mouthful to explain that.

22:10.750 --> 22:12.150
I hope it all makes sense.

22:12.430 --> 22:14.910
Okay, so here's another question for you.

22:15.870 --> 22:18.510
Why is there a plus one here?

22:18.630 --> 22:21.710
Why am I setting this variable to be ten.

22:21.990 --> 22:24.550
But here I'm accessing 11.

22:24.990 --> 22:26.190
This is a tricky point.

22:26.190 --> 22:33.110
You have to remember that the hugging face versions of these models, when they define hidden states.

22:33.230 --> 22:41.190
The first hidden state with index zero is not a transformer block, it is the output of the embeddings

22:41.310 --> 22:46.390
layer at the very beginning of the model, the token embeddings plus the position embeddings.

22:46.830 --> 22:53.990
So that means if you want to manipulate transformer block ten, which is actually the 11th transformer

22:53.990 --> 23:01.870
block, then that corresponds to ten plus one, because this starts counting at zero for the first transformer

23:01.870 --> 23:02.470
block.

23:02.470 --> 23:04.790
But the hidden states start transforming.

23:05.110 --> 23:09.310
Start indexing at one for the first transformer block.

23:09.670 --> 23:13.470
Also quite a mouthful to explain, but I think you get the idea.

23:13.510 --> 23:15.270
Okay, so let me run through that.

23:15.670 --> 23:23.070
And this provides some text confirmation that I wrote into the hook function.

23:23.470 --> 23:23.750
Okay.

23:23.790 --> 23:31.030
And now the rest of this code is basically just visualizing according to a code that I've written earlier.

23:31.030 --> 23:35.140
So taking the mask Ask logits from the original.

23:35.180 --> 23:41.060
This is the clean version of the model, without any manipulations with the version of the model that

23:41.060 --> 23:44.820
I just ran up here where I actually did manipulate.

23:44.820 --> 23:50.620
And this here I'm calling a ridge, and here I'm calling ripple for replace.

23:50.860 --> 23:54.780
Okay, so then here is the interesting analysis.

23:55.020 --> 23:57.500
So again remember blue is from the original.

23:57.500 --> 24:04.460
And what we see is that even though the word he and she do not actually appear in the sentence, the

24:04.460 --> 24:12.460
model is just likely to produce the token for he because yeah, it saw the word engineer and it associates

24:12.460 --> 24:19.100
engineers with the word he more than it associates the word engineer with the token for she.

24:19.620 --> 24:19.900
Okay.

24:19.940 --> 24:21.940
So that is from the original model.

24:21.940 --> 24:26.220
And then we mix them together at 10% and 1%.

24:26.220 --> 24:29.980
And what we got is a complete, complete reversal.

24:30.180 --> 24:36.090
So now the model has basically no activation, very, very little activation for the word he.

24:36.090 --> 24:42.810
And it really activates for the word she almost 100% probability of responding to she.

24:42.850 --> 24:48.330
Now that is not very surprising because the manipulation that we ran was pretty severe.

24:48.330 --> 24:51.010
It was 10% and 90%.

24:51.010 --> 24:58.770
So we're really injecting the representation of the word she into this model okay.

24:58.810 --> 25:01.490
And now let's calculate this bias score.

25:01.650 --> 25:08.570
So you see in the original model the bias was three log units difference in favor of he.

25:08.610 --> 25:10.490
So that means a bias towards he.

25:10.890 --> 25:13.410
And now we've completely reversed it.

25:13.410 --> 25:16.530
So not just reverse it but like to the extreme.

25:16.530 --> 25:22.570
So now we have a bias score of minus ten almost -11 log units.

25:22.570 --> 25:30.650
So this is a really intense like anti bias that we have introduced into this model.

25:31.090 --> 25:35.560
So that's pretty extreme if you like, before going on to the next exercise.

25:35.560 --> 25:41.280
If you are curious, you can try rerunning this exercise a few more times.

25:41.280 --> 25:46.880
So run the code for this exercise a few more times, but change around the mixture levels.

25:46.880 --> 25:57.800
So instead of being 1% and 90%, maybe 30% and 70% or any other combination you'd like to try, this

25:57.800 --> 26:01.360
is the last exercise of this code challenge.

26:01.760 --> 26:10.080
The idea now is to soften the anti-bias editing a little bit and explore whether it matters which layer

26:10.080 --> 26:11.360
we modify.

26:11.920 --> 26:20.440
So change the mixing weights to be half half so evenly, combining the naturally calculated vector with

26:20.440 --> 26:23.440
the one from the model that actually saw the word.

26:23.480 --> 26:32.000
She then have a for loop over all of the different layers and basically repeat the calculation from

26:32.000 --> 26:33.320
exercise three.

26:34.000 --> 26:39.920
Now you don't need to create 24 sets of bar graphs, because that's just a lot of stuff to look at.

26:40.000 --> 26:47.160
So you can just focus on calculating the bias score and store that value per layer.

26:47.560 --> 26:50.160
And then you can visualize it like this.

26:50.280 --> 26:55.040
So the dashed line at zero corresponds to no bias.

26:55.680 --> 27:02.080
So now the question is what is this going to look like as we go deeper into the model.

27:02.480 --> 27:09.480
For example, does the bias score just kind of bop around randomly or does it go monotonically down?

27:09.520 --> 27:11.840
Maybe it goes down and then comes back up.

27:12.040 --> 27:13.840
That is for you to discover.

27:14.720 --> 27:15.240
Cool.

27:15.280 --> 27:18.160
I hope you enjoy working through this exercise.

27:18.200 --> 27:20.320
And now I will show my solution.

27:21.920 --> 27:28.280
Here I am redefining this variable mixture to be 50 over 50.5 0.5.

27:28.920 --> 27:29.400
Okay.

27:29.510 --> 27:32.390
And now here I'm looping over the layers.

27:32.390 --> 27:38.510
Notice that I'm using a looping index variable called layer to replace.

27:38.510 --> 27:43.830
So that means that I do not actually need to redefine this variable explicitly.

27:44.030 --> 27:46.870
It's just going to be redefined on its own.

27:47.070 --> 27:53.470
Inside this for loop, as we are iterating over all of the layers, the number of hidden layers.

27:53.830 --> 28:00.590
So here I am again, grabbing the hidden state from the version of the model when it saw the word she.

28:01.150 --> 28:02.670
So that hidden state and so on.

28:02.670 --> 28:08.270
This code you've seen before, it still needs to be inside this loop, because we want to get the layer

28:08.270 --> 28:12.710
specific embeddings vector and not from any other layer.

28:12.710 --> 28:15.950
So we're not mixing information across different layers here.

28:16.390 --> 28:22.870
And then pushing the mask tokens through the model as you've seen before, and then softmax ING and

28:22.870 --> 28:30.060
calculating the bias for he that is going to be stored in this vector here.

28:30.060 --> 28:31.020
Bias scores.

28:31.380 --> 28:32.420
So I can run through this.

28:32.460 --> 28:36.300
You can see that it's updating each time it runs through a layer.

28:36.780 --> 28:39.180
So goes through pretty fast on the GPU.

28:39.420 --> 28:44.500
I did not actually time this on the CPU, but I guess it's not so bad.

28:44.540 --> 28:46.380
But nice to have the GPU here.

28:47.540 --> 28:49.780
Okay, so now here we can create this plot.

28:49.820 --> 28:56.340
Pretty interesting to see that there is I'm not sure what's going on here, but otherwise a pretty gentle

28:56.340 --> 28:59.780
decrease as we go deeper into the model.

29:00.060 --> 29:08.260
In other words, when we mix the embeddings vector for xi with the embeddings vector for a mask, then

29:08.260 --> 29:14.100
at the very beginning, at the earliest levels of the transformer sequence, it doesn't really have

29:14.100 --> 29:17.940
much of an impact the later we get in the model.

29:17.940 --> 29:23.940
So the closer to the model output and the token prediction, the missing token prediction, the more

29:23.980 --> 29:30.050
that this manipulation actually has an impact, so that is pretty neat to look at.

29:30.090 --> 29:31.570
It's pretty neat to consider.

29:31.570 --> 29:38.010
And essentially what's happening is as we get later into the model, the model is better incorporating

29:38.010 --> 29:42.570
all of the contextual information from surrounding tokens.

29:42.730 --> 29:49.650
So over here, the engineer, the token for engineer is having a bigger impact on the token for the

29:49.650 --> 29:52.570
mask compared to earlier in the model.

29:52.570 --> 29:57.050
When the model is doing more like item level and word level processing.

29:58.490 --> 30:02.930
I want to be clear about a few caveats of this code challenge.

30:03.410 --> 30:12.650
As I mentioned before, it's really easy in complex systems to manipulate the system to give a desired

30:12.650 --> 30:15.370
output in one specific example.

30:15.810 --> 30:22.050
What is much harder, though, of course still possible, is to adjust the model weights so that these

30:22.050 --> 30:30.050
kinds of biases are eradicated from all examples, including when the model is generating new text or

30:30.090 --> 30:33.890
doing something else, like evaluating job applications.

30:34.370 --> 30:40.450
I just want to make sure it's clear to you that this is a hard problem to solve in general, even though

30:40.450 --> 30:46.210
it was extremely easy to solve here in this video for this one specific example.

30:46.770 --> 30:53.170
One thing we did not explore here, for example, is whether you would get the same result if the model

30:53.170 --> 30:58.730
processed the word xi from a different sentence with a different context.

30:59.490 --> 31:06.130
A second point here, which is more about model representations and not about practical implications

31:06.130 --> 31:15.370
of biases, is that averaging two embeddings vectors together relies on an assumption of linearity in

31:15.370 --> 31:17.090
the embeddings space.

31:17.650 --> 31:25.440
So if these concepts like gender are not represented as lines in the embedding space, but instead as

31:25.440 --> 31:28.960
curves or planes or some other weird non-linear shape.

31:29.320 --> 31:33.840
Then linearly averaging together vectors might not make sense.

31:34.280 --> 31:43.160
For example, on a line, the number three is halfway between 2 and 4, but on a curve three might not

31:43.160 --> 31:45.160
be halfway between 2 and 4.

31:45.840 --> 31:49.000
And multiply that by thousands of dimensions.

31:49.000 --> 31:55.040
And it's just really not clear that what we are doing here actually incorporates the geometry of the

31:55.040 --> 31:58.720
representations in the model's embedding space.

31:59.800 --> 32:07.040
That is a deep question about the fundamental nature of how models represent and compute tokens and

32:07.040 --> 32:08.240
representations.

32:08.880 --> 32:12.560
But anyway, linear averaging certainly worked quite well here.

32:12.560 --> 32:20.240
So that does lend some credence to the idea that at least some aspects of the embeddings space can be

32:20.240 --> 32:23.240
reasonably approximated linearly.

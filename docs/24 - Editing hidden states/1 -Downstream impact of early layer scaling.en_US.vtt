WEBVTT

00:02.160 --> 00:04.920
Welcome to this section of the course.

00:05.440 --> 00:13.000
All of the videos in this section are focused on modifying the hidden states, which are the outputs

00:13.000 --> 00:15.200
of the transformer blocks.

00:15.840 --> 00:21.840
Of course, a lot of the concepts and code that you will learn about in this section are relevant for

00:21.840 --> 00:25.840
any kind of causal manipulations in any part of the model.

00:26.480 --> 00:33.160
In this first code demo, I will follow up on an observation from the previous section, which was that

00:33.160 --> 00:40.040
when you modify the activations in one layer, that has an impact on what happens in later layers,

00:40.040 --> 00:42.000
but not in earlier layers.

00:42.800 --> 00:49.440
On the one hand, that is kind of obvious, but it does lend itself to lots of interesting analyses

00:49.440 --> 00:56.320
that we can apply and experiments that we can run, for example, to investigate which layers are most

00:56.320 --> 01:02.360
important for representing different features of text or different contextual information.

01:03.320 --> 01:09.160
There's not much theory or math that you need here, so let me jump right into the overview of the code

01:09.160 --> 01:09.720
demo.

01:10.360 --> 01:17.600
I will use GPT two and create a hook that simply grabs the hidden states that were already calculated

01:17.600 --> 01:24.200
in this layer, and then scale the whole thing down by some scaling factor and then return that scaled

01:24.200 --> 01:24.880
output.

01:25.240 --> 01:27.320
So here you see the hook function.

01:27.520 --> 01:30.200
So I'm grabbing the output of this layer.

01:30.200 --> 01:33.920
And in this case I've hardcoded this to be layer two.

01:34.320 --> 01:42.560
And then I just scale by multiplying this hidden states tensor by some scaling factor, and then pack

01:42.600 --> 01:46.480
up the output again into a tuple before returning it.

01:46.960 --> 01:52.200
And actually you see that the way that I have written this code here does literally nothing.

01:52.200 --> 01:54.760
I set the scaling factor to be one.

01:54.760 --> 01:58.400
So I'm just multiplying the embeddings vectors by one.

01:58.560 --> 02:03.200
But this here this is a globally defined variable.

02:03.200 --> 02:05.160
It's not defined inside.

02:05.160 --> 02:08.640
The function is defined outside of the hook function.

02:08.840 --> 02:10.640
So it's global as global scope.

02:10.640 --> 02:15.360
And that means that I can change this to be less than one or greater than one.

02:15.360 --> 02:22.080
I can change this variable dynamically as I'm running experiments and exploring the impact of scaling.

02:23.200 --> 02:28.360
And also just to be clear, in case this wasn't clear enough from the previous section, this variable

02:28.400 --> 02:32.880
output here, this is the output of this layer.

02:33.280 --> 02:41.480
So before I get this variable output, the model has already run through attention and MLP, adding

02:41.480 --> 02:45.080
those adjustments back onto the residual stream layer norms.

02:45.080 --> 02:45.480
ET cetera.

02:45.480 --> 02:46.120
ET cetera.

02:46.280 --> 02:51.120
Everything that happens in Transformer Block two has already happened.

02:51.120 --> 02:52.920
It's already been calculated.

02:52.920 --> 02:57.480
And here I'm grabbing the results of all of those calculations.

02:57.480 --> 03:03.680
And then I apply this manipulation and then replace the output with this modified version.

03:04.240 --> 03:11.280
Okay, so what I return from this hook function ends up being the input into the next transformer block.

03:12.440 --> 03:16.520
And another reminder this variable here this output variable.

03:16.560 --> 03:18.000
This is a tuple.

03:18.400 --> 03:26.200
It is not a tuple if you hook into the attention or MLP layers directly, but it is a tuple for the

03:26.200 --> 03:29.160
final output of the transformer block.

03:29.680 --> 03:33.400
Now the first element of this tuple, that's the thing that we want.

03:33.560 --> 03:38.080
And it's also what hugging face refers to as the hidden states.

03:38.520 --> 03:45.120
Now, whether there are other elements in this tuple depends on the inputs that you provide when calling

03:45.120 --> 03:47.040
the forward pass through the model.

03:47.480 --> 03:53.240
I'm going to discuss this explicitly and show some examples in a later video in this section.

03:53.640 --> 03:59.920
For now, this tuple actually only has one element, so this variable rest will be empty.

04:00.120 --> 04:03.360
But anyway, it's good to be in the habit of including this.

04:03.480 --> 04:10.160
Otherwise it can cause some serious issues if you do have multiple elements in this output tuple.

04:10.480 --> 04:12.680
Okay, so more on that in a future video.

04:13.720 --> 04:16.160
I have a little bit of text to push through.

04:16.200 --> 04:18.200
It is ten tokens.

04:18.360 --> 04:25.040
And of course there are 13 hidden states 768 dimensions in GPT two.

04:25.080 --> 04:25.640
Small.

04:26.440 --> 04:33.080
Now what I have done is run the forward pass with this hook implanted and storing all of the hidden

04:33.080 --> 04:34.480
states twice.

04:35.080 --> 04:40.000
The first time I did the forward pass, I pushed all the tokens through with this scaling factor of

04:40.000 --> 04:40.520
one.

04:40.920 --> 04:47.320
That means that absolutely nothing happened, except that we wasted like a tiny bit of computation time

04:47.320 --> 04:48.760
just to run this code here.

04:49.200 --> 04:52.800
So that is the unscaled version of the data.

04:52.800 --> 04:57.410
In other words, there were no manipulations or modulations of the activations.

04:57.730 --> 05:01.370
That's a pure copy of the model internals.

05:02.130 --> 05:09.930
Then I did a forward pass again with the same tokens, but now I set the scaling factor to be 0.5.

05:10.370 --> 05:17.090
So that means now that all of the hidden states from transformer block two were scaled by a factor of

05:17.090 --> 05:17.970
one half.

05:18.370 --> 05:23.010
I didn't add any noise or swap the order of the dimensions or anything like that.

05:23.250 --> 05:30.130
All I did was scale everything down, and that means that I now have two sets of hidden states, one

05:30.130 --> 05:36.130
from the original version of the model and one with the scaled vectors in transformer layer two.

05:36.970 --> 05:44.690
So, to gain some insight into the impact of scaling, I calculated the difference between the two sets

05:44.930 --> 05:50.930
of hidden state matrices and then calculated the norm of that matrix.

05:51.450 --> 05:59.450
And the idea is that if the two matrices are identical, then the norm of their difference is exactly

05:59.450 --> 06:00.170
zero.

06:00.570 --> 06:05.650
And that you see here for the embeddings layer and for the first two transformer blocks.

06:06.730 --> 06:11.490
And then here it's in transformer block two where I introduced the scaling.

06:11.850 --> 06:15.570
So now suddenly the matrix difference norm jumps up.

06:15.770 --> 06:18.490
Now that part is not at all surprising.

06:18.490 --> 06:20.810
I scaled it down by a factor of one half.

06:21.090 --> 06:22.850
So of course there is.

06:22.890 --> 06:24.170
There are some differences.

06:24.690 --> 06:28.970
The interesting question is what happens in the layers thereafter?

06:29.530 --> 06:35.810
You can imagine that the model would maybe adapt to the smaller activations and simply figure out a

06:35.810 --> 06:42.210
way to scale up the smaller numbers, so that after a couple of layers, they end up being the same

06:42.210 --> 06:44.170
as without the downsampling.

06:44.850 --> 06:50.690
But actually, though, what ends up happening is that the two sets of matrices continue to diverge

06:50.810 --> 06:57.650
as we get deeper into the model, although they do move towards each other at the final transformer

06:57.650 --> 06:58.170
block.

06:58.810 --> 07:05.690
So there is something at the very end that is able to compensate at least a little bit for the disruption

07:05.690 --> 07:07.650
that happened earlier in the model.

07:08.210 --> 07:13.650
By the way, this plot here shows the results excluding the first token.

07:13.810 --> 07:18.130
It looks quite a bit different when we include the first token in this analysis.

07:18.970 --> 07:25.850
Okay, so is this just a bunch of numbers or does this have any actual impact on the model's behavior.

07:26.090 --> 07:32.210
And by behavior I mean the actual tokens that it generates and the distribution of the final output

07:32.210 --> 07:34.810
logits over the vocab?

07:35.410 --> 07:41.490
I will let you ponder the answer to that question, and what you think will happen to the model's ability

07:41.490 --> 07:44.490
to predict tokens and generate new tokens.

07:45.010 --> 07:51.410
We will do those kinds of investigations in later videos, but here in this demo, I'm really just going

07:51.450 --> 07:55.890
to focus on the code implementations and the model internals.

07:56.530 --> 08:03.530
Anyway, so here is the result of one experiment with one scaling factor of 0.5 compared to no scaling.

08:04.450 --> 08:11.570
The next thing I'll do is repeat exactly this experiment, but looping over a range of scaling factors

08:11.570 --> 08:14.090
from 0.5 to 1.5.

08:14.650 --> 08:16.650
And you can see those results here.

08:16.890 --> 08:19.250
So the left line plot.

08:19.250 --> 08:23.570
So this plot over here shows the same result that I just showed here.

08:23.690 --> 08:27.730
But here this was for one experiment scaling at 0.5.

08:27.890 --> 08:32.810
And here we have a bunch of lines corresponding to different scaling factors.

08:33.290 --> 08:38.250
This purple line up here on the top corresponds to a scaling factor of 0.5.

08:38.250 --> 08:40.450
So that's the one that I showed over here.

08:40.690 --> 08:42.410
And there's a lot of lines here.

08:42.410 --> 08:44.650
And they show a non-linear pattern.

08:44.650 --> 08:46.890
So they go down and then up again.

08:47.290 --> 08:51.690
So therefore I thought it would be easier to view this whole thing as an image.

08:52.010 --> 08:54.130
And that's what you see here on the right.

08:54.290 --> 08:59.370
So each row in this image corresponds to a line in this plot.

08:59.850 --> 09:05.730
And the color in this image corresponds to the height in this line plot over here.

09:06.690 --> 09:13.810
It's pretty neat to see that the impact of scaling increases as we get deeper in the model for all the

09:13.810 --> 09:14.970
scaling factors.

09:15.050 --> 09:20.730
But it becomes relatively more intense for the scaling factors further from one.

09:21.170 --> 09:27.770
And you also see the final transformer block is able to compensate for these different values or these

09:27.890 --> 09:30.210
scaling factors to some extent.

09:30.930 --> 09:31.210
Okay.

09:31.250 --> 09:32.450
So that's the overview.

09:32.490 --> 09:38.250
I will have more to say about all of these plots when I discuss them in code, which is what I will

09:38.250 --> 09:39.130
do right now.

09:40.410 --> 09:42.770
So import some libraries here.

09:42.770 --> 09:45.050
Here I'm importing GPT two.

09:45.170 --> 09:49.570
And here I'm creating the hook just like what you saw in the slides.

09:49.730 --> 09:49.930
Okay.

09:49.970 --> 09:54.570
And so again yeah I'm grabbing the output, unpacking it because this is a tuple.

09:54.770 --> 10:02.210
And here I'm making a copy of that tuple and then scaling it or sorry, copy of the hidden states.

10:02.410 --> 10:04.170
This part is actually not necessary.

10:04.210 --> 10:09.490
You can modulate these directly, but it's never a bad idea to explicitly make a copy.

10:09.810 --> 10:18.610
And then using this in-place method on this PyTorch tensor to multiply by this scaling factor.

10:18.930 --> 10:25.650
So this is an in-place method, which means that it's not like a function where we need to write something

10:25.650 --> 10:26.370
like this.

10:26.690 --> 10:33.330
It just works directly on the object, this tensor, without having to redefine it like this.

10:33.690 --> 10:40.250
And how do you know if a method or a function works in place, or whether it provides an output and

10:40.250 --> 10:41.730
you would need something like this.

10:42.130 --> 10:48.250
Sometimes you just have to look it up, but you can also get an indicator with this underscore at the

10:48.250 --> 10:49.690
end of the function.

10:49.850 --> 10:55.570
In general, in Python, when you see methods or functions that end with an underscore, that doesn't

10:55.570 --> 11:00.130
guarantee that they are in-place operators, but that is fairly common.

11:00.570 --> 11:02.090
Okay, so then I pack it up here.

11:02.130 --> 11:07.690
Actually this is slightly different formulation from what I showed in the screenshot in the code.

11:07.690 --> 11:09.210
But either one of those is fine.

11:09.730 --> 11:10.050
Okay.

11:10.090 --> 11:14.850
So yeah hard coding this to go into block two which is fine.

11:14.850 --> 11:20.210
In the next videos we will make this a little bit more dynamic and soft coded.

11:20.610 --> 11:20.930
Okay.

11:20.970 --> 11:22.010
Here is some text.

11:22.050 --> 11:24.930
Duct tape will be very useful after the apocalypse.

11:25.210 --> 11:26.330
That's probably true.

11:26.330 --> 11:28.250
I don't want to be there to find out.

11:28.570 --> 11:29.890
Okay, let's see here.

11:29.890 --> 11:32.810
I'm redefining the scaling factor as one.

11:33.050 --> 11:36.850
I technically do not need that line of code because it's already up here.

11:36.970 --> 11:44.330
But just because I'm going to call this variable hidden states, pure or clean or unadulterated.

11:44.570 --> 11:50.690
Then I just want to make sure, in case we change this later, that it is really going to be set to

11:50.730 --> 11:53.930
one for these variables over here.

11:54.930 --> 11:55.290
Okay.

11:55.330 --> 12:00.290
So I get the hidden states output and then rename those to hidden states pure.

12:00.530 --> 12:03.490
And then yeah this is what you've seen before.

12:03.970 --> 12:04.290
Okay.

12:04.330 --> 12:06.770
So now I'm running exactly the same code.

12:06.770 --> 12:08.890
So this is still the same same tokens.

12:08.890 --> 12:10.410
Everything like that is the same.

12:10.610 --> 12:15.810
But I'm changing the scaling factor from 1 to 0.5.

12:16.210 --> 12:23.210
And it's really important when you're doing this that the name of the variable is exactly the same as

12:23.250 --> 12:26.290
what goes into this function over here.

12:26.730 --> 12:34.570
When Python runs this function, it is going to search for a variable called scaling underscore factor.

12:34.810 --> 12:38.890
And that variable is not defined here locally in the function.

12:38.890 --> 12:46.210
So therefore Python will search for a global definition of this of of this variable which is here.

12:46.210 --> 12:50.130
But that means that if you would change this to like CamelCase.

12:50.130 --> 12:51.170
If you would do this.

12:51.810 --> 12:53.210
This is not going to work.

12:53.330 --> 12:59.850
Okay, so you need to be very mindful of the spelling and capitalization and all that good stuff.

13:00.410 --> 13:08.450
Okay, so now I get the scaled version of these hidden states here I'm extracting from this layer,

13:08.730 --> 13:15.290
uh, from the first element in the sequence or in the batch, uh, which is only one sequence, uh,

13:15.290 --> 13:17.410
all of the tokens, except for the first one.

13:17.410 --> 13:23.130
I'll show you in a moment what it looks like when we include the first token and all of the embeddings

13:23.130 --> 13:24.010
dimensions.

13:24.170 --> 13:25.890
So get that and then.

13:25.930 --> 13:29.730
So that's for the pure hidden states, the scaled hidden states.

13:29.730 --> 13:34.650
And then I literally just subtract them and calculate the matrix norm.

13:34.850 --> 13:40.570
Technically you could also do all this stuff with PyTorch but I'm just converting it here into numpy.

13:41.170 --> 13:41.450
Okay.

13:41.490 --> 13:47.810
Again the idea is that this is not telling us whether the pure or the scaled versions are bigger or

13:47.930 --> 13:49.730
smaller or how they changed.

13:49.730 --> 13:52.930
If some vectors got bigger, other vectors got smaller.

13:52.930 --> 13:59.250
If they were rotated in different directions, that is all information that is more nuanced than what

13:59.250 --> 14:00.370
the norm will tell you.

14:00.370 --> 14:05.970
The norm simply tells you how far apart are these two matrices from each other.

14:06.090 --> 14:13.090
And if these two matrices are identical so they're not far apart, then their difference matrix will

14:13.090 --> 14:15.450
be zero and the norm will be zero.

14:15.450 --> 14:23.010
And the further the numerical values are in these matrices from each other, the larger the matrix norm

14:23.010 --> 14:23.970
is going to be.

14:25.090 --> 14:26.090
Okay, so that's that.

14:26.090 --> 14:27.450
And then I plot it here.

14:27.490 --> 14:30.450
Now have a look at the y axis over here.

14:30.450 --> 14:35.650
So it looks like most of these difference norms are 100 to 200.

14:36.090 --> 14:37.210
This one's a little bit higher.

14:37.210 --> 14:37.810
Whatever.

14:37.810 --> 14:39.170
So 100 to 200.

14:39.210 --> 14:43.610
Now what I'm going to do is just delete this one over here.

14:43.610 --> 14:49.220
This means that we now have all of the tokens, Including the very first token.

14:49.980 --> 14:54.140
And now we see that these norms went up a lot, right?

14:54.180 --> 14:56.620
Remember it was like 100 to 200.

14:56.620 --> 14:59.260
So those values were all the way down here.

14:59.580 --> 15:06.780
And now it's just exploded by an order of magnitude, simply by including that very first token.

15:06.780 --> 15:13.300
That is the I haven't counted the number of times I've demonstrated in this course that weird things

15:13.300 --> 15:13.700
happen.

15:13.700 --> 15:17.500
On the very first token, it must be at least 20, I think, by now.

15:17.780 --> 15:20.100
So yet another demonstration of that.

15:20.100 --> 15:26.140
And the point is, you know, I think these reminders are important because if you are doing this kind

15:26.140 --> 15:34.100
of research, your entire research program and all of your results can be completely destroyed by incorporating

15:34.100 --> 15:39.420
the first token in a sequence when, particularly when the sequences are relatively small.

15:39.900 --> 15:41.540
Okay, so let's do this again.

15:41.580 --> 15:44.100
Let me run that code again and run this code again.

15:44.100 --> 15:49.340
And now this is shrunk back down to levels that a human would be happy with.

15:49.820 --> 15:58.740
Okay, so now what I do is essentially repeat exactly that code that I just walked you through.

15:58.740 --> 16:07.660
But I'm embedding all of that code into a for loop over a range of scale factors from 0.5 to 1.5.

16:07.900 --> 16:14.900
And then you can see again, I'm being very careful to use exactly the same variable name that I'm using

16:14.900 --> 16:15.900
here, for example.

16:15.940 --> 16:16.140
Yeah.

16:16.180 --> 16:20.580
If I would do scale factor I don't know with a capital R here at the end.

16:21.020 --> 16:26.500
Now, what this means is that the scaling factor is going to be 0.5 for every single run.

16:26.740 --> 16:28.460
Why is it going to be 0.5.

16:28.500 --> 16:33.940
Because Python is going to ignore this variable and look for something that's called this, which I've

16:33.940 --> 16:35.620
previously set to 0.5.

16:36.100 --> 16:36.580
Okay.

16:36.700 --> 16:44.900
Anyway, with GPT two small, uh, and with such a short sequence of tokens, even running this on the

16:45.020 --> 16:46.460
CPU is still very fast.

16:46.460 --> 16:51.260
That was three seconds to do to run this 21 times I think.

16:51.300 --> 16:52.500
Yeah pretty fast.

16:53.020 --> 16:53.340
Okay.

16:53.380 --> 16:53.620
Yeah.

16:53.620 --> 16:55.300
So now I'm creating this plot.

16:55.300 --> 17:03.260
And then what you see here is that all of these lines are going up and then down.

17:03.420 --> 17:03.700
Right.

17:03.740 --> 17:11.060
So they start off very high and each successive line gets lower until we have basically a flat line.

17:11.220 --> 17:17.540
And in theory if we get a scaling factor of exactly one in here, of course that would be a flat line.

17:17.700 --> 17:18.940
And then it comes back up.

17:18.940 --> 17:25.260
So there's this non-linear relationship between the scaling factor and the matrix norm.

17:25.500 --> 17:26.580
It's not surprising.

17:26.620 --> 17:31.940
Again, the matrix norm doesn't tell us exactly what is changing and how it's changing.

17:32.100 --> 17:38.820
All it's telling us is that the two matrices are different from each other and how much the differences

17:38.820 --> 17:39.180
are.

17:40.420 --> 17:46.620
Okay, so that's why I thought it would be nicer to show this in a in an image like this where you can

17:46.620 --> 17:48.340
really see black.

17:48.340 --> 17:49.860
Here the norm is zero.

17:50.020 --> 17:54.500
It's zero because I didn't actually change anything before layer two.

17:54.900 --> 18:03.460
And then there is an almost but not quite symmetric, uh, impact of scaling when we scale below one

18:03.460 --> 18:04.340
and above one.

18:04.340 --> 18:11.460
So it kind of looks like the scaling has more of an impact as we go down, as we shrink compared to

18:11.500 --> 18:14.620
when we expand these vectors.

18:15.740 --> 18:21.900
If you still feel like you're struggling to understand some of the nuances of using forward hooks to

18:21.940 --> 18:29.300
do causal experiments, then don't worry, you will get lots more practice over the next several sections.

18:29.820 --> 18:37.340
One methodological point here it's always a good idea to run a forward pass with no causal interference

18:37.380 --> 18:39.380
to use as a comparison.

18:40.020 --> 18:41.060
So that's it for now.

18:41.140 --> 18:42.420
See you in the next video.

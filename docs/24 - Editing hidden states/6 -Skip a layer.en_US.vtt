WEBVTT

00:01.840 --> 00:08.000
This will be a pretty short video and code demo, as I'm sure you guessed from the title.

00:08.000 --> 00:14.400
I'm going to show you how to completely bypass one of the transformer blocks.

00:14.880 --> 00:21.080
To be honest, I'm not sure if this approach really has a lot of application value because it's a pretty

00:21.080 --> 00:25.320
coarse lesion, but maybe it does have some use.

00:25.320 --> 00:32.400
And anyway, it's a good opportunity to help solidify your understanding of the meaning of the inputs

00:32.600 --> 00:34.360
into the hook functions.

00:34.560 --> 00:43.000
So the idea is that the output from each transformer block is the input to the next transformer block.

00:43.480 --> 00:51.600
What I will show you in this code demo is how to bypass a block completely, such that the output of

00:51.600 --> 00:56.480
block T ends up being the input into block T plus two.

00:57.280 --> 01:03.880
Technically, PyTorch is still going to do all of the calculations in block T plus one, but the hook

01:03.920 --> 01:11.080
that I will implant will replace the output of T plus one with the output of T, and that means that

01:11.080 --> 01:16.130
all the calculations that were done in this transformer block were all for nothing.

01:16.570 --> 01:19.210
So here's how that looks in code.

01:19.450 --> 01:20.730
Here's the hook function.

01:20.730 --> 01:23.290
This is a ludicrously simple function.

01:23.290 --> 01:25.010
There's literally nothing in it.

01:25.210 --> 01:29.730
All I'm doing is returning the input as the output.

01:30.010 --> 01:36.170
So I generated some text to run a forward pass through the model, stored the hidden states, and then

01:36.170 --> 01:42.650
just calculated the matrix norm of the difference in the hidden states between each layer and the previous.

01:42.650 --> 01:47.050
And here you see the impact of implanting this hook over here.

01:47.610 --> 01:54.690
So these numbers here are the matrix norm of the difference between each transformer block and the previous.

01:55.010 --> 02:03.530
And you see that from 6 to 5 we have a norm of zero which means the embeddings vectors did not change

02:03.530 --> 02:06.570
at all as it passed through this layer.

02:07.090 --> 02:14.010
And remember that all of the hidden states actually includes the embeddings matrix as as zero.

02:14.170 --> 02:20.730
So layer five of the transformer block actually corresponds to layer six in the hidden states.

02:21.330 --> 02:25.420
Okay, now I will switch to code and show you in a little bit more detail.

02:26.700 --> 02:30.260
We don't need a lot of fancy libraries for this code demo.

02:30.300 --> 02:34.860
We just need PyTorch and the stuff to import GPT two.

02:35.020 --> 02:38.100
Of course you could use any other model you like here.

02:38.540 --> 02:41.740
So here again is the hook function ridiculously simple?

02:41.900 --> 02:48.460
Basically, all we're doing is saying whatever is the input into the model or into this layer, all

02:48.460 --> 02:55.060
of the calculations, all the attention and MLP adjustments that are taking place inside the transformer

02:55.060 --> 02:59.900
block, and then it eventually spits out some transformed version of the input.

02:59.900 --> 03:03.140
All of that stuff gets completely ignored.

03:03.140 --> 03:07.500
I overwrite it by outputting the input into the model.

03:07.740 --> 03:14.500
And effectively, what that does is tell the transformer to take its input and just directly pass it

03:14.500 --> 03:19.540
along as output, which is the same thing as skipping the layer entirely.

03:19.860 --> 03:21.580
Okay, so run this here.

03:21.580 --> 03:27.540
You see, I'm just implanting this hook into one of the transformer blocks here.

03:27.540 --> 03:28.940
I'm generating some tokens.

03:28.940 --> 03:31.660
There is a lot of liquid water on planet Earth.

03:31.860 --> 03:32.380
Apparently.

03:32.380 --> 03:33.020
That's true.

03:33.160 --> 03:35.240
I read about it on the internet once.

03:35.560 --> 03:35.920
Okay.

03:35.960 --> 03:38.480
And then push those tokens through the model.

03:38.720 --> 03:39.120
Uh, yeah.

03:39.160 --> 03:41.600
11 tokens in that sentence and so on.

03:41.920 --> 03:49.120
And now here I'm looping over all of the hidden state layers from one to the number of hidden state

03:49.120 --> 03:55.920
layers, calculating the difference and then calculating the norm of that difference and just printing

03:55.920 --> 03:56.240
it out.

03:56.240 --> 03:59.000
And here you see that zero that happens.

03:59.000 --> 04:03.520
We get this zero value here because the output equals the input.

04:03.520 --> 04:07.400
And so therefore that layer has been completely skipped over.

04:08.280 --> 04:16.000
The beautiful thing about forward hooks and causal mcinturff research is that it allows you to do really

04:16.000 --> 04:19.560
precise ablations and manipulations.

04:20.160 --> 04:26.320
Cutting out an entire transformer block is much more like a chainsaw than a surgical knife.

04:26.520 --> 04:31.600
So I'm not sure how often you would actually want to do something like this in practice.

04:32.240 --> 04:39.320
That said, I hope you feel that this quick demo increased your understanding of working with hook functions

04:39.440 --> 04:43.160
and the inputs and outputs to the transformer blocks.

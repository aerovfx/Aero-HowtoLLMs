
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../../index.md) > [01 LLM Course](../../index.md) > [LectureStanford](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../../index.md)
- [ğŸ“š Module 01: LLM Course](../../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Lecture 8: LLM Evaluation âš–ï¸

> **TÃ³m táº¯t tá»« khÃ³a há»c Stanford CME 295: Transformers & Large Language Models.**
> BÃ i giáº£ng nÃ y táº­p trung vÃ o bÃ i toÃ¡n khÃ³ nháº¥t trong phÃ¡t triá»ƒn LLM: LÃ m sao Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ (Evaluate) mÃ´ hÃ¬nh má»™t cÃ¡ch chÃ­nh xÃ¡c vÃ  tin cáº­y?

---

## ğŸ“š Má»¥c Lá»¥c
1. [Táº¡i sao Ä‘Ã¡nh giÃ¡ LLM láº¡i khÃ³?](#1-táº¡i-sao-Ä‘Ã¡nh-giÃ¡-llm-láº¡i-khÃ³)
2. [CÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡](#2-cÃ¡c-phÆ°Æ¡ng-phÃ¡p-Ä‘Ã¡nh-giÃ¡)
3. [Metrics truyá»n thá»‘ng (BLEU, ROUGE)](#3-metrics-truyá»n-thá»‘ng-bleu-rouge)
4. [LLM-as-a-Judge](#4-llm-as-a-judge)
5. [Benchmarks phá»• biáº¿n](#5-benchmarks-phá»•-biáº¿n)

---

## 1. Táº¡i sao Ä‘Ã¡nh giÃ¡ LLM láº¡i khÃ³?
KhÃ¡c vá»›i cÃ¡c bÃ i toÃ¡n ML truyá»n thá»‘ng (Classification, Regression) cÃ³ Ä‘Ã¡p Ã¡n Ä‘Ãºng/sai rÃµ rÃ ng, Ä‘áº§u ra cá»§a LLM lÃ  **Free-form Text (VÄƒn báº£n tá»± do)**.
*   **Subjectivity (TÃ­nh chá»§ quan):** Má»™t cÃ¢u tráº£ lá»i cÃ³ thá»ƒ hay vá»›i ngÆ°á»i nÃ y nhÆ°ng dá»Ÿ vá»›i ngÆ°á»i kia.
*   **Variety (Sá»± Ä‘a dáº¡ng):** CÃ³ vÃ´ sá»‘ cÃ¡ch Ä‘á»ƒ diá»…n Ä‘áº¡t cÃ¹ng má»™t Ã½.
*   **Chi phÃ­:** ÄÃ¡nh giÃ¡ thá»§ cÃ´ng bá»Ÿi con ngÆ°á»i (Human Eval) ráº¥t Ä‘áº¯t vÃ  cháº­m.

---

## 2. CÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡
1.  **Human Evaluation:** ChÃ­nh xÃ¡c nháº¥t nhÆ°ng tá»‘n kÃ©m nháº¥t. DÃ¹ng cho giai Ä‘oáº¡n cuá»‘i hoáº·c kiá»ƒm tra ngáº«u nhiÃªn (Spot check).
2.  **Code Evaluation:** DÃ¹ng Unit Test Ä‘á»ƒ cháº¥m Ä‘iá»ƒm code do LLM sinh ra (Pass@k). Ráº¥t chÃ­nh xÃ¡c cho bÃ i toÃ¡n láº­p trÃ¬nh.
3.  **Algorithmic Metrics:** DÃ¹ng cÃ´ng thá»©c toÃ¡n há»c Ä‘á»ƒ so sÃ¡nh vá»›i vÄƒn báº£n máº«u (Reference).
4.  **Model-based Evaluation (LLM-as-a-Judge):** DÃ¹ng má»™t LLM máº¡nh hÆ¡n (vÃ­ dá»¥ GPT-4) Ä‘á»ƒ cháº¥m Ä‘iá»ƒm LLM yáº¿u hÆ¡n.

---

## 3. Metrics truyá»n thá»‘ng (BLEU, ROUGE)
Xuáº¥t phÃ¡t tá»« dá»‹ch mÃ¡y vÃ  tÃ³m táº¯t vÄƒn báº£n.
*   **BLEU (Bilingual Evaluation Understudy):** Äáº¿m sá»‘ tá»« (n-grams) trÃ¹ng láº·p giá»¯a cÃ¢u dá»± Ä‘oÃ¡n vÃ  cÃ¢u máº«u. ChÃº trá»ng Ä‘á»™ chÃ­nh xÃ¡c (Precision).
*   **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** TÆ°Æ¡ng tá»± BLEU nhÆ°ng chÃº trá»ng Ä‘á»™ bao phá»§ (Recall). ThÆ°á»ng dÃ¹ng cho tÃ³m táº¯t.
*   **NhÆ°á»£c Ä‘iá»ƒm:** Chá»‰ báº¯t lá»—i chÃ­nh táº£/tá»« ngá»¯, khÃ´ng hiá»ƒu ngá»¯ nghÄ©a. (VÃ­ dá»¥: "I love you" vÃ  "I adore you" cÃ³ Ã½ nghÄ©a giá»‘ng nhau nhÆ°ng Ä‘iá»ƒm BLEU sáº½ tháº¥p vÃ¬ khÃ´ng trÃ¹ng tá»«). -> *KhÃ´ng cÃ²n phÃ¹ há»£p cho LLM hiá»‡n Ä‘áº¡i.*

---

## 4. LLM-as-a-Judge ğŸ‘¨â€âš–ï¸
PhÆ°Æ¡ng phÃ¡p phá»• biáº¿n nháº¥t hiá»‡n nay Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Chatbot.

**CÆ¡ cháº¿:**
*   ÄÆ°a Prompt + Response cá»§a mÃ´ hÃ¬nh cáº§n cháº¥m + Reference (náº¿u cÃ³) + TiÃªu chÃ­ cháº¥m (Rubric) cho GPT-4 (hoáº·c Claude 3.5).
*   YÃªu cáº§u GPT-4 Ä‘Ã³ng vai giÃ¡m kháº£o, cháº¥m Ä‘iá»ƒm (thang 1-5 hoáº·c 1-10) vÃ  Ä‘Æ°a ra lá»i giáº£i thÃ­ch (Rationale).

**Æ¯u Ä‘iá»ƒm:**
*   Nhanh, ráº», scale tá»‘t.
*   Hiá»ƒu Ä‘Æ°á»£c ngá»¯ nghÄ©a vÃ  sáº¯c thÃ¡i.
*   Äá»™ tÆ°Æ¡ng quan (Correlation) cao vá»›i Ä‘Ã¡nh giÃ¡ cá»§a con ngÆ°á»i.

**NhÆ°á»£c Ä‘iá»ƒm:**
*   **Position Bias:** ThÆ°á»ng Æ°u tiÃªn cÃ¢u tráº£ lá»i xuáº¥t hiá»‡n trÆ°á»›c (hoáº·c sau).
*   **Verbosity Bias:** ThÃ­ch cÃ¢u tráº£ lá»i dÃ i dÃ²ng hÆ¡n.
*   **Self-preference Bias:** ThÃ­ch vÄƒn phong giá»‘ng chÃ­nh nÃ³.

---

## 5. Benchmarks phá»• biáº¿n
CÃ¡c bá»™ Ä‘á» thi tiÃªu chuáº©n Ä‘á»ƒ so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh:
*   **MMLU (Massive Multitask Language Understanding):** Kiáº¿n thá»©c tá»•ng quÃ¡t (ToÃ¡n, LÃ½, HÃ³a, Sá»­...).
*   **GSM8K:** ToÃ¡n tiá»ƒu há»c (cáº§n suy luáº­n nhiá»u bÆ°á»›c).
*   **HumanEval / MBPP:** Láº­p trÃ¬nh Python.
*   **Chatbot Arena (LMSYS):** Báº£ng xáº¿p háº¡ng dá»±a trÃªn bÃ¬nh chá»n mÃ¹ (Blind test) cá»§a cá»™ng Ä‘á»“ng ngÆ°á»i dÃ¹ng thá»±c táº¿ (Elo rating). *ÄÃ¢y Ä‘Æ°á»£c coi lÃ  thÆ°á»›c Ä‘o uy tÃ­n nháº¥t hiá»‡n nay.*

---
*BiÃªn soáº¡n bá»Ÿi Pixiboss - Dá»±a trÃªn Stanford CME 295.*
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [CS229: XÃ¢y Dá»±ng MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n (LLMs) ğŸ§ ](aero_LLM_00_Overview.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_00_Overview.md) |
| [Lecture 1: Transformer Architecture ğŸ¤–](aero_LLM_01_Transformer.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Transformer.md) |
| [Lecture 2: Transformer Tricks & BERT ğŸ› ï¸](aero_LLM_02_Transformer_Tricks.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Transformer_Tricks.md) |
| [Lecture 3: Large Language Models (LLMs) & Inference ğŸš€](aero_LLM_03_Large_Language_Models.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_Large_Language_Models.md) |
| [Lecture 4: LLM Training - Pre-training ğŸ‹ï¸](aero_LLM_04_Training_Pretraining.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_Training_Pretraining.md) |
| [Lecture 5: LLM Tuning (SFT & Parameter Efficient) ğŸ›ï¸](aero_LLM_05_Tuning_PEFT.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Tuning_PEFT.md) |
| [Lecture 6: LLM Reasoning ğŸ§ ](aero_LLM_06_Reasoning.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_Reasoning.md) |
| [Lecture 7: Agentic LLMs & Tool Use ğŸ› ï¸](aero_LLM_07_Agentic_LLMs.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Agentic_LLMs.md) |
| ğŸ“Œ **[Lecture 8: LLM Evaluation âš–ï¸](aero_LLM_08_Evaluation.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_08_Evaluation.md) |
| [Lecture 9: Recap & Current Trends ğŸ”®](aero_LLM_09_Trends.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_Trends.md) |
| [ğŸ› ï¸ Top 12 Repo Quan Trá»ng Cho AI Engineer Tá»‘i Æ¯u LLM](aero_LLM_10_Essential_Tools.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_Essential_Tools.md) |
| [ChÆ°Æ¡ng 1: Tá»•ng Quan Vá» Large Language Models (LLMs) ğŸ§ ](aero_LLM_chapter01_overview_detailed.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_chapter01_overview_detailed.md) |
| [ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t Cá»§a Viá»‡c Huáº¥n Luyá»‡n LLMs ğŸ›ï¸](aero_LLM_chapter02_5pillars_part1.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_chapter02_5pillars_part1.md) |
| [ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t - Part 2 (Evaluation & Systems)](aero_LLM_chapter02_5pillars_part2.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_chapter02_5pillars_part2.md) |
| [ChÆ°Æ¡ng 3: Pre-training â†’ Post-training Pipeline ğŸ”„](aero_LLM_chapter03_training_pipeline.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_chapter03_training_pipeline.md) |
| [ChÆ°Æ¡ng 4 & 5: Mechanisms & Evaluation ğŸ”§ğŸ“Š](aero_LLM_chapter04_05_mechanisms_eval.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_chapter04_05_mechanisms_eval.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

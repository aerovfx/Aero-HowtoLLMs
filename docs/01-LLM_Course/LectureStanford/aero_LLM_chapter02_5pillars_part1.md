
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../../index.md) > [01 LLM Course](../../index.md) > [LectureStanford](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../../index.md)
- [ğŸ“š Module 01: LLM Course](../../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t Cá»§a Viá»‡c Huáº¥n Luyá»‡n LLMs ğŸ›ï¸

> **KhÃ³a há»c:** CS229 - Machine Learning (Stanford)  
> **ChÆ°Æ¡ng:** 2/5 - Training Foundations  
> **Má»¥c tiÃªu:** Hiá»ƒu 5 yáº¿u tá»‘ cá»‘t lÃµi cáº§n thiáº¿t Ä‘á»ƒ xÃ¢y dá»±ng LLM thÃ nh cÃ´ng

---

## ğŸ“š Ná»™i Dung ChÆ°Æ¡ng

1. [Tá»•ng Quan 5 Trá»¥ Cá»™t](#tá»•ng-quan-5-trá»¥-cá»™t)
2. [Trá»¥ Cá»™t 1: Architecture](#trá»¥-cá»™t-1-architecture-kiáº¿n-trÃºc)
3. [Trá»¥ Cá»™t 2: Training Loss & Algorithm](#trá»¥-cá»™t-2-training-loss--algorithm)
4. [Trá»¥ Cá»™t 3: Data](#trá»¥-cá»™t-3-data-dá»¯-liá»‡u)
5. [Trá»¥ Cá»™t 4: Evaluation](#trá»¥-cá»™t-4-evaluation-Ä‘Ã¡nh-giÃ¡)
6. [Trá»¥ Cá»™t 5: Systems](#trá»¥-cá»™t-5-systems-há»‡-thá»‘ng)
7. [Academia vs Industry](#academia-vs-industry)

---

## Tá»•ng Quan 5 Trá»¥ Cá»™t

### **MÃ´ HÃ¬nh SUCCESS = f(Architecture, Loss, Data, Evaluation, Systems)**

```
        ğŸ—ï¸ Architecture
             â†“
        ğŸ“‰ Training Loss
             â†“
        ğŸ“Š Data â†â”€â”€â”€â”€â”€â”€â†’ ğŸ“ˆ Evaluation
             â†“                â†‘
        âš™ï¸ Systems â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
        ğŸ¯ Production LLM
```

### **VÃ­ Dá»¥ Thá»±c Táº¿: GPT-4**

| Trá»¥ Cá»™t | GPT-4 Implementation |
|---------|---------------------|
| **Architecture** | MoE Transformer (8 experts, Top-2) |
| **Loss** | Cross-entropy + PPO (RLHF) |
| **Data** | ~13T tokens (web, books, code) |
| **Evaluation** | MMLU, HumanEval, custom benchmarks |
| **Systems** | 10,000+ A100 GPUs, 100+ days |

---

## Trá»¥ Cá»™t 1: Architecture (Kiáº¿n TrÃºc)

### **Äá»‹nh NghÄ©a**

**Architecture** = Thiáº¿t káº¿ máº¡ng neural, cáº¥u trÃºc tÃ­nh toÃ¡n tá»« input â†’ output.

### **Evolution of LLM Architectures**

```
2017: Transformer (Original)
  â†“
2018: GPT-1 (Decoder-only)
  â†“
2018: BERT (Encoder-only)
  â†“
2019: GPT-2 (Scaled decoder)
  â†“
2020: GPT-3 (Dense transformer, 175B)
  â†“
2021: Switch Transformer (MoE, 1.6T)
  â†“
2023: GPT-4 (MoE + Multimodal)
  â†“
2024: Gemini Ultra (Unified multimodal)
```

### **Key Architectural Components**

#### **A. Attention Mechanisms**

**1. Multi-Head Attention:**
```python
# Pseudo-code
def multi_head_attention(x, num_heads=8):
    # Split into multiple heads
    Q, K, V = split_heads(x, num_heads)
    
    # Scaled dot-product attention
    scores = (Q @ K.T) / sqrt(d_k)
    attn = softmax(scores)
    output = attn @ V
    
    # Concat and project
    return concat_heads(output)
```

**2. Grouped Query Attention (GQA):**
- Used in Llama 2
- Fewer K, V heads than Q heads
- Faster inference

**3. Multi-Query Attention (MQA):**
- Single K, V for all Q heads
- Maximum speed

#### **B. Position Encodings**

| Type | Formula | Used In |
|------|---------|---------|
| **Absolute** | sin/cos | Original Transformer |
| **Relative** | Learnable | T5 |
| **RoPE** | Rotary | Llama, GPT-NeoX |
| **ALiBi** | Attention bias | BLOOM |

**RoPE (Rotary Position Embedding):**
```python
def rope(x, positions):
    # Rotate pairs of dimensions
    freqs = 1.0 / (10000 ** (arange(0, d, 2) / d))
    angles = positions[:, None] * freqs[None, :]
    
    # Apply rotation
    cos, sin = cos(angles), sin(angles)
    x_rotated = rotate_half(x)
    return x * cos + x_rotated * sin
```

#### **C. Mixture of Experts (MoE)**

**Architecture:**
```
Input
  â†“
Gate/Router â”€â”€â†’ Gating scores [sâ‚€, sâ‚, ..., sâ‚‡]
  â†“
Top-K (k=2) â”€â”€â†’ Select 2 highest scores
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Expert0â”‚ Expert1â”‚ Expert2â”‚ Expert3â”‚  â† Only 2 are active
â”‚ Expert4â”‚ Expert5â”‚ Expert6â”‚ Expert7â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
Weighted sum = wâ‚€Â·Eâ‚€(x) + wâ‚Â·Eâ‚(x)
  â†“
Output
```

**Benefits:**
- âœ… Efficient: Only ~12.5% params active (2/16 experts)
- âœ… Specialized: Each expert learns different patterns
- âœ… Scalable: Easy to add more experts

** Challenges:**
- âŒ Load balancing: Some experts underutilized
- âŒ Training complexity: Needs auxiliary loss
- âŒ Serving: Higher memory requirements

### **GPT-4 Architecture Deep Dive**

**Specs (estimated):**
```python
{
    "model_type": "MoE Transformer",
    "num_layers": 120,
    "hidden_size": 18432,
    "num_attention_heads": 128,
    "head_dim": 144,
    "num_experts": 8,
    "experts_active": 2,
    "vocab_size": 100277,
    "context_length": 32768,  # up to 128K
    "total_params": "1.76T",
    "active_params": "~220B per token"
}
```

**Visualization trong llm_viz:**
- Expert grid: 2Ã—4 layout
- Router visualization
- Color coding (active=green, inactive=gray)
- Top-K selection animation

---

## Trá»¥ Cá»™t 2: Training Loss & Algorithm

### **Training Loss**

**Primary: Cross-Entropy Loss**

```python
def cross_entropy_loss(logits, targets):
    """
    logits: [batch, seq_len, vocab_size]
    targets: [batch, seq_len]
    """
    # Softmax to get probabilities
    probs = softmax(logits, dim=-1)
    
    # Negative log likelihood
    loss = -log(probs[range(len(targets)), targets])
    
    return loss.mean()
```

**Formula:**
```
L = -âˆ‘áµ¢ log P(xáµ¢ | xâ‚, ..., xáµ¢â‚‹â‚)
```

**Objective:** Maximize likelihood of correct next token

### **Optimization Algorithms**

#### **A. Adam (GPT-2, GPT-3)**

```python
# Adam parameters
lr = 6e-4  # learning rate
beta1 = 0.9
beta2 = 0.95
epsilon = 1e-8
weight_decay = 0.1

# Update rule
m = beta1 * m + (1 - beta1) * grad
v = beta2 * v + (1 - beta2) * grad**2
update = lr * m / (sqrt(v) + epsilon)
params -= update
```

#### **B. AdamW (Modern LLMs)**

- Decoupled weight decay
- Better generalization
- **Used in:** GPT-4, Llama, Gemini

#### **C. Adafactor (T5, PaLM)**

- Memory-efficient
- Factorized second moments
- Good for huge models

### **Learning Rate Schedule**

**Cosine Decay with Warmup:**
```
Warmup (0-2000 steps):
  lr = base_lr * (step / warmup_steps)

Cosine Decay:
  lr = min_lr + 0.5 * (max_lr - min_lr) * 
       (1 + cos(Ï€ * (step - warmup) / total_steps))
```

**GPT-3 Schedule:**
- Warmup: 375M tokens
- Peak LR: 6e-4
- Decay to: 6e-5
- Total: 300B tokens

### **Gradient Clipping**

```python
# Prevent gradient explosion
max_grad_norm = 1.0
torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
```

### **Mixed Precision Training**

**BF16 (Brain Float 16):**
```python
from torch.cuda.amp import autocast

with autocast(dtype=torch.bfloat16):
    logits = model(inputs)
    loss = criterion(logits, targets)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**Benefits:**
- 2Ã— faster training
- 2Ã— less memory
- No loss in accuracy

---

## Trá»¥ Cá»™t 3: Data (Dá»¯ Liá»‡u)

### **"Data is the New Oil" ğŸ›¢ï¸**

**Táº§m quan trá»ng:**
> "Having 10Ã— more data is often better than having a 10Ã— better algorithm."  
> â€” Andrew Ng

### **Data Sources for LLMs**

| Source | Size | Quality | Examples |
|--------|------|---------|----------|
| **Web Crawl** | ~50T tokens | Low-Medium | Common Crawl |
| **Books** | ~100B tokens | High | Books3, Gutenberg |
| **Code** | ~500B tokens | High | GitHub, StackOverflow |
| **Wikipedia** | ~6B tokens | Very High | Wikipedia dumps |
| **Papers** | ~200B tokens | Very High | arXiv, PubMed |
| **Conversations** | ~10B tokens | Variable | Reddit, forums |

**GPT-3 Training Data:**
```
Common Crawl (filtered): 410B tokens (60%)
WebText2: 19B tokens (22%)
Books1: 12B tokens (8%)
Books2: 55B tokens (8%)
Wikipedia: 3B tokens (3%)
```

### **Data Preprocessing Pipeline**

```
Raw Data
  â†“
1. Deduplication
  â”œâ”€â”€ Exact match removal
  â”œâ”€â”€ Near-duplicate detection (MinHash)
  â””â”€â”€ URL deduplication
  â†“
2. Quality Filtering
  â”œâ”€â”€ Language detection
  â”œâ”€â”€ Perplexity filtering
  â”œâ”€â”€ Toxicity filtering
  â””â”€â”€ PII removal
  â†“
3. Balancing
  â”œâ”€â”€ Domain distribution
  â”œâ”€â”€ Language distribution
  â””â”€â”€ Temporal distribution
  â†“
4. Tokenization
  â””â”€â”€ BPE/SentencePiece
  â†“
Clean Training Data
```

### **Data Quality Metrics**

**Perplexity-based filtering:**
```python
# Train small model on high-quality data
ref_model = train_tiny_gpt(wikipedia + books)

# Filter web data
for doc in web_crawl:
    perplexity = ref_model.perplexity(doc)
    if perplexity < threshold:  # e.g., 1000
        keep(doc)
```

### **Synthetic Data**

**Use cases:**
1. **Math:** Generate problems + solutions
2. **Code:** Create coding challenges
3. **Reasoning:** Chain-of-thought examples

**Example (GPT-4):**
```python
# Generate math problems
prompt = "Generate 100 algebra word problems with step-by-step solutions"
synthetic_data = gpt4.generate(prompt)

# Filter for quality
high_quality = filter_by_correctness(synthetic_data)
```

### **Data Privacy & Ethics**

**Challenge:**
- Personal information in training data
- Copyright issues (books, code)
- Bias amplification

**Solutions:**
- PII removal
- Licensing compliance
- Bias audits
- Opt-out mechanisms

---

## (Continued in next message due to length...)
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

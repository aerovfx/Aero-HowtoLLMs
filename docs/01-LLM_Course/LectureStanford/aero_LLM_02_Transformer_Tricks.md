
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../../index.md) > [01 LLM Course](../../index.md) > [LectureStanford](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../../index.md)
- [ğŸ“š Module 01: LLM Course](../../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Lecture 2: Transformer Tricks & BERT ğŸ› ï¸

> **TÃ³m táº¯t tá»« khÃ³a há»c Stanford CME 295: Transformers & Large Language Models.**
> BÃ i giáº£ng nÃ y Ä‘i sÃ¢u vÃ o cÃ¡c cáº£i tiáº¿n ká»¹ thuáº­t giÃºp Transformer hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n vÃ  sá»± ra Ä‘á»i cá»§a cÃ¡c mÃ´ hÃ¬nh Encoder-only nhÆ° BERT.

---

## ğŸ“š Má»¥c Lá»¥c
1. [Cáº£i tiáº¿n Positional Embeddings (RoPE, ALiBi)](#1-cáº£i-tiáº¿n-positional-embeddings)
2. [Cáº£i tiáº¿n Normalization (LayerNorm vs RMSNorm)](#2-cáº£i-tiáº¿n-normalization)
3. [Tá»‘i Æ°u Attention (Sliding Window, GQA)](#3-tá»‘i-Æ°u-attention)
4. [CÃ¡c há» mÃ´ hÃ¬nh Transformer](#4-cÃ¡c-há»-mÃ´-hÃ¬nh-transformer)
5. [BERT & Encoder-only Models](#5-bert--encoder-only-models)

---

## 1. Cáº£i tiáº¿n Positional Embeddings
Trong bÃ i bÃ¡o gá»‘c, Positional Encoding Ä‘Æ°á»£c cá»™ng trá»±c tiáº¿p vÃ o Input Embedding. Tuy nhiÃªn, cÃ¡c mÃ´ hÃ¬nh hiá»‡n Ä‘áº¡i sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p tiÃªn tiáº¿n hÆ¡n Ä‘á»ƒ xá»­ lÃ½ tá»‘t hÆ¡n Ä‘á»™ dÃ i chuá»—i thay Ä‘á»•i.

### Learned Positional Embedding
*   Há»c má»™t vector riÃªng cho má»—i vá»‹ trÃ­.
*   **Háº¡n cháº¿:** KhÃ´ng thá»ƒ má»Ÿ rá»™ng (extrapolate) cho cÃ¡c chuá»—i dÃ i hÆ¡n Ä‘á»™ dÃ i Ä‘Ã£ tháº¥y trong khi huáº¥n luyá»‡n.

### Rotary Positional Embedding (RoPE) ğŸ”„
*   **Hiá»‡n Ä‘áº¡i nháº¥t:** ÄÆ°á»£c sá»­ dá»¥ng trong Llama, Mistral, PaLM.
*   **CÆ¡ cháº¿:** Thay vÃ¬ cá»™ng vector vá»‹ trÃ­, RoPE **xoay** vector Query vÃ  Key má»™t gÃ³c phá»¥ thuá»™c vÃ o vá»‹ trÃ­ cá»§a chÃºng.
*   **Æ¯u Ä‘iá»ƒm:**
    *   MÃ´ hÃ¬nh há»c Ä‘Æ°á»£c **khoáº£ng cÃ¡ch tÆ°Æ¡ng Ä‘á»‘i** (relative distance) giá»¯a cÃ¡c tá»« má»™t cÃ¡ch tá»± nhiÃªn thÃ´ng qua tÃ­ch vÃ´ hÆ°á»›ng (dot product).
    *   Kháº£ nÄƒng má»Ÿ rá»™ng (extrapolation) tá»‘t hÆ¡n cho cÃ¡c chuá»—i dÃ i.

### ALiBi (Attention with Linear Biases)
*   ThÃªm má»™t bias tÄ©nh vÃ o ma tráº­n Attention score dá»±a trÃªn khoáº£ng cÃ¡ch giá»¯a hai token.
*   ÄÆ¡n giáº£n, khÃ´ng cáº§n há»c tham sá»‘, nhÆ°ng RoPE hiá»‡n nay phá»• biáº¿n hÆ¡n.

---

## 2. Cáº£i tiáº¿n Normalization
Chuáº©n hÃ³a (Normalization) giÃºp mÃ´ hÃ¬nh há»™i tá»¥ nhanh vÃ  á»•n Ä‘á»‹nh hÆ¡n.

*   **Post-Norm (Gá»‘c):** Norm sau khi cá»™ng nhÃ¡nh dÆ° (Residual).
*   **Pre-Norm (Hiá»‡n Ä‘áº¡i):** Norm **trÆ°á»›c** khi vÃ o Attention/FFN. GiÃºp huáº¥n luyá»‡n á»•n Ä‘á»‹nh hÆ¡n vá»›i cÃ¡c mÃ´ hÃ¬nh ráº¥t sÃ¢u.
*   **RMSNorm (Root Mean Square Norm):** Má»™t biáº¿n thá»ƒ cá»§a LayerNorm, bá» qua viá»‡c trá»« giÃ¡ trá»‹ trung bÃ¬nh (mean), chá»‰ chia cho cÄƒn báº­c hai trung bÃ¬nh bÃ¬nh phÆ°Æ¡ng.
    *   *Æ¯u Ä‘iá»ƒm:* TÃ­nh toÃ¡n nhanh hÆ¡n, hiá»‡u quáº£ tÆ°Æ¡ng Ä‘Æ°Æ¡ng. ÄÆ°á»£c dÃ¹ng trong Llama, Gopher.

---

## 3. Tá»‘i Æ°u Attention
Self-Attention cÃ³ Ä‘á»™ phá»©c táº¡p $O(N^2)$ (vá»›i N lÃ  Ä‘á»™ dÃ i chuá»—i), ráº¥t tá»‘n kÃ©m khi chuá»—i dÃ i.

*   **Sliding Window Attention (Cá»­a sá»• trÆ°á»£t):** Má»—i token chá»‰ nhÃ¬n tháº¥y cÃ¡c token lÃ¢n cáº­n trong má»™t cá»­a sá»• nháº¥t Ä‘á»‹nh (vÃ­ dá»¥: Mistral). Giáº£m chi phÃ­ tÃ­nh toÃ¡n nhÆ°ng váº«n giá»¯ Ä‘Æ°á»£c kháº£ nÄƒng hiá»ƒu ngá»¯ cáº£nh nhá» cÃ¡c lá»›p chá»“ng lÃªn nhau (tÆ°Æ¡ng tá»± Receptive field trong CNN).
*   **Grouped Query Attention (GQA):**
    *   *Multi-Head Attention (MHA):* Má»—i Head cÃ³ Q, K, V riÃªng. (Tá»‘n bá»™ nhá»› KV Cache).
    *   *Multi-Query Attention (MQA):* Táº¥t cáº£ Heads chia sáº» chung 1 bá»™ K, V. (Tiáº¿t kiá»‡m nhá»›, giáº£m cháº¥t lÆ°á»£ng).
    *   *GQA:* Trung hÃ²a. Chia Heads thÃ nh cÃ¡c nhÃ³m, má»—i nhÃ³m chia sáº» chung K, V. (CÃ¢n báº±ng tá»‘t nháº¥t giá»¯a tá»‘c Ä‘á»™ vÃ  cháº¥t lÆ°á»£ng, dÃ¹ng trong Llama-2-70b, Llama-3).

---

## 4. CÃ¡c há» mÃ´ hÃ¬nh Transformer
Dá»±a trÃªn kiáº¿n trÃºc, cÃ³ 3 nhÃ¡nh phÃ¡t triá»ƒn chÃ­nh:

1.  **Encoder-Decoder (T5, BART):** Giá»i cÃ¡c tÃ¡c vá»¥ "Text-to-Text" nhÆ° dá»‹ch thuáº­t, tÃ³m táº¯t.
2.  **Encoder-only (BERT, RoBERTa):** Chá»‰ dÃ¹ng pháº§n Encoder. Giá»i cÃ¡c tÃ¡c vá»¥ "Hiá»ƒu ngÃ´n ngá»¯" (NLU) nhÆ° phÃ¢n loáº¡i, tÃ¬m kiáº¿m, NER.
3.  **Decoder-only (GPT, Llama):** Chá»‰ dÃ¹ng pháº§n Decoder. Giá»i cÃ¡c tÃ¡c vá»¥ "Sinh ngÃ´n ngá»¯" (Generative). ÄÃ¢y lÃ  nhÃ¡nh thá»‘ng trá»‹ hiá»‡n nay cá»§a LLMs.

---

## 5. BERT & Encoder-only Models
**BERT (Bidirectional Encoder Representations from Transformers)** lÃ  tÆ°á»£ng Ä‘Ã i cá»§a dÃ²ng Encoder-only.

### Äáº·c Ä‘iá»ƒm:
*   **Bidirectional (Hai chiá»u):** Má»—i tá»« nhÃ¬n tháº¥y toÃ n bá»™ cÃ¢u (trÃ¡i vÃ  pháº£i) cÃ¹ng lÃºc.
*   **Tokens Ä‘áº·c biá»‡t:** `[CLS]` (Ä‘áº¡i diá»‡n cho toÃ n cÃ¢u, dÃ¹ng Ä‘á»ƒ phÃ¢n loáº¡i), `[SEP]` (ngÄƒn cÃ¡ch cÃ¢u).

### QuÃ¡ trÃ¬nh Huáº¥n luyá»‡n (Pre-training)
BERT Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i 2 tÃ¡c vá»¥ tá»± giÃ¡m sÃ¡t (Self-supervised):
1.  **Masked Language Modeling (MLM):** áº¨n Ä‘i 15% sá»‘ tá»« trong cÃ¢u, yÃªu cáº§u mÃ´ hÃ¬nh Ä‘iá»n vÃ o chá»— trá»‘ng. (GiÃºp mÃ´ hÃ¬nh há»c ngá»¯ cáº£nh hai chiá»u).
2.  **Next Sentence Prediction (NSP):** Cho 2 cÃ¢u A vÃ  B, há»i B cÃ³ pháº£i lÃ  cÃ¢u tiáº¿p theo cá»§a A khÃ´ng? (GiÃºp mÃ´ hÃ¬nh hiá»ƒu má»‘i quan há»‡ giá»¯a cÃ¡c cÃ¢u).

### Fine-tuning (Tinh chá»‰nh)
Sau khi Pre-training, BERT táº¡o ra cÃ¡c vector embedding ráº¥t tá»‘t. Ta chá»‰ cáº§n gáº¯n thÃªm má»™t lá»›p Linear nhá» phÃ­a sau Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n cá»¥ thá»ƒ (Sentiment Analysis, Question Answering) vá»›i ráº¥t Ã­t dá»¯ liá»‡u.

### Biáº¿n thá»ƒ
*   **DistilBERT:** DÃ¹ng ká»¹ thuáº­t *Distillation* (ChÆ°ng cáº¥t) Ä‘á»ƒ táº¡o mÃ´ hÃ¬nh nhá» hÆ¡n, nhanh hÆ¡n nhÆ°ng giá»¯ Ä‘Æ°á»£c 97% hiá»‡u nÄƒng cá»§a BERT.
*   **RoBERTa:** Tá»‘i Æ°u hÃ³a BERT (bá» NSP, train lÃ¢u hÆ¡n, dá»¯ liá»‡u nhiá»u hÆ¡n) -> Hiá»‡u nÄƒng tá»‘t hÆ¡n.

---
*BiÃªn soáº¡n bá»Ÿi Pixiboss - Dá»±a trÃªn Stanford CME 295.*
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [CS229: XÃ¢y Dá»±ng MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n (LLMs) ğŸ§ ](aero_LLM_00_Overview.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_00_Overview.md) |
| [Lecture 1: Transformer Architecture ğŸ¤–](aero_LLM_01_Transformer.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Transformer.md) |
| ğŸ“Œ **[Lecture 2: Transformer Tricks & BERT ğŸ› ï¸](aero_LLM_02_Transformer_Tricks.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Transformer_Tricks.md) |
| [Lecture 3: Large Language Models (LLMs) & Inference ğŸš€](aero_LLM_03_Large_Language_Models.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_Large_Language_Models.md) |
| [Lecture 4: LLM Training - Pre-training ğŸ‹ï¸](aero_LLM_04_Training_Pretraining.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_Training_Pretraining.md) |
| [Lecture 5: LLM Tuning (SFT & Parameter Efficient) ğŸ›ï¸](aero_LLM_05_Tuning_PEFT.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Tuning_PEFT.md) |
| [Lecture 6: LLM Reasoning ğŸ§ ](aero_LLM_06_Reasoning.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_Reasoning.md) |
| [Lecture 7: Agentic LLMs & Tool Use ğŸ› ï¸](aero_LLM_07_Agentic_LLMs.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Agentic_LLMs.md) |
| [Lecture 8: LLM Evaluation âš–ï¸](aero_LLM_08_Evaluation.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_Evaluation.md) |
| [Lecture 9: Recap & Current Trends ğŸ”®](aero_LLM_09_Trends.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_Trends.md) |
| [ğŸ› ï¸ Top 12 Repo Quan Trá»ng Cho AI Engineer Tá»‘i Æ¯u LLM](aero_LLM_10_Essential_Tools.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_Essential_Tools.md) |
| [ChÆ°Æ¡ng 1: Tá»•ng Quan Vá» Large Language Models (LLMs) ğŸ§ ](aero_LLM_chapter01_overview_detailed.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_chapter01_overview_detailed.md) |
| [ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t Cá»§a Viá»‡c Huáº¥n Luyá»‡n LLMs ğŸ›ï¸](aero_LLM_chapter02_5pillars_part1.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_chapter02_5pillars_part1.md) |
| [ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t - Part 2 (Evaluation & Systems)](aero_LLM_chapter02_5pillars_part2.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_chapter02_5pillars_part2.md) |
| [ChÆ°Æ¡ng 3: Pre-training â†’ Post-training Pipeline ğŸ”„](aero_LLM_chapter03_training_pipeline.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_chapter03_training_pipeline.md) |
| [ChÆ°Æ¡ng 4 & 5: Mechanisms & Evaluation ğŸ”§ğŸ“Š](aero_LLM_chapter04_05_mechanisms_eval.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_chapter04_05_mechanisms_eval.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

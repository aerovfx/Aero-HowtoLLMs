# Lecture 3: Large Language Models (LLMs) & Inference ๐

> **Tรณm tแบฏt tแปซ khรณa hแปc Stanford CME 295: Transformers & Large Language Models.**
> Bรi giแบฃng nรy tแบญp trung vรo cรกc mรด hรฌnh ngรดn ngแปฏ lแปn (Decoder-only), cรกch mแป rแปng quy mรด (Scaling), kแปน thuแบญt Prompting vร tแปi ฦฐu hรณa suy luแบญn (Inference).

---

## ๐ Mแปฅc Lแปฅc
1. [ฤแปnh nghฤฉa LLM](#1-ฤแปnh-nghฤฉa-llm)
2. [Mixture of Experts (MoE)](#2-mixture-of-experts-moe)
3. [Kแปน thuแบญt Prompting & In-context Learning](#3-kแปน-thuแบญt-prompting--in-context-learning)
4. [Decoding Strategies (Chiแบฟn lฦฐแปฃc giแบฃi mรฃ)](#4-decoding-strategies-chiแบฟn-lฦฐแปฃc-giแบฃi-mรฃ)
5. [Tแปi ฦฐu hรณa Inference (KV Cache, Speculative Decoding)](#5-tแปi-ฦฐu-hรณa-inference)

---

## 1. ฤแปnh nghฤฉa LLM
**Large Language Model (LLM)** thฦฐแปng รกm chแป cรกc mรด hรฌnh:
*   Lร **Language Model** (mรด hรฌnh xรกc suแบฅt dแปฑ ฤoรกn tแปซ tiแบฟp theo).
*   Cรณ kรญch thฦฐแปc **Lแปn** (hรng tแปท tham sแป, huแบฅn luyแปn trรชn hรng nghรฌn tแปท tokens).
*   Kiแบฟn trรบc chแปง ฤแบกo: **Decoder-only Transformer** (bแป qua phแบงn Encoder vร Cross-Attention).

*Vรญ dแปฅ:* GPT-3, PaLM, Llama, Mistral.

---

## 2. Mixture of Experts (MoE) ๐ง
Khi mรด hรฌnh quรก lแปn, chi phรญ tรญnh toรกn cho mแปi lแบงn suy luแบญn rแบฅt cao. **MoE** lร giแบฃi phรกp ฤแป "Mแป rแปng quy mรด mร khรดng tฤng chi phรญ suy luแบญn tฦฐฦกng แปฉng".

*   **ร tฦฐแปng:** Thay lแปp FFN (Feed Forward Network) dรy ฤแบทc bแบฑng nhiแปu "Chuyรชn gia" nhแป (Experts).
*   **Router (Gate):** Mแปt mแบกng con quyแบฟt ฤแปnh xem vแปi mแปi token ฤแบงu vรo, nรชn gแปญi nรณ cho chuyรชn gia nรo xแปญ lรฝ (Vรญ dแปฅ: Cรขu hแปi Toรกn -> Gแปญi cho chuyรชn gia Toรกn).
*   **Sparse Activation (Kรญch hoแบกt thฦฐa):** Dรน cรณ tแปng sแป tham sแป khแปng lแป (vรญ dแปฅ 8x7B), nhฦฐng mแปi lแบงn chแบกy chแป kรญch hoแบกt mแปt phแบงn nhแป (vรญ dแปฅ 2 experts/token).
*   **Lแปฃi รญch:** Training nhanh hฦกn, Inference rแบป hฦกn so vแปi mรด hรฌnh Dense cรนng kรญch thฦฐแปc.
*   **Thรกch thแปฉc:** Cแบงn cรขn bแบฑng tแบฃi (Load balancing) ฤแป trรกnh viแปc mแปt vรi chuyรชn gia lรm viแปc quรก sแปฉc cรฒn sแป khรกc thรฌ ngแปi chฦกi (Routing collapse).

---

## 3. Kแปน thuแบญt Prompting & In-context Learning
LLM cรณ khแบฃ nฤng hแปc tแปซ ngแปฏ cแบฃnh (In-context Learning) mร khรดng cแบงn cแบญp nhแบญt trแปng sแป.

*   **Zero-shot:** Ra lแปnh trแปฑc tiแบฟp (VD: "Dแปch cรขu nรy sang tiแบฟng Anh").
*   **Few-shot:** Cung cแบฅp vรi vรญ dแปฅ mแบซu trฦฐแปc khi hแปi (VD: "Q: Hi A: Chรo / Q: Bye A: Tแบกm biแปt / Q: Thanks A: ...").
*   **Chain-of-Thought (CoT):** Yรชu cแบงu mรด hรฌnh "suy nghฤฉ tแปซng bฦฐแปc" (Let's think step by step). Giรบp cแบฃi thiแปn ฤรกng kแป khแบฃ nฤng giแบฃi toรกn vร suy luแบญn logic.
*   **Self-Consistency:** Hแปi cรนng mแปt cรขu nhiแปu lแบงn (sampling) vร chแปn cรขu trแบฃ lแปi xuแบฅt hiแปn nhiแปu nhแบฅt (Majority voting).

---

## 4. Decoding Strategies (Chiแบฟn lฦฐแปฃc giแบฃi mรฃ)
Lรm sao chแปn tแปซ tiแบฟp theo tแปซ phรขn phแปi xรกc suแบฅt do mรด hรฌnh dแปฑ ฤoรกn?

*   **Greedy Decoding:** Luรดn chแปn tแปซ cรณ xรกc suแบฅt cao nhแบฅt. *Nhฦฐแปฃc ฤiแปm:* Dแป bแป lแบทp, vฤn bแบฃn nhรm chรกn, ฤรดi khi khรดng tแปi ฦฐu toรn cแปฅc.
*   **Beam Search:** Giแปฏ lแบกi K nhรกnh tiแปm nฤng nhแบฅt tแบกi mแปi bฦฐแปc. Tแปi ฦฐu hฦกn Greedy nhฦฐng tแปn kรฉm vร ฤรดi khi vแบซn thiแบฟu tแปฑ nhiรชn.
*   **Sampling (Lแบฅy mแบซu ngแบซu nhiรชn):** Chแปn tแปซ dแปฑa trรชn xรกc suแบฅt (cรณ tรญnh ngแบซu nhiรชn).
    *   **Temperature (Nhiแปt ฤแป):**
        *   $T \to 0$: Trแป vแป Greedy (chรญnh xรกc, รญt sรกng tแบกo).
        *   $T \to \infty$: Phรขn phแปi phแบณng (rแบฅt sรกng tแบกo nhฦฐng dแป nรณi nhแบฃm).
    *   **Top-k Sampling:** Chแป chแปn trong K tแปซ cรณ xรกc suแบฅt cao nhแบฅt.
    *   **Top-p (Nucleus) Sampling:** Chแป chแปn trong nhรณm tแปซ cรณ tแปng xรกc suแบฅt tรญch lลฉy ฤแบกt ngฦฐแปกng P (vรญ dแปฅ 0.9). *Phแป biแบฟn nhแบฅt hiแปn nay.*

---

## 5. Tแปi ฦฐu hรณa Inference
Chแบกy LLM tแปn kรฉm chแปง yแบฟu do bฤng thรดng bแป nhแป (Memory Bound).

### KV Cache
*   Trong quรก trรฌnh sinh tแปซ (Auto-regressive), cรกc token phรญa trฦฐแปc khรดng ฤแปi.
*   Thay vรฌ tรญnh lแบกi Key vร Value cho toรn bแป chuแปi mแปi lแบงn sinh tแปซ mแปi, ta **lฦฐu lแบกi (Cache)** cรกc Key/Value cลฉ vร chแป tรญnh thรชm cho token mแปi nhแบฅt.
*   Giรบp tฤng tแปc ฤแป suy luแบญn ฤรกng kแป nhฦฐng tแปn VRAM.

### PagedAttention (vLLM)
*   Quแบฃn lรฝ bแป nhแป KV Cache giแปng nhฦฐ hแป ฤiแปu hรnh quแบฃn lรฝ RAM (phรขn trang - paging).
*   Giแบฃm lรฃng phรญ bแป nhแป (fragmentation), cho phรฉp batch size lแปn hฦกn -> Tฤng throughput.

### Speculative Decoding (Giแบฃi mรฃ ฤแบงu cฦก)
*   Dรนng mแปt mรด hรฌnh nhแป (Draft model) chแบกy nhanh ฤแป "ฤoรกn" trฦฐแปc vรi tแปซ.
*   Dรนng mรด hรฌnh lแปn (Target model) ฤแป kiแปm tra lแบกi cรกc tแปซ ฤรณ song song.
*   Nแบฟu ฤoรกn ฤรบng -> Chแบฅp nhแบญn hรng loแบกt (Tฤng tแปc). Nแบฟu sai -> Sแปญa lแบกi.
*   Tแบญn dแปฅng viแปc mรด hรฌnh lแปn tรญnh toรกn song song tแปt hฦกn lร chแบกy tuแบงn tแปฑ tแปซng tแปซ.

---
*Biรชn soแบกn bแปi Pixiboss - Dแปฑa trรชn Stanford CME 295.*

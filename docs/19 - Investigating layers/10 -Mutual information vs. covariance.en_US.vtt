WEBVTT

00:02.160 --> 00:11.240
At this point in the course, you now know two quantities of bivariate statistical dependencies mutual

00:11.240 --> 00:13.320
information and covariance.

00:13.600 --> 00:19.880
If you've been wondering how those two relate to each other, then you are definitely going to enjoy

00:19.920 --> 00:21.760
learning from this video.

00:22.200 --> 00:27.600
And if you haven't wondered that question, then that's even better, because now you'll get to learn

00:27.600 --> 00:30.960
something that you didn't even know that you wanted to learn about.

00:31.480 --> 00:34.560
So I can just tell you the short version of the story.

00:34.760 --> 00:44.080
The TLDR covariance is best interpreted with normally distributed data and only measures linear interactions.

00:44.240 --> 00:52.680
Whereas mutual information is a nonlinear measure, it can detect both linear and nonlinear dependencies.

00:53.480 --> 00:59.680
But just because it detects more kinds of interactions doesn't necessarily mean that it's better.

01:00.080 --> 01:08.090
Covariance has a lot of great mathematical properties that make it really useful for analyses, including,

01:08.090 --> 01:10.530
for example, principal components analysis.

01:11.130 --> 01:19.010
Covariance is also fast and numerically stable to calculate, and it doesn't depend on any additional

01:19.010 --> 01:19.930
parameters.

01:20.530 --> 01:27.610
Anyway, the purpose of this video here is to remind you of the formula for covariance.

01:27.730 --> 01:35.410
And then I'll show some Python code demos about how these two measures of dependencies relate to each

01:35.450 --> 01:35.930
other.

01:36.410 --> 01:43.210
And then over the next several videos, you will implement both mutual information and covariance in

01:43.970 --> 01:44.610
data.

01:44.850 --> 01:46.770
So you can see how they compare.

01:47.170 --> 01:52.450
And you will see that there are some insights that can be gained from one or the other analysis.

01:52.970 --> 01:58.650
Sometimes they overlap, but sometimes they give unique patterns of results.

01:59.770 --> 02:05.740
So here is the formula for covariance between two variables X and y.

02:06.420 --> 02:10.580
It's basically just the dot product between two variables.

02:10.580 --> 02:14.060
So element wise multiplication and sum.

02:14.660 --> 02:19.260
Now you do need to make sure both variables are mean centered.

02:19.460 --> 02:25.260
Otherwise there can be a bias introduced by the mean offsets between the variables.

02:25.940 --> 02:35.580
Now this bias by the mean offset actually is a desired result for cosine similarity, but for covariance

02:35.580 --> 02:42.740
and correlation, we actually do not want mean offsets to introduce systematic biases.

02:43.460 --> 02:51.540
And scaling by n minus one is a normalization factor that prevents the covariance term from just trivially

02:51.540 --> 02:54.740
increasing by having more and more data.

02:55.580 --> 02:58.580
So you can think of this as like an average.

02:59.060 --> 03:05.780
Now the reason why you subtract one and not just divide by n has to do with the degrees of freedom of

03:05.780 --> 03:12.420
a covariance analysis, and the fact that we already know that the average value is zero.

03:13.180 --> 03:21.940
So you can see that this is a linear measure of dependency because we only have scalar multiplication

03:21.940 --> 03:23.100
and addition.

03:23.580 --> 03:28.260
There's no logs or any nonlinear operations in this formula.

03:29.060 --> 03:36.500
Now one last point that I would like to make here is that the scale of the data is preserved in covariance.

03:37.140 --> 03:42.860
So imagine we were calculating the covariance of income in a group of people.

03:43.300 --> 03:50.820
Now if both units of both variables have units of, let's say euros, then the covariance has units

03:50.820 --> 03:52.620
of euros squared.

03:53.780 --> 04:00.900
So the fact that covariance retains the scale of the data is sometimes useful, sometimes weird, sometimes

04:00.900 --> 04:04.710
annoying, But always something to be aware of.

04:05.190 --> 04:09.030
And that is going to be relevant in a few videos from now.

04:09.230 --> 04:17.310
When we directly compare covariance across transformer blocks versus mutual information across transformer

04:17.310 --> 04:17.950
blocks.

04:18.670 --> 04:26.110
Now, as I mentioned, uh, two videos ago, mutual information does not have the units of the data

04:26.350 --> 04:31.430
because mutual information is not actually calculated directly from the data.

04:31.750 --> 04:37.310
It's calculated from the probability distribution that is estimated from the data.

04:38.350 --> 04:44.790
Anyway, this is the formula for pairwise mutual a pairwise covariance.

04:44.910 --> 04:50.550
If you have a bunch of variables, a collection of variables, and you want to calculate the covariance

04:50.550 --> 04:58.590
between all pairs of variables, you can run a double for loop over all of the pairs.

04:58.870 --> 05:05.560
But because covariance is linear, then we can use linear algebra to simplify things.

05:06.200 --> 05:13.040
So if you put all of the variables into the columns of a matrix, then you can get all of the pairwise

05:13.040 --> 05:18.600
covariances simply by multiplying the matrix by its transpose.

05:19.200 --> 05:27.440
And this equation here is valid if all of the variables are mean centered before being put into matrix

05:27.440 --> 05:27.920
X.

05:29.960 --> 05:36.240
On this slide, I want to give a comparison of mutual information and covariance.

05:36.600 --> 05:43.520
Now some of this information I've already mentioned before, but it's a good review and expansion of

05:43.520 --> 05:45.160
the important differences.

05:46.120 --> 05:50.920
Okay, this point I've definitely already mentioned and explained the reasoning behind.

05:51.280 --> 05:59.680
So covariance measures only linear interactions, whereas mutual information gets any statistical dependencies

05:59.680 --> 06:00.960
between the variables.

06:01.680 --> 06:08.930
If there are both linear and non-linear interactions, then the covariance will measure only the linear

06:08.970 --> 06:12.650
parts and ignore the non-linear interactions.

06:14.250 --> 06:22.170
Covariance is based on assumptions about the mean and symmetry of dispersion and interpretation of standard

06:22.170 --> 06:23.530
deviations, and so on.

06:23.850 --> 06:31.490
The data don't need to be exactly perfectly normally distributed, but covariance is best interpreted

06:31.490 --> 06:38.490
in data that are at least roughly normally distributed in the sense of having one central peak with

06:38.490 --> 06:41.330
decaying probabilities on either side.

06:42.090 --> 06:50.370
Now it turns out that the pre activations tend to be the fairly normally distributed.

06:50.730 --> 06:58.050
On the other hand, the post non-linearity activations can still be appropriate for covariance analyses

06:58.050 --> 06:59.130
in some cases.

06:59.690 --> 07:04.700
Now, in terms of the assumptions for, Or mutual information.

07:04.740 --> 07:11.780
The thing to keep in mind is that if you do not sample the probability space, then there's literally

07:11.780 --> 07:13.900
no data in that part of space.

07:14.100 --> 07:19.380
So that can be problematic for generalization and interpretation.

07:19.820 --> 07:27.500
So an underlying assumption of mutual information is that your data have fully sampled the probability

07:27.540 --> 07:28.220
space.

07:28.820 --> 07:35.860
You've already seen examples of where that assumption is violated even in the simulated data.

07:36.220 --> 07:44.820
And over the next several videos, you'll see again where in real life activation data that this assumption

07:44.820 --> 07:46.220
is violated.

07:46.620 --> 07:51.340
And you'll also see that there are outliers or extreme values.

07:51.340 --> 07:59.300
And those can be problematic for mutual information analysis because they expand the space for the probability

07:59.300 --> 08:05.030
calculations, but still leave this very sparse sampling of that probability space.

08:05.710 --> 08:12.190
So one approach to dealing with that is to exclude extreme data points for the analysis.

08:12.430 --> 08:18.910
But it's also not really an optimal solution because it means that we are removing some potentially

08:18.910 --> 08:23.750
insightful data in order to preserve statistical robustness.

08:24.310 --> 08:28.590
And that trade off you will often need to make in data science.

08:28.710 --> 08:30.470
Anyway, we'll get back to that point.

08:31.030 --> 08:33.750
So this difference is pretty interesting.

08:33.750 --> 08:39.110
It highlights one advantage of covariance over mutual information.

08:39.670 --> 08:46.150
Covariance is signed, meaning it can take on positive or negative values.

08:46.710 --> 08:48.550
And as you know covariance.

08:49.110 --> 08:53.790
Covariance means that when one variable goes up, the other one goes down.

08:54.830 --> 08:59.510
Mutual information is unsigned, so it cannot be negative.

09:00.030 --> 09:08.350
And that means that negative correlations and positive correlations will both manifest as positive mutual

09:08.350 --> 09:09.830
information values.

09:10.150 --> 09:14.990
And I'll show you an example of that in the Python demo in a few minutes.

09:15.910 --> 09:16.350
Okay.

09:16.390 --> 09:23.350
And this point I've also already mentioned covariance is calculated directly from the data values.

09:23.750 --> 09:31.150
Whereas with mutual information you need to first estimate a probability distribution from the data,

09:31.350 --> 09:35.230
and then the analysis is done on the probability distribution.

09:35.830 --> 09:43.430
Now in principle that's fine, but in practice it means that you need to choose a resolution for estimating

09:43.430 --> 09:45.350
that probability distribution.

09:45.470 --> 09:52.990
And that is a parameter that can change the entropy values and therefore also the mutual information

09:52.990 --> 09:53.750
results.

09:54.990 --> 10:01.230
And finally covariance is definitely from a practical perspective it's better.

10:01.470 --> 10:02.310
It's fast.

10:02.350 --> 10:04.120
It's easy to calculate.

10:04.120 --> 10:11.120
It's robust to undersampling or weird numerical issues, and that's because you're not taking logs of

10:11.160 --> 10:12.920
zero or tiny numbers.

10:13.520 --> 10:20.240
Now, that's not to say that you can never calculate or trust mutual information, of course, but there

10:20.240 --> 10:26.280
are some additional practical concerns for mutual information that you just don't have to worry about

10:26.280 --> 10:27.560
with covariance.

10:28.880 --> 10:29.160
Okay.

10:29.200 --> 10:35.400
With that theoretical discussion out of the way, let me now give you an overview of what I will show

10:35.400 --> 10:37.280
you in the Python demo.

10:38.080 --> 10:45.400
In the first part of the demo, I will simulate some data sets that have linear or nonlinear dependencies,

10:45.640 --> 10:52.240
and that will illustrate several features of covariance and mutual information, including the fact

10:52.240 --> 11:01.600
that nonlinear relationships have zero or at least near zero covariance, whereas those nonlinear dependencies

11:01.600 --> 11:09.810
can be measured with mutual information, and also that both positive and negative covariance values

11:09.810 --> 11:13.370
have positive mutual information values.

11:14.130 --> 11:20.570
In the next part of the demo, I will demonstrate the relationship between covariance and mutual information

11:20.570 --> 11:27.290
in linear interactions by varying the strength of the correlation in simulated data.

11:27.730 --> 11:34.370
And then we'll create a plot that looks like this, where the x axis shows the strength and sign of

11:34.370 --> 11:42.010
the correlation in the data, and the y axis shows either covariance or mutual information.

11:42.530 --> 11:50.250
Now the covariance tracks the linear relationship in the data basically perfectly, plus some noise

11:50.250 --> 11:57.650
and sampling variability, whereas the mutual information shows this U-shaped pattern, which happens

11:57.650 --> 12:00.970
because mutual information is unsigned.

12:01.130 --> 12:07.780
So a negative correlation has the same mutual information as a positive correlation.

12:09.140 --> 12:16.300
This plot here shows a covariance plotted as a function of mutual information, and you can see that

12:16.300 --> 12:19.660
they have a non-linear relationship to each other.

12:19.980 --> 12:20.260
Okay.

12:20.300 --> 12:21.860
So that's the top two plots.

12:22.020 --> 12:26.020
The bottom two plots show the same data the same results.

12:26.020 --> 12:29.540
But I squared the covariance values.

12:29.900 --> 12:33.940
And now you can see that the two measures look quite similar to each other.

12:34.620 --> 12:40.340
The last thing we will explore in this code demo is something I mentioned a couple times already in

12:40.340 --> 12:47.540
the past two videos, and that is the difference between the mutual information calculated manually

12:47.540 --> 12:53.740
as a direct translation of the formula and the scikit learn implementation.

12:54.420 --> 13:02.060
As you saw in a previous video, there are biases and numerical inaccuracies that are introduced by

13:02.060 --> 13:08.350
the manual method, which basically comes from undersampling the entire probability space.

13:09.030 --> 13:13.150
This diagonal line here is the expected result.

13:13.350 --> 13:20.230
If the manual implementation and the scikit learn implementation were exactly identical.

13:20.790 --> 13:28.070
And what you see is that all the dots lie on a line more or less, again with some variation due to

13:28.110 --> 13:31.470
noise and sampling, but they're basically lying on a line.

13:31.470 --> 13:35.390
But that line is shifted above the unity line.

13:35.670 --> 13:36.990
And what does that mean?

13:37.510 --> 13:42.910
That means that the relative mutual information values are all fine.

13:42.910 --> 13:43.990
They're all preserved.

13:43.990 --> 13:50.190
But the manual implementation simply introduces an upward bias, an upward shift.

13:51.310 --> 13:59.030
This is an important result because it has implications for which method you should use for large data

13:59.070 --> 13:59.670
sets.

14:00.070 --> 14:07.350
Now, the thing is that the scikit learn method is just too slow to be of practical use when you have

14:07.350 --> 14:08.870
a ton of data.

14:09.510 --> 14:16.430
So the conclusion here is that as long as you are comparing relative values and not the absolute mutual

14:16.430 --> 14:22.950
information value, it's fine to use the faster and more efficient manual implementation.

14:23.670 --> 14:27.230
All right, let's switch to code and have a look at this demo.

14:28.070 --> 14:30.830
Here are the libraries that we will use here.

14:30.910 --> 14:34.270
And first let me demonstrate an example case.

14:34.270 --> 14:39.790
So I'm actually going to first show the results and then show what the code looks like.

14:40.190 --> 14:43.670
Okay so here we see a positive covariance.

14:43.670 --> 14:47.150
It's pretty clear that the data go up and down together.

14:47.150 --> 14:49.270
So there's a positive correlation here.

14:49.510 --> 14:54.950
And what I did here was literally just multiply the data by minus one.

14:54.950 --> 14:57.230
All I did was flip the sign of the data.

14:57.390 --> 15:01.190
The covariance is exactly identical in magnitude.

15:01.350 --> 15:02.750
It's just the sign flipped.

15:02.790 --> 15:03.030
Okay.

15:03.070 --> 15:05.030
So here we have a negative relationship.

15:05.400 --> 15:11.280
but the mutual information is a positive and it's exactly the same number in both cases.

15:12.240 --> 15:14.520
So let me show you that code here.

15:14.520 --> 15:18.440
So that is so I start by generating variable x.

15:18.560 --> 15:22.880
And then I generate a variable y in four different ways.

15:23.120 --> 15:30.360
So in this case this is a formula for introducing a correlation between two variables.

15:30.360 --> 15:34.280
So here I'm introducing a correlation of 0.7.

15:34.600 --> 15:38.320
And yeah I'm not going to derive this formula.

15:38.320 --> 15:41.200
That's something I discuss in my statistics course.

15:41.200 --> 15:42.400
It doesn't matter here.

15:42.680 --> 15:46.320
The point is that x and y are related to each other.

15:46.520 --> 15:46.760
Okay.

15:46.800 --> 15:48.160
So that's for y1.

15:48.160 --> 15:52.240
And then y2 is literally just the negative of y1.

15:52.280 --> 15:54.880
And that's how we get these two values over here.

15:55.200 --> 15:58.520
Here I calculate the covariance between the variables.

15:58.640 --> 16:03.200
And here is the mutual information between these two variables.

16:03.800 --> 16:04.080
Okay.

16:04.120 --> 16:06.130
So then over here with Y3.

16:06.130 --> 16:07.210
That's pretty interesting.

16:07.210 --> 16:11.970
I define y3 to be x but x squared.

16:12.050 --> 16:18.170
And because there's going to be positive and negative numbers, the square basically becomes a quadratic.

16:18.170 --> 16:22.330
And we get a nice smiley face with a little bit of noise.

16:22.650 --> 16:26.970
So that you see here the smiley face with some noise the covariance.

16:26.970 --> 16:31.370
This is a non-linear relationship in principle should be zero.

16:31.530 --> 16:39.650
You're going to see it be non-zero just because there's sampling variability and noise okay.

16:39.690 --> 16:44.810
So here we have a case where the just by chance that happened to work out to be a bit more symmetric

16:45.010 --> 16:46.090
on the two sides.

16:46.090 --> 16:51.690
So you see that the covariance here is basically zero between x and y.

16:52.170 --> 16:57.970
Now obviously there is a statistical relationship between variable x and variable y.

16:58.210 --> 17:03.170
It's just that that relationship is non-linear and not linear.

17:03.370 --> 17:10.260
So the covariance only measures the linear part of the interactions, which means that the covariance

17:10.260 --> 17:11.260
is zero.

17:11.780 --> 17:13.340
And here we have something similar.

17:13.340 --> 17:16.500
So this is I think I just create this as a cosine.

17:16.780 --> 17:16.980
Yeah.

17:17.020 --> 17:20.140
So I set y to be the cosine of x.

17:20.260 --> 17:22.980
That's why you see it going up and down like this.

17:22.980 --> 17:27.420
Again there's clearly statistical dependencies in the data.

17:27.540 --> 17:33.820
Mutual information can measure those dependencies covariance and also correlation cannot.

17:34.860 --> 17:35.140
Okay.

17:35.180 --> 17:41.940
So that was the first demo here I'm going to run a more simulations of linear interactions.

17:42.100 --> 17:49.980
So I'm going to manipulate the linear relationship between the two variables using the same formula

17:49.980 --> 17:51.220
that I showed above.

17:51.580 --> 17:56.820
So essentially x is random noise y is equal to x.

17:57.060 --> 18:00.020
But then we knock it down by the correlation.

18:00.020 --> 18:07.590
And then we add some innovations that are related to the negative of the reciprocal, the opposite of

18:07.590 --> 18:08.590
the correlation.

18:08.790 --> 18:09.110
Okay.

18:09.150 --> 18:16.750
So basically what's going to happen is over this range of of numbers the data will be.

18:16.790 --> 18:23.430
So the variables x and y will be correlated at between -0.9 and plus 0.9.

18:23.830 --> 18:26.230
All linear correlations.

18:26.230 --> 18:29.270
There's no non-linearities that we are introducing here.

18:29.430 --> 18:33.990
So that means that covariance is an appropriate analysis.

18:34.150 --> 18:41.230
And it also means that mutual information is not going to be able to distinguish between a correlation

18:41.230 --> 18:45.750
of -0.8 and plus 0.8 okay.

18:45.790 --> 18:50.590
So generate the data calculate the covariance and mutual information.

18:50.910 --> 18:54.950
And here is the manual calculation of mutual information.

18:54.950 --> 19:02.590
So remember this is from scikit learn that goes into index one here and then into index two is the manual

19:02.590 --> 19:05.150
calculation of entropy.

19:05.150 --> 19:08.040
And then mutual information over here.

19:08.520 --> 19:12.200
Okay, then I'm just visualizing these results here.

19:12.200 --> 19:16.080
You can see in the second row this is the covariance data.

19:16.080 --> 19:17.480
And I'm just squaring it.

19:17.480 --> 19:18.640
All I'm doing is squaring it.

19:18.640 --> 19:20.880
I'm not changing the data at all.

19:20.880 --> 19:22.560
I'm not squaring the data.

19:22.720 --> 19:29.280
So we still have the linear measure of dependency statistical dependency.

19:29.520 --> 19:34.200
And it's still capturing the linear relationships.

19:34.240 --> 19:36.120
I'm just squaring the results.

19:36.120 --> 19:44.760
The covariance and squaring the covariance gives us not exactly the identical results as mutual information,

19:44.760 --> 19:50.920
but they are certainly very similar to each other, as opposed to when we are just looking at the like

19:50.960 --> 19:52.520
the raw covariance.

19:52.920 --> 19:57.440
Then we see that that follows the pattern of correlations.

19:57.440 --> 20:05.880
But the mutual information only tracks the magnitude of the interactions and not the sign of the interactions.

20:07.120 --> 20:07.480
Okay.

20:07.520 --> 20:14.680
And then the very last thing that I'm doing here is looking at the relationship between the manual implementation

20:14.680 --> 20:19.320
of a mutual information and the scikit learn implementation.

20:19.600 --> 20:26.720
And as I showed in the slides, they are super duper strongly correlated, basically correlated at 0.99.

20:26.800 --> 20:31.400
So it's nearly a perfect correlation with a little bit of variability.

20:31.440 --> 20:33.400
Again that's just due to noise.

20:33.640 --> 20:36.560
If you would increase the sample size for example.

20:36.600 --> 20:39.800
So increase the amount of data that we're generating.

20:39.840 --> 20:43.520
Then this would get even closer to one okay.

20:43.560 --> 20:47.440
And again the conclusion here is that there is a shift.

20:47.440 --> 20:54.720
There's a distributional shift a bias that gets introduced in our manual calculation because we are

20:54.720 --> 21:00.840
undersampling the probability space scikit learn's implementation is a little bit smarter.

21:00.840 --> 21:02.800
It's a little bit more sophisticated.

21:02.960 --> 21:11.770
It has ways of using a kernel density The estimator to try to estimate the entire probability distribution,

21:11.770 --> 21:16.490
and therefore it's a little bit more accurate in terms of the absolute values.

21:16.490 --> 21:18.530
But the relative values don't matter.

21:18.530 --> 21:19.650
That's all fine.

21:19.890 --> 21:26.250
If you would subtract any two pairs, you'd get basically the same result for the manual implementation

21:26.250 --> 21:28.930
versus the scikit learn implementation.

21:30.170 --> 21:36.570
Mutual information and covariance are just different measures of statistical dependencies.

21:37.050 --> 21:40.810
They each have their own advantages and limitations.

21:40.930 --> 21:47.570
Sometimes you can predict which measure is the right one to use, but in lots of cases you just don't

21:47.570 --> 21:49.010
really know a priori.

21:49.050 --> 21:53.810
So you might just pick one, or you might try both and see how they compare.

21:54.330 --> 22:01.610
In fact, in a code challenge in a couple videos from now, you will see that the two measures can sometimes

22:01.610 --> 22:06.770
look dissimilar to each other, and sometimes they look really similar to each other.

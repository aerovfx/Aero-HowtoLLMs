WEBVTT

00:02.320 --> 00:10.720
This is the last video in the sequence on mutual information and how it relates to covariance.

00:11.240 --> 00:18.440
The general goal here in this code challenge will be to calculate mutual information, as you did in

00:18.440 --> 00:27.160
the previous code challenge, and also calculate covariance using the same data pairs and the same conditions

00:27.400 --> 00:35.200
that will allow you to directly compare the features and insights that we can gain from these two measures

00:35.200 --> 00:37.840
of statistical dependencies.

00:38.480 --> 00:44.760
For the data analysis, we're going to separate what are called internal versus terminal punctuation

00:44.760 --> 00:52.600
marks and see how the statistical dependencies differ between internal and terminal punctuations and

00:52.600 --> 00:56.160
how this varies across transformer blocks.

00:56.760 --> 01:02.390
And well, I can already define the distinction between these two punctuation types.

01:02.670 --> 01:10.430
An internal punctuation mark is a punctuation in the middle of a sentence, like a comma and a terminal.

01:10.430 --> 01:17.790
Punctuation is a punctuation mark at the end of a sentence, like a period or an exclamation point.

01:18.590 --> 01:21.510
So here we go with exercise one.

01:21.750 --> 01:27.150
You want to import GPT two medium and push it to the GPU.

01:27.990 --> 01:35.030
In this code challenge, we are only going to use the hidden states, so you do not need to worry about

01:35.030 --> 01:35.670
hooks.

01:36.390 --> 01:41.510
Next, import the text to the book A Heart of Darkness.

01:41.910 --> 01:48.510
It's on Gutenberg.org and we've worked with this book in the past in an earlier code challenge.

01:48.950 --> 01:56.750
Now, the key part of this exercise is to identify internal and terminal punctuation marks and their

01:56.750 --> 01:58.430
indices in the book.

01:59.030 --> 02:05.150
There are actually more punctuation marks than only commas and periods, but in this case, the sample

02:05.150 --> 02:12.030
size is so high from this book that we can really just use these two punctuations and not worry about

02:12.030 --> 02:14.390
any of the other punctuation marks.

02:14.950 --> 02:23.950
Just be mindful that periods are used not only as terminal punctuations, so you'll need to add a filter

02:23.950 --> 02:31.390
or selection, or do something to make sure that you're only getting periods that are used as terminal

02:31.390 --> 02:32.670
punctuation marks.

02:33.110 --> 02:41.510
I found almost 3000 internal punctuation marks, and a little over 2000 terminal punctuation marks in

02:41.510 --> 02:42.190
this book.

02:42.790 --> 02:48.070
If you used a slightly different filter or selection method, you might get slightly different numbers,

02:48.070 --> 02:50.190
but should be somewhere around here.

02:51.110 --> 02:56.860
Then the final part for exercise one is to show a few of the examples.

02:57.220 --> 02:59.100
So here you see some examples.

02:59.100 --> 03:06.860
I just printed out a few examples with nine tokens before and nine tokens after the punctuation mark.

03:06.860 --> 03:09.220
So this is a terminal punctuation.

03:09.260 --> 03:13.100
Actually all these examples are terminal punctuation marks.

03:13.460 --> 03:13.940
By the way.

03:13.940 --> 03:18.740
We're going to use a different context window size for the analysis.

03:18.980 --> 03:24.060
So showing these examples with nine tokens before and nine tokens after.

03:24.100 --> 03:25.940
That's just for visualization.

03:25.980 --> 03:31.620
Just a quick check to make sure that your selection method seems to work okay.

03:31.660 --> 03:35.140
So now you should pause the video and code up this exercise.

03:35.260 --> 03:38.540
And now I will switch to code and discuss my solution.

03:39.580 --> 03:46.540
Here are the libraries we will use, including requests to import the text from the web.

03:46.980 --> 03:55.500
Here I'm importing GPT two medium, and I will push it to the GPU and switch it to eval mode.

03:55.860 --> 04:03.020
And now here this is a variable that we're going to end up using fairly often in the next several exercises.

04:03.020 --> 04:10.900
So I just decided to create a little variable instead of having to type out GPT two config embed over

04:10.900 --> 04:11.660
and over again.

04:11.660 --> 04:15.820
So yeah this is just the number of embeddings dimensions okay.

04:15.860 --> 04:17.380
And then here's the tokenizer.

04:17.700 --> 04:19.860
Here is where I import the text.

04:19.900 --> 04:27.580
Again you can either copy this from an earlier code file or you can Google it and then find the URL

04:27.620 --> 04:28.700
here okay.

04:28.740 --> 04:30.140
Then we get the tokens.

04:30.140 --> 04:35.940
And here I'm just printing out the number of total and unique tokens in here.

04:35.940 --> 04:39.300
So 64,000 tokens in total.

04:39.820 --> 04:45.060
Here I am listing the internal and terminal punctuation marks.

04:45.460 --> 04:53.060
As I mentioned in the slides, there are several internal and terminal punctuation marks because the

04:53.250 --> 04:57.090
sample size in this book is so high, we're not even going to use all of them.

04:57.690 --> 05:00.250
Then we don't really need to worry about these other ones.

05:00.250 --> 05:03.570
I'm just listing them here just for your knowledge.

05:03.570 --> 05:10.570
In case you are curious to continue exploring these kinds of analyses with other types of punctuation.

05:11.170 --> 05:12.610
Okay, so here what I do.

05:12.650 --> 05:14.330
I'll get back to this in a moment.

05:14.330 --> 05:21.930
So basically here what I do is I just look through each individual token and I say, is this token one

05:21.930 --> 05:24.730
of the or in the internal punctuation.

05:24.730 --> 05:26.690
Does it match the internal punctuation?

05:27.130 --> 05:34.690
If so, then this element of this vector is punctuation, which is as long as the number of tokens then

05:34.690 --> 05:35.370
it's one.

05:35.370 --> 05:44.090
So it's going to be zero for no punctuation, one for internal punctuation, and two for terminal punctuation.

05:44.410 --> 05:50.130
And now, with the terminal punctuations, I had to write a little bit of additional code just to make

05:50.170 --> 05:54.170
sure that the next element in the string.

05:54.570 --> 05:58.210
Either before or after is not a number.

05:58.210 --> 06:02.170
And that's just to avoid situations like this where you have a period.

06:02.170 --> 06:06.610
But this is actually not used as a terminal punctuation mark.

06:07.850 --> 06:08.090
Okay.

06:08.130 --> 06:13.010
Then the last thing I want to mention is, yeah, the beginning of these books, as you've seen before,

06:13.450 --> 06:21.290
the beginning of the Gutenberg.org books, is a lot of information about the Gutenberg website and the

06:21.290 --> 06:27.330
service that they provide and contributing to it and so on, some other legal information, and I'm

06:27.330 --> 06:28.490
just not interested in that.

06:28.490 --> 06:34.530
I just wanted to get the punctuation marks from the book itself and not from that preamble.

06:34.530 --> 06:37.850
So therefore I just kind of arbitrarily started at 400.

06:37.890 --> 06:40.290
There's nothing special about this number.

06:40.290 --> 06:43.930
I just wanted to exclude the beginning of the text.

06:44.610 --> 06:45.010
Okay.

06:45.050 --> 06:52.760
So now we can run through all of that and we see that there's yeah, 2000 to 3000 punctuation marks

06:52.760 --> 06:53.760
in this book.

06:54.240 --> 07:00.200
So once you get that code settled, then finding the indices is pretty straightforward.

07:00.200 --> 07:03.920
You just want to know where do we have punctuation one.

07:03.960 --> 07:06.800
Actually I can show you this is punctuation.

07:08.040 --> 07:16.040
Actually maybe I will plot this PLT dot plot is punctuation and I'll plot it like this.

07:16.520 --> 07:23.640
Okay, so now you see there's going to be how many tokens were there in the book like 60, 64,000 okay.

07:23.680 --> 07:26.480
So it's going to be a lot of tokens in this plot.

07:26.520 --> 07:28.400
Most of these are zeros.

07:28.440 --> 07:30.520
A very small number of them are ones.

07:31.080 --> 07:34.320
These are the internal punctuation.

07:34.320 --> 07:37.880
So the commas and some of these are periods okay.

07:37.920 --> 07:39.200
So that's for that plot.

07:39.200 --> 07:42.840
And then here I'm just finding the indices where those occur.

07:43.000 --> 07:47.160
And I'm printing out a small handful of the terminal punctuations.

07:47.160 --> 07:48.280
Just the first five.

07:48.320 --> 07:52.280
You can also check this for the internal punctuations also.

07:52.280 --> 07:52.720
Fine.

07:53.680 --> 07:55.800
Now for exercise two.

07:56.200 --> 08:00.800
The main goal here is to get the hidden state activations.

08:01.080 --> 08:09.320
To begin, create two batches of token sequences, one for the internal punctuation marks and one for

08:09.320 --> 08:11.400
the terminal punctuation marks.

08:12.000 --> 08:21.560
Each of the two sequences should have 31 tokens in total, where the token index at position 20 is the

08:21.560 --> 08:23.760
punctuation mark, so that's the target.

08:24.200 --> 08:30.400
You can then run two forward passes, one for the internal and one for the terminal.

08:30.400 --> 08:37.640
And then you can confirm that the sizes of the activations and the number of hidden states that you

08:37.640 --> 08:40.640
get are consistent with these numbers here.

08:40.640 --> 08:52.110
So 250 sequences, 31 tokens per sequence and 1024 is the number of embeddings dimensions in GPT two

08:52.350 --> 08:53.070
medium.

08:53.470 --> 08:59.950
And yeah, by the way, the only reason to use 250 sequences here is just to keep the sample size a

08:59.950 --> 09:00.870
bit small.

09:00.870 --> 09:06.190
So the analyses are a little easier to work with, a bit faster for the code to run.

09:06.430 --> 09:13.030
Of course, if you're doing this in a real analysis for real research, then you could follow exactly

09:13.030 --> 09:16.990
the same analysis approach that we're doing in this code challenge.

09:16.990 --> 09:22.110
But you would want this number to be much, much bigger from, you know, this entire book.

09:22.150 --> 09:26.950
You can sample from lots of other books, from Wikipedia, from Twitter, and so on.

09:27.310 --> 09:35.350
Now, because the model is on the GPU, you will need to copy the batches over to the GPU, but for

09:35.350 --> 09:42.830
the rest of the exercises, it will be convenient to have the hidden states activations on the CPU and

09:42.830 --> 09:47.390
also in numpy format instead of in PyTorch format.

09:47.870 --> 09:55.670
So therefore, the final part of this exercise is to bring the hidden states from the GPU back to the

09:55.670 --> 10:01.110
CPU, convert them into numpy, and then you can make some visualizations.

10:01.350 --> 10:07.630
So here on the left you see the activations averaged over all the punctuation marks.

10:07.830 --> 10:15.790
So averaging over all 250 sequences from index 20 from one hidden layer.

10:15.790 --> 10:17.070
And that's what you see here.

10:17.590 --> 10:24.590
It's interesting to see that there are clearly some extreme activation values here, which means that

10:24.590 --> 10:31.630
there are a small number of dimensions that really care a lot about these punctuation marks.

10:32.190 --> 10:36.230
Now here the x axis is the embedding dimension.

10:36.350 --> 10:41.110
And the scatter plot here shows the internal and the terminal activations.

10:41.390 --> 10:45.820
And then what you see over here is a scatter plot of the activations.

10:45.820 --> 10:49.780
Here are the internal and here are the terminal activations.

10:49.820 --> 10:56.700
Again, each of these dots is the average over all of the sequences across the batch.

10:57.140 --> 11:04.380
And basically what this shows is that the model is responding overall fairly similarly to internal versus

11:04.380 --> 11:06.180
terminal punctuation marks.

11:06.660 --> 11:13.060
So each dot in this plot is a dimension of the from the embeddings.

11:13.380 --> 11:13.780
Okay.

11:13.820 --> 11:20.940
So yeah, it's going to be interesting to see how these averages here compare to the mutual information

11:20.940 --> 11:24.340
and covariance values that we will calculate later.

11:24.460 --> 11:27.660
But this is good enough for exercise two.

11:27.980 --> 11:33.660
So now you can pause the video and get these activations and generate that plot.

11:33.980 --> 11:39.700
And when you're ready or if you get stuck come back to the video and I will walk you through my solution.

11:40.700 --> 11:42.420
So batch size of 20.

11:42.570 --> 11:44.530
There's, uh, 250.

11:44.570 --> 11:44.890
Sorry.

11:44.930 --> 11:52.810
There's going to be, uh, 20 tokens before each punctuation and ten tokens after each punctuation mark,

11:52.810 --> 11:54.370
each of the 250.

11:54.370 --> 11:58.290
So that's going to give us 31 tokens per sequence.

11:58.810 --> 12:03.290
Here I am initializing these matrices for the batches.

12:03.450 --> 12:11.450
So I have uh yeah PyTorch format batch size by context pre plus context post plus one.

12:11.450 --> 12:15.250
And the plus one is for the terminal or the punctuation mark.

12:15.490 --> 12:17.890
And we want these to be integers of course.

12:18.290 --> 12:20.610
Okay so here is pretty straightforward.

12:20.610 --> 12:24.010
I'm just looping over 250.

12:24.210 --> 12:32.650
And then I'm grabbing one of the punctuation location indices and creating a little batch of text around

12:32.650 --> 12:37.090
that or token sequence around that token location.

12:37.090 --> 12:41.690
So minus context pre and then plus context post.

12:42.010 --> 12:46.330
So then do that for internal and terminal punctuations.

12:46.330 --> 12:47.370
Pretty straightforward.

12:47.770 --> 12:49.730
Here I'm running the forward pass.

12:49.730 --> 12:51.410
So I have to do this twice.

12:51.530 --> 12:53.610
And I'm naming the variables differently.

12:53.610 --> 12:57.090
So I get output internal and output terminal.

12:57.090 --> 13:01.090
So I don't overwrite the two sets of activations.

13:01.090 --> 13:05.210
This is the confirmation that we get sizes that we expect.

13:05.210 --> 13:12.130
So there are 24 uh transformer blocks in GPT two medium.

13:12.130 --> 13:18.330
And of course the first one of these is the embeddings the initial token plus position embeddings.

13:18.890 --> 13:19.210
Okay.

13:19.250 --> 13:22.210
So now this stuff lives on the GPU.

13:22.530 --> 13:24.050
We want it on the CPU.

13:24.090 --> 13:25.970
We want it in numpy format.

13:26.170 --> 13:30.770
And also just for convenience I'm creating shorter variable names.

13:31.130 --> 13:37.690
So that's what I do here I define these two variables Hidden states, House intern and turbine.

13:37.890 --> 13:45.000
And then I'm just appending these with each hidden state from each layer, and then detach to remove

13:45.000 --> 13:52.560
it from all the gradient and overhead information, bring it to the CPU and into numpy format.

13:52.800 --> 13:53.600
So there you go.

13:53.640 --> 13:54.400
And now here.

13:54.440 --> 13:54.640
Yeah.

13:54.680 --> 13:59.400
Just to confirm, by the way, there's no specific reason why this needs to be a list.

13:59.440 --> 14:04.760
You could also set this up to be one big matrix a one big tensor.

14:04.960 --> 14:11.720
But I chose to leave it as a list just so it's consistent with how hugging face organizes the hidden

14:11.720 --> 14:13.040
states variables here.

14:13.600 --> 14:22.320
Okay, so yeah, this is just confirmation 250 sequences, 31 tokens and a dimensionality of embeddings.

14:22.320 --> 14:24.760
Dimensionality of 1024.

14:25.200 --> 14:27.280
And then finally to visualize.

14:27.280 --> 14:33.200
So this is just yeah a little bit of data to give us some context about what the data look like.

14:33.560 --> 14:41.600
Here I am getting the average of the internal and and terminal punctuations from this one particular

14:41.600 --> 14:43.480
layer randomly chosen.

14:43.720 --> 14:45.080
Uh, and uh, yeah.

14:45.120 --> 14:49.960
Then this is the exact token index of the punctuation mark.

14:50.120 --> 14:57.960
And now I'm averaging over the first axis, which means that I am averaging over all of the, uh, sequences

14:57.960 --> 14:58.560
in the batch.

14:58.560 --> 15:02.640
So 250 sequences averaging over all of them.

15:02.640 --> 15:04.720
And that's what I show in these plots here.

15:05.600 --> 15:15.800
Now for exercise three, first write a Python function that calculates trimmed mutual information and

15:15.800 --> 15:24.680
covariance for an input pair of variables, the mutual information function you already have from several

15:24.680 --> 15:26.040
previous videos.

15:26.280 --> 15:33.520
So you really just need to copy one of those functions from an earlier notebook and then add covariance

15:33.520 --> 15:41.950
to the calculation once you have that function, you can calculate the mutual information and covariance

15:42.150 --> 15:50.990
for all the pairs of punctuation marks separately for internal versus terminal punctuations, and that

15:50.990 --> 15:55.510
will allow you to visualize the data like this for example.

15:56.110 --> 16:01.150
So this is just mutual information and only in layer one.

16:01.630 --> 16:09.070
Now to clarify this is a layer index one which means it's the first transformer block.

16:09.270 --> 16:16.550
Remember that in these hidden states the first hidden state layer is actually the output of the embeddings.

16:16.550 --> 16:23.270
So this is the first layer where the embeddings are really processed and transformed by the attention

16:23.270 --> 16:24.910
and MLP mechanisms.

16:26.030 --> 16:34.910
Anyway, these matrices show pairwise mutual information values for each pair of punctuation marks.

16:34.950 --> 16:37.030
This one is just for internal.

16:37.070 --> 16:38.670
This is just for terminal.

16:39.030 --> 16:42.230
And the lines over here show the histograms.

16:42.230 --> 16:49.670
So the distributions of all of the unique non-zero elements in this matrix.

16:49.670 --> 16:51.790
So just the lower triangle here.

16:52.190 --> 16:55.550
So this figure shows the mutual information.

16:55.670 --> 17:01.710
And then you can make the same plot again but for covariance instead of mutual information.

17:02.030 --> 17:09.870
And well this is a really striking difference in this case the mutual information and the covariance

17:09.870 --> 17:13.270
look really qualitatively very different from each other.

17:13.790 --> 17:18.870
And you also see this really striking pattern of clustering here.

17:18.870 --> 17:26.190
So there are three groups of covariance values in the data for the internal punctuations.

17:26.830 --> 17:31.750
Now looking at these two figures separately is interesting.

17:32.070 --> 17:39.980
But maybe we can gain some better insights into the relationship or lack of relationship between mutual

17:39.980 --> 17:44.940
information and covariance by plotting them directly against each other.

17:45.340 --> 17:48.460
And that's the goal of this third plot here.

17:48.860 --> 17:55.540
So here you take all of the unique covariance and mutual information elements.

17:55.660 --> 17:59.460
Of course, they have to be matched by the sequence pairs.

18:00.380 --> 18:03.180
And then you plot them as a function of each other.

18:03.500 --> 18:06.820
So in this plot, the x axis shows covariance.

18:06.980 --> 18:10.260
The y axis shows mutual information.

18:10.460 --> 18:14.340
And I've also calculated the correlation coefficient between them.

18:14.460 --> 18:16.780
You can do this with the Pearson correlation.

18:16.980 --> 18:20.900
And that appears in the title of each of these graphs.

18:21.020 --> 18:23.620
So this is for the internal punctuation.

18:23.620 --> 18:25.820
This should say internal not intermediate.

18:26.260 --> 18:28.540
This is for the terminal punctuations.

18:28.700 --> 18:35.700
And here on the right this plot is literally just the superposition of the intermediate and internal

18:35.700 --> 18:36.540
scatter plots.

18:36.540 --> 18:42.620
So this plot plus this plot overlaid, and that gives us this plot over here.

18:43.460 --> 18:47.180
Now the information in this plot is redundant.

18:47.220 --> 18:50.540
It's exactly the same as what I'm showing in these two plots.

18:50.540 --> 18:58.900
But the reason for the redundancy is that when we put all of the information into one subplot, one

18:58.900 --> 19:06.860
panel, then that will allow us to visualize this relationship between mutual information and covariance

19:07.020 --> 19:09.380
across all of the layers.

19:09.420 --> 19:12.180
Now, that's not what we're doing in this exercise.

19:12.180 --> 19:15.260
It's what we will do in the next exercise.

19:15.660 --> 19:22.580
So here in exercise three you can focus on making these three figures, these three plots for layer

19:22.580 --> 19:25.860
one and then repeat for layer 20.

19:25.900 --> 19:30.770
So to do this you don't need to copy and paste into a new code cell.

19:30.810 --> 19:38.530
All you do is change the variable name so that instead of these data being drawn from layer one, they're

19:38.530 --> 19:40.410
drawn from layer 20.

19:40.850 --> 19:44.410
Again, don't worry about any of the other layers in the model.

19:44.410 --> 19:47.330
We'll get to that in the next exercise.

19:48.130 --> 19:51.570
So now you can pause the video and work your way through this.

19:51.610 --> 19:55.090
And now I will switch to code and discuss these findings.

19:55.770 --> 20:02.410
Here is the function to calculate mutual information I change the function name very slightly from the

20:02.410 --> 20:04.450
previous code challenge.

20:04.570 --> 20:05.250
And yeah.

20:05.250 --> 20:05.850
So let's see.

20:05.850 --> 20:13.290
All of this code is identical to the previous, uh, the functions that we had in previous videos.

20:13.490 --> 20:15.530
And now I've added one line here.

20:15.730 --> 20:21.570
So the covariance is the element wise multiplication of the two variables.

20:21.690 --> 20:23.490
They have to be mean centered.

20:23.490 --> 20:25.530
So that's why I subtract the mean.

20:25.730 --> 20:26.890
And then we sum.

20:27.170 --> 20:29.760
And then we divide by n minus one.

20:29.760 --> 20:32.520
So that is the covariance between this pair.

20:32.680 --> 20:40.320
And now this is a pretty interesting set of pair of analyses because we have identical data but just

20:40.320 --> 20:44.000
slightly different measures of statistical dependencies.

20:44.440 --> 20:49.560
And now I'm outputting a tuple of the mutual information value and the covariance value.

20:50.160 --> 20:50.560
Okay.

20:50.600 --> 20:56.080
So then this is just a little sanity check about the indexing.

20:56.080 --> 20:59.400
So I'm just getting one batch one sequence.

20:59.400 --> 21:03.480
And this particular index context pre.

21:03.680 --> 21:10.400
And just to confirm that it actually is a period in this case it's just a punctuation mark.

21:10.400 --> 21:15.280
For example you know if you were indexing like this then that would be wrong.

21:15.320 --> 21:19.640
And all of these subsequent analyses would be not based on punctuations.

21:20.040 --> 21:20.480
Okay.

21:20.840 --> 21:21.520
Very nice.

21:21.520 --> 21:24.920
So now I'm going to run this cell of code here.

21:25.360 --> 21:30.680
Here I am looping over all of these sequences in the batch.

21:30.680 --> 21:37.520
So b I is looping from zero to batch size, and then j goes from b I plus one.

21:37.560 --> 21:39.760
You've seen this now many times.

21:40.120 --> 21:42.600
Here I am extracting the data.

21:42.720 --> 21:52.960
So uh, this particular token, uh, this particular pair of sequences and all of the embeddings dimensions,

21:53.120 --> 21:57.360
and that's what I calculate mutual information and covariance over.

21:58.400 --> 22:01.960
Now, I do not explicitly say what threshold to use.

22:02.320 --> 22:06.040
Uh, I'm here using a threshold of Z greater than five.

22:06.400 --> 22:09.480
You can try exploring different values if you like.

22:09.640 --> 22:09.920
Okay.

22:09.960 --> 22:12.560
And then the outputs of this are a tuple.

22:12.560 --> 22:18.080
So I have mutual information uh internal and covariance internal.

22:18.200 --> 22:19.960
And now I repeat for terminal.

22:20.000 --> 22:22.400
This code is the same as this.

22:22.400 --> 22:24.520
But the variable names are different.

22:25.040 --> 22:30.670
So yeah this code cell takes approximately 40s 35 to 40s to run.

22:30.990 --> 22:34.470
And now here I'm going to do some of the visualizations.

22:34.510 --> 22:43.590
Now we already know that we just need to export or extract the non-zero values from this matrix.

22:43.710 --> 22:50.230
And we want to vectorize them which facilitates calculating histograms and also making scatter plots.

22:50.710 --> 22:55.150
Now in the previous code challenge what I did was use this function.

22:55.150 --> 23:01.350
So triangle upper indices and the one means to ignore the diagonal itself.

23:01.350 --> 23:04.670
So we just get the elements above the diagonal.

23:04.670 --> 23:10.670
So previously I was using this entire function as an input into the matrices.

23:10.830 --> 23:11.990
In this code challenge.

23:11.990 --> 23:16.390
When I was programming this, I thought it would be nice to have a separate variable.

23:16.390 --> 23:18.190
So here you see that variable.

23:18.190 --> 23:19.190
It's a tuple.

23:19.190 --> 23:26.430
It is the the row and column indices for Extracting these elements that we want.

23:26.990 --> 23:27.310
Okay.

23:27.350 --> 23:29.150
And then we do the visualization.

23:29.150 --> 23:33.350
So this part I'm not going to go into a huge amount of detail about.

23:33.350 --> 23:35.030
So creating the images.

23:35.230 --> 23:38.710
And yeah like this is the real core of the code here.

23:38.710 --> 23:42.910
Everything else is just making the plot look a little bit nicer.

23:42.910 --> 23:45.550
So setting the color limits and et cetera.

23:45.590 --> 23:46.030
ET cetera.

23:46.070 --> 23:54.030
All this stuff here I'm calculating the histograms and yeah, also plotting code is important but not

23:54.030 --> 23:56.230
really that interesting okay.

23:56.270 --> 23:58.830
So here is for mutual information.

23:58.830 --> 24:01.670
And here is for the covariance.

24:02.030 --> 24:08.350
All of this code, everything in this cell is literally just copied from this cell.

24:09.150 --> 24:10.030
From this cell.

24:10.030 --> 24:12.910
And all I did was change the variable names.

24:12.910 --> 24:13.590
Uh, where am I?

24:13.870 --> 24:15.790
Change the variable names here.

24:15.790 --> 24:19.750
And also, you know, some other little things like the color limits and so on.

24:20.870 --> 24:21.270
Okay.

24:21.350 --> 24:23.020
Super duper striking, right?

24:23.060 --> 24:26.820
We see these big clusters here really strong clustering.

24:27.100 --> 24:30.620
And here we can also see that in the plot.

24:30.900 --> 24:37.860
So here we see the covariance really shows up as three different clusters but not so much in the mutual

24:37.860 --> 24:38.860
information.

24:38.980 --> 24:47.580
So in fact when we look at the data for mutual information for the internal punctuations, you do see

24:47.620 --> 24:55.700
kind of a double peak here, but it's not really as clear in the mutual information as it is in the

24:56.060 --> 24:57.140
covariance.

24:57.340 --> 25:04.060
So it looks like there might be some additional clustering here, although that is pretty small relative

25:04.060 --> 25:05.860
to this big blob over here.

25:06.300 --> 25:06.500
Okay.

25:06.540 --> 25:11.380
And then this is the plot with both of these superimposed on top of each other.

25:12.140 --> 25:16.700
Again, this panel here has no new information.

25:16.940 --> 25:25.140
But in the next exercise we're going to generate this plot for all 25 layers, and so therefore, having

25:25.140 --> 25:29.940
one plot is a little bit easier than having two plots for 25 layers.

25:30.420 --> 25:35.860
Okay, so, uh, yeah, when you first look at this, you might think that there are different kinds

25:35.860 --> 25:39.460
of commas, you know, three different categories of commas.

25:39.620 --> 25:46.620
Maybe we need to go back to the text and see if, you know, these really correspond to three different

25:46.620 --> 25:46.900
things.

25:46.900 --> 25:49.300
And we're inappropriately mixing them together.

25:49.660 --> 25:55.260
Uh, so therefore we can, uh, have a look at layer 20.

25:55.300 --> 25:56.420
Where do I have this?

25:57.500 --> 26:00.060
Uh, that's all the way back up here.

26:00.100 --> 26:00.340
Yeah.

26:00.380 --> 26:00.740
Okay.

26:00.780 --> 26:05.980
So change this to layer 20, and, uh, don't need to rerun that code.

26:05.980 --> 26:10.340
I do need to rerun the visualization, which I'll do in a moment.

26:10.860 --> 26:12.180
So here's the thing.

26:12.380 --> 26:21.130
If these data really were three distinct categories in the text, then it would be reasonable to expect

26:21.130 --> 26:26.650
that these this strong clustering would be preserved throughout the entire model.

26:26.650 --> 26:33.010
If these are really distinct from each other, and what you will see in a moment is what you've probably

26:33.010 --> 26:38.170
already seen in your own code, is that these are really not at all distinct.

26:38.170 --> 26:41.610
So this they're distinct early in the model.

26:41.610 --> 26:43.810
They are not distinct later in the model.

26:43.970 --> 26:50.450
And in the next exercise you're actually going to see these three clusters merge as we move through

26:50.490 --> 26:52.850
the different layers okay.

26:52.890 --> 26:56.090
So I'm going to go back and replot that in a moment.

26:56.090 --> 27:01.690
But I would just like to make one comment about this very strong negative correlation.

27:02.530 --> 27:09.410
Now what's happening in here is that we have these three distinct groups and that pulls the correlation

27:09.410 --> 27:10.130
down.

27:10.290 --> 27:16.810
Although when you look within any one group, there isn't really a clear correlation between covariance

27:16.810 --> 27:18.210
and mutual information.

27:18.290 --> 27:19.850
Maybe here, but here.

27:19.850 --> 27:21.530
If anything it's positive.

27:21.810 --> 27:25.250
Whereas this overall correlation is negative.

27:25.530 --> 27:28.730
There's actually a name in statistics for this phenomenon.

27:28.730 --> 27:31.250
It's called Simpson's paradox.

27:31.410 --> 27:37.370
Simpson like the cartoon The Simpson's, but it's named after the statistician Simpson.

27:37.770 --> 27:42.170
So Simpson's paradox is also called the subgroups paradox.

27:42.170 --> 27:50.090
And basically it's when you get an artificially inflated correlation that is actually a confound.

27:50.090 --> 27:56.170
It's a specious correlation, because what's really happening is that you have separate groups in here.

27:56.410 --> 27:59.730
Okay, that's just a little discussion point about statistics.

27:59.730 --> 28:03.730
Now let me go back and run this code.

28:04.250 --> 28:06.930
And so here looks quite different.

28:06.930 --> 28:10.090
And here run the covariance code.

28:10.130 --> 28:12.450
And now this also looks really different.

28:12.490 --> 28:16.530
These color values are no longer appropriate.

28:16.760 --> 28:18.800
You can see that here on the x axis.

28:19.280 --> 28:22.120
So I'm not going to go back and change the color values.

28:22.120 --> 28:24.200
But you can mess around with this if you like.

28:24.360 --> 28:26.920
It's the Vmin and Vmax parameters.

28:26.920 --> 28:30.560
But you do see the full numerical range of the distribution here.

28:31.000 --> 28:38.840
That is also interesting because it means that the mutual information numerical values are roughly in

28:38.840 --> 28:41.880
the same range for layer one and layer 20.

28:42.920 --> 28:46.280
But the covariance values are way, way, way larger.

28:46.280 --> 28:52.120
They're like an order of magnitude larger in layer 20 compared to layer one.

28:52.440 --> 28:58.880
I will let you ponder that why that might be the case, what it might mean if it's problematic.

28:59.080 --> 29:00.080
I'll let you ponder that.

29:00.080 --> 29:04.120
I'm going to talk about that more in the next exercise.

29:04.280 --> 29:07.320
But anyway, now we can generate this plot again.

29:07.320 --> 29:09.440
And several things are striking.

29:09.480 --> 29:13.840
First of all, we do not see any clusters here.

29:14.240 --> 29:22.520
Uh, And we also see that covariance and mutual information are now much more similar to each other

29:22.520 --> 29:23.600
qualitatively.

29:23.920 --> 29:30.160
And in fact, if you would square the covariance, you would see that it looks even more like a linear

29:30.160 --> 29:31.120
relationship.

29:31.360 --> 29:38.280
You remember from several videos ago when I talked about mutual information and covariance in the same

29:38.440 --> 29:38.840
lecture.

29:38.840 --> 29:44.880
When I introduced that, I said that mutual information tends to look like squared covariance.

29:45.080 --> 29:47.720
Now, that's not a mathematically trivial relationship.

29:47.720 --> 29:49.360
They're very different analyses.

29:49.560 --> 29:51.720
But yeah, you can see this here as well.

29:52.760 --> 29:56.240
Congrats on making it this far in the code challenge.

29:56.240 --> 30:02.480
We're not yet finished, but I'm going to cut the video here because I want to encourage you to take

30:02.480 --> 30:07.520
a short break and do all the things I always suggest you do on these short breaks.

30:07.520 --> 30:12.840
When you're ready, come back to the next video and we will finish off this code challenge.

WEBVTT

00:01.840 --> 00:09.200
Another great analysis technique to have in your toolkit is called effective dimensionality.

00:09.720 --> 00:16.800
It's very similar to compression and dimension reduction, and provides some insights into the nature

00:16.800 --> 00:20.840
of the shape of the data space inside the model.

00:21.400 --> 00:28.320
That's important because the information that the model can process is limited by the dimensionality

00:28.320 --> 00:29.680
of the data space.

00:30.240 --> 00:37.760
As a very simple example, imagine that the only dimension you could use to describe the conditions

00:37.760 --> 00:40.120
outside was temperature.

00:40.680 --> 00:45.440
Now, temperature certainly is an informative metric, but it's not everything.

00:45.840 --> 00:53.000
If you have the humidity as well, then that tells you a lot more information about what it's like outside.

00:53.560 --> 00:58.680
So temperature and humidity would mean two dimensions of information.

00:59.240 --> 01:02.810
And then perhaps cloud coverage would be a third dimension.

01:03.370 --> 01:11.330
So the more dimensions the information is represented and processed in, the more contextual details

01:11.330 --> 01:14.010
the model can represent and calculate.

01:14.810 --> 01:22.210
Now, the precise interpretation of dimensionality here gets pretty tricky because there are so many

01:22.210 --> 01:25.330
nonlinear transformations throughout the model.

01:25.530 --> 01:34.810
And every linear transformation inside the model is both preceded and followed by a nonlinear transformation.

01:35.450 --> 01:42.490
Nonetheless, effective dimensionality is still pretty interesting and I think can be quite useful in

01:42.530 --> 01:44.370
mech interp analyses.

01:44.970 --> 01:49.690
So let me begin with a reminder about principal components analysis.

01:49.890 --> 01:54.530
The singular value decomposition and percent variance explained.

01:55.650 --> 02:04.010
Actually, if you are unfamiliar with SVD, then I recommend going back to this video from earlier in

02:04.010 --> 02:06.330
the course to get a refresher.

02:06.770 --> 02:14.010
You do not need to watch the entire video, just the beginning part of the lecture where I provided

02:14.010 --> 02:22.170
a high level intro to the singular value decomposition briefly and as a reminder of the reminder.

02:22.370 --> 02:28.050
The idea is that we have a data set stored as a two dimensional matrix.

02:28.210 --> 02:29.650
So rows and columns.

02:30.090 --> 02:33.010
And then we take the SVD of that matrix.

02:33.130 --> 02:37.490
And we get the singular values and singular vectors.

02:38.010 --> 02:43.130
Now earlier in the course we were actually using these singular vectors.

02:43.410 --> 02:50.570
But now here in this video and also in the next video, we don't actually care about the vectors per

02:50.570 --> 02:50.810
se.

02:50.850 --> 02:56.090
We're really just going to focus on the singular values for the analyses.

02:56.970 --> 03:04.940
The singular values encode the importance of each direction in the data, and importance in the context

03:04.940 --> 03:10.100
of PCA is the amount of variance along that direction.

03:11.060 --> 03:19.260
So, for example, in this data set, PCA would identify this line as the most important direction in

03:19.260 --> 03:26.340
the data, because the most amount of variance in the data aligns with this vector with this direction.

03:27.500 --> 03:33.180
So this vector, this direction here would have a large singular value.

03:33.940 --> 03:38.460
The next most important direction in this data would be this one.

03:38.460 --> 03:41.780
And its singular value will be smaller.

03:42.580 --> 03:49.220
Now we can plot all of the singular values in a plot, and that would look something like this.

03:49.540 --> 03:56.620
This kind of plot is variously called the SVD spectrum, the singular value spectrum, or the scree

03:56.660 --> 03:58.060
plot, or something like that.

03:58.420 --> 04:01.580
Now here's the cool thing about singular values.

04:02.020 --> 04:08.020
All of the singular vectors have unit norm, which means a magnitude of one.

04:08.340 --> 04:16.700
And that means that the vectors themselves do not actually contain any information other than direction.

04:17.780 --> 04:21.740
In other words, all of the variability in the data.

04:21.860 --> 04:28.900
So all the variance, all the covariance across all the variables, it's all stored in the singular

04:28.900 --> 04:29.820
values.

04:30.220 --> 04:37.100
And technically it's the squared singular values because of the relationship between singular values

04:37.100 --> 04:40.420
and eigenvalues of the data matrix transpose.

04:40.860 --> 04:45.940
If you've taken a course on linear algebra, then that statement should sound familiar.

04:46.140 --> 04:48.060
And if not, then don't worry about it.

04:48.100 --> 04:56.900
The point is that 100% of the variability in the data is stored in the collection of singular values,

04:57.460 --> 05:05.190
and therefore we can calculate how much variability is stored in each component or each direction using

05:05.190 --> 05:06.070
this formula.

05:06.070 --> 05:14.790
Here, where r squared is a general statistical term used to indicate the proportion of variance explained,

05:15.230 --> 05:18.350
and the sigmas are the singular values.

05:18.750 --> 05:27.030
So square all of the singular values, sum them all up, and then divide that into each of the individual

05:27.030 --> 05:28.510
singular values.

05:28.950 --> 05:34.710
That gives you the proportion of variance accounted for by each principal component.

05:34.990 --> 05:42.110
And you can multiply this by 100 to get the percent variance explained.

05:43.470 --> 05:44.710
So that's all fine.

05:44.710 --> 05:48.070
But how do we get to effective dimensionality.

05:48.630 --> 05:53.190
In fact the dimensionality of the data space is kind of trivial.

05:53.350 --> 05:59.590
It's simply the number of tokens that you have in the sequence or the number of neurons in the layer.

05:59.790 --> 06:02.270
Whichever of those two is smaller.

06:02.710 --> 06:07.590
But that is the mathematical definition of the ambient dimensionality.

06:08.110 --> 06:14.990
The effective dimensionality is actually determined by the data, not by the number of columns in the

06:14.990 --> 06:15.750
matrix.

06:16.190 --> 06:22.870
And I'd like to give you an example to help you understand this concept of effective dimensionality.

06:22.990 --> 06:24.590
And then I'll get into the math.

06:25.230 --> 06:27.390
So think about a piece of paper.

06:27.790 --> 06:31.670
Technically this piece of paper is three dimensional.

06:31.910 --> 06:39.350
But one of the dimensions is so thin that it cannot be used for any practical purpose.

06:40.110 --> 06:45.470
So we really only use a piece of paper as a two dimensional object.

06:45.910 --> 06:49.830
Of course, physically it exists in three dimensions.

06:49.990 --> 06:57.320
But effectively the thickness dimension is so thin that it has no application value.

06:57.840 --> 07:03.920
So we can say that a piece of paper has an effective dimensionality of two.

07:04.760 --> 07:07.400
Here are some examples with data.

07:07.440 --> 07:09.760
Let's first look at this one on the left.

07:10.240 --> 07:12.680
These data live in two dimensions.

07:12.680 --> 07:17.320
There are two axes, but all of the data points sit on a line.

07:17.800 --> 07:24.880
So although the ambient dimensionality is two, the data really live in a one dimensional subspace.

07:25.320 --> 07:31.840
So we can say that the effective dimensionality is one, although technically the true dimensionality

07:31.840 --> 07:32.560
is two.

07:33.760 --> 07:36.000
Now how about this one over here?

07:36.440 --> 07:39.560
Certainly these data are strongly correlated.

07:39.760 --> 07:44.000
Would you say that the effective dimensionality here is 1 or 2?

07:44.720 --> 07:51.000
It's not really clear what the answer should be, because it depends on what you think of the variability

07:51.240 --> 07:53.200
off of the diagonal.

07:53.800 --> 07:58.840
Is this noise or is this important meaningful variability?

07:59.480 --> 08:06.440
We could compress the data down to one dimension and then say that everything off the perfect diagonal

08:06.440 --> 08:07.400
is noise.

08:07.600 --> 08:13.080
We don't think it carries any meaningful information that we want to preserve in the analysis.

08:13.800 --> 08:18.320
And that is basically how compression by dimension reduction works.

08:19.000 --> 08:27.440
So let's say we have some threshold, and we say that the variability on this smaller axis is considered

08:27.440 --> 08:34.880
noise if it's smaller than some amount, and we consider it meaningful and we do not want to compress

08:34.880 --> 08:35.440
it out.

08:35.480 --> 08:37.640
If it's bigger than that threshold.

08:38.800 --> 08:47.600
And what do we take as a threshold in the context of PCA and the SVD, we can take the percent of total

08:47.600 --> 08:50.760
data variability as a threshold.

08:51.400 --> 09:00.690
So what you see in these plots here are the singular value spectrum of a data matrix converted to percent

09:00.730 --> 09:05.530
variance, using the formula that I showed in the previous slide.

09:06.250 --> 09:13.050
And this is the kind of pattern that you see very often in all sorts of data in biological systems and

09:13.050 --> 09:14.770
physical systems and so on.

09:15.050 --> 09:21.250
The first couple of components tend to account for a relatively large amount of variability.

09:21.730 --> 09:23.010
And then that goes down.

09:23.010 --> 09:28.570
And the other components account for a relatively small amount of variability.

09:29.090 --> 09:33.290
And what you see over here is exactly the same data.

09:33.290 --> 09:35.650
But now it's the cumulative sum.

09:36.170 --> 09:41.810
So the first point down here is equal to this point over here it's around 25.

09:42.290 --> 09:45.290
And then the second point here is around 40.

09:45.490 --> 09:48.810
That is the cumulative sum of the first two points here.

09:48.810 --> 09:52.130
So 25 plus 15 and so on.

09:53.050 --> 10:00.730
So the interpretation of this cumulative variance explained plot is that, for example, if you take

10:00.730 --> 10:09.170
the first 20 components, then that together all first 20 components together accounts for around 70%

10:09.170 --> 10:12.810
of the total variability of the data set.

10:13.570 --> 10:17.650
So now we can pick some threshold, let's say 80%.

10:17.930 --> 10:26.250
And we can ask how many components in the data or in the PCA of the data does it take to account for

10:26.290 --> 10:30.330
80% of the variance of the entire data set?

10:30.850 --> 10:35.250
And that is the number that we take to be the effective dimensionality.

10:35.850 --> 10:38.890
So let's say there are 100 components in total.

10:39.050 --> 10:43.890
In this case we would say the effective dimensionality is around 55.

10:44.450 --> 10:48.970
Now we cannot actually visualize a 100 dimensional space.

10:49.250 --> 10:53.490
But the interpretation would be the same as this piece of paper.

10:53.810 --> 10:59.020
So inside the LM there is a 100 dimensional space.

10:59.180 --> 11:06.340
But all of the token embeddings are actually only really occupying 55 of those dimensions.

11:06.540 --> 11:11.780
The other 45 dimensions are kind of like the width of the paper.

11:12.220 --> 11:19.540
Now to be clear, a cumulative percent variance explained threshold of 80 is way too low.

11:20.020 --> 11:27.220
Normally you would use 95 or 99%, maybe 99.9% of the variance.

11:27.580 --> 11:34.060
But for illustration purposes, I didn't feel like going out to all of the 1000 components that are

11:34.100 --> 11:40.700
actually in this dataset, just because I wanted to visualize this descent over here to the left.

11:41.060 --> 11:47.260
So I'm just drawing these lines at 80, just to give you a visual intuition of the analysis.

11:47.380 --> 11:51.980
And you will see how this actually looks when I switch to code in a few minutes.

11:52.500 --> 11:55.550
And also just to be clear about the interpretation.

11:55.830 --> 12:03.270
All of the dimensions are technically mathematically present, but there's such a tiny amount of variability

12:03.270 --> 12:08.830
in these later components that we consider it to be noise or unimportant.

12:09.350 --> 12:17.310
Of course, that is a huge assumption, and it is certainly very possible that tiny amounts of variance

12:17.310 --> 12:23.710
in just the right dimensions, in just the right neurons are actually really important for guiding the

12:23.710 --> 12:25.030
model forwards.

12:26.110 --> 12:34.030
Nonetheless, you can run this kind of analysis and then compare the results for different layers or

12:34.030 --> 12:40.630
different data sets, different token types or categories, different parts of speech, different context,

12:40.630 --> 12:44.590
windows, etc., etc. it's quite a versatile method.

12:44.750 --> 12:51.550
And even though there are some arbitrary features like what number to use as a threshold, the overall

12:51.550 --> 12:58.470
patterns are going to be pretty similar when you compare for different categories or different layers.

12:59.070 --> 13:06.990
So in the Python demo, in this video I will show you how to write and use and run the code to generate

13:06.990 --> 13:08.830
these kinds of analyses.

13:08.990 --> 13:16.390
And then in the code challenge in the next video, you are going to adapt and expand these methods to

13:16.430 --> 13:21.070
a much bigger model, to more layers and to a couple of different data sets.

13:22.430 --> 13:25.670
So here's what we're going to do in the Python demo.

13:26.030 --> 13:29.670
I'll start by importing GPT two XL.

13:30.190 --> 13:32.070
You have seen this model before.

13:32.110 --> 13:38.910
It has 48 transformer blocks and an embeddings dimensionality of 1600.

13:39.310 --> 13:46.190
And that is great because it means that when we set up the effective dimensionality analysis, we can

13:46.190 --> 13:54.000
apply it to every single layer in exactly the same way, with exactly the same parameters and thresholds.

13:54.720 --> 14:01.160
The tokens I will use to process the data and get the activations are from the book through the Looking

14:01.160 --> 14:04.520
glass, which we've used several times before in this course.

14:05.080 --> 14:11.560
And as a reference, what I'm going to do is completely scramble all of these tokens.

14:12.000 --> 14:18.880
So it's exactly the same tokens in both analyses, but just with different orderings.

14:19.360 --> 14:26.200
So that's a pretty interesting comparison, because any difference that we observe in effective dimensionality

14:26.320 --> 14:34.600
cannot be attributable to the tokens themselves, but instead are entirely attributable to the order

14:34.760 --> 14:36.560
in which the tokens appear.

14:37.720 --> 14:41.880
Now for this analysis we will use the hidden state activations.

14:41.880 --> 14:43.480
We're not using any hooks.

14:43.640 --> 14:48.560
You know how to do that yourself if you want to follow up on those kinds of analyses.

14:48.680 --> 14:53.760
But to keep it simple and to give some variety, I'm just going to show you.

14:53.800 --> 14:55.400
Hidden state activations.

14:55.880 --> 15:03.760
Remember that the hidden state activations correspond to the final outputs of each layer in the model,

15:04.080 --> 15:09.320
including the initial embeddings, and then all of the transformer blocks.

15:10.000 --> 15:17.840
So here is what the for loop looks like over all the layers where I extract the activations mean center.

15:17.880 --> 15:26.760
Perform PCA, or at least the initial part of PCA using the SVD, and then calculate the percent variance

15:26.760 --> 15:27.480
explained.

15:27.480 --> 15:35.720
And here I find the number of components that it takes to account for 95% of the variability.

15:35.880 --> 15:41.440
I will unpack what this line of code means and what it does when I switch to code.

15:41.760 --> 15:49.280
This whole analysis here is then repeated for the shuffled tokens, and that will give me a plot that

15:49.280 --> 15:50.360
looks like this.

15:50.680 --> 15:58.530
So the color lines up here show the cumulative percent variance explained for the real tokens and the

15:58.570 --> 15:59.970
shuffled tokens.

16:00.410 --> 16:07.450
And the line color corresponds to the hidden layer with yellow lines going deeper into the model.

16:07.970 --> 16:16.290
And then the scatter plot down here shows the transformer block on the x axis, and the number of dimensions

16:16.290 --> 16:21.570
it takes to get to 95% variance explained on the y axis.

16:22.090 --> 16:29.290
It's pretty interesting to see not only the laminar profile of dimensionality, expansion and contraction

16:29.290 --> 16:37.090
and expansion, and a little bit of contraction again, but also the real difference between the real

16:37.130 --> 16:41.690
tokens, the real token sequence order and the shuffled tokens.

16:42.050 --> 16:44.930
And over here is also an interesting data point.

16:44.970 --> 16:51.810
Remember that the first hidden state is the output of the token plus position embeddings.

16:52.010 --> 16:55.210
Now, those appear to be exactly identical.

16:55.210 --> 16:57.330
Or maybe really, really, really close.

16:57.610 --> 17:05.250
Of course, that is a sensible result, considering that there is no context processing that happens

17:05.250 --> 17:07.370
at the level of the embeddings.

17:07.610 --> 17:12.770
The context modulations only start happening in the transformer blocks.

17:12.930 --> 17:15.530
In particular in the attention subblocks.

17:16.850 --> 17:22.690
Well, I'm sure you are super duper excited to see the code and start experimenting around with it.

17:22.690 --> 17:25.050
So let's now switch to Python.

17:25.890 --> 17:27.930
Here are some libraries to import.

17:27.930 --> 17:33.930
Here I'm importing GPT two XL and pushing it to the GPU.

17:34.130 --> 17:37.090
And of course I'm setting it to eval mode.

17:37.570 --> 17:39.490
Here I'm incorporating the text.

17:39.530 --> 17:45.970
We are actually going to look at some different texts in the next exercise in the code challenge.

17:46.090 --> 17:49.890
But for here I'm just taking actually not that much.

17:49.890 --> 17:52.900
Just a couple of thousand tokens actually.

17:52.900 --> 17:55.140
Just really 1000 token.

17:55.180 --> 17:59.100
And the reason why I start here, let me just show you the beginning of this.

17:59.740 --> 18:06.580
So the beginning of the book is actually just this preliminary information about Project Gutenberg and

18:06.580 --> 18:06.860
so on.

18:06.860 --> 18:09.860
So I just wanted to cut out that information.

18:09.860 --> 18:15.020
So I'm just getting 1000 tokens from the actual book itself.

18:15.420 --> 18:15.700
Okay.

18:15.740 --> 18:18.740
So those are the tokens that I will push through the model.

18:18.980 --> 18:23.620
And here are the shuffled tokens that we will use as a reference.

18:23.740 --> 18:25.340
So it's all the same tokens.

18:25.340 --> 18:33.740
All I'm doing is randomly, completely randomizing the order of the tokens that are already present.

18:33.740 --> 18:36.460
So it's a pretty interesting experimental approach.

18:36.460 --> 18:41.260
It is literally exactly the same tokens that go into the model.

18:41.260 --> 18:43.900
But of course the sequence is different.

18:44.540 --> 18:44.900
Okay.

18:44.940 --> 18:51.660
So then here I'm pushing the real and the shuffle Shuffled tokens through the model, checking the size

18:51.660 --> 18:57.980
of the output and creating a variable called num hidden for the number of hidden layers.

18:58.100 --> 19:03.180
That's just a variable that will be useful for the, uh, for the for loops later on.

19:03.500 --> 19:10.380
Okay, so we see that the hidden states is of size one by 1000 by 1600.

19:10.540 --> 19:20.900
So one sequence in this batch and 1000 tokens and 1600 dimensions, uh, in the uh, embeddings and

19:20.900 --> 19:24.220
the number of hidden states, very, uh, tensors.

19:24.220 --> 19:28.220
So the number of these data matrices that we have is 49.

19:28.260 --> 19:34.820
That corresponds to the 48 transformer blocks in GPT two XL plus.

19:34.820 --> 19:40.900
The first one is the output of the token embeddings plus the position embeddings.

19:41.460 --> 19:41.700
Okay.

19:41.740 --> 19:44.860
And then here I calculate the dimensionality metrics.

19:44.860 --> 19:49.910
I already showed this code in the uh slides.

19:50.070 --> 19:57.350
So I'm looping over all of the layers, extract the activations, uh, bring them back from the GPU

19:57.350 --> 20:01.070
to the CPU and then numpy ify them.

20:01.550 --> 20:08.630
By the way, this here this code works if you have exactly one sequence in your batch.

20:08.750 --> 20:14.630
If you have multiple sequences in a batch, then you would need to modify this code a little bit.

20:14.630 --> 20:19.990
So just something to keep in mind if you are going to use this code for other analyses.

20:20.510 --> 20:21.630
Uh, here I am.

20:21.670 --> 20:22.590
Mean centering.

20:22.590 --> 20:29.790
That's an important first step in, uh, principal components analysis and SVD and Eigendecomposition.

20:29.790 --> 20:36.790
If you do not mean center, then the first component simply points to the mean offset, which is not

20:36.790 --> 20:39.310
really what we want in these kinds of analyses.

20:39.350 --> 20:45.710
We want all the components to point along the directions of covariance and not the mean offset.

20:45.710 --> 20:47.790
So the entire data is shifted.

20:47.790 --> 20:53.160
It's translated so that it's centered at the origin of the space.

20:54.240 --> 20:54.560
Okay.

20:54.600 --> 20:58.000
Then here I do singular value decomposition.

20:58.040 --> 21:06.720
This function returns the left singular vectors, the singular values, and the right singular vectors.

21:06.800 --> 21:10.600
So it gives three outputs for this particular analysis.

21:10.640 --> 21:13.440
We do not care about the vectors.

21:13.440 --> 21:15.480
We only want the values.

21:15.480 --> 21:21.280
So that's why I'm just exporting one over here the second of these outputs.

21:21.360 --> 21:23.560
So let me first show you what this looks like.

21:24.280 --> 21:25.200
Uh let's see.

21:25.200 --> 21:28.440
So I can just write out s you can see it's a big vector.

21:28.480 --> 21:29.960
There's a lot of these numbers.

21:29.960 --> 21:32.240
There's a thousand of them in particular.

21:32.240 --> 21:34.800
And actually, you know maybe I will start with this.

21:35.240 --> 21:38.400
So x dot shape, this is uh yeah.

21:38.440 --> 21:43.880
It's the activations matrix, the output of one layer.

21:44.080 --> 21:48.160
And it's 1000 tokens by 1600 Dimensions.

21:48.320 --> 21:48.640
Okay.

21:48.680 --> 21:53.240
So then we get s dot shape and there are a thousand of those.

21:53.400 --> 21:56.280
And why are there a thousand and not 1600?

21:56.480 --> 22:04.200
Because it's whatever is the smaller of the two dimensions in the data matrix for linear algebra reasons

22:04.200 --> 22:06.760
that you can learn about in a linear algebra course.

22:07.240 --> 22:07.560
Okay.

22:07.600 --> 22:09.800
Let's uh, let's just have a quick look.

22:11.320 --> 22:14.400
So here we see the singular value spectrum.

22:14.400 --> 22:16.640
It looks like it's very large in the beginning.

22:16.640 --> 22:17.800
And then it goes down.

22:17.840 --> 22:24.520
Now these raw values here, these are related to the scale of the data to the numbers in the data.

22:24.760 --> 22:28.560
And that's fine if you have some way to interpret that.

22:28.560 --> 22:35.640
But mostly what we want to do with these singular values, especially for a principal components analysis

22:35.640 --> 22:43.920
or compression or dimensionality analysis, is to transform them into percent of variance accounted

22:43.920 --> 22:44.360
for.

22:44.880 --> 22:46.240
And that you see here.

22:46.240 --> 22:54.450
So each individual singular value squared, divided by the sum of all of the singular values squared.

22:54.650 --> 23:01.730
This gives me proportion of variance explained, and then I multiply it by 100 and that gives me percent

23:01.930 --> 23:03.410
of variance explained.

23:03.410 --> 23:07.770
So now I can plot that one and that you see here.

23:07.770 --> 23:12.410
So the first component accounts for around 23% of the variance.

23:12.410 --> 23:19.010
And all of these components down here account for a tiny tiny fraction of the variance like way less

23:19.010 --> 23:20.170
than 1%.

23:20.450 --> 23:28.130
So, you know, if you think back to this analogy with a piece of paper, this is like a 1000 dimensional

23:28.130 --> 23:37.530
piece of paper, but, you know, maybe two or 800 or 900 of these sides of the paper are so thin that

23:37.570 --> 23:39.730
they contain very little information.

23:39.730 --> 23:44.690
They are technically present in the piece of paper, but they're super duper thin.

23:44.810 --> 23:53.130
And really all the thickness in the paper is happening in the first several dozen of these, uh, components.

23:53.770 --> 23:54.090
Okay.

23:54.130 --> 24:00.730
So now we can transform this into the cumulative sum, and that looks like this.

24:00.730 --> 24:08.010
So now if we want to know how many components, how many directions in the data space does it take to

24:08.050 --> 24:11.090
account for 90% of the variance?

24:11.090 --> 24:15.250
All we have to do is read off 90% over here.

24:15.250 --> 24:18.370
So 90 that corresponds to I don't know what this is like.

24:18.410 --> 24:26.130
Maybe, uh, one 190 components gets us to 90% variance.

24:27.290 --> 24:33.410
So now what we can do to analyze this more concretely is say which of these.

24:33.450 --> 24:35.890
Actually, let me just show you what this looks like in numbers.

24:36.290 --> 24:39.930
So it starts off as around 22%.

24:40.330 --> 24:43.090
And then it keeps getting monotonically higher.

24:43.250 --> 24:49.940
But the rate at which it increases is is decreasing because all of these successive components account

24:49.940 --> 24:56.860
for like 0.010.00 3% variance from this component to the next.

24:57.100 --> 24:57.460
Okay.

24:57.500 --> 25:06.180
So now what we want to do is find how many components it takes to account for, let's say, 95% of the

25:06.180 --> 25:07.060
variance.

25:07.180 --> 25:16.500
So one way we can do that is having this boolean here and saying which of these vectors are above 95.

25:16.860 --> 25:19.780
And what we want is this value over here.

25:19.900 --> 25:28.820
That is the first component index at which the cumulative sum of all of the variances explained accounts

25:28.820 --> 25:31.860
for more than 95%.

25:32.060 --> 25:34.660
So then we can use numpy dot where.

25:35.100 --> 25:38.620
And basically that tells us, you know, this is the answer that we're looking for.

25:39.660 --> 25:42.300
So I hope that explanation made sense.

25:42.300 --> 25:47.660
That is the explanation of this line of code down here.

25:48.340 --> 25:48.700
Okay.

25:48.740 --> 25:51.540
So that is all of this code here.

25:51.860 --> 25:54.740
This code here is exactly the same code.

25:54.740 --> 25:56.140
It's copy pasted.

25:56.260 --> 25:58.380
I removed some of the comments.

25:58.420 --> 26:05.300
The only difference is using this variable here for the shuffled tokens instead of the real tokens.

26:05.500 --> 26:12.300
Uh, and then yeah, here I'm putting the results into a different element on this final dimension for

26:12.300 --> 26:14.020
these results matrices here.

26:14.420 --> 26:17.980
This code cell here is a little bit of an aside.

26:18.020 --> 26:23.740
This is just the code that I wrote to generate the figure that I showed in the slides.

26:24.300 --> 26:26.020
Uh, you don't really need it.

26:26.060 --> 26:32.180
It's not really a core part of this Python demo, but I just thought I would include it anyway just

26:32.220 --> 26:33.780
because I used it in the slides.

26:34.140 --> 26:37.780
Okay, now what I want to do is show the visualization.

26:37.940 --> 26:42.740
So, uh, let me show the plot first, and then I'll go back and explain each part of it.

26:43.020 --> 26:47.590
So this plot here shows the cumulative variance explained.

26:47.590 --> 26:51.790
That's literally exactly the plot that I showed you a moment ago.

26:51.790 --> 26:54.950
But here I'm showing it as lines instead of dots.

26:55.110 --> 26:59.390
And I'm using a separate color for each of the layers.

26:59.950 --> 27:09.310
And then to get the effective dimensionality at 95% of variance explained, what we do is find the 95

27:09.310 --> 27:10.950
percentile up here.

27:11.110 --> 27:18.790
And for each layer, what is the number of components that matches that line at 95.

27:19.070 --> 27:19.390
Okay.

27:19.430 --> 27:23.590
So then we extract that for all of these lines and all of these lines.

27:23.590 --> 27:26.710
And then I can make this scatter plot that you saw here.

27:27.150 --> 27:33.790
So it's pretty interesting to compare the real vectors versus the shuffled token vectors.

27:33.910 --> 27:39.950
And essentially what you see is that they overlap pretty consistently for the first, let's say quarter

27:39.990 --> 27:41.870
of the Transformers.

27:42.430 --> 27:43.670
And what does this tell us?

27:43.670 --> 27:51.550
This tells us that within each of these transformers, the amount of space in the total data matrix

27:51.550 --> 27:59.350
space that the actual token processing is taking up is pretty comparable for the shuffled and the real

27:59.390 --> 28:00.150
tokens.

28:00.350 --> 28:02.670
And it's only later in the model.

28:02.910 --> 28:08.750
Later in the transformer blocks, that we start to see some real differentiation where the shuffled

28:08.790 --> 28:13.030
tokens are actually occupying a slightly smaller subspace.

28:13.190 --> 28:17.710
And the real tokens are occupying a larger space.

28:17.830 --> 28:19.950
And why does this generally get bigger?

28:20.190 --> 28:28.510
Remember that as we get deeper into the model, the model is learning to incorporate more and more context

28:28.630 --> 28:35.350
into the processing of the current token in order to transform it into a next token.

28:35.750 --> 28:43.550
So that is pretty difficult to do, and therefore the complexity of the representations and the calculations

28:43.550 --> 28:44.960
Populations increases.

28:44.960 --> 28:51.920
And so the model needs to basically expand the effective dimensionality of the analysis here.

28:52.480 --> 28:55.320
Of course you see these are also a little bit unusual.

28:55.320 --> 28:56.800
These are a bit outliers here.

28:56.960 --> 28:59.320
This is the final transformer block.

28:59.320 --> 29:06.800
This is what goes directly to the embeddings matrix and then to the transformed into text after one

29:06.800 --> 29:08.680
token is selected.

29:09.400 --> 29:16.640
Once you get the concept of this analysis approach you can apply it to lots of different data sets.

29:17.120 --> 29:24.400
And now the exact numerical results will depend on the threshold, which itself is a bit arbitrary.

29:24.440 --> 29:29.400
You know why use 95% variance explained as a threshold?

29:29.400 --> 29:31.240
Why not 94%?

29:31.240 --> 29:34.200
Or how about 98.327%?

29:34.800 --> 29:41.840
So it is a good idea to explore different thresholds and interpret relative dimensionalities and not

29:41.840 --> 29:43.400
the exact number.

29:44.200 --> 29:51.040
Also, keep in mind that there really are no linear calculations inside a deep learning model as big

29:51.040 --> 29:54.160
as an LN inside the model.

29:54.320 --> 30:02.120
Every linear calculation like dot product, for example, has a nonlinear transform before it and after

30:02.120 --> 30:02.480
it.

30:02.880 --> 30:09.920
So there are like slices of linearity, but those are sandwiched in between non-linearities.

30:10.480 --> 30:14.800
So I am a huge fan of using linear analysis methods.

30:14.800 --> 30:23.040
They tend to be stable and robust and easier to interpret, but a compelling result of a linear analysis

30:23.200 --> 30:28.440
does not mean that the computation itself inside the model is linear.

30:29.560 --> 30:36.400
Anyway, if you enjoyed this code demo, then you are definitely going to enjoy the code challenge in

30:36.400 --> 30:41.880
the next video, where you will get to build on the knowledge and skills that you gained here.

WEBVTT

00:02.440 --> 00:12.440
This is the first video in a collection of several videos on mutual information covariance and mechanistic

00:12.440 --> 00:13.640
interpretability.

00:14.280 --> 00:22.800
It is in the section on Examining Layers, but mutual information and covariance are useful and general

00:22.800 --> 00:29.800
analysis methods that work for lots of different situations and many different features of the model,

00:29.800 --> 00:31.840
and not just layers per se.

00:32.680 --> 00:41.000
So the purpose of this video is to introduce you to the formula and interpretation of mutual information,

00:41.320 --> 00:46.960
and I'll show you how to implement it using several different approaches in Python.

00:47.480 --> 00:56.000
So this video is not really about LX per se, but the methods that you will use in later videos.

00:56.480 --> 01:00.780
Mutual information is based on a quantity called entropy.

01:01.100 --> 01:06.300
So let me first introduce you to the math and interpretation of entropy.

01:06.460 --> 01:09.780
And then we can switch to discussing mutual information.

01:10.580 --> 01:17.300
Actually the word entropy is used in several different contexts and it has several different meanings.

01:17.820 --> 01:25.460
You might be familiar with the concept of entropy in physics and thermodynamics, where that term entropy

01:25.460 --> 01:32.900
refers to the fact that order tends to move towards disorder, and things that are neat and organized

01:32.900 --> 01:36.540
eventually become chaotic and disorganized.

01:37.060 --> 01:43.780
Unfortunately, this interpretation of entropy leads to some very difficult existential thoughts about

01:43.780 --> 01:44.500
the universe.

01:44.500 --> 01:45.460
But don't worry.

01:45.700 --> 01:53.580
The way that we will use entropy here in data analysis is a different interpretation from Claude Shannon,

01:54.100 --> 02:01.550
and it is much more upbeat and happy than the Then the thermodynamics interpretation in the context

02:01.590 --> 02:03.670
of information theory.

02:04.110 --> 02:10.750
Entropy is a quantification of the amount of variability or unexpectedness.

02:10.790 --> 02:14.750
It's also usually called surprise in this literature.

02:15.150 --> 02:16.110
In a data set.

02:16.990 --> 02:25.510
So this graph over here shows entropy on the y axis and the probability of an event occurring on the

02:25.510 --> 02:26.590
x axis.

02:27.150 --> 02:35.070
Now when the probabilities are close to zero or close to one, then there isn't much unexpectedness

02:35.070 --> 02:35.790
happening.

02:36.350 --> 02:42.590
For example, if we think about the probability of the sun coming up in the morning, that's a very

02:42.590 --> 02:43.710
high probability.

02:43.990 --> 02:48.510
Therefore, you are not surprised when the sun comes up in the morning.

02:49.150 --> 02:55.790
On the other hand, something that has a probability of 0.5 has a lot of entropy.

02:56.190 --> 02:58.610
So here you can think about flipping a coin.

02:59.050 --> 03:05.850
The coin has an equal chance of landing on heads or tails, and therefore the outcome is more variable.

03:06.250 --> 03:12.890
Now, in the context of information theory that probability has a high entropy.

03:13.530 --> 03:19.450
So you can also think of the entropy as being like the predictability of a system.

03:19.850 --> 03:25.930
So if you can accurately predict a particular outcome then the entropy will be low.

03:26.370 --> 03:31.330
But if you cannot predict an outcome then the entropy is higher.

03:31.930 --> 03:35.850
Now these are all somewhat vague and imprecise statements.

03:35.970 --> 03:38.210
And I'll show you the formula on the next slide.

03:38.330 --> 03:45.490
But I do want to lighten the mood a little bit after the previous slide, and say that entropy in information

03:45.490 --> 03:52.570
theory states that everything and everyone is great and happy, and we all like being surprised because

03:52.570 --> 03:55.780
surprises convey more Information.

03:56.660 --> 03:58.380
Okay, let's do some math.

03:59.620 --> 04:03.700
Here is the formula for entropy in information theory.

04:04.100 --> 04:07.540
It's often indicated using a capital H.

04:08.300 --> 04:11.180
The formula itself is pretty straightforward.

04:11.180 --> 04:17.180
You need to know the probability of certain events taking place or being observed.

04:17.700 --> 04:23.180
And then you simply multiply the probability by the log of the probability.

04:23.380 --> 04:27.660
And then you sum up over all of the possible data categories.

04:27.660 --> 04:30.340
So that would be n data categories.

04:30.340 --> 04:32.140
And then you have your entropy.

04:33.060 --> 04:41.260
Now because probabilities are all between 0 and 1 the log of those probabilities will be negative.

04:41.700 --> 04:45.180
And so the sum will necessarily be negative.

04:45.700 --> 04:52.140
Therefore we add a negative sign out front just so that entropy ends up being positive.

04:52.380 --> 04:55.430
And that just makes the interpretation a little bit easier.

04:56.230 --> 05:02.710
You can also see that there is a potential problem with this formula when the probability is zero.

05:03.270 --> 05:07.230
So the log of zero is not a finite number.

05:07.230 --> 05:10.790
So we can have numerical problems with this calculation.

05:11.950 --> 05:14.870
In practice there are a few ways to deal with this.

05:14.990 --> 05:22.470
One way is to ignore from the analysis any probabilities that are exactly zero.

05:23.070 --> 05:28.430
And another option is to add a very tiny number inside the log.

05:28.710 --> 05:35.870
Now, that number would be so small that it doesn't really impact the analysis for non-zero probabilities.

05:36.190 --> 05:43.390
And if the probability is zero, then the log of that tiny number will be some really large negative

05:43.390 --> 05:44.030
number.

05:44.150 --> 05:49.430
But then it gets multiplied by the probability out here, which is still zero.

05:49.950 --> 05:55.810
Anyway, I'll show you some of these Approaches for numerical stability when I switch to code.

05:56.450 --> 06:04.290
Now, one difficulty of using entropy is that this formula is really designed for categorical events,

06:04.450 --> 06:12.850
like a coin landing on heads or tails, or a token index, or a grammatical category like noun or verb

06:12.850 --> 06:14.010
or adjective.

06:14.650 --> 06:24.210
But we are going to use entropy and mutual information on activation values, which are continuous numbers.

06:24.570 --> 06:26.650
So that causes some headaches.

06:26.650 --> 06:32.330
And the solution is to use histograms to approximate probabilities.

06:32.930 --> 06:39.650
And unfortunately that also can lead to some headaches because it means that we need to choose the number

06:39.650 --> 06:43.250
of bins and the method of binning the data.

06:43.890 --> 06:48.970
And these parameter choices can impact the entropy estimation.

06:49.610 --> 06:52.430
I'll talk about this in more detail in code.

06:52.710 --> 06:59.350
Although to be honest, a really detailed discussion of this is more for a course on statistics or machine

06:59.350 --> 06:59.830
learning.

06:59.830 --> 07:03.030
So I'm not going to get into a ton of detail about it here.

07:03.910 --> 07:13.070
Anyway, the upshot of all this math is that when the data set has a lot of variability, then entropy

07:13.070 --> 07:14.070
is higher.

07:14.710 --> 07:21.670
And the flip side of that interpretation is that when entropy is low, it means that the data has a

07:21.670 --> 07:23.430
lot of redundancies.

07:23.430 --> 07:26.070
That means the data are highly predictable.

07:26.830 --> 07:31.430
In other words, entropy is a measure of the variability of the data.

07:32.750 --> 07:39.310
But you're already familiar with another quantification of variability, which is the standard deviation

07:39.310 --> 07:40.310
or variance.

07:40.790 --> 07:44.030
So how does entropy relate to variance.

07:44.670 --> 07:53.400
Well, the short version of the story is that variance is a linear measure of variability, whereas

07:53.400 --> 07:56.600
entropy is a nonlinear measure.

07:57.240 --> 08:04.240
So entropy also doesn't really make assumptions about the distribution of the data, whereas variance

08:04.240 --> 08:06.640
is based on interpreting the mean.

08:07.240 --> 08:15.200
And so variance and also standard deviation are mainly interpretable or mainly suitable for data that

08:15.200 --> 08:21.720
are at least somewhat normally distributed, and that is not the case for entropy with entropy.

08:21.760 --> 08:26.200
You can have any data distribution that happens to be observed.

08:27.240 --> 08:34.080
In the next video, I'm going to talk more about the relationship between mutual information and covariance.

08:34.080 --> 08:39.520
So I'll also get back to this discussion in later videos.

08:39.880 --> 08:43.360
So let's now talk about mutual information.

08:43.680 --> 08:52.610
If you think of this Venn diagram as the entropy in variable x and this is the entropy in variable y.

08:53.090 --> 08:55.570
Then where they overlap over here.

08:55.930 --> 08:58.570
That is the mutual information.

08:59.530 --> 09:04.810
I will show two formulas for calculating mutual information on the following slide.

09:05.050 --> 09:13.370
But essentially the idea is that mutual information is designed to quantify how much you can know about

09:13.370 --> 09:16.770
one variable if you measure another variable.

09:17.330 --> 09:19.450
And we can think about two extremes.

09:19.450 --> 09:25.370
Let's say you measure the amount of hydrogen in different stars in the Milky Way galaxy.

09:25.770 --> 09:29.690
And you also measured the price of Bitcoin in the past ten years.

09:30.130 --> 09:35.130
Those two variables have literally nothing whatsoever to do with each other.

09:35.250 --> 09:39.170
And so the mutual information between them would be zero.

09:40.010 --> 09:46.970
On the other hand, let's say you measure the price of Bitcoin and the price of Ethereum over the past

09:46.970 --> 09:47.950
ten years.

09:48.510 --> 09:52.310
Now, those two variables are closely related to each other.

09:52.550 --> 09:59.750
They're not identical, but it certainly is the case that knowing the price of Ethereum gives you some

09:59.750 --> 10:02.430
indication of the price of Bitcoin.

10:02.990 --> 10:09.990
So the mutual information between those two variables would be significantly larger than zero.

10:11.350 --> 10:18.590
And what exactly could you say about the price of Bitcoin if you knew the price of Ethereum using mutual

10:18.590 --> 10:19.390
information?

10:20.310 --> 10:25.230
Well here mutual information is not so super duper helpful.

10:25.630 --> 10:32.190
If you want to make an actual prediction about the value of one variable given the value of another

10:32.190 --> 10:32.950
variable.

10:33.310 --> 10:37.270
Then mutual information is just not the proper analysis.

10:37.750 --> 10:45.670
So mutual information tells you about the overlap of predictability of one variable and another, but

10:45.670 --> 10:52.570
it does not tell you the exact values of one variable, given what you know about another variable.

10:54.130 --> 10:59.210
Mutual information is often indicated using the capital letter I.

10:59.930 --> 11:05.810
So here you see two different but equivalent formulas for mutual information.

11:06.450 --> 11:11.290
Now mutual information is always calculated between two variables.

11:11.290 --> 11:15.530
It's a bivariate measure of statistical dependency.

11:16.170 --> 11:20.290
So x and y would be two different variables.

11:20.370 --> 11:26.170
Let's say for example two different neurons in an MLP transformer Subblock.

11:27.050 --> 11:35.450
Now this first formula here looks an awful lot like entropy, except that the joint probability of x

11:35.450 --> 11:40.530
and Y is normalized by the individual probabilities.

11:41.770 --> 11:48.020
This second formula here is more similar to the Venn diagram that I showed in the previous slide, which

11:48.020 --> 11:49.140
you also see here.

11:49.580 --> 11:55.940
So in other words, you add up the entropies of each of the two variables individually, and then you

11:55.980 --> 12:00.460
subtract off what you can learn about the two of them together.

12:01.020 --> 12:07.660
Now, it might initially seem strange that we are adding the individual entropies and then subtracting

12:07.660 --> 12:14.260
off the joint entropy, but remember that entropy is defined using a minus sign.

12:14.660 --> 12:22.940
So you can actually think about inverting these signs to say that it's the joint entropy minus the entropies

12:22.940 --> 12:25.260
of the two individual variables.

12:26.180 --> 12:33.380
By the way, you might have noticed that the formula I used for entropy had a log two base in a previous

12:33.380 --> 12:36.260
slide, whereas here I'm using the natural log.

12:36.820 --> 12:40.940
Now in practice you would consistently use one or the other.

12:40.940 --> 12:48.270
So either log base two or the natural log, but the base of the logarithm doesn't actually matter in

12:48.310 --> 12:51.190
most applications of mutual information.

12:51.710 --> 12:58.350
It does change the units of the results, for example bits versus nats.

12:58.390 --> 13:04.710
In this case, but that doesn't change the relative mutual information between different variables or

13:04.710 --> 13:05.910
different conditions.

13:06.550 --> 13:14.030
I would also like to point out that all of these formulas for entropy and mutual information are not

13:14.030 --> 13:16.670
based on the data themselves.

13:16.830 --> 13:22.550
They're based on the probabilities of the data values or the data bins.

13:23.270 --> 13:29.630
And that means that the scale of the data has no impact on mutual information.

13:30.510 --> 13:37.990
So for example, if you would multiply the data values by a billion, that literally has no impact on

13:37.990 --> 13:40.030
the mutual information value.

13:40.750 --> 13:47.730
And I mentioned that as a bit of a segue to comparing mutual information to covariance, which you will

13:47.730 --> 13:49.530
learn about in the next video.

13:49.730 --> 13:53.050
And it will also be relevant a couple of videos thereafter.

13:54.290 --> 14:00.730
Anyway, as I mentioned a few slides ago, this formula is pretty straightforward when you have discrete

14:00.730 --> 14:01.970
categories.

14:02.130 --> 14:05.970
But we are working with continuous activation numbers.

14:05.970 --> 14:07.530
So floating point numbers.

14:07.810 --> 14:15.330
And so all of these calculations have to be based on estimating probabilities using histograms and joint

14:15.370 --> 14:16.290
histograms.

14:16.730 --> 14:19.850
And that leads me to the Python demo.

14:20.170 --> 14:28.930
The purpose of the Python demo is to show you the basics of implementing entropy and mutual information.

14:29.290 --> 14:35.970
I will introduce some additional nuances to mutual information over the next several videos.

14:36.570 --> 14:44.670
So to start with I'm going to generate some bivariate data that has a statistical dependency built in.

14:44.870 --> 14:46.110
And that's what you see here.

14:46.430 --> 14:51.390
Now it's just two arbitrary variables, and it's clear that they are correlated.

14:51.390 --> 14:53.350
So they share some information.

14:53.870 --> 15:01.710
And the picture you see over here is the discretized joint histogram of these two variables.

15:02.790 --> 15:10.310
Entropy and mutual information is not actually calculated from the original full resolution data.

15:10.310 --> 15:15.630
Instead it's calculated from this map of counts.

15:16.190 --> 15:24.150
Now that is tricky because the number of bins that you use to generate this lower resolution histogram

15:24.750 --> 15:31.910
is a parameter of the analysis, and you will get different results if you use, for example, 15 versus

15:31.910 --> 15:35.430
25 versus 50 bins in the variables.

15:36.430 --> 15:41.480
You can also see that there are a lot of bins with no data points in them.

15:41.880 --> 15:47.760
Now that can create fatal numerical errors in the formula with the logarithm.

15:47.920 --> 15:50.040
So we need to address that problem.

15:51.080 --> 15:57.120
Now from this 2D histogram we can calculate the bivariate entropy.

15:57.480 --> 16:04.240
But to calculate mutual information we also need the entropies of each variable separately.

16:04.880 --> 16:11.600
Now it turns out that once we have this 2D histogram, the entropies of the individual variables can

16:11.600 --> 16:15.560
be calculated simply by taking the marginal average.

16:15.560 --> 16:22.360
So basically the projection of this blob onto the x axis and onto the y axis.

16:22.840 --> 16:24.080
And that's what you see here.

16:24.080 --> 16:29.240
These are the entropies the distributions from which entropy is calculated.

16:29.480 --> 16:32.000
Of the two individual variables.

16:32.520 --> 16:37.200
And then we can use all of those numbers to calculate mutual information.

16:38.050 --> 16:44.970
After showing you this demo of how to calculate mutual information by directly translating the formulas

16:44.970 --> 16:53.330
into code, I will do another demo where I simulate data that have no relationship whatsoever to each

16:53.330 --> 16:53.970
other.

16:53.970 --> 16:57.050
So two completely independent variables.

16:58.170 --> 17:04.170
Now we know from theory that the expected mutual information should be zero.

17:05.010 --> 17:11.850
And of course with some sampling variability and small sample sizes, the mutual information might be

17:11.850 --> 17:15.530
a little bit above zero, but it should be pretty close to zero.

17:16.090 --> 17:21.570
However, we are going to get values of around 0.5, maybe even higher.

17:22.090 --> 17:30.130
That is much bigger than what we can expect from sampling variability, and it reflects some systematic

17:30.130 --> 17:35.770
biases that are introduced by the discretization and histogram binning.

17:36.450 --> 17:43.350
So therefore I will then show you another method of calculating entropy, which actually only makes

17:43.350 --> 17:48.790
it worse, although it's still kind of interesting to see this alternative approach.

17:49.270 --> 17:57.510
And then finally I will use a method for calculating mutual information from the scikit learn library.

17:57.710 --> 18:05.070
This uses a different and more accurate way of estimating the probability distributions, and that gives

18:05.070 --> 18:11.390
us a mutual information value for uncorrelated variables very close to zero.

18:11.590 --> 18:18.390
This little bit above zero is to be expected giving given, you know, noise and sampling variability

18:18.390 --> 18:18.950
and so on.

18:20.110 --> 18:27.430
Anyway, I will have more to say about this scikit learn function over the next several videos.

18:27.910 --> 18:29.350
It is good to use.

18:29.350 --> 18:35.350
It can be more accurate than just implementing the formula directly, but it's also quite slow.

18:35.680 --> 18:43.080
and the direct implementation here actually still is quite accurate, just with a constant bias that

18:43.080 --> 18:44.280
is introduced.

18:44.880 --> 18:52.520
So you don't actually always need or maybe even want to use the scikit learn implementation.

18:53.160 --> 18:58.440
Anyway, I will now switch to code and show you how mutual information is calculated.

18:58.440 --> 19:01.800
And of course I encourage you to follow along in the code.

19:03.040 --> 19:10.760
Mostly I will just calculate entropy and mutual information directly based on the formulas using numpy.

19:11.000 --> 19:15.320
But then towards the end, as I mentioned, I will use some scikit learn tools.

19:15.320 --> 19:23.600
In particular, this function mutual information regression which is in the sklearn featureselection

19:23.640 --> 19:24.480
library.

19:24.480 --> 19:29.040
So let's import those here I'm collecting or creating the data.

19:29.280 --> 19:32.240
So I'm going to generate 200 data points.

19:32.400 --> 19:37.060
Random numbers for x y is also random numbers.

19:37.060 --> 19:39.860
However, then I'm adding x onto it.

19:40.060 --> 19:46.860
This is just a very simple way of introducing a statistical dependency between two variables.

19:46.860 --> 19:52.700
So y equals x plus some innovations if you will, or a bit of noise.

19:53.220 --> 20:00.260
Okay here I'm taking the histogram of these two data these two variables using 15 bins.

20:00.380 --> 20:05.300
And then the plotting code I encourage you to look over but it's fairly straightforward.

20:05.300 --> 20:08.260
So just creating a scatter plot here.

20:08.420 --> 20:13.260
Here is an image of the 2D histogram data.

20:13.420 --> 20:17.980
So this just reproduces a figure that I showed in the slides a moment ago.

20:18.340 --> 20:24.380
And basically each one of these blocks is color coded according to the count, which you can interpret

20:24.380 --> 20:25.660
as probability.

20:25.660 --> 20:31.180
In fact, if you would normalize this then you would get exactly the probability values.

20:31.500 --> 20:39.400
And again you also see that lots of values around here around the edges have a count of zero, which

20:39.400 --> 20:44.960
means they will also have a probability of zero, which means we're going to be taking the log of zero,

20:45.000 --> 20:46.840
which is illegal in math.

20:46.840 --> 20:50.440
And so we will need to address that problem somehow.

20:51.160 --> 20:51.480
Okay.

20:51.520 --> 20:54.960
So here what I'm doing is scaling by the sum.

20:55.160 --> 21:00.280
This converts this matrix here of counts into probabilities.

21:00.440 --> 21:06.840
So the sum over all of the p underscore z matrix values will be one.

21:07.280 --> 21:11.080
And then we get the marginal probability distributions.

21:11.080 --> 21:17.160
So the probability over x and over y simply by calculating the marginal sum.

21:17.400 --> 21:19.200
So again we're summing over.

21:19.360 --> 21:22.160
So all the columns are summing over all the rows.

21:22.200 --> 21:29.000
And that gives us the projection onto the two axes which is the marginal probability distributions for

21:29.000 --> 21:30.920
each of the variables.

21:31.490 --> 21:34.250
Here I'm calculating mutual information.

21:34.410 --> 21:34.690
Sorry.

21:34.730 --> 21:35.130
Entropy.

21:35.170 --> 21:36.570
Here I'm calculating entropy.

21:37.210 --> 21:40.370
So remember there's a minus sign and then a summation.

21:40.690 --> 21:48.530
Inside the summation we multiply the probability values by the log of the probability values.

21:48.970 --> 21:56.850
And then because some of these values will be zero I'm adding on epsilon just some very very small number.

21:57.130 --> 22:05.210
And as I mentioned in the slides the idea is that when these probabilities are non-zero then adding

22:05.210 --> 22:07.970
this tiny number does basically nothing.

22:08.530 --> 22:15.930
But when this value is exactly zero, then this log is going to be, I don't know, like minus, you

22:15.930 --> 22:19.970
know, -150 or something really large and negative.

22:20.010 --> 22:22.090
But at least it won't crash.

22:22.450 --> 22:26.130
It's not we're not going to get an error because we're taking the log of zero.

22:26.410 --> 22:32.230
And it doesn't matter what this number is because it multiplies this probability here, which is not

22:32.230 --> 22:33.990
summed with the EPs.

22:33.990 --> 22:36.470
So this actually can be zero.

22:37.150 --> 22:37.470
Okay.

22:37.510 --> 22:40.390
So that's the entropy for these two variables.

22:40.670 --> 22:42.710
It's 3.5 whatever that means.

22:42.710 --> 22:47.750
I can show you very quickly that when we change the number of bins.

22:47.910 --> 22:50.030
So let's make this be 25.

22:50.590 --> 22:53.550
And now we can calculate this again.

22:53.670 --> 22:55.550
So now just keep this in mind.

22:55.550 --> 23:00.510
These two entropy values are around 3.5 for each of the variables.

23:00.510 --> 23:03.150
And now they're around for a little bit above four.

23:03.630 --> 23:06.950
And well I guess I did regenerate the data.

23:06.950 --> 23:10.070
So maybe a better demo would be to comment this out.

23:10.070 --> 23:12.750
So I'm not getting new data, but that's okay.

23:12.790 --> 23:14.190
Actually yeah I will do that now.

23:14.430 --> 23:14.750
Okay.

23:14.790 --> 23:16.390
So comment this out.

23:17.270 --> 23:19.870
So now the data are exactly the same.

23:20.150 --> 23:24.670
But now I'm going back to 15 bins okay.

23:24.710 --> 23:32.560
And now the entropy is for a little bit above for that was with 25 bins and now with 15 bins, it goes

23:32.560 --> 23:35.880
back to being 3.33.4 or so.

23:36.320 --> 23:44.520
This is one of the tricky aspects of calculating entropy and mutual information from continuous variables,

23:44.520 --> 23:50.960
which include the activation values that we'll be using over the next several videos.

23:50.960 --> 23:54.200
But that's a level of nuance that I won't get into here.

23:54.680 --> 23:59.720
Okay, so here I'm getting the entropy over Z.

23:59.720 --> 24:01.320
So this is the joint entropy.

24:01.480 --> 24:07.520
You can see the formula is exactly the same as entropy for x and y.

24:08.000 --> 24:13.720
And here is just the literal implementation of the formula that I showed in the slides.

24:13.720 --> 24:20.320
So the entropy of x plus the entropy of y minus the entropy of Z or the joint entropy.

24:20.360 --> 24:23.000
And now we get a value that is close to one.

24:23.000 --> 24:25.040
So 0.96.

24:25.240 --> 24:30.540
So now what I want to do is go back and uh, let's see.

24:30.700 --> 24:31.660
Uncomment that.

24:31.660 --> 24:33.180
So we're generating new data.

24:33.180 --> 24:36.780
But I'm going to write x times zero.

24:37.140 --> 24:40.500
So that means that's the same thing as just deleting this actually okay.

24:40.540 --> 24:44.460
So now x and y are completely uncorrelated.

24:44.620 --> 24:46.300
You can see that here as well.

24:46.300 --> 24:51.740
You don't see any obvious linear or non-linear relationships between these variables.

24:51.860 --> 24:56.220
It's just a big isotropic cloud of dust okay.

24:56.260 --> 25:00.820
So now what is the uh the entropy is about the same.

25:00.820 --> 25:01.580
That's fine.

25:01.580 --> 25:03.900
What is the mutual information between these.

25:03.940 --> 25:06.580
Well we expect it to be exactly zero.

25:06.620 --> 25:08.700
That's the theoretical expectation.

25:09.380 --> 25:14.900
In practice when we are sampling data it's never going to be exactly zero.

25:14.900 --> 25:17.900
But we do expect it to be pretty close to zero.

25:18.060 --> 25:26.750
However here we get a value of 0.43, which is still a pretty respectably high amount of mutual information.

25:27.350 --> 25:34.830
Okay, so now what I'm going to do is come up with a different way of binning the data.

25:34.990 --> 25:41.630
So instead of binning the data to create equal sized bins where there may or may not be data values

25:41.790 --> 25:49.950
in each bin, here I'm using the percentile function to create equal sized partitions of the data.

25:50.030 --> 25:56.190
So now I'm not partitioning the range of the data into 15 equal chunks.

25:56.310 --> 26:06.230
Instead, I'm partitioning the actual data values such that there are 15 equal sized bins of data values.

26:06.430 --> 26:11.150
So all the data values are in the same number of bins.

26:11.750 --> 26:11.990
Okay.

26:11.990 --> 26:16.950
And then I basically repeat this calculation here it gets even higher.

26:16.950 --> 26:20.230
So this is unfortunately not a great solution.

26:20.550 --> 26:24.150
But here I'm using the scikit learn function.

26:24.280 --> 26:27.600
Mutual information regression.

26:28.000 --> 26:34.120
And now it's a little bit trickier to implement because you cannot just implement your two variables

26:34.120 --> 26:34.760
like this.

26:35.200 --> 26:43.720
Instead I this function expects one of the matrices to be or one of the inputs to be a matrix a two

26:43.720 --> 26:45.880
dimensional NumPy array.

26:46.160 --> 26:49.480
Now we don't have two dimensional matrices.

26:49.480 --> 26:51.800
These are matrices of numbers.

26:51.920 --> 26:53.760
These are just two vectors.

26:54.120 --> 26:58.040
So the easiest solution is just to reshape this.

26:58.040 --> 27:01.200
And actually let me show you what this does.

27:01.640 --> 27:04.560
So I can say x dot shape.

27:05.320 --> 27:08.520
And that is 200 by nothing.

27:08.520 --> 27:10.840
So this is a dimensionless array.

27:11.400 --> 27:21.000
And now when I reshape this it becomes a 200 by one matrix which is like a yeah it's like a column vector.

27:21.440 --> 27:21.760
Okay.

27:21.800 --> 27:27.420
So that suddenly works, and now we get a value that is much closer to zero.

27:27.660 --> 27:33.140
And as I mentioned in the slides, this is a numerical value for mutual information.

27:33.260 --> 27:38.700
Given a finite sample data set that you can expect to be zero.

27:38.700 --> 27:40.860
So this is just a little bit of sampling error.

27:41.540 --> 27:47.140
And what is the difference between the code that I wrote above and this function here.

27:47.180 --> 27:50.580
Basically this function is a bit more sophisticated.

27:50.580 --> 27:59.300
It uses non-parametric kernel density estimators to estimate a probability distribution that is more

27:59.300 --> 28:05.020
accurate than simply counting the number of observations in different bins.

28:05.540 --> 28:07.060
So it is more accurate.

28:07.180 --> 28:11.460
You will see in the next video or two videos for now.

28:11.780 --> 28:18.620
Over the next several videos, you will see that this method is first of all really slow, so it can

28:18.620 --> 28:24.000
be intractably Unusably slow if you have a lot of data to process.

28:24.000 --> 28:31.600
And you will also see that the difference here between 0.4 and something very close to zero.

28:31.920 --> 28:33.560
That's just a constant offset.

28:33.560 --> 28:39.600
So all of the relative mutual information values are preserved.

28:39.720 --> 28:48.640
And we just have this constant offset due to this estimation difficulty with with bins with values of

28:48.640 --> 28:49.520
zero in them.

28:50.840 --> 28:54.760
So that's the intro to Mutual information.

28:55.040 --> 28:56.760
It's a good measure to know about.

28:56.760 --> 29:05.240
It's used relatively often in science for measuring non-linear statistical dependencies between variables.

29:05.920 --> 29:13.880
In the next video, I will show you how to implement mutual information in an LLM during token processing,

29:14.080 --> 29:20.240
and then I will discuss the relationship between mutual information and covariance.

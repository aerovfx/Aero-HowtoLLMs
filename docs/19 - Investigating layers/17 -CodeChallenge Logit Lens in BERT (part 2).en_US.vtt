WEBVTT

00:02.480 --> 00:08.480
This is the continuation of the code challenge that we started in the previous video.

00:08.480 --> 00:13.720
So now we're starting with exercise four and there's six exercises in total.

00:14.280 --> 00:19.480
So we are almost ready to do the logit lens analysis with Bert.

00:20.000 --> 00:27.920
The thing is that the way that I taught you to do a logit lens analysis in the previous video was maybe

00:27.960 --> 00:30.960
not entirely the best way to explain it.

00:31.320 --> 00:38.440
Here's the thing it's not just that you multiply the intermediate layers activations by the embeddings

00:38.440 --> 00:39.240
matrix.

00:39.720 --> 00:47.040
Instead, what you really need to do is push the activations through the final part of the model, the

00:47.040 --> 00:48.920
final module in the model.

00:49.720 --> 00:59.320
Now the thing is, in an open AI GPT architecture, the final module at the end of the model is nothing

00:59.320 --> 01:02.000
more than one embeddings matrix.

01:02.000 --> 01:02.560
That's it.

01:03.320 --> 01:10.480
So that approach actually did work for GPT two, but it won't really work here with Bert.

01:10.920 --> 01:16.560
I mean, the multiplication will work, but it's not going to achieve the appropriate result.

01:17.080 --> 01:19.200
And that's what I am showing here.

01:19.200 --> 01:22.560
And that's basically the goal of this exercise.

01:23.080 --> 01:31.400
So the final like paragraph, if you will, of the GPT model this is GPT two is it contains only this

01:31.400 --> 01:35.400
one linear object, just this one linear layer.

01:35.760 --> 01:40.840
And that means we can just pull out the weights and use matrix multiplication.

01:41.360 --> 01:48.880
However, the final paragraph of the Bert model, that's what this screenshot shows, is not just this

01:48.880 --> 01:54.160
decoder, it's a sequence of transformations and normalizations.

01:55.320 --> 02:02.720
Now the thing is, in the full model, all of the weights are trained to work with this final head of

02:02.720 --> 02:06.360
the model, including these intermediate activations.

02:06.880 --> 02:10.440
So this is like the UN embeddings section.

02:10.440 --> 02:12.000
It's the embeddings module.

02:12.000 --> 02:14.520
It's not just this one matrix.

02:15.080 --> 02:19.160
And that's basically what you are going to do in this exercise.

02:19.440 --> 02:26.760
So you want to actually make a token prediction in the correct way and also in the incorrect way.

02:27.680 --> 02:28.040
Okay.

02:28.080 --> 02:30.440
So that was all just a bit of background.

02:30.440 --> 02:33.120
Now let me give you some more detail.

02:33.400 --> 02:38.160
So first run a forward pass with one token masked out.

02:38.520 --> 02:44.840
I chose to mask out the word way exactly the same as in exercise one.

02:45.200 --> 02:52.440
Only this time we really do want the Hidden States activations, because you want to test the logit

02:52.440 --> 02:55.960
lens analysis on one transformer block.

02:56.560 --> 03:04.750
Okay, so the wrong way to do it is to extract the weights matrix from this final decoder module, this

03:04.750 --> 03:09.630
final decoder layer, and then multiply the activations.

03:09.630 --> 03:17.910
That's matrix A of some intermediate transformer block layer by the transpose of this embeddings matrix.

03:17.910 --> 03:19.030
So that's the wrong way.

03:19.470 --> 03:23.590
The correct way to do it is to push the activations from.

03:23.630 --> 03:31.350
So one of the hidden states activations through this entire module, this entire layer of the model.

03:31.350 --> 03:38.390
And so that you could do by grabbing the predictions and then pushing the activations through the predictions.

03:39.470 --> 03:45.030
So I used a layer towards the end, I think it was layer 21 out of 24.

03:45.070 --> 03:46.230
I forget exactly.

03:46.590 --> 03:54.190
But anyway, when you plot the z squared logits for both the correct and the incorrect approaches,

03:54.390 --> 03:57.470
you will see quite a difference between these results.

03:57.870 --> 04:04.990
In fact, the z scores for the incorrect way almost never even get above three.

04:05.350 --> 04:09.230
So this kind of just looks like a distribution of noise.

04:09.670 --> 04:16.390
But when you do it the correct way, you'll get a much more sensible result with a ginormous Z score.

04:16.550 --> 04:22.590
As you know, almost 30 standard deviations away from the mean of the distribution.

04:23.910 --> 04:25.150
So I hope that makes sense.

04:25.150 --> 04:28.430
Now you should pause the video and get to work.

04:28.590 --> 04:32.150
And now I will switch to code and discuss my solution.

04:33.350 --> 04:36.630
Here I am printing out the model architecture again.

04:36.670 --> 04:39.790
This is already printed out at the top of the script.

04:39.790 --> 04:46.990
I just wanted to have it here again as a reminder for how to access these different layers here.

04:47.390 --> 04:47.830
Okay then.

04:47.830 --> 04:49.950
This is just a convenience variable.

04:49.950 --> 04:53.110
This is the number of hidden states that we're going to get.

04:53.430 --> 04:59.710
It's the number of layers in the encoder part which is this corresponds to the number of transformer

04:59.710 --> 05:00.350
blocks.

05:00.510 --> 05:06.110
And of course you know it is plus one for the initial embeddings from the hidden States.

05:06.510 --> 05:08.590
Okay, so now I'm running a forward pass.

05:08.590 --> 05:15.950
Here we are getting the hidden states, and I have no idea why this code is repeated here.

05:15.950 --> 05:17.910
I think I will just delete this.

05:17.910 --> 05:21.350
That's probably just a little mistake that I have that in there twice.

05:21.630 --> 05:27.310
Okay, so there's 25 hidden states, layers in total, 24 transformer blocks.

05:27.430 --> 05:34.950
And the shape of the hidden states for this one particular layer, this is just for conformation is

05:34.950 --> 05:41.030
one sequence 14 tokens and 1024 embeddings dimensions.

05:41.510 --> 05:41.830
Okay.

05:41.870 --> 05:44.230
So here let me actually go back up here.

05:44.230 --> 05:49.350
So we have model dot predictions decoder.

05:49.350 --> 05:52.270
And then we can get the weights out of this.

05:52.270 --> 05:55.830
And that is what this does over here.

05:55.990 --> 05:58.990
So this is just the embeddings matrix.

05:58.990 --> 06:05.190
And then I transpose it here I'm getting the activations from hidden states layer minus three.

06:05.190 --> 06:08.230
So out of 25 it's layer 22.

06:08.550 --> 06:15.910
And then here I multiply the activations that I grabbed here by the embeddings matrix transpose that

06:15.910 --> 06:19.990
I extracted here, as I mentioned in the slides a moment ago.

06:20.230 --> 06:28.870
This does work for GPU architectures, but that's because in the GPT architecture, all of this module,

06:28.870 --> 06:32.670
this entire layer here is literally just one layer.

06:33.150 --> 06:33.550
Okay.

06:33.590 --> 06:34.710
So let's see.

06:34.710 --> 06:35.910
So there we do that.

06:35.910 --> 06:37.710
Bring it back to the CPU.

06:38.110 --> 06:40.390
And now here I'm doing it the correct way.

06:40.390 --> 06:45.950
So I take the activations I push them through this entire module here.

06:45.950 --> 06:51.670
All of the business that's inside model predictions.

06:51.670 --> 06:55.350
And then of course I detach that and bring that back to the CPU.

06:55.750 --> 07:02.910
And now here when we do the visualization, we can already see that this does not give us a sensible

07:02.910 --> 07:03.590
result.

07:03.750 --> 07:05.900
The predicted token here is hash.

07:05.900 --> 07:06.860
Hash wives.

07:07.140 --> 07:08.460
That's not correct.

07:08.460 --> 07:14.340
And also when you look at the distribution, I mean, this does not look like the model has really picked

07:14.500 --> 07:16.620
a specific token.

07:16.820 --> 07:23.740
And the reason is that what we're doing here is removing a lot of important transformations that the

07:23.740 --> 07:30.260
model has learned to get from logits to the vocab dimensionality.

07:30.260 --> 07:32.020
So here it works quite well.

07:33.100 --> 07:40.140
You have been patiently waiting a very long time to finally get to the logit lens analysis.

07:40.700 --> 07:47.300
I appreciate your patience, and I hope you feel that you learned a lot on the way to exercise five.

07:47.940 --> 07:55.180
So this exercise involves implementing the analysis, the logit lens, and doing some visualizations.

07:55.180 --> 08:00.100
And then we'll make a heat map of the results in the next exercise.

08:00.740 --> 08:04.340
So loop through all of the layers in the model.

08:04.340 --> 08:06.220
So all the transformer blocks.

08:06.220 --> 08:14.140
Of course, you can ignore the initial embeddings, uh, matrix and also loop through all the tokens

08:14.140 --> 08:20.260
in the sequence to mask them out and get hidden states activations from the forward pass.

08:20.780 --> 08:28.940
Then you should Z score all of the activations from each layer for each of the masked tokens, and store

08:28.980 --> 08:33.780
that Z score for whatever is the maximum predicted token.

08:34.220 --> 08:41.060
So this is essentially just combining what you have already done in previous exercises.

08:41.060 --> 08:48.700
So just a lot of copy pasting code to implement exercise five at the first and the final layers you

08:48.700 --> 08:50.900
can print out the predicted text.

08:50.940 --> 08:54.940
This is also what we did in the previous video.

08:55.140 --> 08:58.980
So here you see, the way you do anything is the way you do everything.

08:59.180 --> 09:05.700
And then it will be interesting to see what the predicted text looks like at the first transformer block.

09:05.740 --> 09:08.380
And at the final transformer block.

09:08.740 --> 09:12.860
Next you can make line plots that look like this.

09:13.300 --> 09:17.980
Now in this graph, every line corresponds to a different layer.

09:17.980 --> 09:25.620
So a different transformer block and the y axis is the z score of the maximum token at each position.

09:26.260 --> 09:29.940
And the x axis corresponds to the token position.

09:30.420 --> 09:37.900
Now I've labeled the text here on the x axis ticks according to the original text, and that's for your

09:37.900 --> 09:38.660
reference.

09:38.820 --> 09:45.020
Of course, for each of the layers, the predicted words are not necessarily going to be the actual

09:45.020 --> 09:47.060
words from the original text.

09:47.580 --> 09:54.220
So to see what those predicted words are, you would need to do a different visualization like a text

09:54.260 --> 09:54.940
heatmap.

09:54.940 --> 09:57.780
And we'll get to that in the next exercise.

09:57.940 --> 10:06.500
But here the goal with this visualization is to see that the z scores generally increase as we get deeper

10:06.500 --> 10:13.260
into the model, I will have a few more things to say about this, including a short discussion about

10:13.260 --> 10:18.180
whether it's appropriate or even valid to draw lines between these points here.

10:18.460 --> 10:22.580
But anyway, I will get to that when I switch to code.

10:22.700 --> 10:29.620
The final part of this exercise is to visualize these exact same results, but in an image.

10:30.100 --> 10:37.420
So all of the data in this image here are exactly the same as the data for the lines in this plot.

10:37.460 --> 10:40.900
Here it's just visualized in a different way.

10:41.300 --> 10:49.820
So each row here corresponds to a layer, and the color corresponds to the z score of the predicted

10:49.860 --> 10:52.020
token from each layer.

10:52.020 --> 10:54.140
And for each token position.

10:55.340 --> 11:02.660
I think that both of these representations have advantages, but also potential for misinterpretation.

11:02.820 --> 11:09.620
So I think it's useful for you to see both of these visualizations, and then in your own investigations,

11:09.620 --> 11:14.020
you have a bigger arsenal of visualization methods.

11:14.780 --> 11:15.420
All right.

11:15.460 --> 11:19.780
Now is your time to pause the video and work through this exercise.

11:20.060 --> 11:23.940
And now I will switch to code and continue this discussion.

11:25.180 --> 11:29.340
I will store two sets of results in two matrices.

11:29.540 --> 11:35.100
So I have the z scores for the predicted tokens, the masked out tokens.

11:35.220 --> 11:41.780
And that is the size of the hidden states by the number of tokens in the text.

11:42.060 --> 11:44.300
And then the actual predicted tokens.

11:44.300 --> 11:45.900
And that's also the same size.

11:45.900 --> 11:50.980
But here we can just use integers because these are just integer tokens.

11:51.500 --> 11:53.500
Okay here I have a for loop.

11:53.940 --> 11:58.300
This is basically copied from exercise four.

11:58.500 --> 12:04.660
I think your exercise three I forget which exercise is copied from, but uh, yeah, you you remember.

12:04.940 --> 12:12.610
Okay, so I make a copy of all of the tokens, and then I replace one of the tokens with the mask.

12:13.090 --> 12:20.370
Actually, in this case, I'm actually moving the tokens to the GPU only when I'm inputting these token

12:20.370 --> 12:22.570
sequences into the model.

12:22.850 --> 12:24.290
Okay, get the outputs.

12:24.290 --> 12:25.810
Hidden states equals true.

12:26.050 --> 12:33.970
And now once I have all of the hidden states activations for this particular token, this particular

12:33.970 --> 12:38.770
masked token, now I can loop over all of the layers.

12:38.890 --> 12:44.330
And again all of this code is code you've seen in earlier exercises.

12:44.450 --> 12:46.770
So here I get the activations.

12:46.770 --> 12:49.690
Here I get the apply the logit lens.

12:49.690 --> 12:57.010
And then I'm getting the maximum token from the logits and z scoring over here.

12:57.010 --> 12:58.930
So I store the predicted token.

12:59.170 --> 13:03.890
And I also store the z score of the predicted token.

13:04.330 --> 13:10.570
If you did this slightly differently, that's also fine, for example, in the previous video.

13:10.570 --> 13:17.810
So actually two videos ago when I introduced the logit lens method, I didn't actually store the token

13:17.810 --> 13:18.330
index.

13:18.330 --> 13:20.530
I was storing the actual token text.

13:20.610 --> 13:21.850
You could also do it that way.

13:21.930 --> 13:23.010
Totally fine.

13:23.250 --> 13:24.210
Uh, let's see.

13:24.370 --> 13:26.170
Here are the predictions.

13:26.170 --> 13:32.090
So at the first transformer block we have fellowship accreditation, blah blah blah blah blah.

13:32.130 --> 13:36.690
Lots of fellowships, nothing whatsoever to do with the actual text.

13:36.730 --> 13:39.050
The model is doing horrible here.

13:39.090 --> 13:40.810
It's getting it completely wrong.

13:41.130 --> 13:47.970
And then by the final layer, this you've seen before, the way you do it, the way you do, not technically

13:47.970 --> 13:51.290
really 100% correct, but pretty close.

13:51.290 --> 13:55.610
And it certainly gets the right idea of the missing tokens.

13:56.170 --> 14:01.010
And by the way, also slight difference from a couple videos ago.

14:01.330 --> 14:02.570
There I was.

14:02.570 --> 14:07.810
Only I didn't even run this analysis for the first embeddings layer here.

14:07.810 --> 14:10.410
I actually ran it over all of the layers.

14:10.410 --> 14:17.090
But then here I'm only looking at the predicted tokens from the first layer, so I'm still ignoring

14:17.370 --> 14:21.970
the embeddings matrix and just looking at the first transformer block.

14:22.090 --> 14:25.570
Multiple correct ways to set up these kinds of analyses.

14:26.410 --> 14:30.330
So here I plot a line for each of the layers.

14:30.490 --> 14:36.930
And with this here, I'm just skipping the first and the final token, because that corresponds to the

14:36.970 --> 14:41.410
CLS and Sep tokens at the beginning and at the end.

14:41.410 --> 14:44.930
And I don't really care about those for this visualization.

14:45.490 --> 14:45.810
Okay.

14:45.850 --> 14:52.410
So I briefly mentioned whether it was appropriate to have lines in between the dots here.

14:52.530 --> 14:59.450
Now the thing is in statistics and data visualization, if you have a line connecting to individual

14:59.450 --> 15:06.290
points, then you are assuming that there are values that you're linearly interpolating between those

15:06.290 --> 15:07.130
two points.

15:07.130 --> 15:11.010
And there would be a value that would be somewhere between way and you.

15:11.250 --> 15:13.890
And that would have a Z score of 11.

15:14.410 --> 15:20.530
Now the thing is that is that's just doesn't work for discrete categories like this.

15:20.530 --> 15:26.970
There is nothing in between way and you, they are just discrete, categorically different.

15:26.970 --> 15:28.410
They are sorted, they're ordered.

15:28.410 --> 15:34.570
But there's no like way point five and you point five that you can mix together.

15:34.690 --> 15:42.010
So technically formally in terms of data visualization the lines here are kind of inappropriate.

15:42.210 --> 15:48.810
But I think that it just doesn't really look as good with without the lines.

15:48.810 --> 15:53.450
I think it's harder to visualize what's really going on here without the lines.

15:53.450 --> 16:03.850
So in my humble opinion, it is fine to violate statistical rules, data visualization rules when the

16:03.850 --> 16:11.330
advantage of when the gain in interpretability Outweighs the potential for misinterpretation.

16:11.490 --> 16:13.730
Okay, that's really all I want to say about that.

16:13.770 --> 16:19.050
What you do see in general is as we get deeper into the model, the Z scores.

16:19.050 --> 16:25.970
So the separation of the predicted token versus the rest of the tokens increases.

16:26.410 --> 16:26.690
Okay.

16:26.730 --> 16:31.850
And then here I have exactly the same data but now shown as an image.

16:32.090 --> 16:40.570
So here again you can see that as we get deeper into the model we get better and better separation of

16:40.570 --> 16:46.690
the predicted tokens compared to the, uh, the rest of the distribution of tokens.

16:47.690 --> 16:53.450
It's also pretty interesting to see which this visualization highlights that this visualization doesn't

16:53.450 --> 17:00.810
show as well that some of these tokens are the model was separating them earlier compared to other tokens.

17:00.970 --> 17:09.480
So for example, this token do uh, took the model, you know, maybe like 18 layers before it got really

17:09.480 --> 17:12.720
great separation here versus this one.

17:12.720 --> 17:20.080
The the model had a much easier time earlier in the model earlier in the transformer blocks really popping

17:20.080 --> 17:24.080
out this predicted token from the rest of the distribution.

17:25.680 --> 17:29.880
This is the final exercise in this code challenge.

17:30.200 --> 17:33.320
You do not need to do any new analyses.

17:33.360 --> 17:35.960
You just need to create this heat map.

17:36.160 --> 17:43.120
So that's going to involve finding some other notebook where you created a heat map for the logit lens,

17:43.120 --> 17:51.680
and then copying it and making some minor modifications for this model size and this text sequence length.

17:52.120 --> 18:00.480
And again, the color behind each token corresponds to the min max scaled z transformed activations.

18:01.120 --> 18:07.920
Now, because this is an entire matrix, there are actually a few different ways that you can min max

18:07.920 --> 18:08.920
scale the data.

18:08.920 --> 18:09.000
To.

18:09.480 --> 18:18.160
For example, you can min max scale the entire matrix, or you can scale the matrix row wise, or you

18:18.160 --> 18:20.400
can scale the matrix column wise.

18:20.640 --> 18:25.120
And by that I mean you could have within each column here.

18:25.120 --> 18:32.520
So within each token you could scale all of these data to go from 0 to 1.

18:32.760 --> 18:36.840
Or you could scale within the row or you could scale the entire matrix.

18:36.840 --> 18:38.840
There's no single right or wrong.

18:39.160 --> 18:42.800
And you can make arguments for multiple ways of doing it.

18:43.080 --> 18:49.160
You might be able to guess just from looking at this graph, the way that I set up the min max scale.

18:49.160 --> 18:55.160
But as I mentioned, you can explore different ways when you switch to code, which is what you should

18:55.160 --> 18:56.080
do now.

18:57.400 --> 19:05.840
I min max scaled the entire matrix so I get from the whole matrix I get one minimum and one maximum.

19:05.960 --> 19:12.640
So that means from the entire matrix there will be one value with zero one value of one, and everything

19:12.640 --> 19:14.240
else is in between.

19:14.640 --> 19:14.880
Okay.

19:14.920 --> 19:16.040
And then, uh, yeah.

19:16.080 --> 19:19.080
Here you just see those results visualized.

19:19.080 --> 19:21.520
So from that matrix I'm just flattening it out.

19:21.520 --> 19:23.520
This is just for visualization.

19:23.520 --> 19:31.120
Here we get zero and then we go all the way up to the maximum predicted token is going to have a value

19:31.120 --> 19:33.000
of one okay.

19:33.040 --> 19:41.480
So uh, all of this code here to do the visualization for the heat map is exactly the same as, uh,

19:41.600 --> 19:44.640
the video where I introduced the logit lens analysis.

19:44.640 --> 19:46.000
That was two videos ago.

19:46.320 --> 19:53.280
And yeah, as I mentioned there, it's the code here is a little bit simpler compared to a regular text

19:53.280 --> 19:57.000
heat map, uh, because of the matrix organization.

19:57.000 --> 20:02.080
But otherwise, yeah, I think the code is, uh, confusing the first time you see it.

20:02.080 --> 20:05.800
But once you work through it line by line, it gets a bit easier.

20:06.040 --> 20:13.440
Mainly the changes that I made from a couple videos ago was just to modify some of these little parameters

20:13.440 --> 20:21.480
here, just so that it looks a bit better with this length of a token sequence and this number of layers.

20:22.040 --> 20:28.320
So it's pretty interesting to see that the model is basically doing its complete junk.

20:28.360 --> 20:32.560
I mean, this is total absolute garbage here in the beginning.

20:32.560 --> 20:36.240
And then it kind of seems like the model just gets it right.

20:36.280 --> 20:38.240
So somewhere around layer.

20:38.400 --> 20:40.960
I don't know, 13, 14, 15 ish.

20:41.160 --> 20:48.440
The model just seems to suddenly snap into getting the final predictions, which are sometimes correct,

20:48.440 --> 20:49.840
sometimes incorrect.

20:49.840 --> 20:54.240
But even when they're incorrect, the model just picked it here.

20:54.240 --> 20:59.200
And then it stayed with that prediction for basically half of the model.

21:00.600 --> 21:02.760
A few final take home points.

21:02.920 --> 21:11.000
One is that it's really important for you to have at least a good, solid, basic conceptual and mathematical

21:11.190 --> 21:18.270
understanding of how these different analyses work, and how different companies and AI organizations

21:18.270 --> 21:19.830
put their models together.

21:20.310 --> 21:28.070
And that's because although most analyses will work on most models, there are often little modifications

21:28.070 --> 21:29.270
that need to be made.

21:29.390 --> 21:36.430
And if you do not have an understanding of the analyses and you're just copying and pasting code, the

21:36.430 --> 21:43.390
risk of making mistakes or misinterpretations or bugs that you don't know how to fix is just really

21:43.390 --> 21:43.750
high.

21:44.630 --> 21:51.070
A feature of models that I would also like to highlight here is that there are some quantifications

21:51.070 --> 21:59.630
of internal calculations and representations that suggest smooth transitions across the layers, whereas

21:59.670 --> 22:07.190
other analyses, like what you saw here suggest more of like a discontinuous or a sudden phase transition.

22:08.510 --> 22:15.270
Now, it's perhaps not so surprising that we find a result like this with the logit lens method, because

22:15.270 --> 22:23.390
we forced the model to make a categorical prediction, whereas the internal activations are continuous

22:23.430 --> 22:24.270
measures.

22:25.070 --> 22:31.030
But the fact that different analyses can seem to reveal qualitatively different conclusions.

22:31.550 --> 22:36.350
That seems confusing and weird, but it actually is very typical.

22:36.750 --> 22:40.750
It's a typical phenomenon that happens with very complex systems.

22:41.630 --> 22:41.870
Okay.

22:41.910 --> 22:47.870
And then the final point that I will mention here is something I mentioned at the beginning of the previous

22:47.870 --> 22:52.030
video when I introduced you to the logit lens, and that is that.

22:52.190 --> 22:58.070
Yeah, I've just shown you one outcome measure which is categorically forcing a token prediction.

22:58.430 --> 23:02.150
But there's lots of other things you can do with this logit lens approach.

23:02.310 --> 23:09.870
If you're interested, you can read more about it in the link that I provided in the previous code file

23:09.870 --> 23:11.150
for the previous video.

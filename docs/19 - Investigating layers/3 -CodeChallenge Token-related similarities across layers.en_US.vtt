WEBVTT

00:02.080 --> 00:08.880
This code challenge follows directly from the previous video, so make sure you've gone through that

00:08.880 --> 00:12.680
video and the code before starting this one.

00:13.160 --> 00:20.200
What we are going to do here is follow up on the approach that I showed you in the previous video,

00:20.200 --> 00:28.920
and extend it by adding some additional analyses, and also by analyzing how the characteristics change

00:29.200 --> 00:31.360
over the different layers.

00:32.320 --> 00:40.040
Exercise one is mostly copy paste from the code for the previous video, although there are a few changes

00:40.040 --> 00:42.160
and a few things that you can leave out.

00:42.320 --> 00:45.720
And this is basically a list of what you want to accomplish here.

00:46.200 --> 00:50.400
So one difference is that you want to use GPT two XL.

00:50.400 --> 00:54.480
So the large the extra large version of GPT two.

00:54.920 --> 01:03.020
So make sure that the size and shape parameters in your code are soft coded so that they will work with

01:03.020 --> 01:06.500
the bigger model, which has more layers and more parameters.

01:07.300 --> 01:13.740
Now you can connect to a GPU if you want, although there isn't a whole lot of feedforward processing

01:13.900 --> 01:15.660
to do in this code challenge.

01:15.660 --> 01:20.420
So I actually just ran the whole thing on the CPU and it's fine.

01:20.900 --> 01:28.460
By the end of this exercise, you want to have all the activations for the targets and non-target tokens

01:28.660 --> 01:33.380
and using the same sentences from the previous video.

01:33.780 --> 01:41.660
And then you also want to have that upper triangular matrix mask from which we can extract the individual

01:41.660 --> 01:47.020
submatrices and the interactions across the different attention Submatrices.

01:47.620 --> 01:55.860
Now you do not need to calculate cosine similarity or correlation or any other analyses in this exercise.

01:55.860 --> 01:57.300
We'll get to that later.

01:57.420 --> 02:03.040
Here everything is just about getting the code set up for later exercises.

02:03.240 --> 02:10.480
Also, a brief reminder that the target token in the full sentence is actually space her, not just

02:10.480 --> 02:12.480
her, with no preceding space.

02:13.360 --> 02:17.680
Okay, so now you can pause the video and switch to code.

02:18.000 --> 02:24.720
I will switch to Python very briefly, just to give a quick overview of the code for this exercise.

02:25.760 --> 02:27.760
So some libraries to import.

02:27.760 --> 02:31.360
Here you can see I'm importing Gpt2 Excel.

02:31.960 --> 02:33.360
This is just copied over.

02:33.360 --> 02:37.880
In fact all of the Tokenizers for GPT two are the same.

02:37.880 --> 02:41.040
All the variants use exactly the same tokenizer.

02:41.280 --> 02:43.600
And I'm switching the model to eval mode.

02:43.600 --> 02:48.360
And this is just a convenience variable for the number of embeddings dimensions.

02:48.360 --> 02:53.640
So I don't need to keep writing out all of these characters multiple times throughout the code.

02:54.040 --> 02:57.440
Okay, this is the same as what you've seen before.

02:57.480 --> 03:01.530
I'm hooking the, uh, Q vk matrices.

03:01.770 --> 03:03.770
And yeah using attention.

03:04.050 --> 03:05.010
C attention.

03:05.210 --> 03:14.850
And the output of that linear transformation is the q, v and k matrices, or the vectors all concatenated

03:14.850 --> 03:19.930
together into one wide matrix, which I will later on separate.

03:20.650 --> 03:21.050
Let's see.

03:21.090 --> 03:25.290
Here are the sentences literally copy pasted from earlier.

03:25.490 --> 03:27.810
Uh, tokenize push through the model.

03:27.850 --> 03:30.770
Yeah, this takes around a minute on the CPU.

03:30.810 --> 03:35.130
It's really not so bad and we don't need the GPU for anything else.

03:35.130 --> 03:37.130
This is the only thing we're going to do here.

03:37.410 --> 03:45.570
And then this is the code to create that matrix mask that I explained in the previous video.

03:45.770 --> 03:50.490
So you can see all the same code from the or mostly the same code from the previous video.

03:50.930 --> 03:56.010
Uh, and I'm just leaving out some printing and, and visualizations and so on.

03:56.770 --> 04:00.670
We are still not quite ready to do the analyses.

04:01.110 --> 04:07.470
First, I want you to write a function that will get layer specific activations.

04:07.750 --> 04:14.790
The only tricky part is that you need to find the index of the target token in each sentence, because

04:14.790 --> 04:20.230
the word her can appear in different positions in different sentences.

04:20.830 --> 04:28.950
So the idea of this function is that you're going to use it in the next exercise to get all the activations

04:28.950 --> 04:35.630
from one layer, so that and that is the layer index that you input into this function.

04:36.270 --> 04:42.190
Now on the one hand, you can literally copy paste all of this code from the previous video.

04:42.550 --> 04:49.790
But on the other hand, because this is the only thing you need to do for exercise two I strongly recommend

04:49.830 --> 04:57.750
coding this one from scratch just based on these comments here, without copy pasting from the previous

04:57.750 --> 04:58.390
video.

04:58.670 --> 05:05.490
This is a great opportunity to make sure that you're comfortable with finding a target token in a short

05:05.490 --> 05:09.090
sequence and working with activations vectors.

05:10.010 --> 05:15.290
Okay, so now you should pause the video and again I will switch to code, but just briefly to make

05:15.290 --> 05:17.290
sure that this syntax is clear.

05:18.330 --> 05:23.290
So inside this function I initialize a variable.

05:23.290 --> 05:24.650
So we have the length.

05:25.050 --> 05:29.810
So the number of sentences and then the number of embeddings dimensions times three.

05:30.090 --> 05:36.450
Of course that's for the concatenation of the queries the keys and the values vectors.

05:36.770 --> 05:38.770
And then I loop over sentences.

05:38.930 --> 05:39.410
And yeah.

05:39.450 --> 05:47.090
Also as a reminder of stuff I've said in the previous video, if you have the sequences, if the batches

05:47.090 --> 05:53.730
are set up such that the target word is in the same position, the same ordinal position, for example,

05:53.730 --> 06:00.030
let's say the target token that you want to grab the activations for is always the final token in the

06:00.030 --> 06:00.790
sequence.

06:00.790 --> 06:03.550
You do not actually need a for loop here.

06:03.550 --> 06:07.270
You can just index something you know it would be something like this.

06:07.350 --> 06:12.470
All the sentences, the final token and then all of the dimensions.

06:12.590 --> 06:20.150
But, uh, yeah, the, uh, the location of the target token can be different in every sentence.

06:20.630 --> 06:20.910
Okay.

06:20.950 --> 06:23.070
And then this is for the target.

06:23.070 --> 06:26.310
And this is for the comparison, the non-target.

06:26.310 --> 06:34.270
These are whatever token or word happens to come before the word her in each of these sentences.

06:35.630 --> 06:38.310
This exercise is a big one.

06:38.510 --> 06:47.630
What you're going to do is create a for loop that loops over all of the layers and calculates and organizes

06:47.670 --> 06:50.870
a bunch of analyses from within each layer.

06:51.430 --> 06:56.470
There's no visualization here that comes in the next two exercises.

06:56.630 --> 07:01.650
But here is the list of things to do for exercise three.

07:02.090 --> 07:05.410
So here we have a big loop over all the layers.

07:05.570 --> 07:10.570
And first you call the function that you wrote in exercise two.

07:10.970 --> 07:17.970
This will give you tensors for all the activations for the targets and for the non targets.

07:18.450 --> 07:25.690
The next thing to do is get the variances over all of the targets and averaged within each attention

07:25.690 --> 07:26.730
submatrix.

07:26.970 --> 07:35.650
So to be clear for this piece of the code, here you have 54 activation values for each neuron.

07:36.290 --> 07:41.770
Then you calculate the variance across all of those 54 neurons.

07:42.130 --> 07:44.650
That's going to give you 1600.

07:44.650 --> 07:56.610
So 1600 variance values corresponding to 1600 neurons query neurons and 1600 because this is GPT two

07:56.850 --> 07:57.690
XL.

07:58.010 --> 08:01.590
Of course that number would be 768.

08:01.790 --> 08:12.430
In the small version, then you average across all of those 1600 variance values to get one number corresponding

08:12.470 --> 08:18.830
to the average of the token activation variances from the cue population.

08:19.510 --> 08:24.110
Then all this stuff that I just described that was for the query neuron.

08:24.110 --> 08:29.350
So then you repeat that for the keys and you repeat that for the values.

08:29.990 --> 08:34.630
So that means you're going to get three numbers from each layer.

08:34.990 --> 08:41.310
And then you repeat that analysis for the targets and for the non targets.

08:41.470 --> 08:44.550
So in fact it's actually six numbers per layer.

08:44.950 --> 08:45.310
Okay.

08:45.350 --> 08:54.190
The interpretation of this result is that if the model processes the word her in exactly the same way

08:54.310 --> 08:59.040
in every sentence, then the variance will be basically zero.

08:59.640 --> 09:08.320
And the larger the variance, the more the preceding context changes how the model processes the target

09:08.320 --> 09:09.200
word her.

09:09.840 --> 09:15.320
And of course, that might change across the different layers of the model, the different transformer

09:15.320 --> 09:16.000
blocks.

09:17.160 --> 09:17.480
Okay.

09:17.520 --> 09:19.360
So that's for the variances.

09:19.360 --> 09:26.120
For the means you do exactly the same procedure, except you get the average activation value instead

09:26.120 --> 09:29.320
of the variance over all of the tokens.

09:30.080 --> 09:40.320
The next thing to do is to calculate cosine similarities across all 5000 neurons in q, k, and v separately

09:40.320 --> 09:43.360
for the targets and the non targets.

09:43.640 --> 09:46.520
Now this we did in the previous video.

09:47.160 --> 09:55.600
Once you have those big cosine similarity matrices, you can then apply the matrix mask that you created

09:55.600 --> 10:05.020
in exercise one And isolate all of the cosine similarities within each of the three matrices and across

10:05.020 --> 10:07.220
all the pairs of matrices.

10:08.140 --> 10:10.860
And then you can make histograms again.

10:10.900 --> 10:14.340
That is the same as what we did in the previous video.

10:14.380 --> 10:21.100
You can feel free to copy and modify or rewrite from scratch if you'd like a bigger challenge.

10:21.660 --> 10:23.620
Okay, so I hope that all makes sense.

10:23.620 --> 10:25.900
There's a lot of work to do here.

10:26.020 --> 10:30.580
And this for loop actually gets pretty long once you fill in all of this code.

10:31.180 --> 10:32.980
But I believe in you.

10:32.980 --> 10:34.180
I know you can do it.

10:34.180 --> 10:36.500
So pause the video and get to work.

10:36.540 --> 10:38.900
And now I will show my solution.

10:39.980 --> 10:43.340
So here we are in this very long code cell.

10:43.380 --> 10:45.780
There's really it gets really quite long.

10:46.180 --> 10:46.460
Okay.

10:46.500 --> 10:50.500
So in the very beginning I'm initializing lots and lots of stuff.

10:50.620 --> 10:54.140
Now just a little helpful tip about coding.

10:54.460 --> 10:58.120
What I often do is initialize Later.

10:58.120 --> 11:00.280
So I will start writing code.

11:00.280 --> 11:05.360
And then as I'm writing the code before I actually run the code, I'm starting to write the code.

11:05.680 --> 11:10.480
Uh, and then I think about how big does this matrix need to be?

11:10.800 --> 11:16.240
And then I can come up to the top of the code cell and then initialize it and so on.

11:16.280 --> 11:18.560
Now a lot of these are basically the same.

11:18.560 --> 11:25.280
They all get initialized to the same size, which is the number of layers, the number of transformer

11:25.280 --> 11:28.040
blocks by the number of edges.

11:28.040 --> 11:35.400
So this is the uh the histogram values I call it y for the histogram heights, the number of edges,

11:35.400 --> 11:37.800
the number of boundaries minus one.

11:38.360 --> 11:38.680
Okay.

11:38.720 --> 11:41.280
So all of those get initialized.

11:41.280 --> 11:48.400
And then here I have the initializations for the matrices of all of the variances and all of the means.

11:48.400 --> 11:50.720
So that's two by layers by three.

11:51.160 --> 11:54.640
The two corresponds to the target versus non-target.

11:54.800 --> 12:01.060
The layers is yeah the number of transformer blocks, and the three corresponds to the three attention

12:01.060 --> 12:02.500
sub matrices.

12:02.900 --> 12:10.940
Now there are some arbitrary and just like personal preference choices to make in these kinds of analyses.

12:10.940 --> 12:18.100
For example, if you chose to set up these matrices like this, to have it be layers by target or non-target,

12:18.100 --> 12:25.100
by, uh, by sub matrix, that's completely fine as long as your code is all internally consistent,

12:25.140 --> 12:26.420
it's totally fine.

12:26.580 --> 12:33.780
If you wanted to break this up into two matrices like you can see here, I'm separating this into target

12:33.780 --> 12:34.620
and non-target.

12:34.620 --> 12:36.380
So you could also do that here.

12:36.740 --> 12:41.540
And then you would just have something like this also completely fine.

12:41.540 --> 12:47.980
And these things if you, you know, if you want to just uh, you could initialize these to be for target

12:47.980 --> 12:54.860
and non I mean some of this is if I would start writing this code again from scratch, I'd probably

12:54.900 --> 12:59.000
initialize some of these variables to be different sizes anyway.

12:59.960 --> 13:01.800
Now here is the main for loop.

13:01.960 --> 13:08.400
Okay, so this is just calling the function from the previous exercise that we wrote to extract all

13:08.440 --> 13:09.520
the activations.

13:09.960 --> 13:12.280
Here I am getting the variances.

13:12.280 --> 13:21.440
So all of the variances across the first dimension that corresponds to the different sentences.

13:21.440 --> 13:24.880
So the 54 presentations of the word her.

13:25.400 --> 13:27.120
And so I get the variances there.

13:27.120 --> 13:34.080
And then I'm averaging over the variances for the first third and for the middle third and for the later

13:34.080 --> 13:34.640
third.

13:35.000 --> 13:44.200
Now you also know how to use this function called split which is a method that operates on PyTorch tensors.

13:44.360 --> 13:52.360
So if you want to so if you chose to torch.cat to convert this into a tensor and use split and putting

13:52.360 --> 13:56.200
the, uh, you know, all the parameters in here, that's totally great.

13:56.200 --> 13:57.300
That's totally fine.

13:57.300 --> 14:01.220
And then, yeah, maybe you find that a little bit easier for this stuff here.

14:01.620 --> 14:02.260
All right.

14:02.420 --> 14:03.380
Uh, let me see.

14:03.420 --> 14:03.820
Okay.

14:04.220 --> 14:05.980
So that is for the variances.

14:05.980 --> 14:07.180
Here's for the means.

14:07.180 --> 14:10.020
This code is exactly the same as this code.

14:10.060 --> 14:13.780
The only difference is that here I'm calculating the mean.

14:13.780 --> 14:16.340
And here I'm calculating the variance.

14:16.820 --> 14:22.780
By the way, in case you are curious why I said to calculate variance instead of standard deviation,

14:23.220 --> 14:26.660
the variance is the standard deviation squared.

14:26.660 --> 14:32.380
So that means the variance can kind of highlight the differences between different groupings a little

14:32.420 --> 14:36.140
bit better because the the numbers are squared.

14:36.140 --> 14:37.140
So they just get bigger.

14:37.140 --> 14:41.980
It's just a little bit easier to make separations between different groups.

14:42.580 --> 14:42.980
Okay.

14:43.220 --> 14:50.220
Uh let's see here is calculating all to all cosine similarity for the targets and non targets.

14:50.380 --> 14:57.630
And here I am extracting the specific submatrices uh exactly as I showed before.

14:57.870 --> 15:02.430
Okay, so now that we have all of these cosine similarities.

15:02.470 --> 15:10.230
Now I'm generating the histograms and you can see it's very similar to the previous video.

15:10.230 --> 15:12.430
And also it's just a lot of copy pasting.

15:12.430 --> 15:18.910
So you you know the hardest part of coding this particular exercise I think is just making sure you

15:18.910 --> 15:21.670
are changing all of the variables correctly.

15:21.670 --> 15:24.070
So it's a good idea to go through these.

15:24.110 --> 15:30.750
And also what I often do is if you click on one variable, all the other variables that have exactly

15:30.750 --> 15:36.470
the same spelling, you can see that they get this kind of background light blue highlight.

15:36.830 --> 15:37.870
So this is actually good.

15:37.870 --> 15:40.150
Let's say you made a typo.

15:40.230 --> 15:41.870
You forgot to change this.

15:41.870 --> 15:46.750
So now when I click on this one I can see that this variable also lights up.

15:46.750 --> 15:49.830
This also gets highlighted which is incorrect.

15:50.110 --> 15:51.950
So it's good to do that.

15:51.990 --> 15:58.650
And what you want to confirm here is that this variable doesn't get highlighted anywhere else, indicating

15:58.650 --> 16:00.130
that I'm only using it here.

16:01.250 --> 16:05.090
Now we're starting to get to some cool visualizations.

16:05.210 --> 16:10.690
I think some of these curves will look familiar to you from the previous video.

16:11.130 --> 16:18.730
Remember that in the previous video we focused on just one layer and we just used the small version

16:18.770 --> 16:20.050
of GPT two.

16:20.690 --> 16:26.610
So here now we have one line per layer and it's a bigger model.

16:26.610 --> 16:29.290
But the visualization is basically the same.

16:29.570 --> 16:35.810
So you created the histograms of cosine similarities in the previous exercise.

16:35.930 --> 16:39.170
So now you don't have any new analyses to run.

16:39.170 --> 16:45.410
You're just visualizing the analyses that you completed in the previous exercise.

16:45.890 --> 16:50.010
So the line color in these plots corresponds to the layers.

16:50.170 --> 16:53.450
And that you see in the color axis here on the right.

16:53.970 --> 17:02.510
It is really striking to see this smooth transition from the cosine similarities being extreme and large

17:02.510 --> 17:08.630
in magnitude towards shifting to zero as we get deeper into the model.

17:09.310 --> 17:16.110
Now, the reason why that happens is that in the early layers, the model is actually still processing

17:16.110 --> 17:23.310
the word her, and the word her is the same for all of the activations that these data come from.

17:23.630 --> 17:30.670
But the deeper we get into the model, the more the tokens shift from being about the current token

17:30.990 --> 17:34.510
to being about predicting the subsequent token.

17:34.830 --> 17:39.670
And every sentence in this data set has a unique context.

17:39.830 --> 17:47.430
So the set of tokens that the processing shifts towards is going to be different for each sentence,

17:47.430 --> 17:53.470
which means that the cosine similarities will be reduced, which means that the distributions will shift

17:53.470 --> 17:55.110
more towards zero.

17:55.610 --> 18:00.970
So you can visualize every layer as a line with a different color like this.

18:01.250 --> 18:05.730
And you can also make images of the distributions like this.

18:06.170 --> 18:12.730
So what you're looking at now these two images is the same as these two line plots.

18:13.170 --> 18:18.330
But now every line in this plot is a row in this image.

18:18.570 --> 18:25.290
And the color in the image corresponds to the y axis height in the line plots.

18:25.810 --> 18:31.170
Now, both of these ways of visualizing are really interesting and complementary.

18:31.530 --> 18:38.610
They technically show identical data, but I think you get different kinds of information more easily

18:38.610 --> 18:40.570
from one or the other plot.

18:40.650 --> 18:43.410
So you can try both and see what you think.

18:44.370 --> 18:49.130
I really enjoy making these kinds of visualizations and I hope that you do too.

18:49.650 --> 18:55.630
Anyway, now I will switch to the code and show my solution and discuss the results in a little bit

18:55.630 --> 18:56.550
more detail.

18:57.630 --> 19:03.710
The plotting code here is only slightly more complicated than the code from the previous video.

19:03.950 --> 19:09.790
In fact, you can copy the plotting code from the previous video and just, you know, put it inside

19:09.790 --> 19:10.550
a for loop.

19:10.670 --> 19:15.950
If you want to get these colors to look really nice and fancy, then you can try doing something like

19:15.950 --> 19:16.230
this.

19:16.230 --> 19:17.310
You can add this.

19:17.350 --> 19:25.150
Of course, if you want to color just using the built in color chooser from Python, that's also fine.

19:25.630 --> 19:28.390
Okay, so let's see, I don't have a whole lot to say.

19:28.830 --> 19:35.430
Uh, you've seen me use this kind of scheme before where I color the different lines and then generate

19:35.470 --> 19:40.830
a color bar that maps the color onto the layer number.

19:40.950 --> 19:43.510
So feel free to copy that and modify it.

19:43.830 --> 19:46.430
You can use different color maps.

19:46.430 --> 19:47.830
I like this plasma.

19:47.830 --> 19:53.710
Sometimes I do plasma R for plasma reverse that you see in uh.

19:53.710 --> 19:54.230
Wait, sorry.

19:54.270 --> 19:56.610
I also need to change this one.

19:56.650 --> 19:57.050
Oh yeah.

19:57.050 --> 19:57.850
Well oops.

19:57.850 --> 19:58.810
Because it's in a loop.

19:58.810 --> 20:01.010
I can't really change all of them.

20:01.050 --> 20:01.450
Uh, okay.

20:01.490 --> 20:03.850
I would have to change all of them or none of them.

20:03.850 --> 20:08.650
But anyway, just showing you that you can reverse the, uh, color order.

20:08.650 --> 20:12.770
So let me switch that back to plasma and this back to plasma.

20:12.770 --> 20:16.770
So basically if you want to reverse the color map, you have to reverse it on all of them.

20:16.970 --> 20:17.450
Okay.

20:17.490 --> 20:20.250
Anyway, uh, let's see what we can see.

20:20.530 --> 20:27.210
So as I already explained in the slides, essentially we see a shift in the distributions of cosine

20:27.210 --> 20:32.370
similarities from being more towards the extreme with, uh, earlier layers.

20:32.370 --> 20:37.890
So closer to the input, that's where the model is processing each token.

20:38.290 --> 20:43.410
And as you go deeper into the model, more towards the output of the model.

20:44.330 --> 20:51.050
Now the model is shifting its processing towards trying to predict what is the next token going to be.

20:51.330 --> 20:58.500
Now the next token depends not just on the word her, but on all the other words that preceded the word

20:58.500 --> 20:58.740
her.

20:58.780 --> 21:02.220
And that is different for every sentence in this batch.

21:02.220 --> 21:09.180
And so therefore, we expect this kind of shift into the, uh, into smaller cosine similarities.

21:09.700 --> 21:16.460
As you also know from the previous video, from the end of the previous video, a lot of these, uh,

21:16.540 --> 21:24.220
tails of the distributions here are actually attributable to average offsets to mean offsets in the

21:24.380 --> 21:31.540
attention vectors that are driving the cosine similarity, similarity values to the extreme.

21:31.540 --> 21:38.020
So you can see that those vectors are also shifting closer to each other in their average values or

21:38.020 --> 21:39.700
their distributional shifts.

21:39.980 --> 21:44.340
The other part of this exercise was to show the images.

21:44.340 --> 21:45.340
And that's what I do here.

21:45.340 --> 21:48.500
It's all the same data that I'm plotting above.

21:48.860 --> 21:55.280
Uh, I'm just showing using I am show now some of these like the maximum value here.

21:55.280 --> 21:56.680
This is a bit arbitrary.

21:56.720 --> 22:02.360
Often what ends up happening is you pick one value here, and then you look at the plot and you say,

22:02.400 --> 22:06.160
whoa, this is like completely, completely saturated.

22:06.160 --> 22:09.000
So let's try, you know, something much, much bigger.

22:09.200 --> 22:15.800
And you just kind of iterate through this procedure several times and then, uh, yeah, you will eventually

22:15.840 --> 22:17.840
find some value that seems to work.

22:18.200 --> 22:21.160
That said, these are non-normalized counts.

22:21.160 --> 22:27.840
If you would transform these into density estimates, for example, then it would be a little bit easier

22:27.960 --> 22:30.440
to specify a value a priori.

22:30.720 --> 22:35.800
But yeah, I mean, in general it's sometimes a little bit tricky to find these colors.

22:35.920 --> 22:42.760
Another option you can do is to pick the color based on the empirical maximum value.

22:42.920 --> 22:46.040
Now that sometimes works, but sometimes doesn't work.

22:46.200 --> 22:50.320
You can also try picking the max and scaling it down.

22:50.320 --> 22:53.960
So say for example 70% of the maximum value.

22:54.100 --> 22:55.140
That still doesn't work.

22:55.380 --> 23:02.300
Yeah, it's very tricky to set these color limits, because you will find that what you think is a really

23:02.300 --> 23:08.140
good number for one data set is just not going to work for another data set.

23:08.260 --> 23:11.620
In this case we have some really extreme values.

23:11.780 --> 23:15.660
And so that's why it's hard to get this color scheme to work.

23:16.140 --> 23:22.700
Anyway, as long as you have the color bars indicated with some explanation of what the color maps onto,

23:23.020 --> 23:24.260
it's totally fine.

23:24.300 --> 23:27.100
There's nothing wrong with arbitrary color scaling.

23:28.060 --> 23:35.620
And finally, we have exercise five, where you will visualize the means and the variances.

23:36.140 --> 23:38.620
Here you can make four scatterplots.

23:38.740 --> 23:46.420
You can see that these scatter plots have the layer on the x axis and three different markers for the

23:46.540 --> 23:50.580
query the keys and the values vectors.

23:51.260 --> 23:58.960
So what you plot here are the average activations for the targets and the non-targets.

23:59.080 --> 24:06.120
And then down here we have for the average variances again for targets and non-targets.

24:06.640 --> 24:12.200
Obviously I'm not showing you the results here because I want you to create this plot and interpret

24:12.200 --> 24:13.000
it yourself.

24:13.360 --> 24:20.080
But what you want to look for here are any kinds of patterns over the laminar dimension.

24:20.080 --> 24:26.200
So over the x axis for example, are the means or the variances pretty steady.

24:26.240 --> 24:30.280
As you go deeper into the model, do they increase?

24:30.280 --> 24:33.120
Do they decrease some weird nonlinear trends?

24:33.320 --> 24:37.480
Are there any noticeable differences between the different matrices?

24:38.160 --> 24:45.760
Remember from earlier in the course that the Q and k vectors are fairly similar to each other, in that

24:45.760 --> 24:53.780
they combine to determine the probability that they're corresponding tokens get emphasized in the attention

24:53.780 --> 24:57.340
sublayer, whereas the V vectors.

24:57.340 --> 25:05.180
So the value vectors, that's what carries the actual information that gets weighted by the softmax.

25:05.380 --> 25:10.900
And then that is what gets passed on to the next layer, the MLP subblock.

25:11.500 --> 25:17.140
This is the last exercise in this code challenge, and I hope you enjoy working your way through it.

25:17.420 --> 25:18.940
And now I will switch to code.

25:19.700 --> 25:24.700
The code here is fairly straightforward, especially if you ignore all of this stuff, which is just

25:24.700 --> 25:27.540
making the plots look a little bit nicer.

25:27.580 --> 25:34.540
Really, all I'm doing is plotting the means across the three different matrices, and for the targets

25:34.540 --> 25:35.980
and non-targets.

25:36.340 --> 25:37.340
It's that simple.

25:37.380 --> 25:41.060
Okay, so let's have a look at the means for the targets.

25:41.100 --> 25:44.860
The non-targets we see the means are generally close to zero.

25:44.980 --> 25:52.740
Remember these are averages across a lot of, you know, the 54 different presentations of the word

25:52.740 --> 26:00.600
her and also the 1600 different neurons within each of these submatrices.

26:01.000 --> 26:08.240
So the means being close to zero does not mean that the mean of any given q vector, for example, is

26:08.240 --> 26:11.400
not, uh, is also must be zero.

26:11.520 --> 26:18.480
So you could actually have some diversity in the averages, but at the population level the averages

26:18.480 --> 26:20.320
are pretty close to zero.

26:20.720 --> 26:22.760
And then yeah of course this is for targets.

26:22.760 --> 26:25.840
So it's exactly the same word in every case.

26:26.280 --> 26:32.680
And here we have the non-targets where you really see this consistent distributional shift, especially

26:32.680 --> 26:40.720
as you get towards the middle part of the model where the model is starting to shift from processing

26:40.720 --> 26:48.280
the local features to processing more global features about the entire sentence or the whole sequence.

26:48.560 --> 26:49.920
Okay, so that's for the means.

26:49.960 --> 26:53.890
Now here we have the same plot but for the variances.

26:53.930 --> 26:57.730
And now this one is super interesting, especially here with the targets.

26:57.730 --> 27:04.410
You see that the variances across all three of these attention submatrices start off very small, really

27:04.410 --> 27:12.010
close to zero, and then they start increasing kind of linearly, almost monotonically until you get

27:12.010 --> 27:13.050
towards the very end.

27:13.050 --> 27:19.050
And then you really start to see shifts between the different, uh, divergence across the different

27:19.050 --> 27:21.490
submatrices of attention.

27:22.410 --> 27:28.370
And again, you know, it's the same basic interpretation at the kind of, you know, bird's eye view

27:28.370 --> 27:31.810
of a LM, that what's happening is here.

27:31.810 --> 27:34.250
The model is processing the word her.

27:34.450 --> 27:39.050
And, you know, it's just processing that individual token.

27:39.050 --> 27:46.250
And the later you get in the layer, the more information, the more context information and predictive

27:46.250 --> 27:53.150
information about the subsequent token gets packed on to these embeddings vectors.

27:53.150 --> 27:55.550
So that means they're starting to diverge.

27:55.550 --> 27:58.630
They get more and more distinct from each other.

27:58.790 --> 28:07.030
And for the non-targets, it's even higher because you have both token level differences and also sequence

28:07.030 --> 28:08.430
level differences.

28:08.430 --> 28:12.670
So they're the variance is just a higher consistently throughout.

28:13.470 --> 28:15.870
I hope you enjoyed this code challenge.

28:15.990 --> 28:23.230
I like these kinds of large scaled zoom out analyses, because I think that they really help provide

28:23.230 --> 28:29.150
some background context for the general flow of information in an LLM.

28:29.790 --> 28:36.070
On the other hand, it's also fair to criticize these kinds of approaches by saying that they do not

28:36.110 --> 28:45.950
really reveal a lot of very specific insights into how llms work, but these kinds of zoomed out characteristics

28:45.950 --> 28:51.670
also tend to be more reproducible, which is a pretty comforting thought in my opinion.

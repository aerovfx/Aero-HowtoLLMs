WEBVTT

00:02.080 --> 00:10.040
As is often the case in this course, this code challenge follows directly from the previous video and

00:10.040 --> 00:11.040
code demo.

00:11.640 --> 00:20.440
So in the previous video I calculated RSA and category selectivity for two attention matrices in one

00:20.440 --> 00:21.000
layer.

00:21.520 --> 00:29.520
Now that you have this set of knowledge and skills and code, you are going to extend this analysis

00:29.520 --> 00:32.720
to a larger model and to all of the layers.

00:33.120 --> 00:41.440
It will be pretty interesting to see whether and how these analysis results change as we traverse deeper

00:41.440 --> 00:42.480
into the model.

00:43.280 --> 00:49.280
Actually, most of the hard work for you to do in this code challenge is in exercise two.

00:49.800 --> 00:53.320
Exercise one is just about setting everything up.

00:53.880 --> 01:00.560
You can copy from the notebook file from the previous video, or it might be easier just to make a copy

01:00.560 --> 01:05.350
of the entire notebook file, rename it and start to modify it.

01:05.830 --> 01:10.310
There are a few changes that you need to make here for exercise one.

01:10.830 --> 01:14.870
So in the previous video I used GPT two medium.

01:15.270 --> 01:18.590
Here you are going to work with GPT two large.

01:19.070 --> 01:26.710
And you also want to hook all three attention matrices, not just the two that I did in the previous

01:26.750 --> 01:27.470
video.

01:28.070 --> 01:32.870
Now there's only one batch of tokens to run, and the sequences are not so long.

01:33.830 --> 01:37.950
So you can actually run this whole code challenge on the CPU.

01:38.510 --> 01:46.390
The end result of this exercise will be a set of the activations from the query, the keys, and the

01:46.390 --> 01:48.030
values vectors.

01:48.190 --> 01:52.310
You can print out all of the dictionary keys to confirm.

01:52.750 --> 01:57.710
Make sure that you have everything from all the transformer blocks and the three attention matrices,

01:58.190 --> 02:01.150
and the shape of any one of those should match.

02:01.390 --> 02:02.470
this size.

02:02.470 --> 02:12.550
So 34 sentences, five tokens per sentence and 1280 embeddings dimensions from each of these matrices.

02:13.470 --> 02:17.750
Okay, so now you should pause the video and work through this exercise.

02:17.870 --> 02:24.230
And now I will switch to code, although just briefly just to give an overview of what I changed relative

02:24.230 --> 02:26.150
to the previous video.

02:26.910 --> 02:29.910
Typical libraries here is let me see.

02:29.950 --> 02:32.790
Import GPT two large.

02:33.590 --> 02:35.110
And here's the hooks.

02:35.110 --> 02:43.670
This is almost identical to the code from the previous video, except here I'm also storing the V matrices.

02:43.670 --> 02:52.430
So the values matrices and then the surgery for the implantation here is in a for loop over all of the

02:52.430 --> 02:53.270
layers.

02:53.270 --> 02:55.230
So I can run that code.

02:55.350 --> 03:01.670
This code I did not change from the previous video, although I just copied and pasted things around

03:01.820 --> 03:02.300
a little bit.

03:02.300 --> 03:06.420
So it's fewer, uh, pieces of code overall.

03:06.700 --> 03:12.780
And then finally we run all of the data through the model and just confirm that we get all the keys

03:12.780 --> 03:13.780
that we expect.

03:13.780 --> 03:20.940
So all of the, uh, matrices, the tensors that we expect and that the activations tensors are the

03:20.940 --> 03:22.420
size that we expect.

03:23.260 --> 03:30.860
This exercise is where you will run all of the analyses, although you don't do any visualizations until

03:30.860 --> 03:32.420
exercise three.

03:33.020 --> 03:37.500
So this for loop here shows an overview of what you should do.

03:38.020 --> 03:42.060
So you loop over all of the layers in the model.

03:42.380 --> 03:52.460
And then within each layer you calculate the cosine similarities for q and k and v vectors independently

03:52.460 --> 03:53.380
of each other.

03:53.740 --> 04:02.180
And you can confirm that each of these matrices should be 34 by 34, corresponding to the 34 For sentences.

04:02.620 --> 04:09.860
And of course, we really only care about the final token in each of these sentences because the first

04:09.900 --> 04:17.620
four tokens are identical, and the fifth token is that unique token that comes from the three categories.

04:18.180 --> 04:26.820
So from each of these cosine similarity matrices, you can then calculate selectivity in the indices

04:26.820 --> 04:30.860
and store them in a three by three matrix.

04:31.380 --> 04:40.180
The 36 corresponds to the 36 transformer layers in GPT two large, and the three corresponds to the

04:40.180 --> 04:43.460
three matrices so q, k, and v.

04:44.460 --> 04:52.140
Then you can calculate and store the RSA and store those values in a different matrix, but of the same

04:52.140 --> 04:55.060
size as the selectivity indices.

04:55.620 --> 05:02.420
So this means you're going to have an RSA score for each layer in the model and for each of the three

05:02.460 --> 05:04.300
attention matrices.

05:05.420 --> 05:09.100
So now you can pause the video and code up this analysis.

05:09.300 --> 05:16.020
As I mentioned multiple times before, you can make this exercise easier or more challenging depending

05:16.020 --> 05:20.100
on how much code you want to copy from the previous video.

05:20.500 --> 05:22.620
Anyway, now I will switch to code.

05:23.540 --> 05:29.300
There's not a whole lot for me to discuss in this exercise solution.

05:29.300 --> 05:35.420
It's basically just the code that I showed in the previous exercise, the previous video.

05:35.460 --> 05:42.860
Sorry, separately for Q and K that's that you've seen in previous video and also now for V.

05:42.860 --> 05:49.900
But of course the code is exactly the same, just working off of a different activations tensor.

05:50.380 --> 05:54.860
Here I calculate the selectivity indices also exactly the same.

05:54.860 --> 06:01.580
And then here I am storing them into this matrix selectivity indices.

06:01.860 --> 06:03.050
And then for the RSA.

06:03.090 --> 06:03.290
Yeah.

06:03.330 --> 06:04.890
Also very straightforward here.

06:06.010 --> 06:08.170
Finally the visualization.

06:08.530 --> 06:14.970
You can create two scatter plots that both have the transformer layer on the x axis.

06:15.210 --> 06:18.170
And here you show the selectivity index.

06:18.290 --> 06:20.330
And here you show the RSA.

06:20.810 --> 06:26.050
And of course you want to look for any patterns across the different matrices.

06:26.210 --> 06:35.090
And as a function of depth in the model I hope you find this visualization to be insightful and interesting.

06:35.210 --> 06:36.930
And now I will switch to code.

06:38.010 --> 06:40.610
The code here is also quite straightforward.

06:40.610 --> 06:47.250
I have three separate lines of plotting for each row in these matrices.

06:47.410 --> 06:51.290
And you can actually plot this all in a little for loop if you prefer.

06:51.450 --> 06:57.130
But yeah, I have it in separate lines just to separate the visual appearance of them.

06:57.650 --> 06:57.970
Okay.

06:58.050 --> 07:00.250
So this is quite interesting.

07:00.250 --> 07:02.570
We can interpret the RSA first.

07:02.640 --> 07:05.720
First of all, all of the values are quite strong.

07:05.720 --> 07:06.600
So really high.

07:06.640 --> 07:11.960
Generally all above 0.84 ish and going all the way up to 0.96.

07:12.120 --> 07:20.920
Now you don't really see a lot of consistency in terms of increases or decreases with transformer layers,

07:20.920 --> 07:26.760
or real noticeable differences between the different pairs of matrices.

07:26.880 --> 07:33.840
So this tells us that the way that the embeddings vectors for the different tokens in the different

07:33.840 --> 07:39.440
categories are related to each other within each matrix is pretty consistent.

07:39.680 --> 07:47.320
Uh, within the each individual layer and across all of the layers and across the different matrices.

07:47.560 --> 07:49.160
So nice to see.

07:49.200 --> 07:54.320
But, uh, yeah, we can't really make a lot of interesting interpretations out of this.

07:54.560 --> 07:57.480
It's just that the RSA scores are generally pretty high.

07:58.680 --> 07:59.960
And then we get over here.

07:59.960 --> 08:02.560
Now we do see some striking differences.

08:02.920 --> 08:10.840
So the selectivity indices in general are high in the beginning when we are closer to the input, closer

08:10.840 --> 08:15.880
to the text, as it appeared when it first entered the model into the embeddings.

08:16.000 --> 08:24.400
So that means that early in the language model, the processing is closer to the semantic category.

08:24.600 --> 08:32.880
And later on, as we get deeper into the model, the category of the semantic category of the different

08:32.920 --> 08:35.640
tokens matters less and less.

08:35.760 --> 08:42.600
So basically this tells us that by this point, the model doesn't really care so much about the specific

08:42.600 --> 08:49.520
semantic category of the word, and that's probably because the preceding context doesn't actually include

08:49.520 --> 08:55.960
any of the category specific information about space or furniture or fruit.

08:56.320 --> 09:04.040
So, uh, now this result here, the difference between Q and K, this tiny little difference here,

09:04.040 --> 09:05.040
close to one.

09:05.040 --> 09:08.720
That is what we focused on in the previous video.

09:08.760 --> 09:09.800
That was a different model.

09:09.800 --> 09:13.600
That was GPT two medium this GPT two large.

09:13.800 --> 09:15.840
But you see the same pattern here.

09:16.120 --> 09:23.400
And on the other hand you see in the values matrix we get in general much stronger selectivity.

09:23.400 --> 09:30.480
And it has this interesting smile pattern where the selectivity is higher but decreasing.

09:30.480 --> 09:34.760
But then actually around the middle of the model it starts to increase again.

09:34.760 --> 09:37.240
So more category selectivity.

09:37.240 --> 09:44.680
As we get deeper into the model, you will continue to see throughout the rest of this course that we

09:44.680 --> 09:52.080
can use the same or very similar analyses on different parts of the model to reveal different dynamics

09:52.080 --> 09:52.920
in the model.

09:53.240 --> 10:00.440
And you will also see that we can do different analyses on the same data to look for different patterns

10:00.440 --> 10:02.600
and reveal different features.

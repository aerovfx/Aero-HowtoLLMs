WEBVTT

00:02.120 --> 00:10.720
Now that you understand the basics of the logit lens, it's time to expand your knowledge of that analysis.

00:10.920 --> 00:13.960
Learn a bit more about the Bert model.

00:14.400 --> 00:22.800
Experience the joy of getting error messages and working with GPUs and CPUs, and learn more about standardizing

00:22.800 --> 00:28.160
data and visualizing internal model activations in various ways.

00:28.760 --> 00:31.000
So there's quite a lot to do in this code challenge.

00:31.000 --> 00:32.480
So let's begin.

00:33.080 --> 00:43.480
The goal of exercise one is to import the large Bert model and have a review about masked token prediction

00:43.600 --> 00:44.440
in Bert.

00:44.880 --> 00:50.120
Now for starters we're going to work with this Bert large version of the model.

00:50.120 --> 00:53.440
We actually haven't worked with this particular model yet.

00:53.480 --> 00:57.080
We've only been working with the smaller version of Bert.

00:57.600 --> 01:03.420
So once you import it, you can have a quick look through the model, see how it differs from the smaller

01:03.420 --> 01:03.980
version.

01:04.740 --> 01:10.820
Now you don't need to do like a rigorous quantitative comparison, but just check for typical things

01:10.820 --> 01:16.380
that you look for in a model description, like the number of embeddings dimensions, the number of

01:16.380 --> 01:21.740
transformer blocks, the names of the different components of the model, and so on.

01:22.620 --> 01:26.620
Then you want to move that model to the GPU.

01:27.260 --> 01:36.100
Now, to be honest, this entire code challenge could easily be done just on the CPU, but I want you

01:36.100 --> 01:43.140
to have some more practice with moving text and data back and forth between the GPU and the CPU.

01:44.060 --> 01:47.700
I hope that will cause some errors and a bit of confusion.

01:48.020 --> 01:52.580
And the reason why I hope that it does is because I think that's worth it.

01:52.580 --> 01:59.860
Because having more experience with integrating across different processors and sorting the code out

01:59.860 --> 02:06.160
and dealing with error messages and so on, That definitely pays off in the long term, even though

02:06.160 --> 02:08.480
it's a bit frustrating in the short term.

02:09.720 --> 02:18.920
Anyway, the main part of this exercise is a reminder of how to do masked token prediction in a Bert

02:18.960 --> 02:19.720
model.

02:20.080 --> 02:25.080
Here is the text that we will work with for the rest of this code challenge.

02:25.360 --> 02:29.880
The way you do anything is the way you do everything.

02:29.880 --> 02:34.320
That's one of my favorite sayings, as you know from the previous video.

02:34.960 --> 02:43.360
Anyway, write out this sentence, but replace the word way with the word mask and with square brackets

02:43.360 --> 02:44.040
around it.

02:44.400 --> 02:53.040
You will remember from earlier in the course that the Bert model has a special token for this text,

02:53.080 --> 02:59.400
with the word mask with square brackets around it, and the model has been trained to make predictions

02:59.400 --> 03:02.400
about what token should be in that position.

03:03.260 --> 03:09.260
Once you have this token sequence, you can print it out like this and show all the tokens over here,

03:09.500 --> 03:12.780
and then you can do a forward pass through the model.

03:13.420 --> 03:19.180
Now we are not doing logit lens just yet, not in this exercise.

03:19.420 --> 03:23.540
So here for this exercise you only need the final logits.

03:23.540 --> 03:25.660
You do not need the hidden states.

03:26.140 --> 03:28.500
And then you can make a plot that looks like this.

03:28.780 --> 03:34.860
Find the token with the maximum logit and convert that back to text.

03:35.340 --> 03:38.020
Now in this particular case, it worked out well.

03:38.060 --> 03:40.900
The model predicted the correct token.

03:41.860 --> 03:42.300
Okay.

03:42.340 --> 03:46.020
So now you can pause the video and code up this exercise.

03:46.220 --> 03:49.780
And I will now switch to code and show my solution.

03:51.300 --> 03:56.860
Here are all of the libraries that I will use for this entire code challenge.

03:56.980 --> 03:59.860
And here I'm importing Bert-large.

04:00.020 --> 04:04.600
So previously we have been working with Bert unnamed Named Uncased.

04:04.960 --> 04:06.400
But without writing anything.

04:06.400 --> 04:10.880
Here you are implicitly importing the small version of Bert.

04:10.880 --> 04:15.160
So we haven't worked with this yet, so we will have a look at what this looks like.

04:15.560 --> 04:17.640
So here I'm setting up the GPU.

04:17.800 --> 04:24.520
Here I'm moving the model to the GPU and then switching the model into eval mode.

04:25.400 --> 04:28.960
So here we see the large version of Bert.

04:29.000 --> 04:32.160
Now of course Bert models get much bigger than this one.

04:32.360 --> 04:35.000
But this is the one that we are going to work with here.

04:35.240 --> 04:39.280
Okay, so it has an embeddings dimension of 1024.

04:39.520 --> 04:43.720
You can compare that to 768 for example.

04:44.000 --> 04:49.680
And yeah this one has 24 layers 24 transformer blocks.

04:49.800 --> 04:52.720
Again you can compare that to 12 for example.

04:53.120 --> 04:53.360
Okay.

04:53.400 --> 05:01.400
And otherwise we have seen before that the naming convention and the matrix storage convention is different

05:01.400 --> 05:03.520
for Bert compared to GPT.

05:03.920 --> 05:05.190
So it's good practice.

05:05.230 --> 05:10.790
Whenever you're working with a model or a new model or a model that you haven't worked with in a while,

05:10.990 --> 05:17.550
to just print out the description and have a look through and remind yourself of the different names

05:17.550 --> 05:19.710
for the different matrices and so on.

05:20.750 --> 05:21.030
Okay.

05:21.070 --> 05:26.830
And then I would also like to point out this final little module at the end.

05:26.870 --> 05:33.790
Now when we worked with the GPT models, for example, in the previous video, there was the little

05:33.790 --> 05:37.030
module at the end that was called LM underscore head.

05:37.390 --> 05:39.350
So language model head.

05:39.350 --> 05:42.590
And that had literally just one piece at the end of it.

05:42.630 --> 05:45.270
It was basically the equivalent of this.

05:45.470 --> 05:47.430
It was a linear object.

05:47.430 --> 05:49.590
So it was basically just a matrix.

05:49.790 --> 05:53.190
But here we have this whole module called predictions.

05:53.350 --> 06:01.510
And that has a couple of matrix multiplications with a Gelu nonlinear activation in between there's

06:01.550 --> 06:02.390
a layer norm.

06:02.390 --> 06:09.050
And then we get to the final layer where we go from the embeddings dimension, which is what we're in

06:09.050 --> 06:11.530
all the way up here, still in the embeddings dimension.

06:11.530 --> 06:16.490
And then we go out to the vocab dimension and that is called the decoder.

06:16.610 --> 06:17.010
Okay.

06:17.050 --> 06:20.770
So that will be relevant for the next exercise.

06:21.330 --> 06:21.650
Okay.

06:21.690 --> 06:22.930
So here we have the text.

06:22.970 --> 06:25.170
The way you do anything is the blank.

06:25.490 --> 06:26.890
You do everything.

06:26.930 --> 06:30.570
Of course you know that this should be the word way.

06:31.050 --> 06:36.130
And also just a quick reminder this particular model is called uncased.

06:36.170 --> 06:39.250
That means it's been trained with all lowercase.

06:39.250 --> 06:45.770
So that means that there is no difference between the with a capital and the with a lowercase.

06:46.130 --> 06:49.450
I still like it to be capital because I'm very old school.

06:49.450 --> 06:51.410
I like proper capitalizations.

06:51.850 --> 06:52.450
Anyway.

06:52.610 --> 06:53.890
Uh, tokenize.

06:54.130 --> 07:00.490
Uh, and then, yeah, just printing out, uh, what this tokenized version of the text looks like.

07:01.570 --> 07:03.410
Okay, here I do the forward pass.

07:03.450 --> 07:10.430
Uh, notice I'm not getting the hidden states here For this particular exercise, we only need the final

07:10.430 --> 07:12.230
logits from the output.

07:12.630 --> 07:16.990
So here I have the mass token ID that is here.

07:17.030 --> 07:18.750
That is something I defined here.

07:18.750 --> 07:25.350
So where does where do the tokens equal the mask token ID.

07:25.350 --> 07:29.590
So this gives me the location of the mask here.

07:30.190 --> 07:30.470
Okay.

07:30.510 --> 07:33.750
So then I'm getting the logits for that task location.

07:34.150 --> 07:41.550
Uh putting it back to the CPU, bringing it from the GPU back to the CPU and then finding the maximum

07:41.710 --> 07:42.510
logit.

07:42.670 --> 07:47.310
And then I'm plotting and finding what that maximum value corresponds to.

07:47.750 --> 07:49.990
And that is the token way.

07:50.030 --> 07:50.990
So that's great.

07:51.190 --> 07:53.150
That is an accurate prediction.

07:54.550 --> 08:00.790
Exercise two is actually just a small addition to exercise one.

08:01.190 --> 08:05.590
You do not need to run any new analyses or forward passes.

08:05.750 --> 08:07.810
Just take the final output.

08:07.810 --> 08:08.610
Logits.

08:08.810 --> 08:17.130
Make a histogram of them, including a vertical dashed line over here to indicate the maximum token.

08:17.130 --> 08:21.770
Logit that we figured out in the previous exercise was the word weigh.

08:22.530 --> 08:28.050
And then I want you to z score this entire distribution.

08:28.050 --> 08:30.650
And then you can generate this plot.

08:30.810 --> 08:34.890
This is exactly the same plot you saw in the previous exercise.

08:34.890 --> 08:37.930
So token index on the x axis.

08:37.930 --> 08:41.850
And previously it was the logits on the y axis.

08:41.850 --> 08:46.210
Here it is the z score on the y axis.

08:46.890 --> 08:54.770
Remember that the formula for the z score, the z transformation, is each data point minus the mean

08:54.810 --> 09:01.170
of all the data points in the distribution, divided by the standard deviation of the distribution.

09:01.650 --> 09:05.570
So we're going to be z scoring a lot in this code challenge.

09:06.050 --> 09:10.790
One of the big Advantages of z scoring data in general.

09:10.790 --> 09:18.030
And also here in this code challenge, is that we can interpret these numbers as standard deviation

09:18.030 --> 09:21.790
units away from the center of the distribution.

09:22.310 --> 09:29.950
In other words, the model activation for the correct token was over ten standard deviations away from

09:29.950 --> 09:33.830
the center of the distribution of all the other tokens.

09:33.910 --> 09:34.430
That's a lot.

09:34.470 --> 09:36.670
Ten standard deviations is really a lot.

09:36.870 --> 09:43.830
It shows how well the model was able to predict the missing token, and also how good the separation

09:43.830 --> 09:48.790
is of the predicted token relative to all of the other tokens.

09:49.390 --> 09:57.310
Now, this histogram is also important to expect, because the interpretability of the z score depends

09:57.310 --> 10:01.550
on the data being at least roughly Gaussian distributed.

10:01.990 --> 10:09.050
Now it doesn't need to be a perfect normal distribution, but you do want to see one peak that decays

10:09.050 --> 10:10.450
on both sides.

10:10.450 --> 10:17.410
And if you see that in the distribution, then the Z score is going to be interpretable as standard

10:17.410 --> 10:19.970
deviation units away from the mean.

10:21.130 --> 10:23.210
So that's it for this exercise.

10:23.330 --> 10:26.010
Now you can pause the video and get to work.

10:26.330 --> 10:31.290
I will now switch to code, but only briefly because there isn't really that much to discuss.

10:31.450 --> 10:40.130
For this solution, the most important line of code in this exercise solution is this line here.

10:40.170 --> 10:40.530
Oops.

10:40.930 --> 10:43.050
This this line over here.

10:43.170 --> 10:50.730
So we take the data minus the mean of the data and divide by the standard deviation of the data.

10:51.130 --> 10:58.690
Now if you are familiar with statistics and Python, you might be wondering whether we need to set the

10:58.730 --> 11:02.410
denominator degrees of freedom to be one and all.

11:02.410 --> 11:06.050
I will say, I don't want to get into a long discussion about that.

11:06.050 --> 11:12.630
Basically, it has to do with whether you consider these values to be a sample or a population.

11:12.830 --> 11:19.630
But all I can say here is that it doesn't really matter because the sample size is so large.

11:19.630 --> 11:22.630
So we have over 30,000 data points.

11:22.630 --> 11:23.390
In fact, it is.

11:23.390 --> 11:25.470
I don't even remember the exact number.

11:25.510 --> 11:26.190
Let me see.

11:26.670 --> 11:35.270
So whether you use the denominator degrees of freedom it of 1 or 0 is the difference between 30,500

11:35.270 --> 11:39.950
and 22 versus 30,521.

11:39.950 --> 11:44.030
So in practice it really does not matter at all.

11:44.150 --> 11:44.510
Okay.

11:44.550 --> 11:51.710
But I just wanted to mention that for the statistics aficionados in the audience here okay.

11:51.750 --> 11:57.230
Anyway, so this is transforming all of the data to a z score okay.

11:57.310 --> 11:59.870
And then the rest of this is just plotting here.

11:59.870 --> 12:01.710
I'm doing some stuff with Grid Spec.

12:01.710 --> 12:07.310
And that's just because I want the plot to have unequal sizes.

12:07.310 --> 12:08.270
So this one.

12:08.270 --> 12:12.090
So basically I set up Uh, three uh, subplots.

12:12.090 --> 12:17.330
And then this plot only occupies the first plot and this one occupies the other two.

12:17.890 --> 12:22.410
You know, I've mentioned this several times in the beginning of the course, but I haven't mentioned

12:22.410 --> 12:24.090
it so much recently.

12:24.090 --> 12:29.450
But if you do the visualizations differently from how I do it, totally fine.

12:29.490 --> 12:35.730
The point of this course is for you to learn methods to investigate language models.

12:35.730 --> 12:39.450
I really don't care about the details of the plotting.

12:39.490 --> 12:40.530
That's totally.

12:40.570 --> 12:41.850
That's so unimportant.

12:42.850 --> 12:43.170
Okay.

12:43.210 --> 12:46.450
Anyway, uh, yeah, you can look through all of this code.

12:46.450 --> 12:48.090
It's not terribly interesting.

12:48.090 --> 12:50.810
I'm basically just transforming, uh, or.

12:50.970 --> 12:56.090
Yeah, beautifying these plots according to how I think looks nice.

12:57.610 --> 13:00.050
Now for exercise three.

13:00.490 --> 13:05.010
The goal here is to repeat the masked token prediction.

13:05.290 --> 13:08.370
But for all of the words in a for loop.

13:08.610 --> 13:12.670
So we're still not yet doing the logit lens analysis.

13:12.910 --> 13:19.870
But this analysis here in exercise three is setting us up for that, because we're going to do the logit

13:19.870 --> 13:26.470
lens analysis over all the layers, over the hidden states, and also over all of the tokens.

13:27.030 --> 13:32.630
So to do this exercise you set up a for loop over all the tokens.

13:32.870 --> 13:38.750
Replace each token in turn with the word mask with square brackets around it.

13:39.390 --> 13:41.070
Then you do a forward pass.

13:41.190 --> 13:42.790
Grab all the activations.

13:42.790 --> 13:50.910
Find the maximum token prediction for each missing token, and this list I've printed out here just

13:50.910 --> 13:53.550
for confirmation of the masking.

13:54.070 --> 14:00.550
So in this set of lists, each row corresponds to an iteration in the for loop.

14:00.870 --> 14:05.710
And you can see the number three appearing along the diagonal over here.

14:06.110 --> 14:07.950
And sorry 103.

14:07.950 --> 14:12.650
And the token 103 corresponds to the mask token.

14:12.650 --> 14:18.770
So this is just some confirmation that each token in turn was replaced with the mask.

14:19.970 --> 14:24.290
The end result of this exercise will be a table that you generate.

14:24.450 --> 14:25.250
You print out.

14:25.250 --> 14:27.450
The table looks something like this.

14:27.610 --> 14:30.650
So in the left column we have the target words.

14:31.170 --> 14:39.330
The middle column shows the predicted words, and the right column shows the z value for each of the

14:39.330 --> 14:40.930
predicted words.

14:41.530 --> 14:48.410
So in this case what you see here, the model correctly predicted the first token in the sentence.

14:48.450 --> 14:55.850
It's also pretty interesting that the model predicted that the CLS token should be a period, and that's

14:55.850 --> 15:02.730
pretty interesting, because it means that the model understood from context that this is a full sentence,

15:03.010 --> 15:08.330
and so the token preceding a full sentence is likely to be a period.

15:09.010 --> 15:09.570
Anyway.

15:09.930 --> 15:16.040
You will discover what the rest of the predicted tokens look like when you pause the video and switch

15:16.040 --> 15:18.680
to Python, which is what you should do now.

15:18.960 --> 15:20.800
And now I will switch to code.

15:22.080 --> 15:24.400
Here I am printing out the text again.

15:24.440 --> 15:29.400
It's actually very slightly different from the text I wrote in exercise one.

15:29.800 --> 15:37.440
Apparently I'm not using a period here, and I guess I did settle down to, uh, not capitalizing everything.

15:37.440 --> 15:38.040
But anyway.

15:38.400 --> 15:41.080
Uh, okay, so then tokenize the full text.

15:41.080 --> 15:44.280
So there's no masks in here and here.

15:44.520 --> 15:54.840
I loop over all of the tokens in the text, and then here I make a copy of the tokens and so dot clone

15:54.840 --> 15:58.600
to make a copy, and then we have to send it to the GPU.

15:58.840 --> 16:06.360
And then I am replacing each index in this for loop with the mask from this tokenizer.

16:06.600 --> 16:14.180
It's also fine if you would write something like tokenizer dot encode and then uh like this.

16:14.180 --> 16:15.580
That would also work.

16:15.580 --> 16:22.060
But it's nice to know about this specific feature so I can actually just show you this here.

16:22.500 --> 16:26.900
So tokenizer mask token ID and it is 103.

16:27.380 --> 16:27.860
Okay.

16:27.900 --> 16:29.980
So uh, so that's the mask.

16:29.980 --> 16:36.740
And then yeah here I'm just printing out this little, uh, row just to show that each token in turn

16:36.740 --> 16:39.060
is replaced with the mask token.

16:39.620 --> 16:43.300
Then I push all those mask tokens through the model.

16:43.340 --> 16:45.420
Now, of course, you could also let me see.

16:45.420 --> 16:50.580
You could move the mask tokens to the device, uh, to the GPU over here.

16:50.580 --> 16:51.300
Like this.

16:51.500 --> 16:52.820
That's also fine.

16:53.060 --> 16:54.820
And then here I get the output.

16:54.820 --> 16:55.540
Logits.

16:55.580 --> 17:01.140
Of course, we only have one sequence in this batch, so that's why I'm indexing zero over here.

17:01.500 --> 17:06.020
And then we want the logits for the missing token.

17:06.020 --> 17:07.740
For the masked token.

17:07.820 --> 17:09.940
Bring that one back to the CPU.

17:10.340 --> 17:19.440
And then here I find the token associated with the largest logit and then decode that to get text,

17:19.440 --> 17:24.400
and then I get the z score for that specific token.

17:24.400 --> 17:31.880
So this is that particular predicted token minus the mean of the distribution divided by the standard

17:31.880 --> 17:33.760
deviation of the distribution.

17:34.160 --> 17:36.480
So I can run that that runs quite fast.

17:36.480 --> 17:43.520
If you're doing all of this on the CPU then maybe it takes, I don't know, ten 15 seconds for uh,

17:43.520 --> 17:46.040
let me see for this line to run.

17:46.040 --> 17:48.840
But otherwise yeah, it's less than one second.

17:49.120 --> 17:49.480
Okay.

17:49.520 --> 17:51.520
So yeah, that's just the confirmation.

17:51.680 --> 17:57.120
And now we can print out the entire, uh, text that Bert predicted.

17:57.280 --> 18:01.560
So the sequence, the predicted sequence is the way you do it.

18:01.600 --> 18:02.480
The way you do.

18:03.080 --> 18:08.200
Not quite as profound as the original version, but it does make a lot of sense.

18:08.200 --> 18:14.120
And each of these individual words, even when they are technically incorrect.

18:14.120 --> 18:20.260
So in terms of categorical accuracy, this word is wrong because it's not anything.

18:20.620 --> 18:27.620
But in terms of the contextual accuracy and the semantic accuracy, grammatical accuracy, it actually

18:27.620 --> 18:29.220
is correct the way you do it.

18:29.220 --> 18:29.900
Period.

18:29.940 --> 18:31.660
That is a complete sentence.

18:32.020 --> 18:37.860
And then it's also interesting to just qualitatively have a look at these z scores.

18:37.900 --> 18:41.380
And sometimes these z scores are higher.

18:41.500 --> 18:44.540
And that tends to be when the model was correct.

18:44.660 --> 18:50.420
And like these cases here where the model was incorrect the z scores are a little bit lower.

18:50.460 --> 18:53.860
Of course here it's also just around eight and a half.

18:54.940 --> 18:59.740
There are six exercises in total in this code challenge.

18:59.900 --> 19:02.540
And you have now completed the first three.

19:02.980 --> 19:05.460
So I'm going to break the video here.

19:05.620 --> 19:07.700
Encourage you to take a short break.

19:07.740 --> 19:14.420
And when you feel refreshed and relaxed and ready to continue, then come back to the next video.

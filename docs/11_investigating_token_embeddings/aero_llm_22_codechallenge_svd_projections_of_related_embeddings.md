
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [11 investigating token embeddings](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Ãnh Xáº¡ SVD CÃ¡c Dáº£i Äiá»ƒm NhÃºng CÃ³ Quan Há»‡ ChÃ©o

## TÃ³m táº¯t

Má»™t trong nhá»¯ng giá»›i háº¡n khi phÃ¢n tÃ­ch toÃ n bá»™ KhÃ´ng gian Embeddings báº±ng SVD (Singular Value Decomposition) lÃ  sá»± bÃ£o hÃ²a nhiá»…u - nhá»¯ng thÃ nh pháº§n chÃ­nh (Principal Components) thÆ°á»ng Ä‘Ã¡nh máº¥t Ä‘á»™ chi tiáº¿t do pháº£i gÃ¡nh Ä‘á»¡ má»™t tá»· lá»‡ phÆ°Æ¡ng sai khá»•ng lá»“ cá»§a cáº£ trÄƒm ngÃ n cá»¥m tá»« dá»‹ biá»‡t. Giáº£i phÃ¡p Ä‘á»‘i trá»ng lÃ  khoanh vÃ¹ng tá»a Ä‘á»™ vi mÃ´: Thay vÃ¬ nÃ©n toÃ n thá»ƒ máº£ng BERT, ta táº¡o ra cÃ¡c "Ma tráº­n Con" (Submatrices) chá»©a duy nháº¥t tá»• há»£p token mang Ä‘áº·c trÆ°ng nhÃ³m (VD: TÃªn quá»‘c gia, chá»¯ sá»‘). BÃ i phÃ¢n tÃ­ch dÆ°á»›i Ä‘Ã¢y minh chá»©ng nÄƒng lá»±c cá»§a SVD trong viá»‡c tÃ¬m ra nhá»¯ng trá»¥c liÃªn káº¿t Ä‘á»“ng dáº¡ng áº©n dáº­t dÆ°á»›i cÃ¡c nhÃ³m tá»« vá»±ng cá»¥ thá»ƒ.

---

## 1. Kiáº¿n Táº¡o Ma Tráº­n Con (Submatrices Embeddings)

Cáº¥u hÃ¬nh thá»­ nghiá»‡m Ä‘Æ°á»£c thiáº¿t káº¿ dá»±a trÃªn 2 táº­p dá»¯ liá»‡u máº«u trÃ­ch tá»« mÃ´ hÃ¬nh BERT:
1. **Táº­p 10 Chá»¯ sá»‘ Ä‘Æ¡n:** `["0", "1", "2", ..., "9"]`
2. **Táº­p 10 Quá»‘c gia LiÃªn Minh ChÃ¢u Ã‚u (EU):** `["France", "Germany", "Italy", "Spain", ...]` (Chá»n lá»c Æ°u tiÃªn cÃ¡c quá»‘c gia khÃ´ng bá»‹ bÄƒm ngang bá»Ÿi tokenizer Ä‘á»ƒ Ä‘áº£m báº£o luáº­t *1 word = 1 token*).

Hai ma tráº­n con nháº­n Ä‘Æ°á»£c (Matrix $M_{\text{digits}}$ vÃ  $M_{\text{EU}}$) cÃ³ chung kÃ­ch thÆ°á»›c $10 \times 768$.
TÃ­nh Ä‘á»™c láº­p phÃ¢n phá»‘i (Orthogonality mapping) Ä‘Æ°á»£c kháº³ng Ä‘á»‹nh ngay tá»« bÆ°á»›c thá»­ nghiá»‡m khi Ma tráº­n VectÆ¡ Trung BÃ¬nh (Mean vectors) cá»§a táº­p Chá»¯ sá»‘ vÃ  táº­p EU tráº£ vá» má»©c tÆ°Æ¡ng quan cá»±c tháº¥p ($r \approx 0.01$). Äiá»u nÃ y chá»©ng minh 2 Ä‘Ã¡m mÃ¢y tá»a Ä‘á»™ nÃ y bay xa nhau hoÃ n toÃ n trong cáº¥u trÃºc dáº£i ngÃ¢n hÃ  768 chiá»u.

---

## 2. Loáº¡i Bá» ÄÆ°á»ng Tiá»‡m Cáº­n Báº±ng Ká»¹ Thuáº­t Dá»‹ch TÃ¢m (Mean-Centering)

TrÆ°á»›c khi tiáº¿n hÃ nh phÃ¢n rÃ£ nhÃ¢n ma tráº­n $M$, má»i cáº¥u trÃºc dá»¯ liá»‡u hÃ¬nh há»c tuyáº¿n tÃ­nh Ä‘á»u pháº£i tiáº¿n hÃ nh lÃ¹i tÃ¢m (Mean Centering).
TÃ­nh tá»‹nh tiáº¿n nÃ y cÆ°a bá» khoáº£ng cÃ¡ch dÆ° thá»«a tá»« Ä‘iá»ƒm $0$ Ä‘áº¿n lÃµi Ä‘Ã¡m mÃ¢y dá»¯ liá»‡u:

$$
\hat{M}_{i} = M_{i} - \mu
$$

*(Vá»›i $\mu$ lÃ  vector trung bÃ¬nh cá»±c Ä‘áº¡i cÃ³ Ä‘á»™ dÃ i báº±ng sá»‘ cá»™t kÃ­ch thÆ°á»›c D=768).*

Khi Mean-centering Ä‘Æ°á»£c thá»±c thi cháº·t cháº½, Ä‘Æ°á»ng quang phá»• giÃ¡ trá»‹ suy biáº¿n (Singular value spectrum / Scree plot) tá»« SVD sáº½ cÃ³ Ä‘áº·c tÃ­nh rá»—ng dÆ° táº¡i giÃ¡ trá»‹ cuá»‘i cÃ¹ng. NÃ³i cÃ¡ch khÃ¡c, thuáº­t toÃ¡n cÆ°a Ä‘i má»™t *báº­c tá»± do* (Rank minus 1), biá»ƒu diá»…n báº±ng viá»‡c singular value cuá»‘i cÃ¹ng sáº½ Ä‘Ã¢m tháº³ng vá» $0$. Náº¿u khÃ´ng lÃ¹i tÃ¢m, trá»¥c phÃ¢n phá»‘i SVD sáº½ dá»“n toÃ n bá»™ sá»± khÃ¡c biá»‡t vÃ o Component-1 (Trá»¥c thá»© 1), lÃ m sai lá»‡ch kháº£ nÄƒng Ä‘á»c hiá»ƒu Component-2.

---

## 3. KhÃ¡m PhÃ¡ Ã NghÄ©a SVD Báº±ng PhÃ©p Chiáº¿u Nghá»‹ch Táº­p Há»£p (Over-Projections)

### KhÃ¡i Niá»‡m PhÃ©p Chiáº¿u Rá»™ng RÃ£i:
Sau khi SVD thÃ nh cÃ´ng $\hat{M}_{\text{EU}} = U \Sigma V^T$, chÃºng ta thu Ä‘Æ°á»£c chÃ¹m Vector riÃªng biá»‡t Ä‘áº·c táº£ tÃ­nh "*ChÃ¢u Ã‚u*" náº¯m giá»¯ táº¡i hÃ ng thá»© tá»± Ä‘áº§u tiÃªn cá»§a Ä‘a giÃ¡c $V^T$ (KÃ­ hiá»‡u $V_{\text{top}}$).

PhÃ©p mÃ u giáº£i thÃ­ch náº±m á»Ÿ bÆ°á»›c sau: Thay vÃ¬ giá»›i háº¡n kháº£o sÃ¡t trÃªn 10 nÆ°á»›c ChÃ¢u Ã‚u, ta láº¥y **toÃ n bá»™ 30.000 tokenizer cÃ²n láº¡i cá»§a há»‡ BERT**, trá»« Ä‘i $\mu_{\text{EU}}$, rá»“i nhÃ¢n tÃ­ch vÃ´ hÆ°á»›ng Ä‘á»• bÃ³ng toÃ n bá»™ 30.000 tá»« nÃ y lÃªn trá»¥c $V_{\text{top}}$:

$$
\text{Projections} = (E_{\text{all\_tokens}} - \mu_{\text{EU}}) \cdot V_{\text{top}}
$$

### Diá»…n Dá»‹ch ChÃ³p Äá»“ Thá»‹ (Extremes Projections):
Thá»‘ng kÃª 30 token cÃ³ tÃ­ch vÃ´ hÆ°á»›ng vÄƒng ra xa nháº¥t trÃªn Trá»¥c $V_{\text{top}}$ (Top positive / Top negative Projections) má»Ÿ ra chÃ¢n trá»i cÆ¡ cháº¿ mÃ¡y há»c:
- á» dáº£i cá»±c Ã¢m cá»§a Trá»¥c ChÃ¢u Ã‚u, chÃºng ta báº¯t gáº·p nhá»¯ng tá»« vá»±ng khÃ´ng há» náº±m trong nhÃ³m gá»‘c Ä‘Ã o táº¡o nhÆ°ng cÃ¹ng má»™t há»‡ trá»¥c Ä‘á»‹a lÃ½ ngÃ´n ngá»¯ nhÆ°: *Latvian, Tallinn, Vilnius, Estonian*.
- á» dáº£i cá»§a ma tráº­n Chá»¯ Sá»‘, cÃ¡c cá»±c Ä‘oan dá»± Ä‘oÃ¡n kÃ©o theo sá»± xuáº¥t hiá»‡n cá»§a cÃ¡c chuá»—i text format sá»‘ (VD: *Seven, Null, Zero, Divided*), chá»©ng tá» trá»¥c khÃ´ng gian toÃ¡n há»c cÃ³ kháº£ nÄƒng ná»‘i káº¿t hÃ¬nh dÃ¡ng sá»‘ ("7") vá»›i kÃ½ hiá»‡u vÄƒn báº£n ("Seven").

---

## 4. Káº¿t luáº­n

Sá»± phÃ¢n máº£ng Ma tráº­n con (Submatrices Extracting) cung cáº¥p má»™t khung kÃ­nh lÃºp máº¡nh máº½ giáº£m bá»›t nhiá»…u loáº¡n ngáº«u nhiÃªn cá»§a toÃ n bá»™ thÆ° viá»‡n ngÃ´n ngá»¯ tá»± nhiÃªn. PhÆ°Æ¡ng phÃ¡p láº¥y SVD táº¡o ra ma tráº­n V, rá»“i Ä‘em toÃ n bá»™ Ä‘áº¡i dÆ°Æ¡ng Embeddings pháº£n kÃ­ch dá»™i ngÆ°á»£c chiáº¿u rá»i lÃªn $V$ chÃ­nh lÃ  má»™t chiáº¿c kÃ­nh rá»i Ä‘Ã¨n soi sÃ¡ng cáº¥u trÃºc ná»™i máº¡c (Mech Interpretability) cho tháº¥y cÃ¡ch hÃ ng tá»· ma tráº­n thÃ´ng sá»‘ Neural Network mÃ³c ná»‘i khÃ¡i niá»‡m cá»§a con ngÆ°á»i thÃ nh máº¡ng nhá»‡n tÃ­nh toÃ¡n.

---

## TÃ i liá»‡u tham kháº£o

1. **Turian, J., et al. (2010).** *Word representations: A simple and general method for semi-supervised learning.* ACL.
2. **Deerwester, S., et al. (1990).** *Indexing by latent semantic analysis.* JASIS.
3. TÃ i liá»‡u thá»±c hÃ nh Ä‘á»‹nh lÆ°á»£ng *SVD projections of related embeddings*.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero llm 01 codechallenge cosine similarity advanced part 1](aero_llm_01_codechallenge_cosine_similarity_advanced_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_codechallenge_cosine_similarity_advanced_part_1_.md) |
| [aero llm 02 codechallenge cosine similarity advanced part 2](aero_llm_02_codechallenge_cosine_similarity_advanced_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_codechallenge_cosine_similarity_advanced_part_2_.md) |
| [Theo DÃµi DÃ²ng Cháº£y Cosine Similarity TrÃªn Trá»¥c VÄƒn Báº£n ChuyÃªn Tuáº§n Tá»± (Word Sequences)](aero_llm_03_codechallenge_cosine_similarity_in_word_sequences.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_cosine_similarity_in_word_sequences.md) |
| [Nghá»‡ Thuáº­t Váº½ Báº£n Äá»“ Nhiá»‡t Ma Tráº­n NhÃºng Báº±ng CÆ°á»ng Äá»™ Tá»« (Coloring Cosine Similarity)](aero_llm_04_codechallenge_coloring_cosine_similarity.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_coloring_cosine_similarity.md) |
| [áº¢o áº¢nh Cá»§a TrÃ­ Tuá»‡ ToÃ¡n Há»c Trong NgÃ´n Ngá»¯: Sá»©c Máº¡nh Cá»§a Random Embeddings](aero_llm_05_codechallenge_can_random_embeddings_be_interpreted.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_codechallenge_can_random_embeddings_be_interpreted.md) |
| [PhÆ°Æ¡ng PhÃ¡p T-SNE VÃ  Thuáº­t ToÃ¡n PhÃ¢n Cá»¥m DBSCAN: Chiáº¿u KhÃ´ng Gian Äa Chiá»u Cho LLMs](aero_llm_06_t_sne_projection_and_dbscan_clustering_theory_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_t_sne_projection_and_dbscan_clustering_theory_.md) |
| [PhÃ¢n Cá»¥m Ngá»¯ NghÄ©a Qua PhÃ©p Chiáº¿u t-SNE & Máº­t Äá»™ DBSCAN (Python)](aero_llm_07_t_sne_projection_and_dbscan_clustering_python_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_t_sne_projection_and_dbscan_clustering_python_.md) |
| [ThÃ¡ch Thá»©c Code: TÃ¬m Lá»— Há»•ng PhÃ¢n Cá»¥m Báº±ng Bá»™ Lá»c Báº£ng Chá»¯ CÃ¡i Chá»¯ X](aero_llm_08_codechallenge_cluster_the_x_terms.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_codechallenge_cluster_the_x_terms.md) |
| [PhÃ¢n RÃ£ Token, NhÃºng VÃ  PhÃ¢n Cá»¥m Biá»ƒu TÆ°á»£ng Emojis Báº±ng Äá»“ Thá»‹ Máº­t Äá»™](aero_llm_09_codechallenge_tokenize_embed_and_cluster_happy_emojis.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_tokenize_embed_and_cluster_happy_emojis.md) |
| [PhÃ¢n TÃ­ch RSA (Representational Similarity Analysis) Giá»¯a CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_10_rsa_representational_similarity_analysis_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_rsa_representational_similarity_analysis_.md) |
| [PhÃ¢n TÃ­ch Äá»™ Lá»‡ch RSA (Part 1): So SÃ¡nh Sá»± Báº¥t Äá»“ng Giá»¯a KhÃ´ng Gian GloVe 50D vÃ  300D](aero_llm_11_codechallenge_compare_embeddings_with_rsa_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_compare_embeddings_with_rsa_part_1_.md) |
| [PhÃ¢n TÃ­ch Äá»™ Lá»‡ch RSA (Part 2): Äá»‘i Chiáº¿u TÆ°Æ¡ng Quan Pearson Cho Khoáº£ng CÃ¡ch Cosine](aero_llm_12_codechallenge_compare_embeddings_with_rsa_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_compare_embeddings_with_rsa_part_2_.md) |
| [So SÃ¡nh KhÃ´ng Gian NhÃºng: Word2Vec VÃ  GPT-2 Qua PhÃ¢n TÃ­ch RSA](aero_llm_13_codechallenge_word2vec_vs_gpt2.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_codechallenge_word2vec_vs_gpt2.md) |
| [Bá»‘ Cá»¥c Äá»“ Thá»‹ Máº¡ng (Network Graph) ThÃ´ng Qua Ma Tráº­n Cosine Similarity](aero_llm_14_codechallenge_graph_representation_of_cosine_similarities.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_graph_representation_of_cosine_similarities.md) |
| [Sá»‘ Há»c Tuyáº¿n TÃ­nh vÃ  RÃºt TrÃ­ch TÆ°Æ¡ng Äá»“ng Giá»¯a CÃ¡c Tá»« NhÃºng (Word Embeddings Analogies)](aero_llm_15_embeddings_arithmetic_and_analogies.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_embeddings_arithmetic_and_analogies.md) |
| [Vá»¡ Má»™ng Vá» Sá»‘ Há»c Vector TÆ°Æ¡ng ÄÆ°Æ¡ng (Soft-Coded Analogies) TrÃªn Word2Vec](aero_llm_16_codechallenge_soft_coded_analogies_in_word2vec.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_soft_coded_analogies_in_word2vec.md) |
| [Thiáº¿t Láº­p VÃ  Diá»…n Giáº£i Trá»¥c Ngá»¯ NghÄ©a Tuyáº¿n TÃ­nh (Linear Semantic Axes)](aero_llm_17_creating_and_interpreting_linear_semantic_axes.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_creating_and_interpreting_linear_semantic_axes.md) |
| [Khai ThÃ¡c Thuáº­t ToÃ¡n k-NN Cho TÃ¬m Kiáº¿m Tá»« Äá»“ng NghÄ©a TrÃªn BERT](aero_llm_18_knn_for_synonym_searching_in_bert.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_knn_for_synonym_searching_in_bert.md) |
| [Cáº¡nh Tranh TÃ¬m Tá»« Äá»“ng NghÄ©a BERT vs GPT: CÆ¡ Cháº¿ Tokenization Äa KÃ½ Tá»±](aero_llm_19_codechallenge_bert_v_gpt_knn_kompetition.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_codechallenge_bert_v_gpt_knn_kompetition.md) |
| [Sá»± Dá»‹ch Chuyá»ƒn VÃ  Äá»“ng Tá»“n Biá»ƒu Diá»…n Giá»¯a CÃ¡c KhÃ´ng Gian NhÃºng](aero_llm_20_research_on_translating_embeddings_spaces.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_research_on_translating_embeddings_spaces.md) |
| [PhÃ¢n TÃ­ch ChÃ¹m Quang Phá»• Suy Biáº¿n (Singular Value Spectrum) Cá»§a KhÃ´ng Gian NhÃºng](aero_llm_21_singular_value_spectrum_of_embeddings_submatrices.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_singular_value_spectrum_of_embeddings_submatrices.md) |
| ğŸ“Œ **[Ãnh Xáº¡ SVD CÃ¡c Dáº£i Äiá»ƒm NhÃºng CÃ³ Quan Há»‡ ChÃ©o](aero_llm_22_codechallenge_svd_projections_of_related_embeddings.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_22_codechallenge_svd_projections_of_related_embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

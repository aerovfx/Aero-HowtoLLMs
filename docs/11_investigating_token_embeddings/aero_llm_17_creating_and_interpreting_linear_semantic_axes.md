
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [11 investigating token embeddings](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thiáº¿t Láº­p VÃ  Diá»…n Giáº£i Trá»¥c Ngá»¯ NghÄ©a Tuyáº¿n TÃ­nh (Linear Semantic Axes)

## TÃ³m táº¯t

CÃ¡c chiá»u trong khÃ´ng gian nhÃºng cá»§a há»‡ mÃ´ hÃ¬nh tá»± há»“i quy lá»›n (Autoregressive LLMs) thÆ°á»ng Ä‘Æ°á»£c gÃ¡n cho má»™t tÃ­nh cháº¥t tháº§n bÃ­ khi mÃ  cÃ¡c nhÃ  khoa há»c cÃ³ thá»ƒ cá»™ng trá»« cÃ¡c Ä‘áº¡i lÆ°á»£ng Ä‘á»‹nh danh Ä‘á»ƒ tÃ¬m cÃ¡c gÃ³c Ä‘á»™ ngá»¯ phÃ¡p $VD: Vector TÆ°Æ¡ng lai - Vector QuÃ¡ khá»© = Trá»¥c thá»i gian$. BÃ i bÃ¡o khoa há»c nÃ y chá»©ng minh táº§m áº£nh hÆ°á»Ÿng cá»§a thao tÃ¡c Chuáº©n hÃ³a hÃ¬nh há»c (Normalization) Ä‘á»‘i chiáº¿u kháº£ nÄƒng sÃ ng lá»c tÃ­n hiá»‡u vá»›i má»™t bá»™ lá»c thÃ´ sÆ¡ trong quÃ¡ trÃ¬nh lÃ m nÃ©t "Trá»¥c ngá»¯ nghÄ©a tuyáº¿n tÃ­nh" cá»§a cá»¥m nhÃºng.

---

## 1. Trá»¥c Ngá»¯ NghÄ©a: ÄÆ°á»ng Váº½ Logic Cháº¡y XuyÃªn Ma Tráº­n

KhÃ´ng gian tá»« vá»±ng Word2Vec, theo lÃ½ thuyáº¿t, chá»©a kháº£ nÄƒng biá»ƒu diá»…n nhá»¯ng khÃ¡i niá»‡m tÆ°Æ¡ng pháº£n á»Ÿ hai phÃ­a cá»§a má»™t Ä‘Æ°á»ng tháº³ng. Giáº£ sá»­ ta muá»‘n xÃ¡c láº­p má»™t **Trá»¥c Thá»i Gian (Time Axis)**, phÃ©p tÃ­nh láº¥y Ä‘iá»ƒm nÃºt (anchor points) lÃ  hai tá»a Ä‘á»™ Ä‘áº·c trÆ°ng Ä‘á»‘i láº­p "Past" vÃ  "Future":

\vec{v}_{\text{TimeAxis}} = \vec{v}_{\text{future}} - \vec{v}_{\text{past}}

Má»™t khi Ä‘Ã£ xÃ¡c láº­p Ä‘Æ°á»£c $\vec{v}_{\text{TimeAxis}}$, má»i vector nhÃºng $\vec{w}$ báº¥t ká»³ khi chiáº¿u (project) lÃªn trá»¥c nÃ y sáº½ tráº£ vá» há»‡ sá»‘ (projection scalar) dá»± Ä‘oÃ¡n má»©c Ä‘á»™ "thuá»™c vá» tÆ°Æ¡ng lai" hay "hoÃ i niá»‡m quÃ¡ khá»©" thÃ´ng qua phÃ©p TÃ­ch vÃ´ hÆ°á»›ng (Dot product).

---

## 2. Tiá»n Xá»­ LÃ½ HÃ¬nh Há»c (Geometric Pre-Normalization)

Cáº¡m báº«y tiá»m áº©n cá»§a viá»‡c trá»« Ä‘i hai tá»a Ä‘á»™ thÃ´ náº±m á»Ÿ "Sá»©c náº·ng vi phÃ¢n" cá»§a má»—i token. Nhá»¯ng tá»« vá»±ng thÃ´ng thÆ°á»ng nhÆ°ng vÃ´ nghÄ©a (stop words nhÆ° "the", "an", "is") chá»©a vector embeddings má» vá»›i chiá»u dÃ i chuáº©n ngáº¯n (low $L2-norm$). NgÆ°á»£c láº¡i cÃ¡c tá»« ngá»¯ cáº£nh trá»ng Ä‘iá»ƒm sáº½ cÃ³ $\vec{v}$ vá»›i chiá»u dÃ i cá»±c Ä‘áº¡i Ä‘Ã¢m xa khá»i gá»‘c tá»a Ä‘á»™ $0$.

Náº¿u ta láº¥y $\vec{v}_{\text{future}} - \vec{v}_{\text{the}}$, Ä‘Ã¡p Ã¡n sáº½ bá»‹ nghiÃªng lá»‡ch (bias) khá»•ng lá»“ vá» phÃ­a Ä‘áº§u Ä‘iá»ƒm "future" khiáº¿n cho trá»¥c khÃ´ng gian thÃ nh pháº©m bá»‹ trÆ°á»£t gÃ³c máº¥t tÃ­nh Ä‘á»‘i xá»©ng tÆ°Æ¡ng sinh. Váº¥n Ä‘á» Ä‘Æ°á»£c giáº£i quyáº¿t báº±ng viá»‡c báº¯t buá»™t **Chuáº©n HÃ³a (Normalization)** Ä‘á»™ dÃ i tá»«ng thÃ nh pháº§n trÆ°á»›c khi thá»±c hiá»‡n quy Ä‘á»•i trá»¥c:

\hat{v}_{\text{future}} = \frac{\vec{v}_{\text{future}}}{\|\vec{v}_{\text{future}}\|}

$$
\hat{v}_{\text{past}} = \frac{\vec{v}_{\text{past}}}{\|\vec{v}_{\text{past}}\|} Trá»¥c ngá»¯ nghÄ©a thá»±c thá»¥ (Normalized Axis) pháº£i Ä‘Æ°á»£c thiáº¿t láº­p trÃªn hai vector chuáº©n quy cÃ³ Ä‘á»™ dÃ i giá»›i háº¡n trong vÃ²ng viá»n cáº§u báº±ng 1: \vec{v}_{\text{TimeAxisNorm}} = \hat{v}_{\text{future}} - \hat{v}_{\text{past}}
$$


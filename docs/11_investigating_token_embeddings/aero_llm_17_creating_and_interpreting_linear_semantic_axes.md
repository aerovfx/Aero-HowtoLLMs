
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [11 investigating token embeddings](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thiáº¿t Láº­p VÃ  Diá»…n Giáº£i Trá»¥c Ngá»¯ NghÄ©a Tuyáº¿n TÃ­nh (Linear Semantic Axes)

## TÃ³m táº¯t

CÃ¡c chiá»u trong khÃ´ng gian nhÃºng cá»§a há»‡ mÃ´ hÃ¬nh tá»± há»“i quy lá»›n (Autoregressive LLMs) thÆ°á»ng Ä‘Æ°á»£c gÃ¡n cho má»™t tÃ­nh cháº¥t tháº§n bÃ­ khi mÃ  cÃ¡c nhÃ  khoa há»c cÃ³ thá»ƒ cá»™ng trá»« cÃ¡c Ä‘áº¡i lÆ°á»£ng Ä‘á»‹nh danh Ä‘á»ƒ tÃ¬m cÃ¡c gÃ³c Ä‘á»™ ngá»¯ phÃ¡p (VD: Vector TÆ°Æ¡ng lai - Vector QuÃ¡ khá»© = Trá»¥c thá»i gian). BÃ i bÃ¡o khoa há»c nÃ y chá»©ng minh táº§m áº£nh hÆ°á»Ÿng cá»§a thao tÃ¡c Chuáº©n hÃ³a hÃ¬nh há»c (Normalization) Ä‘á»‘i chiáº¿u kháº£ nÄƒng sÃ ng lá»c tÃ­n hiá»‡u vá»›i má»™t bá»™ lá»c thÃ´ sÆ¡ trong quÃ¡ trÃ¬nh lÃ m nÃ©t "Trá»¥c ngá»¯ nghÄ©a tuyáº¿n tÃ­nh" cá»§a cá»¥m nhÃºng.

---

## 1. Trá»¥c Ngá»¯ NghÄ©a: ÄÆ°á»ng Váº½ Logic Cháº¡y XuyÃªn Ma Tráº­n

KhÃ´ng gian tá»« vá»±ng Word2Vec, theo lÃ½ thuyáº¿t, chá»©a kháº£ nÄƒng biá»ƒu diá»…n nhá»¯ng khÃ¡i niá»‡m tÆ°Æ¡ng pháº£n á»Ÿ hai phÃ­a cá»§a má»™t Ä‘Æ°á»ng tháº³ng. Giáº£ sá»­ ta muá»‘n xÃ¡c láº­p má»™t **Trá»¥c Thá»i Gian (Time Axis)**, phÃ©p tÃ­nh láº¥y Ä‘iá»ƒm nÃºt (anchor points) lÃ  hai tá»a Ä‘á»™ Ä‘áº·c trÆ°ng Ä‘á»‘i láº­p "Past" vÃ  "Future":
$$
\vec{v}_{\text{TimeAxis}} = \vec{v}_{\text{future}} - \vec{v}_{\text{past}}
$$
Má»™t khi Ä‘Ã£ xÃ¡c láº­p Ä‘Æ°á»£c $\vec{v}_{\text{TimeAxis}}$, má»i vector nhÃºng $\vec{w}$ báº¥t ká»³ khi chiáº¿u (project) lÃªn trá»¥c nÃ y sáº½ tráº£ vá» há»‡ sá»‘ (projection scalar) dá»± Ä‘oÃ¡n má»©c Ä‘á»™ "thuá»™c vá» tÆ°Æ¡ng lai" hay "hoÃ i niá»‡m quÃ¡ khá»©" thÃ´ng qua phÃ©p TÃ­ch vÃ´ hÆ°á»›ng (Dot product).

---

## 2. Tiá»n Xá»­ LÃ½ HÃ¬nh Há»c (Geometric Pre-Normalization)

Cáº¡m báº«y tiá»m áº©n cá»§a viá»‡c trá»« Ä‘i hai tá»a Ä‘á»™ thÃ´ náº±m á»Ÿ "Sá»©c náº·ng vi phÃ¢n" cá»§a má»—i token. Nhá»¯ng tá»« vá»±ng thÃ´ng thÆ°á»ng nhÆ°ng vÃ´ nghÄ©a (stop words nhÆ° "the", "an", "is") chá»©a vector embeddings má» vá»›i chiá»u dÃ i chuáº©n ngáº¯n (low $L2-norm$). NgÆ°á»£c láº¡i cÃ¡c tá»« ngá»¯ cáº£nh trá»ng Ä‘iá»ƒm sáº½ cÃ³ $\vec{v}$ vá»›i chiá»u dÃ i cá»±c Ä‘áº¡i Ä‘Ã¢m xa khá»i gá»‘c tá»a Ä‘á»™ $0$.

Náº¿u ta láº¥y $\vec{v}_{\text{future}} - \vec{v}_{\text{the}}$, Ä‘Ã¡p Ã¡n sáº½ bá»‹ nghiÃªng lá»‡ch (bias) khá»•ng lá»“ vá» phÃ­a Ä‘áº§u Ä‘iá»ƒm "future" khiáº¿n cho trá»¥c khÃ´ng gian thÃ nh pháº©m bá»‹ trÆ°á»£t gÃ³c máº¥t tÃ­nh Ä‘á»‘i xá»©ng tÆ°Æ¡ng sinh. Váº¥n Ä‘á» Ä‘Æ°á»£c giáº£i quyáº¿t báº±ng viá»‡c báº¯t buá»™t **Chuáº©n HÃ³a (Normalization)** Ä‘á»™ dÃ i tá»«ng thÃ nh pháº§n trÆ°á»›c khi thá»±c hiá»‡n quy Ä‘á»•i trá»¥c:
$$
\hat{v}_{\text{future}} = \frac{\vec{v}_{\text{future}}}{\|\vec{v}_{\text{future}}\|}
$$
$$
\hat{v}_{\text{past}} = \frac{\vec{v}_{\text{past}}}{\|\vec{v}_{\text{past}}\|}
$$
Trá»¥c ngá»¯ nghÄ©a thá»±c thá»¥ (Normalized Axis) pháº£i Ä‘Æ°á»£c thiáº¿t láº­p trÃªn hai vector chuáº©n quy cÃ³ Ä‘á»™ dÃ i giá»›i háº¡n trong vÃ²ng viá»n cáº§u báº±ng $1$:
$$ 
\vec{v}_{\text{TimeAxisNorm}} = \hat{v}_{\text{future}} - \hat{v}_{\text{past}} 
$$
TÃ­nh khÆ°á»¡ng bá»©c khÃ´ng gian nÃ y tÆ°á»›c Ä‘i áº£o áº£nh phÆ°Æ¡ng sai tá»« Ä‘á»™ lá»›n module, khiáº¿n há»‡ quy chiáº¿u chá»‰ táº­p trung vÃ o khÃ¡c biá»‡t phÆ°Æ¡ng hÆ°á»›ng gÃ³c (Cosine direction divergence).

---

## 3. Há»‡ Quáº£ Tá»« Nhá»¯ng Bá»™ Lá»c CÄƒn Báº£n (Tokens Filtering Rule)

Khi tiáº¿n hÃ nh cháº¥m Ä‘iá»ƒm (Cosine similarity mapping) má»™t "Trá»¥c ranh giá»›i thá»i gian" vá»›i má»™t bá»™ tá»« Ä‘iá»ƒn lÃªn Ä‘áº¿n hÃ ng triá»‡u tá»« vá»±ng cáº¯t ra tá»« Wikipedia, má»™t sá»‘ káº¿t quáº£ láº¡ láº«m Ã¢m cá»±c cÃ³ thá»ƒ ná»• ra (nhá»¯ng liÃªn káº¿t token nhiá»…u nhÆ° Ä‘á»‹a chá»‰ URL, kÃ½ tá»± lá»—i, chá»¯ Latin viáº¿t táº¯t trá»™n láº«n Ä‘iá»ƒm ngáº«u nhiÃªn). Äá»ƒ khá»­ cÃ¡c yáº¿u tá»‘ nhiá»…u nÃ y, logic Lá»c nhÃ£n tá»« vá»±ng (Filters) Ä‘Æ°á»£c bá»• sung:
- **Chuáº©n Cá»± ly Chá»¯ cÃ¡i:** Tá»« vá»±ng yÃªu cáº§u $> 2$  letters.
- **Biá»ƒu thá»©c chÃ­nh quy (Alphanumeric Mask):** Chá»‰ nhá»¯ng máº£ng kÃ­ tá»± hoÃ n toÃ n cáº¥u táº¡o tá»« chá»¯ há»‡ ABC.

Bá»™ lá»c nÃ y quÃ©t qua ma tráº­n tá»« nhÃºng $E \in \mathbb{R}^{V \times D}$ (vá»›i $V=3.000.000$). Káº¿t quáº£ loáº¡i bá» Ä‘áº¿n $70\%$ khá»‘i lÆ°á»£ng vocab cá»§a Word2vec chá»‰ chá»©a rÃ¡c thÃ´ng tin (noise artifacts). Máº£ng rÃºt lÃµi giÃºp tÄƒng tá»‘c Ä‘áº¡o hÃ m bá»™ nhá»› trÃªn $E_{filtered}$, cho phÃ©p $\text{CosSim}(\vec{v}_{\text{axis}}, E_{filtered}^T)$ tÃ¬m trÃºng nhá»¯ng váº¡ch Ä‘Ã­ch tá»« vá»±ng ("pass performance", "yesterday") náº±m Ä‘Ãºng phá»• Ã¢m cá»§a trá»¥c mÃ  khÃ´ng dÃ­nh báº«y ngáº«u nhiÃªn (false positive correlation).

---

## 4. Káº¿t luáº­n

CÃ¡c thá»±c thá»ƒ LLMs vá»›i biá»ƒu Ä‘á»“ Attention khÃ´ng hoáº¡t Ä‘á»™ng báº±ng phÃ¡p thuáº­t - chÃºng lÃ  tá»• há»£p bÄƒm rÃ£ nhá»¯ng lá»›p Norm (Normalization) xáº¿p chá»“ng nhau vÃ  nhá»¯ng thá»§ thuáº­t mÃ ng lá»c nhá» cáº¥u thÃ nh má»™t kiáº¿n trÃºc phi tuyáº¿n ká»³ vÄ©. TÃ­nh toÃ¡n cÃ¡c Ä‘iá»ƒm lÃ¢n cáº­n ngá»¯ nghÄ©a trÃªn máº¡ng NÆ¡-ron yÃªu cáº§u sá»± nghiÃªm ngáº·t Ä‘á»‹nh chuáº©n (vector normalizations) nháº±m khÃ´ng Ä‘á»ƒ cho tÃ­nh Ä‘a dáº¡ng ngáº«u nhiÃªn cá»§a khá»‘i lÆ°á»£ng tá»a Ä‘á»™ chiá»…m lÄ©nh Ä‘á»™ cÃ¢n báº±ng tuyáº¿n tÃ­nh cá»§a hÃ¬nh há»c ngÃ´n ngá»¯ há»c.

---

## TÃ i liá»‡u tham kháº£o

1. **Bolukbasi, T., et al. (2016).** *Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings*. NeurIPS. (Ãp dá»¥ng trá»¥c ngá»¯ nghÄ©a khá»­ thiÃªn vá»‹ giá»›i tÃ­nh).
2. **Li, Y., et al. (2015).** *Representation Learning for Semantic Composition*. ACL.
3. TÃ i liá»‡u bÃ i giáº£ng *Investigating token embeddings - Creating and interpreting linear semantic axes*.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero llm 01 codechallenge cosine similarity advanced part 1](aero_llm_01_codechallenge_cosine_similarity_advanced_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_codechallenge_cosine_similarity_advanced_part_1_.md) |
| [aero llm 02 codechallenge cosine similarity advanced part 2](aero_llm_02_codechallenge_cosine_similarity_advanced_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_codechallenge_cosine_similarity_advanced_part_2_.md) |
| [Theo DÃµi DÃ²ng Cháº£y Cosine Similarity TrÃªn Trá»¥c VÄƒn Báº£n ChuyÃªn Tuáº§n Tá»± (Word Sequences)](aero_llm_03_codechallenge_cosine_similarity_in_word_sequences.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_cosine_similarity_in_word_sequences.md) |
| [Nghá»‡ Thuáº­t Váº½ Báº£n Äá»“ Nhiá»‡t Ma Tráº­n NhÃºng Báº±ng CÆ°á»ng Äá»™ Tá»« (Coloring Cosine Similarity)](aero_llm_04_codechallenge_coloring_cosine_similarity.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_coloring_cosine_similarity.md) |
| [áº¢o áº¢nh Cá»§a TrÃ­ Tuá»‡ ToÃ¡n Há»c Trong NgÃ´n Ngá»¯: Sá»©c Máº¡nh Cá»§a Random Embeddings](aero_llm_05_codechallenge_can_random_embeddings_be_interpreted.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_codechallenge_can_random_embeddings_be_interpreted.md) |
| [PhÆ°Æ¡ng PhÃ¡p T-SNE VÃ  Thuáº­t ToÃ¡n PhÃ¢n Cá»¥m DBSCAN: Chiáº¿u KhÃ´ng Gian Äa Chiá»u Cho LLMs](aero_llm_06_t_sne_projection_and_dbscan_clustering_theory_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_t_sne_projection_and_dbscan_clustering_theory_.md) |
| [PhÃ¢n Cá»¥m Ngá»¯ NghÄ©a Qua PhÃ©p Chiáº¿u t-SNE & Máº­t Äá»™ DBSCAN (Python)](aero_llm_07_t_sne_projection_and_dbscan_clustering_python_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_t_sne_projection_and_dbscan_clustering_python_.md) |
| [ThÃ¡ch Thá»©c Code: TÃ¬m Lá»— Há»•ng PhÃ¢n Cá»¥m Báº±ng Bá»™ Lá»c Báº£ng Chá»¯ CÃ¡i Chá»¯ X](aero_llm_08_codechallenge_cluster_the_x_terms.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_codechallenge_cluster_the_x_terms.md) |
| [PhÃ¢n RÃ£ Token, NhÃºng VÃ  PhÃ¢n Cá»¥m Biá»ƒu TÆ°á»£ng Emojis Báº±ng Äá»“ Thá»‹ Máº­t Äá»™](aero_llm_09_codechallenge_tokenize_embed_and_cluster_happy_emojis.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_tokenize_embed_and_cluster_happy_emojis.md) |
| [PhÃ¢n TÃ­ch RSA (Representational Similarity Analysis) Giá»¯a CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_10_rsa_representational_similarity_analysis_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_rsa_representational_similarity_analysis_.md) |
| [PhÃ¢n TÃ­ch Äá»™ Lá»‡ch RSA (Part 1): So SÃ¡nh Sá»± Báº¥t Äá»“ng Giá»¯a KhÃ´ng Gian GloVe 50D vÃ  300D](aero_llm_11_codechallenge_compare_embeddings_with_rsa_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_compare_embeddings_with_rsa_part_1_.md) |
| [PhÃ¢n TÃ­ch Äá»™ Lá»‡ch RSA (Part 2): Äá»‘i Chiáº¿u TÆ°Æ¡ng Quan Pearson Cho Khoáº£ng CÃ¡ch Cosine](aero_llm_12_codechallenge_compare_embeddings_with_rsa_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_compare_embeddings_with_rsa_part_2_.md) |
| [So SÃ¡nh KhÃ´ng Gian NhÃºng: Word2Vec VÃ  GPT-2 Qua PhÃ¢n TÃ­ch RSA](aero_llm_13_codechallenge_word2vec_vs_gpt2.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_codechallenge_word2vec_vs_gpt2.md) |
| [Bá»‘ Cá»¥c Äá»“ Thá»‹ Máº¡ng (Network Graph) ThÃ´ng Qua Ma Tráº­n Cosine Similarity](aero_llm_14_codechallenge_graph_representation_of_cosine_similarities.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_graph_representation_of_cosine_similarities.md) |
| [Sá»‘ Há»c Tuyáº¿n TÃ­nh vÃ  RÃºt TrÃ­ch TÆ°Æ¡ng Äá»“ng Giá»¯a CÃ¡c Tá»« NhÃºng (Word Embeddings Analogies)](aero_llm_15_embeddings_arithmetic_and_analogies.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_embeddings_arithmetic_and_analogies.md) |
| [Vá»¡ Má»™ng Vá» Sá»‘ Há»c Vector TÆ°Æ¡ng ÄÆ°Æ¡ng (Soft-Coded Analogies) TrÃªn Word2Vec](aero_llm_16_codechallenge_soft_coded_analogies_in_word2vec.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_soft_coded_analogies_in_word2vec.md) |
| ğŸ“Œ **[Thiáº¿t Láº­p VÃ  Diá»…n Giáº£i Trá»¥c Ngá»¯ NghÄ©a Tuyáº¿n TÃ­nh (Linear Semantic Axes)](aero_llm_17_creating_and_interpreting_linear_semantic_axes.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_17_creating_and_interpreting_linear_semantic_axes.md) |
| [Khai ThÃ¡c Thuáº­t ToÃ¡n k-NN Cho TÃ¬m Kiáº¿m Tá»« Äá»“ng NghÄ©a TrÃªn BERT](aero_llm_18_knn_for_synonym_searching_in_bert.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_knn_for_synonym_searching_in_bert.md) |
| [Cáº¡nh Tranh TÃ¬m Tá»« Äá»“ng NghÄ©a BERT vs GPT: CÆ¡ Cháº¿ Tokenization Äa KÃ½ Tá»±](aero_llm_19_codechallenge_bert_v_gpt_knn_kompetition.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_codechallenge_bert_v_gpt_knn_kompetition.md) |
| [Sá»± Dá»‹ch Chuyá»ƒn VÃ  Äá»“ng Tá»“n Biá»ƒu Diá»…n Giá»¯a CÃ¡c KhÃ´ng Gian NhÃºng](aero_llm_20_research_on_translating_embeddings_spaces.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_research_on_translating_embeddings_spaces.md) |
| [PhÃ¢n TÃ­ch ChÃ¹m Quang Phá»• Suy Biáº¿n (Singular Value Spectrum) Cá»§a KhÃ´ng Gian NhÃºng](aero_llm_21_singular_value_spectrum_of_embeddings_submatrices.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_singular_value_spectrum_of_embeddings_submatrices.md) |
| [Ãnh Xáº¡ SVD CÃ¡c Dáº£i Äiá»ƒm NhÃºng CÃ³ Quan Há»‡ ChÃ©o](aero_llm_22_codechallenge_svd_projections_of_related_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_codechallenge_svd_projections_of_related_embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [11 investigating token embeddings](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Sá»‘ Há»c Tuyáº¿n TÃ­nh vÃ  RÃºt TrÃ­ch TÆ°Æ¡ng Äá»“ng Giá»¯a CÃ¡c Tá»« NhÃºng (Word Embeddings Analogies)

## TÃ³m táº¯t

Kháº£ nÄƒng thá»±c hiá»‡n cÃ¡c phÃ©p toÃ¡n Ä‘áº¡i sá»‘ phi tuyáº¿n dáº¡ng `Vector(King) - Vector(Man) + Vector(Woman) = Vector(Queen)` lÃ  má»™t trong nhá»¯ng minh há»a trá»±c quan kinh Ä‘iá»ƒn nháº¥t khi nháº¯c tá»›i sá»©c máº¡nh cá»§a KhÃ´ng gian nhÃºng tá»« (Word Embeddings). BÃ i bÃ¡o nÃ y phÃ¢n tÃ¡ch báº£n cháº¥t hÃ¬nh há»c cá»§a sá»‘ há»c vector trong nghiÃªn cá»©u ngÃ´n ngá»¯, lÃ m rÃµ phÃ©p trá»« chiá»u khÃ´ng gian (Dimension subtraction) tÆ°Æ¡ng quan tá»›i logic diá»…n giáº£i ngá»¯ nghÄ©a (semantic linear axes) nhÆ° tháº¿ nÃ o. TrÃ¡i nghá»‹ch vá»›i nhá»¯ng lá»i tÃ¢ng bá»‘c tá»« truyá»n thÃ´ng, bÃ i viáº¿t cÅ©ng Ä‘Æ°a ra nhá»¯ng sá»± giá»›i háº¡n vÃ  rá»§i ro tá»« gÃ³c nhÃ¬n HÃ¬nh há»c Ä‘áº¡i sá»‘ tuyáº¿n tÃ­nh cá»§a Embeddings.

---

## 1. Giá»›i thiá»‡u: Chuyá»ƒn Ngá»¯ NghÄ©a ThÃ nh CÃ¡c Tá»• Há»£p Tuyáº¿n TÃ­nh

Ká»ƒ tá»« sá»± ra Ä‘á»i phÃ´i thai cá»§a Word2Vec (Mikolov 2013) vÃ  sau Ä‘Ã³ lÃ  GloVe hay Transformers, há»‡ thá»‘ng AI chuyá»ƒn Ä‘á»•i cáº¥u trÃºc ngá»¯ nghÄ©a tiáº¿ng anh khÃ´ khan sang má»™t khÃ´ng gian Ä‘iá»ƒm Euclidean ($N$-dimensions). Náº¿u sá»± há»c hÃ³a nÃ y tháº­t sá»± báº¯t nguá»“n tá»« má»™t lÃ½ thuyáº¿t táº­p há»£p, cÃ¡c hÆ°á»›ng vÃ  Ä‘á»™ dá»c vector báº¯t buá»™c pháº£i mang tÃ­nh cáº¥u hÃ¬nh tuyáº¿n tÃ­nh Ä‘á»ƒ tráº£ lá»i cho nhá»¯ng tá»· lá»‡ tÆ°Æ¡ng Ä‘á»“ng (Analogies). 

Trong khÃ´ng gian nÃ y, khoáº£ng cÃ¡ch tá»« "Man" (Nam giá»›i) Ä‘áº¿n "King" (Vua) Ä‘Æ°á»£c Ä‘á»‹nh hÃ¬nh bá»Ÿi vector báº£n cháº¥t (VÃ­ dá»¥: Sá»± quyá»n lá»±c). Do Ä‘Ã³ viá»‡c sao chÃ©p vector báº£n cháº¥t nÃ y lÃªn tá»a Ä‘á»™ cá»§a "Woman" (Ná»¯ giá»›i) trÃªn lÃ½ thuyáº¿t pháº£i tráº£ vá» má»™t vá»‹ trÃ­ ráº¥t gáº§n vá»›i Ä‘áº¡i lÆ°á»£ng mang Ã½ nghÄ©a "Sá»± quyá»n lá»±c cá»§a ná»¯ giá»›i" - tá»©c lÃ  "Queen" (Ná»¯ hoÃ ng).

---

## 2. Ká»¹ Thuáº­t TÃ­nh ToÃ¡n Sá»‘ Há»c Embeddings (Vector Arithmetic)

Khá»Ÿi táº¡o há»‡ thá»‘ng khÃ´ng gian vector liÃªn tá»¥c (Continuous Vector Space Model), phÃ©p suy luáº­n tÆ°Æ¡ng Ä‘á»“ng bao hÃ m theo cÃ¡c bÆ°á»›c:

### 2.1 PhÆ°Æ¡ng trÃ¬nh Tuyáº¿n tÃ­nh TÆ°Æ¡ng quan cÆ¡ sá»Ÿ
Gá»i $v_w \in $\mathbb${R}^D$ lÃ  biá»ƒu diá»…n vector $D$ chiá»u cá»§a tá»« $w$. PhÆ°Æ¡ng trÃ¬nh sá»‘ há»c cá»‘t lÃµi láº¥y Ã½ tÆ°á»Ÿng tá»« quy luáº­t hÃ¬nh bÃ¬nh hÃ nh (Parallelogram law):

$$

$$

v_{analogy} = v_{king} - v_{man} + v_{woman}

$$

$$

ÄÃ¢y lÃ  quÃ¡ trÃ¬nh triá»‡t tiÃªu (subtract) má»™t vector thuá»™c tÃ­nh trá»«u tÆ°á»£ng (nhÆ° *giá»›i tÃ­nh*) vÃ  tiÃªm (inject) vÃ o má»™t thÃ nh pháº§n thuá»™c tÃ­nh khÃ¡c. 

### 2.2 Thuáº­t toÃ¡n Argmax vá»›i Cosine Similarity
VÃ¬ $v_{analogy}$ khÃ´ng cháº¯c cháº¯n Ä‘Ã¡p tháº³ng vÃ o tÃ¢m cá»§a má»™t tá»« vá»±ng xÃ¡c thá»±c cÃ³ sáºµn (do Ä‘á»™ trÃ´i dáº¡t - concept drift trong khÃ´ng gian nhiá»…u), bÃ i toÃ¡n hiá»‡n ra dÆ°á»›i dáº¡ng má»™t hÃ m tÃ¬m Ä‘iá»ƒm lÃ¢n cáº­n gáº§n nháº¥t (Nearest Neighbors Search):

$$

$$

\text{target\_word} = \text{argmax}_{w \in V \setminus \{king, man, woman\}} \cos(v_{analogy}, v_w)

$$

$$

Trong Ä‘Ã³:
- $\cos(A, B) = \frac{A \cdot B}{\\midA\\mid\\midB\\mid}$ tÃ­nh báº±ng ma tráº­n khoáº£ng cÃ¡ch Gram.
- Táº­p tÃ¬m kiáº¿m $V$ pháº£i loáº¡i bá» cÃ¡c tá»« náº±m á»Ÿ pháº§n nÃ³n gá»‘c nháº±m ngÄƒn cáº£n mÃ´ hÃ¬nh tÃ¡i sinh ra Ä‘Ã¡p Ã¡n táº§m thÆ°á»ng do sá»± bÃ¹ng ná»• cá»§a quy chuáº©n chuáº©n hÃ³a L2 (L2 constraints).

---

## 3. Ranh Giá»›i áº¢o áº¢nh: Sá»± Thiáº¿t Thiáº¿u Tuyáº¿n TÃ­nh á» Äá»“ Thá»‹ NgÃ´n Ngá»¯ Phá»©c Táº¡p

Tuy Ä‘áº¡t Ä‘á»™ kinh ngáº¡c cao trÃªn nhá»¯ng tá»« cÃ³ Ä‘á»™ phá»• biáº¿n cá»¥ thá»ƒ, khi lÃ m toÃ¡n á»Ÿ nhá»¯ng khÃ¡i niá»‡m mÆ¡ há»“ nhÆ° Ä‘á»‹nh nghÄ©a "Trá»¥c thá»i gian" (Time axis), viá»‡c tÃ¬m Ä‘iá»ƒm lÃ¢n cáº­n vá»›i ma tráº­n `v_{tomorrow} - v_{yesterday}` Ä‘a pháº§n tháº¥t báº¡i vÃ  tráº£ vá» Ä‘iá»ƒm dá»± Ä‘oÃ¡n lÃ  nhá»¯ng nhiá»…u ngáº«u nhiÃªn.

**Do ÄÃ¢u Náº£y Sinh Hiá»‡n TÆ°á»£ng PhÃ¢n Äá»©t Sá»± Kiá»‡n?**
- **HÃ¬nh Há»c Cung Phi Tuyáº¿n:** Sá»‘ há»c cá»™ng trá»« tá»± suy diá»…n ráº±ng khÃ´ng gian thÃ´ng tin (Latent space distribution) tuÃ¢n theo mÃ´ hÃ¬nh Ä‘áº¡i sá»‘ tuyáº¿n tÃ­nh Euclid. NhÆ°ng thá»±c cháº¥t cÃ¡c Ä‘a táº¡p vi phÃ¢n phÃ¢n bá»• (embeddings manifolds) bá»‹ bÃ³p mÃ©o qua cáº¥u trÃºc phi tuyáº¿n hÃ m Softmax hoáº·c Relu cá»§a hÃ m máº¡ng Neural Networks.
- **Tiáº¿ng Vá»ng Truyá»n ThÃ´ng (Cherry Picking Artifact):** Khi truyá»n thÃ´ng cÃ´ng nghá»‡ liÃªn tá»¥c lan truyá»n phÃ©p toÃ¡n "King - Man", nÃ³ che giáº¥u Ä‘i sá»± tháº­t ráº±ng phÆ°Æ¡ng trÃ¬nh nÃ y lÃ  báº£n thiáº¿t káº¿ do con ngÆ°á»i lá»“ng ghÃ©p (hand-picked), khÃ´ng mang tÃ­nh tá»•ng quÃ¡t cho cÃ¡c cáº¥u trÃºc cÃº phÃ¡p vÄƒn pháº¡m vÄ© mÃ´ vÃ  ngÃ´n ngá»¯ áº©n dá»¥ (metaphors). ÄÃ¡nh giÃ¡ sá»± tinh vi cá»§a AI trÃªn sá»± phÃ¡c há»a giáº£n Ä‘Æ¡n hoÃ n nguyÃªn chá»§ nghÄ©a (reductionist view) tiá»m tÃ ng cÃ¡c lá»— há»•ng phÃ²ng thá»§ AI Safety, lÃ£ng quÃªn rá»§i ro tá»« logic láº­p trÃ¬nh Ä‘á»©t gÃ£y.

---

## 4. Káº¿t luáº­n

MÃ´ hÃ¬nh vector tuyáº¿n tÃ­nh sá»‘ há»c trÃªn cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n xÃ¡c nháº­n Ä‘á»‹nh dáº¡ng thÃ´ng tin trong AI chá»©a má»™t tráº­t tá»± khÃ´ng gian liÃªn tiáº¿p. Thay vÃ¬ dá»±a dáº«m má»™t cÃ¡ch mÃª tÃ­n ráº±ng phÃ©p cá»™ng trá»« Ä‘Æ¡n giáº£n cÃ³ thá»ƒ giáº£i pháº«u toÃ n bá»™ Ä‘áº·c tÃ­nh tÃ­nh toÃ¡n (Computational linearity) cá»§a LLM, chÃºng ta cáº§n hÆ°á»›ng Ä‘áº¿n má»™t phá»• vi phÃ¢n sÃ¢u rá»™ng hÆ¡n Ä‘á»ƒ gá»¡ gáº¡c máº¡ng lÆ°á»›i thÃ´ng Ä‘iá»‡p chÃ©o cá»§a nÃ£o bá»™ AI.

---

## TÃ i liá»‡u tham kháº£o

1. **Mikolov, T., et al. (2013).** *Linguistic Regularities in Continuous Space Word Representations.* NAACL-HLT. (Sá»± khá»Ÿi Ä‘áº§u cá»§a phÆ°Æ¡ng trÃ¬nh Analogy).
2. **Levy, O., & Goldberg, Y. (2014).** *Linguistic Regularities in Sparse and Explicit Word Representations.* CoNLL.
3. **Ethayarajh, K., et al. (2019).** *How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings.* EMNLP.
4. TÃ i liá»‡u bÃ i giáº£ng *Tokens investigating - Embeddings arithmetic and analogies*.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero llm 01 codechallenge cosine similarity advanced part 1](aero_llm_01_codechallenge_cosine_similarity_advanced_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_codechallenge_cosine_similarity_advanced_part_1_.md) |
| [aero llm 02 codechallenge cosine similarity advanced part 2](aero_llm_02_codechallenge_cosine_similarity_advanced_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_codechallenge_cosine_similarity_advanced_part_2_.md) |
| [Theo DÃµi DÃ²ng Cháº£y Cosine Similarity TrÃªn Trá»¥c VÄƒn Báº£n ChuyÃªn Tuáº§n Tá»± (Word Sequences)](aero_llm_03_codechallenge_cosine_similarity_in_word_sequences.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_cosine_similarity_in_word_sequences.md) |
| [Nghá»‡ Thuáº­t Váº½ Báº£n Äá»“ Nhiá»‡t Ma Tráº­n NhÃºng Báº±ng CÆ°á»ng Äá»™ Tá»« (Coloring Cosine Similarity)](aero_llm_04_codechallenge_coloring_cosine_similarity.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_coloring_cosine_similarity.md) |
| [áº¢o áº¢nh Cá»§a TrÃ­ Tuá»‡ ToÃ¡n Há»c Trong NgÃ´n Ngá»¯: Sá»©c Máº¡nh Cá»§a Random Embeddings](aero_llm_05_codechallenge_can_random_embeddings_be_interpreted.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_codechallenge_can_random_embeddings_be_interpreted.md) |
| [PhÆ°Æ¡ng PhÃ¡p T-SNE VÃ  Thuáº­t ToÃ¡n PhÃ¢n Cá»¥m DBSCAN: Chiáº¿u KhÃ´ng Gian Äa Chiá»u Cho LLMs](aero_llm_06_t_sne_projection_and_dbscan_clustering_theory_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_t_sne_projection_and_dbscan_clustering_theory_.md) |
| [PhÃ¢n Cá»¥m Ngá»¯ NghÄ©a Qua PhÃ©p Chiáº¿u t-SNE & Máº­t Äá»™ DBSCAN (Python)](aero_llm_07_t_sne_projection_and_dbscan_clustering_python_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_t_sne_projection_and_dbscan_clustering_python_.md) |
| [ThÃ¡ch Thá»©c Code: TÃ¬m Lá»— Há»•ng PhÃ¢n Cá»¥m Báº±ng Bá»™ Lá»c Báº£ng Chá»¯ CÃ¡i Chá»¯ X](aero_llm_08_codechallenge_cluster_the_x_terms.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_codechallenge_cluster_the_x_terms.md) |
| [PhÃ¢n RÃ£ Token, NhÃºng VÃ  PhÃ¢n Cá»¥m Biá»ƒu TÆ°á»£ng Emojis Báº±ng Äá»“ Thá»‹ Máº­t Äá»™](aero_llm_09_codechallenge_tokenize_embed_and_cluster_happy_emojis.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_tokenize_embed_and_cluster_happy_emojis.md) |
| [PhÃ¢n TÃ­ch RSA (Representational Similarity Analysis) Giá»¯a CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_10_rsa_representational_similarity_analysis_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_rsa_representational_similarity_analysis_.md) |
| [PhÃ¢n TÃ­ch Äá»™ Lá»‡ch RSA (Part 1): So SÃ¡nh Sá»± Báº¥t Äá»“ng Giá»¯a KhÃ´ng Gian GloVe 50D vÃ  300D](aero_llm_11_codechallenge_compare_embeddings_with_rsa_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_compare_embeddings_with_rsa_part_1_.md) |
| [PhÃ¢n TÃ­ch Äá»™ Lá»‡ch RSA (Part 2): Äá»‘i Chiáº¿u TÆ°Æ¡ng Quan Pearson Cho Khoáº£ng CÃ¡ch Cosine](aero_llm_12_codechallenge_compare_embeddings_with_rsa_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_compare_embeddings_with_rsa_part_2_.md) |
| [So SÃ¡nh KhÃ´ng Gian NhÃºng: Word2Vec VÃ  GPT-2 Qua PhÃ¢n TÃ­ch RSA](aero_llm_13_codechallenge_word2vec_vs_gpt2.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_codechallenge_word2vec_vs_gpt2.md) |
| [Bá»‘ Cá»¥c Äá»“ Thá»‹ Máº¡ng (Network Graph) ThÃ´ng Qua Ma Tráº­n Cosine Similarity](aero_llm_14_codechallenge_graph_representation_of_cosine_similarities.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_graph_representation_of_cosine_similarities.md) |
| ğŸ“Œ **[Sá»‘ Há»c Tuyáº¿n TÃ­nh vÃ  RÃºt TrÃ­ch TÆ°Æ¡ng Äá»“ng Giá»¯a CÃ¡c Tá»« NhÃºng (Word Embeddings Analogies)](aero_llm_15_embeddings_arithmetic_and_analogies.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_15_embeddings_arithmetic_and_analogies.md) |
| [Vá»¡ Má»™ng Vá» Sá»‘ Há»c Vector TÆ°Æ¡ng ÄÆ°Æ¡ng (Soft-Coded Analogies) TrÃªn Word2Vec](aero_llm_16_codechallenge_soft_coded_analogies_in_word2vec.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_soft_coded_analogies_in_word2vec.md) |
| [Thiáº¿t Láº­p VÃ  Diá»…n Giáº£i Trá»¥c Ngá»¯ NghÄ©a Tuyáº¿n TÃ­nh (Linear Semantic Axes)](aero_llm_17_creating_and_interpreting_linear_semantic_axes.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_creating_and_interpreting_linear_semantic_axes.md) |
| [Khai ThÃ¡c Thuáº­t ToÃ¡n k-NN Cho TÃ¬m Kiáº¿m Tá»« Äá»“ng NghÄ©a TrÃªn BERT](aero_llm_18_knn_for_synonym_searching_in_bert.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_knn_for_synonym_searching_in_bert.md) |
| [Cáº¡nh Tranh TÃ¬m Tá»« Äá»“ng NghÄ©a BERT vs GPT: CÆ¡ Cháº¿ Tokenization Äa KÃ½ Tá»±](aero_llm_19_codechallenge_bert_v_gpt_knn_kompetition.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_codechallenge_bert_v_gpt_knn_kompetition.md) |
| [Sá»± Dá»‹ch Chuyá»ƒn VÃ  Äá»“ng Tá»“n Biá»ƒu Diá»…n Giá»¯a CÃ¡c KhÃ´ng Gian NhÃºng](aero_llm_20_research_on_translating_embeddings_spaces.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_research_on_translating_embeddings_spaces.md) |
| [PhÃ¢n TÃ­ch ChÃ¹m Quang Phá»• Suy Biáº¿n (Singular Value Spectrum) Cá»§a KhÃ´ng Gian NhÃºng](aero_llm_21_singular_value_spectrum_of_embeddings_submatrices.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_singular_value_spectrum_of_embeddings_submatrices.md) |
| [Ãnh Xáº¡ SVD CÃ¡c Dáº£i Äiá»ƒm NhÃºng CÃ³ Quan Há»‡ ChÃ©o](aero_llm_22_codechallenge_svd_projections_of_related_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_codechallenge_svd_projections_of_related_embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

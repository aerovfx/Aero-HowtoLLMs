
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [16 Interfering with attention](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ thÃ¡ch Láº­p trÃ¬nh: VÃ¡ lá»—i Head vÃ  Token trong tÃ¡c vá»¥ IOI (Head and Token Patching in IOI)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y trÃ¬nh bÃ y káº¿t quáº£ cá»§a má»™t thá»­ thÃ¡ch láº­p trÃ¬nh nÃ¢ng cao nháº±m cÃ´ láº­p cÃ¡c thÃ nh pháº§n tÃ­nh toÃ¡n chá»‹u trÃ¡ch nhiá»‡m cho tÃ¡c vá»¥ Nháº­n dáº¡ng TÃ¢n ngá»¯ GiÃ¡n tiáº¿p (IOI). Báº±ng cÃ¡ch thá»±c hiá»‡n vÃ¡ lá»—i (patching) nháº¯m má»¥c tiÃªu vÃ o duy nháº¥t má»™t Attention Head vÃ  má»™t Token cuá»‘i cÃ¹ng trong chuá»—i, nghiÃªn cá»©u phÃ¢n tÃ­ch sá»± thay Ä‘á»•i cá»§a logit difference (IOI score). Káº¿t quáº£ cho tháº¥y cÃ¡c can thiá»‡p á»Ÿ má»©c Ä‘á»™ nÃ y táº¡o ra hiá»‡u á»©ng cá»±c ká»³ tinh vi, cá»§ng cá»‘ lÃ½ thuyáº¿t vá» cáº¥u trÃºc cá»™ng dá»“n cá»§a residual stream vÃ  vai trÃ² "tinh chá»‰nh" cá»§a tiá»ƒu khá»‘i Attention. BÃ¡o cÃ¡o cÅ©ng tháº£o luáº­n vá» mÃ´ táº£ toÃ¡n há»c cá»§a Transformer Block Ä‘á»ƒ giáº£i thÃ­ch sá»± khÃ¡c biá»‡t giá»¯a cÃ¡c cáº¥p Ä‘á»™ can thiá»‡p.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Tiáº¿p ná»‘i cÃ¡c thá»±c nghiá»‡m vÃ¡ lá»—i Hidden States, thá»­ thÃ¡ch nÃ y yÃªu cáº§u má»©c Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n: thay vÃ¬ ghi Ä‘Ã¨ toÃ n bá»™ thÃ´ng tin táº¡i má»™t táº§ng, chÃºng ta chá»‰ thay Ä‘á»•i má»™t "máº£nh" thÃ´ng tin nhá» nháº¥t cÃ³ thá»ƒ â€“ má»™t Head cá»¥ thá»ƒ táº¡i Token dá»± Ä‘oÃ¡n. Má»¥c tiÃªu lÃ  xÃ¡c Ä‘á»‹nh xem liá»‡u cÃ³ nhá»¯ng "Name Mover Heads" (cÃ¡c Ä‘áº§u dá»‹ch chuyá»ƒn tÃªn) cá»¥ thá»ƒ nÃ o Ä‘Ã³ng vai trÃ² then chá»‘t trong viá»‡c giáº£i quyáº¿t mÃ¢u thuáº«n giá»¯a Subject vÃ  Indirect Object hay khÃ´ng.

---

## 2. PhÆ°Æ¡ng PhÃ¡p Thá»±c Nghiá»‡m (Methodology)

### 2.1. Can thiá»‡p Vi pháº«u (Precision Patching)
Sá»­ dá»¥ng mÃ´ hÃ¬nh GPT-2 Large vá»›i vÃ²ng láº·p kÃ©p qua 36 táº§ng vÃ  20 heads:
- **Äá»‘i tÆ°á»£ng can thiá»‡p:** Chá»‰ duy nháº¥t token cuá»‘i cÃ¹ng (token dá»± Ä‘oÃ¡n) cá»§a chuá»—i recipient Ä‘Æ°á»£c thay tháº¿ báº±ng hoáº¡t hÃ³a tá»« chuá»—i donor.
- **Vá»‹ trÃ­:** Lá»›p `c_proj` (pre-hook) Ä‘á»ƒ Ä‘áº£m báº£o can thiá»‡p xáº£y ra trÆ°á»›c khi cÃ¡c heads bá»‹ trá»™n láº«n.
- **Chá»‰ sá»‘ Ä‘o lÆ°á»ng:** IOI Score Ä‘Æ°á»£c tÃ­nh báº±ng sai lá»‡ch logit giá»¯a Ä‘Ã¡p Ã¡n Ä‘Ãºng (Sally) vÃ  Ä‘Ã¡p Ã¡n sai (Sam).

### 2.2. Ká»¹ thuáº­t Reshaping
Dá»¯ liá»‡u táº¡i `c_proj` cÃ³ kÃ­ch thÆ°á»›c `[batch, tokens, heads * head_dim]`. Viá»‡c `reshape` thÃ nh `[batch, tokens, heads, head_dim]` lÃ  báº¯t buá»™c Ä‘á»ƒ cÃ³ thá»ƒ chá»‰ Ä‘á»‹nh chÃ­nh xÃ¡c head cáº§n vÃ¡ mÃ  khÃ´ng lÃ m áº£nh hÆ°á»Ÿng Ä‘áº¿n cÃ¡c heads lÃ¢n cáº­n.

---

## 3. Káº¿t Quáº£ VÃ  PhÃ¢n TÃ­ch (Results & Analysis)

### 3.1. PhÃ¢n tÃ­ch Báº£n Ä‘á»“ TÃ¡c Ä‘á»™ng (Impact Mapping)
- **Quan sÃ¡t:** KhÃ¡c vá»›i viá»‡c vÃ¡ Hidden States (táº¡o ra sá»± sá»¥t giáº£m IOI score khá»•ng lá»“), viá»‡c vÃ¡ tá»«ng head Ä‘Æ¡n láº» chá»‰ táº¡o ra nhá»¯ng thay Ä‘á»•i ráº¥t nhá», thÆ°á»ng khÃ´ng thá»ƒ phÃ¡t hiá»‡n Ä‘Æ°á»£c báº±ng máº¯t thÆ°á»ng trÃªn biá»ƒu Ä‘á»“ quÃ©t (scatter plot).
- **Xu hÆ°á»›ng:** Hiá»‡u á»©ng thÆ°á»ng rÃµ rÃ ng hÆ¡n á»Ÿ cÃ¡c táº§ng cuá»‘i cá»§a mÃ´ hÃ¬nh, nÆ¡i cÃ¡c thÃ´ng tin ngá»¯ cáº£nh Ä‘Ã£ Ä‘Æ°á»£c tinh luyá»‡n Ä‘á»ƒ chuáº©n bá»‹ cho viá»‡c giáº£i mÃ£ token. Má»™t sá»‘ head lÃ m giáº£m nháº¹ IOI score, trong khi sá»‘ khÃ¡c láº¡i lÃ m tÄƒng â€“ cho tháº¥y sá»± phÃ¢n hÃ³a chá»©c nÄƒng cá»§a cÃ¡c Attention Heads.

### 3.2. Giáº£i thÃ­ch ToÃ¡n há»c vá» sá»± khÃ¡c biá»‡t
Äáº§u ra cá»§a má»™t Transformer Block ($x_{out}$) Ä‘Æ°á»£c mÃ´ táº£ bá»Ÿi phÆ°Æ¡ng trÃ¬nh:
$$x_{out} = x_{in} + \Delta Attention(LN(x_{in})) + \Delta MLP(LN(x_{in} + \Delta Attention))$$
Trong Ä‘Ã³:
- $\Delta Attention$ lÃ  tá»•ng Ä‘Ã³ng gÃ³p cá»§a táº¥t cáº£ cÃ¡c heads.
- Can thiá»‡p cá»§a chÃºng ta chá»‰ nháº¯m vÃ o $1/N_{heads}$ cá»§a thÃ nh pháº§n $\Delta Attention$ táº¡i duy nháº¥t má»™t vá»‹ trÃ­ token.
- Do Ä‘Ã³, pháº§n lá»›n thÃ´ng tin trong $x_{out}$ váº«n Ä‘áº¿n tá»« $x_{in}$ (residual stream) vÃ  Ä‘áº§u ra cá»§a MLP, giáº£i thÃ­ch táº¡i sao hiá»‡u á»©ng láº¡i vÃ´ cÃ¹ng tinh vi.

---

## 4. Tháº£o Luáº­n: Sá»± PhÃ¡t triá»ƒn cá»§a Diá»…n giáº£i há»c CÆ¡ há»c
Káº¿t quáº£ nÃ y minh chá»©ng cho má»™t quy luáº­t trong khoa há»c: khi kiáº¿n thá»©c tÄƒng lÃªn, cÃ¡c thá»±c nghiá»‡m sáº½ chuyá»ƒn tá»« "bÃºa táº¡" (sledgehammer) sang "vi pháº«u" (å¾®æ‰‹æœ¯). 
- Viá»‡c má»™t can thiá»‡p nhá» Ä‘áº¿n má»©c gáº§n nhÆ° vÃ´ hÃ¬nh váº«n táº¡o ra sá»± thay Ä‘á»•i vá» sá»‘ liá»‡u logit lÃ  má»™t minh chá»©ng cho tÃ­nh nháº¡y cáº£m vÃ  Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh Transformer.
- Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u á»©ng lá»›n nhÆ° vÃ¡ Hidden States, chÃºng ta cáº§n xÃ¡c Ä‘á»‹nh vÃ  vÃ¡ Ä‘á»“ng thá»i má»™t "nhÃ³m" cÃ¡c heads cÃ³ chá»©c nÄƒng tÆ°Æ¡ng Ä‘á»“ng (circuit analysis).

---

## 5. Káº¿t Luáº­n
Thá»­ thÃ¡ch nÃ y kháº³ng Ä‘á»‹nh ráº±ng Attention Heads khÃ´ng hoáº¡t Ä‘á»™ng Ä‘á»™c láº­p Ä‘á»ƒ táº¡o ra Ã½ nghÄ©a, mÃ  chÃºng Ä‘Ã³ng gÃ³p nhá»¯ng "tinh chá»‰nh" (tweaks) nhá» vÃ o má»™t dÃ²ng cháº£y thÃ´ng tin khá»•ng lá»“. Viá»‡c tÃ¬m kiáº¿m "Name Mover Heads" Ä‘Ã²i há»i sá»± káº¿t há»£p giá»¯a phÃ¢n tÃ­ch Ä‘á»‹nh lÆ°á»£ng chÃ­nh xÃ¡c vÃ  má»™t khung lÃ½ thuyáº¿t vá»¯ng cháº¯c vá» dÃ²ng cháº£y thÃ´ng tin trong residual stream.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. Thá»­ thÃ¡ch Precision Head Patching trÃªn tÃ¡c vá»¥ IOI dá»±a trÃªn `aero_LLM_07_CodeChallenge Head and token patching in IOI.md`. PhÃ¢n tÃ­ch Ä‘á»‹nh lÆ°á»£ng vÃ  mÃ´ táº£ toÃ¡n há»c cá»§a Transformer Block.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Cáº¯t bá» Attention Head vÃ  Dá»± Ä‘oÃ¡n Token (Head Ablation and Token Prediction)](aero_LLM_01_Head ablation and token prediction.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Head ablation and token prediction.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Dá»± Ä‘oÃ¡n Token sau khi Cáº¯t bá» Head (Pháº§n 1)](aero_LLM_02_CodeChallenge Token prediction after head ablations (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_CodeChallenge Token prediction after head ablations (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Dá»± Ä‘oÃ¡n Token sau khi Cáº¯t bá» Head (Pháº§n 2)](aero_LLM_03_CodeChallenge Token prediction after head ablations (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Token prediction after head ablations (part 2).md) |
| [TÃ¡c Ä‘á»™ng cá»§a viá»‡c "Táº¯t tiáº¿ng" Head lÃªn Äá»™ tÆ°Æ¡ng Ä‘á»“ng Cosine (Impact of Head-Silencing on Cosine Similarity)](aero_LLM_04_Impact of head-silencing on cosine similarity.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_Impact of head-silencing on cosine similarity.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: GPT-2 cÃ³ thá»±c sá»± thÃ­ch Pizza Dá»©a? (Má»™t nghiÃªn cá»©u vá» can thiá»‡p Attention chÃ­nh xÃ¡c)](aero_LLM_05_CodeChallenge Does GPT2 like pineapple pizza.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_CodeChallenge Does GPT2 like pineapple pizza.md) |
| [VÃ¡ lá»—i Attention Head trong tÃ¡c vá»¥ Nháº­n dáº¡ng TÃ¢n ngá»¯ GiÃ¡n tiáº¿p (Attention Head Patching in IOI)](aero_LLM_06_Attention head patching in IOI.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_Attention head patching in IOI.md) |
| ğŸ“Œ **[Thá»­ thÃ¡ch Láº­p trÃ¬nh: VÃ¡ lá»—i Head vÃ  Token trong tÃ¡c vá»¥ IOI (Head and Token Patching in IOI)](aero_LLM_07_CodeChallenge Head and token patching in IOI.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_07_CodeChallenge Head and token patching in IOI.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [16 Interfering with attention](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# VÃ¡ lá»—i Attention Head trong tÃ¡c vá»¥ Nháº­n dáº¡ng TÃ¢n ngá»¯ GiÃ¡n tiáº¿p (Attention Head Patching in IOI)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y má»Ÿ rá»™ng ká»¹ thuáº­t vÃ¡ hoáº¡t hÃ³a (Activation Patching) tá»« má»©c Ä‘á»™ Hidden States (toÃ n bá»™ transformer block) xuá»‘ng má»©c Ä‘á»™ tinh vi hÆ¡n lÃ  cÃ¡c cÃ¡ thá»ƒ Attention Head. Sá»­ dá»¥ng tÃ¡c vá»¥ Nháº­n dáº¡ng TÃ¢n ngá»¯ GiÃ¡n tiáº¿p (Indirect Object Identification - IOI), nghiÃªn cá»©u thá»±c hiá»‡n viá»‡c "cáº¥y ghÃ©p" (transplanting) hoáº¡t hÃ³a cá»§a head tá»« má»™t chuá»—i donor sang má»™t chuá»—i recipient. Káº¿t quáº£ thá»±c nghiá»‡m cho tháº¥y má»™t sá»± khÃ¡c biá»‡t Ä‘Ã¡ng kinh báº¡t: trong khi viá»‡c vÃ¡ Hidden States táº¡o ra sá»± thay Ä‘á»•i hÃ nh vi triá»‡t Ä‘á»ƒ, viá»‡c vÃ¡ táº¥t cáº£ Attention Heads chá»‰ táº¡o ra nhá»¯ng tÃ¡c Ä‘á»™ng má» nháº¡t. PhÃ¡t hiá»‡n nÃ y dáº«n Ä‘áº¿n má»™t tháº£o luáº­n sÃ¢u sáº¯c vá» vai trÃ² cá»§a tiá»ƒu khá»‘i Attention nhÆ° má»™t cÆ¡ cháº¿ Ä‘iá»u chá»‰nh (tweaking) thay vÃ¬ lÃ  nguá»“n thay Ä‘á»•i chÃ­nh cho cÃ¡c vector biá»ƒu diá»…n.

---

## 1. Má»Ÿ Äáº§u (Introduction)
TÃ¡c vá»¥ IOI (vÃ­ dá»¥: "Sam vÃ  Sally Ä‘i cÃ´ng viÃªn, Sam táº·ng quÃ  cho...") yÃªu cáº§u mÃ´ hÃ¬nh pháº£i xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c Ä‘á»‘i tÆ°á»£ng khÃ´ng láº·p láº¡i (Sally). Trong cÃ¡c nghiÃªn cá»©u trÆ°á»›c, viá»‡c vÃ¡ (patching) toÃ n bá»™ Hidden States Ä‘Ã£ chá»©ng minh Ä‘Æ°á»£c sá»± tá»“n táº¡i cá»§a "cÃ¡c táº§ng quan trá»ng" nÆ¡i tri thá»©c ngá»¯ phÃ¡p Ä‘Æ°á»£c lÆ°u trá»¯. BÃ¡o cÃ¡o nÃ y Ä‘i sÃ¢u vÃ o viá»‡c pháº«u thuáº­t cÃ¡c Attention Heads Ä‘á»ƒ xem liá»‡u chÃºng ta cÃ³ thá»ƒ cÃ´ láº­p hÃ nh vi nÃ y á»Ÿ má»©c Ä‘á»™ head hay khÃ´ng.

---

## 2. PhÆ°Æ¡ng PhÃ¡p Thá»±c Nghiá»‡m (Methodology)

### 2.1. PhÃ¢n biá»‡t Hidden States Patching vÃ  Head Patching
- **Hidden States (Tráº¡ng thÃ¡i áº©n):** LÃ  Ä‘áº§u ra cuá»‘i cÃ¹ng cá»§a Transformer Block, trá»±c tiáº¿p hÃ¬nh thÃ nh dá»± Ä‘oÃ¡n token tiáº¿p theo. Patching á»Ÿ Ä‘Ã¢y lÃ  thay Ä‘á»•i toÃ n bá»™ "niá»m tin" cá»§a mÃ´ hÃ¬nh táº¡i táº§ng Ä‘Ã³.
- **Attention Heads:** Chá»‰ tÃ­nh toÃ¡n cÃ¡c Ä‘iá»u chá»‰nh nhá» (adjustments) dá»±a trÃªn ngá»¯ cáº£nh Ä‘á»ƒ cá»™ng vÃ o residual stream. Patching á»Ÿ Ä‘Ã¢y chá»‰ lÃ  thay Ä‘á»•i "cÃ¡ch mÃ´ hÃ¬nh nhÃ¬n vÃ o ngá»¯ cáº£nh".

### 2.2. Quy trÃ¬nh Ká»¹ thuáº­t
- **BÆ°á»›c 1:** Cháº¡y Forward Pass trÃªn chuá»—i A (Donor) Ä‘á»ƒ láº¥y vÃ  lÆ°u trá»¯ hoáº¡t hÃ³a cá»§a táº¥t cáº£ cÃ¡c Heads thÃ´ng qua Hook.
- **BÆ°á»›c 2:** Cháº¡y Forward Pass trÃªn chuá»—i B (Recipient), Ä‘á»“ng thá»i dÃ¹ng Hook Ä‘á»ƒ ghi Ä‘Ã¨ hoáº¡t hÃ³a cá»§a cÃ¡c Heads báº±ng dá»¯ liá»‡u tá»« chuá»—i A.
- **Cáº¥u trÃºc Hook:** Sá»­ dá»¥ng `Forward Pre-hook` trÃªn lá»›p `c_proj` Ä‘á»ƒ can thiá»‡p vÃ o cÃ¡c Heads trÆ°á»›c khi chÃºng bá»‹ trá»™n láº«n bá»Ÿi ma tráº­n $W_O$.

---

## 3. Káº¿t Quáº£ VÃ  PhÃ¢n TÃ­ch (Results & Analysis)

### 3.1. Sá»± chÃªnh lá»‡ch vá» hiá»‡u quáº£ (Discrepancy)
- **Quan sÃ¡t:** Ngay cáº£ khi vÃ¡ toÃ n bá»™ 12 heads trong má»™t táº§ng cá»§a GPT-2 Small, chá»‰ sá»‘ IOI (xÃ¡c suáº¥t cá»§a Sally so vá»›i Sam) chá»‰ dá»‹ch chuyá»ƒn nháº¹. MÃ´ hÃ¬nh khÃ´ng bao giá» hoÃ n toÃ n bá»‹ Ä‘Ã¡nh lá»«a Ä‘á»ƒ chá»n Sam (tÃ¢n ngá»¯ sai) nhÆ° khi vÃ¡ Hidden States.
- **TÃ­nh nháº¥t quÃ¡n:** Hiá»‡n tÆ°á»£ng nÃ y láº·p láº¡i trÃªn cáº£ GPT-2 Medium, cho tháº¥y Ä‘Ã¢y lÃ  Ä‘áº·c tÃ­nh kiáº¿n trÃºc chá»© khÃ´ng pháº£i do quy mÃ´ tham sá»‘.

### 3.2. Giáº£i thÃ­ch cÆ¡ há»c
Táº¡i sao viá»‡c vÃ¡ Heads láº¡i yáº¿u hÆ¡n nhiá»u so vá»›i Hidden States? 
- **LÃ½ do:** Hidden States bao gá»“m cáº£ Ä‘áº§u ra cá»§a Attention vÃ  máº¡ng MLP. Quan quan trá»ng hÆ¡n, Attention chá»‰ Ä‘Ã³ng gÃ³p má»™t pháº§n nhá» (Residual) vÃ o vector biá»ƒu diá»…n tá»•ng thá»ƒ. Khi chÃºng ta chá»‰ vÃ¡ Head, chÃºng ta chá»‰ Ä‘ang thay Ä‘á»•i "pháº§n bá»• sung" ngá»¯ cáº£nh, trong khi "ná»™i dung gá»‘c" (tá»« cÃ¡c táº§ng trÆ°á»›c Ä‘Ã³ trong residual stream) váº«n Ä‘Æ°á»£c báº£o toÃ n máº¡nh máº½.

---

## 4. Tháº£o Luáº­n: Deterministic Logic trong Coding
NghiÃªn cá»©u nháº¥n máº¡nh táº§m quan trá»ng cá»§a cÃ¡c sanity checks:
- Viá»‡c so sÃ¡nh trá»±c tiáº¿p cÃ¡c tensor sau khi vÃ¡ (`head_xb == head_xa`) lÃ  bÆ°á»›c báº¯t buá»™c Ä‘á»ƒ xÃ¡c nháº­n Hook hoáº¡t Ä‘á»™ng Ä‘Ãºng. 
- Sá»± phá»©c táº¡p cá»§a viá»‡c `reshape` (tá»« embeddings sang heads) lÃ  cáº§n thiáº¿t Ä‘á»ƒ chuáº©n bá»‹ cho cÃ¡c can thiá»‡p nháº¯m má»¥c tiÃªu vÃ o duy nháº¥t má»™t head trong cÃ¡c thá»­ thÃ¡ch tiáº¿p theo.

---

## 5. Káº¿t Luáº­n
Viá»‡c vÃ¡ Attention Heads tiáº¿t lá»™ ráº±ng cÃ¡c heads hoáº¡t Ä‘á»™ng nhÆ° nhá»¯ng bá»™ tinh chá»‰nh tinh vi. Máº·c dÃ¹ chÃºng mang thÃ´ng tin ngá»¯ cáº£nh quan trá»ng, nhÆ°ng sá»©c máº¡nh cá»§a chÃºng bá»‹ giá»›i háº¡n bá»Ÿi cáº¥u trÃºc residual stream. Nhá»¯ng tháº£o luáº­n tiáº¿p theo sáº½ nháº¯m vÃ o viá»‡c mÃ´ táº£ toÃ¡n há»c cho sá»± khÃ¡c biá»‡t nÃ y vÃ  cÃ¡ch cÃ´ láº­p cÃ¡c "Heads chuyÃªn biá»‡t" cho tÃ¡c vá»¥ IOI.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. ThÃ­ nghiá»‡m Head Patching trÃªn tÃ¡c vá»¥ IOI dá»±a trÃªn `aero_LLM_06_Attention head patching in IOI.md`. So sÃ¡nh sá»± khÃ¡c biá»‡t giá»¯a can thiá»‡p Hidden States vÃ  Attention Subblock.
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

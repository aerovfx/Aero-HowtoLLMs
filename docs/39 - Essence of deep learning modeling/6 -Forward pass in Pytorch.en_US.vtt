WEBVTT

00:02.240 --> 00:10.160
In this lecture, I will provide an overview of the forward pass of artificial neural networks and how

00:10.160 --> 00:15.520
those operations are stored and implemented in the PyTorch library.

00:16.400 --> 00:22.120
Not to be honest, if this is the first time you are ever seeing this material, then you will probably

00:22.160 --> 00:25.960
find the main part of this course to be a little challenging.

00:26.400 --> 00:31.200
So I hope that you already have some exposure to deep learning in PyTorch.

00:31.640 --> 00:37.480
And this lecture is some review and maybe also some additional novel explorations.

00:38.720 --> 00:45.320
This slide is a reminder of the fundamental mechanism of artificial neural networks.

00:45.640 --> 00:50.040
We have a unit here which unfortunately is called a neuron.

00:50.400 --> 00:57.920
It's unfortunate naming because this has literally nothing whatsoever to do with actual biological neurons,

00:58.080 --> 01:01.840
not in their form or their function or their calculations.

01:02.200 --> 01:03.360
But that's how it goes.

01:04.040 --> 01:10.640
Anyway, the x's here are inputs into this unit, and the W's are the weights.

01:10.920 --> 01:19.480
Now this one over here x0 is colored differently because this is called the bias term.

01:19.840 --> 01:23.800
It's an offset that doesn't actually come from anything in the data.

01:24.160 --> 01:27.760
It's just an extra number that gets added to this unit.

01:27.920 --> 01:34.640
That allows the unit to shift its distribution of activations, which turns out to be really useful

01:34.640 --> 01:36.360
for the model's performance.

01:36.960 --> 01:41.720
It's the same thing as an intercept term in a regression.

01:42.800 --> 01:50.800
Anyway, this unit here calculates a weighted sum of these inputs, which is then passed through an

01:50.800 --> 01:58.960
activation function, typically a non-linear activation function like ReLU or Jlu or sigmoid, depending

01:58.960 --> 02:03.600
on the specific network architecture and the goal of this unit.

02:04.160 --> 02:06.720
So this is one unit.

02:06.880 --> 02:13.960
And then in practice, you stitch lots of these units together in different layers of the model.

02:14.520 --> 02:20.640
All the way over here to the left we have the inputs that actually are coming from the data measured

02:20.640 --> 02:22.280
from the outside world.

02:22.640 --> 02:25.400
Maybe these are pixel values from an image.

02:25.560 --> 02:33.080
Maybe they are numerical values corresponding to medical information, or maybe they are numbered tokens

02:33.080 --> 02:35.160
that come from written language.

02:35.720 --> 02:43.120
And then all the way at this side of the model you have some output, which is a prediction that the

02:43.120 --> 02:45.160
model makes about the world.

02:45.720 --> 02:52.760
Again, maybe that's a simple binary prediction, like whether a patient has a disease or not, or maybe

02:52.760 --> 02:56.810
it's a prediction about which word should be printed Did next.

02:57.450 --> 03:03.490
And then all of these layers in the middle are called hidden layers, because they are not directly

03:03.490 --> 03:06.170
connected to data from the outside world.

03:06.450 --> 03:07.650
But here's the thing.

03:07.890 --> 03:12.690
All of these units, I engage in identical calculations.

03:12.690 --> 03:19.850
They're all doing exactly the same thing, which is linear weighted sum, and that is implemented as

03:19.850 --> 03:26.010
the dot product between the inputs, so the x's and the corresponding weights.

03:26.010 --> 03:30.730
And then that is passed through a simple nonlinear activation function.

03:31.330 --> 03:38.330
Of course the values of the inputs and the values of the weights are different for each unit, but the

03:38.330 --> 03:42.730
elementary mathematical operation is exactly the same.

03:43.410 --> 03:46.930
Now notice the direction of the arrows on this graph.

03:46.970 --> 03:54.930
They all go from the input which corresponds to real world data, images or text, and all the outputs

03:55.090 --> 03:57.530
flow to the right towards the output.

03:57.890 --> 04:03.770
So that is called the forward pass where information flows from input.

04:04.330 --> 04:08.690
So initial input to the final output of the deep learning model.

04:09.730 --> 04:15.090
It's also sometimes called the feed forward pass or the forward sweep or something similar.

04:15.090 --> 04:20.570
There's a couple of different terms that people use now with modern architectures.

04:20.570 --> 04:25.090
It's not necessarily the case that all the information only goes forward.

04:25.250 --> 04:31.210
Sometimes there are recurrent connections where a little bit of residual information is fed back from

04:31.210 --> 04:36.210
later layers to earlier layers, but that's more the occasional exception.

04:36.210 --> 04:43.810
Mostly information flows forwards from the initial inputs, from the data, all the way to the output

04:44.410 --> 04:45.970
during the forward pass.

04:46.610 --> 04:51.090
Okay, then we have backpropagation, often abbreviated backprop.

04:51.490 --> 04:58.850
Backprop only occurs during model training, not when you're actually using the model for applications.

04:59.250 --> 05:06.210
So backprop involves adjusting each of these weights, each of these arrows according to the model's

05:06.210 --> 05:07.050
performance.

05:07.130 --> 05:14.490
So if the model is not performing very well, then it will adjust the values of these weights to increase

05:14.490 --> 05:16.770
the performance on the next batch.

05:17.810 --> 05:21.890
But anyway, I'm just focusing on the forward pass in this video.

05:23.250 --> 05:27.890
Now for language models the architecture is a little bit more complicated.

05:28.370 --> 05:34.010
So we have at the inputs a numerical representations of characters and words.

05:34.170 --> 05:35.570
Those are called tokens.

05:36.050 --> 05:38.850
So those are the inputs that are coming in.

05:39.250 --> 05:41.930
Then we have lots of hidden layers.

05:42.170 --> 05:46.530
Now in large language models these have very specific architectures.

05:46.530 --> 05:48.730
They're called attention heads and so on.

05:48.770 --> 05:54.850
But in a conceptual sense they are basically just hidden layers because they do not directly receive

05:54.850 --> 06:01.090
input from the outside world, nor are they directly connected to the outputs of the model.

06:02.250 --> 06:10.250
And then the outputs of a language model correspond to, or at least can be transformed into the probabilities

06:10.250 --> 06:17.770
of selecting tokens, which are then translated into pieces of text like characters and words.

06:18.490 --> 06:19.650
Now the thing is.

06:22.490 --> 06:26.730
That we like to think about these units or artificial neurons.

06:27.130 --> 06:31.330
We think about them as being the atoms of computation in the models.

06:31.330 --> 06:39.250
But in practice, in the actual Python implementation, you don't actually construct these units directly.

06:39.370 --> 06:41.690
They are implicitly created.

06:42.010 --> 06:49.930
Instead, we create the matrices that correspond to all of these weights or all of these arrows here.

06:50.370 --> 06:57.980
And that is implemented through a PyTorch class called linear with a capital L, so nn dot linear.

06:59.180 --> 07:06.460
So when you see non-linear in the code, that's actually creating these matrices that correspond to

07:06.500 --> 07:10.740
the weights, which are all of the arrows that I'm drawing in this diagram.

07:12.220 --> 07:12.620
Okay.

07:12.780 --> 07:15.300
In a moment I will switch to Python.

07:15.300 --> 07:17.900
I will go through a demo of this in code.

07:18.380 --> 07:24.140
But before switching to code, I want to give you a heads up of what you're going to see and what to

07:24.180 --> 07:25.140
look out for.

07:25.580 --> 07:33.020
So we are going to create a class in Python that will instantiate a deep learning model.

07:33.580 --> 07:38.100
Now, just to give you a heads up in advance, this is not an actual language model.

07:38.420 --> 07:45.180
I'm using words that correspond to language model components, but this is a super simple, watered

07:45.180 --> 07:47.620
down collection of random numbers.

07:47.980 --> 07:55.460
The point here is to focus on the code and the mechanics and the operations, not on the actual accuracy

07:55.460 --> 07:58.700
of this model as a top of the line transformer.

07:59.820 --> 08:05.300
So I'll explain each of these lines in more detail in the code, but you can see that we are creating

08:05.300 --> 08:09.660
an input layer, some hidden layers and an output layer.

08:09.780 --> 08:14.740
So that's done in the init section which stands for initialization.

08:15.180 --> 08:23.340
Then down here we have the forward pass which is implementing the weighted linear sum plus non-linearity

08:23.340 --> 08:25.380
that I described a few slides ago.

08:25.820 --> 08:32.100
Now you don't actually literally see any calculations being done in here, but I'm going to show you

08:32.100 --> 08:38.300
how we can reproduce what for example self embedding is actually implementing.

08:39.660 --> 08:48.060
Then we are going to create an instance of that model, inspect its properties and see how to access

08:48.140 --> 08:51.340
some of the parts, like for example, the weights matrices.

08:52.660 --> 09:00.100
The next thing we will do is manually calculate a forward pass using dot products as implemented through

09:00.140 --> 09:08.300
matrix multiplication, and we will see that our manual calculations perfectly reconstruct what PyTorch

09:08.340 --> 09:12.300
is doing using its NN linear class.

09:13.460 --> 09:20.700
Finally, I will illustrate to you the basic concept of using a language model for next token prediction.

09:21.180 --> 09:28.820
What we're going to do is start with some text, translate that text into numbers, push all of those

09:28.820 --> 09:34.860
numbers through the model in a forward pass, and then make predictions for subsequent tokens.

09:35.780 --> 09:40.460
This is literally how actual language models like ChatGPT work.

09:40.780 --> 09:49.820
But of course this is a tiny matrix with random numbers and no training, whereas ChatGPT is absolutely

09:49.820 --> 09:54.380
mammoth and has undergone a mind boggling amount of training.

09:55.020 --> 10:00.020
But understanding all of that is the goal for the rest of the course for now.

10:00.420 --> 10:04.700
Let's switch to Python and go through this code in more detail.

10:06.780 --> 10:14.660
Here I import numpy and matplotlib, as well as PyTorch and some submodules in PyTorch.

10:15.340 --> 10:21.980
Here is where we will create our own class for a simple deep learning model.

10:22.500 --> 10:29.580
Now the thing is, in PyTorch we need to keep track of a lot of little variables, components of the

10:29.580 --> 10:32.900
computational graphs, gradients, and so on.

10:32.900 --> 10:41.260
So there's a lot of methods and properties and attributes that any deep learning model in PyTorch needs

10:41.260 --> 10:47.470
to have that we do not want to have to rewrite every single time we create a new model.

10:47.830 --> 10:55.310
So what we do is inherit a lot of those methods and attributes from MN dot module.

10:55.310 --> 10:57.070
And that's basically what's going on in here.

10:57.070 --> 11:05.510
So this allows us to absorb some important PyTorch features into this class that we're creating without

11:05.510 --> 11:09.350
having to explicitly write them down in this class.

11:09.870 --> 11:10.110
Okay.

11:10.150 --> 11:14.870
And so in this actual class what I define are just two methods.

11:14.870 --> 11:19.470
So init for initialization and forward for forward pass.

11:20.030 --> 11:23.830
In the initialization I create an embedding layer.

11:23.870 --> 11:25.630
This is also a linear layer.

11:25.630 --> 11:31.310
You'll learn later in the course about the relationship between embedding and layer.

11:31.310 --> 11:35.830
But basically this is also a linear layer okay.

11:35.910 --> 11:44.310
And this is going to go from 27 to 127 corresponds to the number of letters in the English language.

11:44.310 --> 11:46.590
So 26 letters plus space.

11:47.030 --> 11:50.750
We're going to use that for pushing data through the model later.

11:50.910 --> 11:54.830
And then 100 is just kind of an arbitrary number that I chose.

11:54.830 --> 11:58.070
That's the number of units in the hidden layers.

11:58.510 --> 12:03.430
And now here I have a tension block uh, in a transformer.

12:03.430 --> 12:10.150
So there's two components to every attention layer which you'll learn about later on in the course.

12:10.150 --> 12:12.430
There's what's called the attention itself.

12:12.430 --> 12:19.990
And then there's the feed forward part, which is often abbreviated as MLP for multi-layer perceptron.

12:20.390 --> 12:20.790
Okay.

12:20.830 --> 12:29.550
So at the input we go from 27 characters, 27 letters and space to a dimensionality of 100.

12:29.590 --> 12:34.510
We stay at 100 by 100, and then we go down to 64.

12:34.670 --> 12:40.910
And then we have this output layer, which for reasons that will become clear to you later, is called

12:40.910 --> 12:43.110
an unembedding Embedding matrix.

12:43.270 --> 12:45.670
And that goes down to 27.

12:46.430 --> 12:50.710
So we start the model with 27 possible inputs.

12:50.710 --> 12:54.350
And we finish the model with 27 possible outputs.

12:54.590 --> 12:58.390
That means that we can input letters into the model.

12:58.390 --> 13:01.190
And the model is going to output letters back.

13:01.750 --> 13:02.150
Okay.

13:02.190 --> 13:03.430
So that's that.

13:03.430 --> 13:05.110
Here is the forward pass.

13:05.110 --> 13:10.030
This is what uh gets calculated when you push data through the model.

13:10.030 --> 13:14.550
So the forward pass involves taking the inputs which are x.

13:14.670 --> 13:19.110
So these are the letters converted into integers.

13:19.430 --> 13:22.950
Those get pushed through the first layer which is the embedding.

13:23.270 --> 13:28.070
Uh so and then this is calculating the weighted linear uh sum.

13:28.070 --> 13:30.790
And then here we get a non-linear activation.

13:31.110 --> 13:33.230
And then we have the attention block.

13:33.270 --> 13:38.350
It's not quite accurate to call this attention block but we're just having some fun here.

13:38.990 --> 13:42.590
Uh so then the data get pushed through this again.

13:42.630 --> 13:45.110
This is linear weighted combination in.

13:45.110 --> 13:47.670
Here are where all the weights get stored.

13:47.670 --> 13:49.670
And here's the nonlinear activation.

13:49.670 --> 13:53.430
And then again through the MLP block the embeddings.

13:53.430 --> 13:59.910
And finally we return the values of y which we are going to convert back into letters.

14:01.190 --> 14:01.550
Okay.

14:01.590 --> 14:02.870
So run that cell.

14:03.070 --> 14:08.190
And here I'm just creating an instance I call this variable LM for language model.

14:08.590 --> 14:14.070
And here we see some information about the model that I just created.

14:14.070 --> 14:16.350
So there's an embedding layer.

14:16.590 --> 14:20.550
And then there's an attention layer an MLP and an embedding.

14:20.870 --> 14:28.390
You are going to see text printed out like this many times throughout the course when we are investigating,

14:28.630 --> 14:32.790
for example, the Bert model, the GPT two model and so on.

14:33.230 --> 14:42.470
Now we can access all of the numbers is stored inside the model by using periods to access these different

14:42.470 --> 14:43.390
properties.

14:43.390 --> 14:47.910
So LM dot MLP that's going to give us access to this.

14:47.950 --> 14:49.910
And let me already show you what this looks like.

14:50.270 --> 14:53.070
So this is what happens when you just write LM.

14:53.350 --> 14:58.510
If you write LM dot MLP you see some more information.

14:58.510 --> 15:00.030
It's a linear layer.

15:00.230 --> 15:03.830
It's 100 coming in and 64 coming out.

15:03.950 --> 15:05.110
It has a bias.

15:05.670 --> 15:07.670
So there are bias terms in here.

15:08.070 --> 15:09.870
And then we can access the weights.

15:09.870 --> 15:16.950
That is the actual matrix that connects all of the neurons in the previous layer to all the neurons

15:16.950 --> 15:17.910
in this layer.

15:18.350 --> 15:20.350
And what is the size of that.

15:20.390 --> 15:22.870
Well we can do that with dot shape.

15:23.110 --> 15:29.710
Now remember that I said up here that there are a lot of methods and properties and so on.

15:29.710 --> 15:35.110
There's just a lot of things that these classes need to have to work within PyTorch.

15:35.230 --> 15:38.400
And I'm not actually literally defining them here.

15:38.680 --> 15:42.800
Instead, I am inheriting them from nn dot module.

15:43.000 --> 15:47.760
So let's see what are all the properties that we have in this variable.

15:47.800 --> 15:52.440
LM so all sorts of stuff in here that I did not define.

15:52.640 --> 15:54.160
This one I defined right.

15:54.200 --> 15:55.880
We defined dot forward.

15:56.040 --> 15:57.320
So that is in here.

15:57.520 --> 16:05.360
And yeah there's just a lot of other things going on here like register load state dict post hook okay

16:05.400 --> 16:07.360
I did not define that in the class.

16:07.520 --> 16:11.400
We inherited that method from PyTorch.

16:11.840 --> 16:16.920
Uh, this actually is important, these uh methods to give us hooks.

16:16.920 --> 16:23.000
We're going to use that in uh, later on in the course when learning about mechanistic interpretability.

16:23.160 --> 16:29.680
And these hooks allow us to access the information, the activations that are being passed through the

16:29.680 --> 16:32.680
model during the forward pass.

16:33.880 --> 16:34.360
Okay.

16:34.480 --> 16:35.080
Um, yeah.

16:35.120 --> 16:40.560
I don't really want to get into this too much, but just to say that, you know, this is an established

16:40.760 --> 16:44.960
class that comes in PyTorch, if you're curious, you can read a little bit more about it.

16:45.360 --> 16:45.880
Okay.

16:45.920 --> 16:53.320
What I want to do now is show you that we can exactly reproduce what is happening up.

16:53.320 --> 16:58.120
Let me get rid of this up here in the model.

16:58.160 --> 16:59.760
Let me go back like this.

16:59.800 --> 17:00.040
Okay.

17:00.080 --> 17:00.760
This is good.

17:01.160 --> 17:09.760
Okay, now I stated, I claimed that when we write linear and then we pass some data into it, some

17:09.760 --> 17:17.560
inputs in variable x, I said that what we're actually doing here is calculating a linear weighted combination

17:17.800 --> 17:26.120
of these inputs x with the weights that are stored in attention and then passing that through a non-linear

17:26.120 --> 17:26.960
activation.

17:26.960 --> 17:29.920
But in here you do not see any additions.

17:29.920 --> 17:34.440
You don't see any multiplications, you don't see dot products or matrix multiplication.

17:34.680 --> 17:40.440
All you see is the name of this component of the model and parentheses.

17:40.480 --> 17:40.800
Okay.

17:40.840 --> 17:42.600
So what is this actually doing?

17:43.120 --> 17:49.520
It is literally just doing a weighted combination of the inputs where the weights come from, these

17:49.520 --> 17:53.520
parameters that we already just looked at here okay.

17:53.560 --> 18:01.000
And so to demonstrate that to you what I'm doing here is coming up with some random input, passing

18:01.000 --> 18:04.080
it through the LM attention.

18:04.600 --> 18:06.760
And then yeah.

18:06.800 --> 18:08.040
So getting that output.

18:08.240 --> 18:16.080
And then what I'm doing is taking that same input here and multiplying it by the weights matrix transpose.

18:16.080 --> 18:18.720
The transpose is just for housekeeping stuff.

18:18.720 --> 18:24.840
It's just the orientation with which PyTorch organizes the weights matrices.

18:25.680 --> 18:34.480
So matrix multiplication uh, plus the bias term, this would be the x0 term that I showed in the slides.

18:34.520 --> 18:35.000
Okay.

18:35.360 --> 18:41.200
And the idea is that we want to see that output one and output two are the same.

18:41.440 --> 18:48.200
If those are the same, then it means that this module here is literally just implementing this weighted

18:48.200 --> 18:52.480
combination plus the bias which is also still weighted combination.

18:52.720 --> 18:57.320
By the way these parentheses here are not necessary for the calculation.

18:57.640 --> 19:00.920
I think it just helps the equation look a little bit nicer.

19:01.400 --> 19:01.800
Okay.

19:02.160 --> 19:03.920
So I hope that all makes sense.

19:04.040 --> 19:07.400
What I'm doing here is just checking that their shapes are the same.

19:07.720 --> 19:13.760
If the sizes of these outputs were different, then we would absolutely know that they are calculating

19:13.760 --> 19:14.680
something different.

19:15.240 --> 19:15.480
Okay.

19:15.520 --> 19:19.560
And now what I'm doing is just plotting one as a function of the other.

19:19.680 --> 19:27.520
And you see that they all fall on the unity line, which means that our manual calculation here is literally,

19:27.520 --> 19:30.360
literally exactly the same as this here.

19:31.400 --> 19:31.880
Okay.

19:31.960 --> 19:37.530
In practice, you don't really want to just do this, partly because there's so many components to the

19:37.530 --> 19:38.050
model.

19:38.290 --> 19:45.730
So many different weights, matrices and also non-linear modules do actually implement more methods.

19:45.730 --> 19:47.490
They do more than just this.

19:47.490 --> 19:49.770
But this is the key mathematical component.

19:50.090 --> 19:50.410
Okay.

19:50.450 --> 19:53.170
So I hope that takes some of the mystery out of that.

19:53.690 --> 20:01.650
Uh, the last thing I want to do in this code demo is show you how to push model, push data through

20:01.650 --> 20:07.090
this model, and get tokens out, which we can transform back into letters.

20:07.610 --> 20:14.450
So I'm importing this string library, which is yeah, just getting out all of the letters here.

20:14.450 --> 20:15.610
So A through Z.

20:16.170 --> 20:16.410
Okay.

20:16.450 --> 20:22.010
Now here is some text that we are going to process I like corn, we all like corn right.

20:22.370 --> 20:29.330
Uh and then let's see I'm going to tokenize the text, which basically just means assign integers to

20:29.370 --> 20:31.890
each of these letters.

20:31.890 --> 20:36.570
So A is going to be zero, B is one, C is two and so on.

20:37.010 --> 20:38.930
And now so that's what I do here.

20:38.930 --> 20:42.450
So I loop through and I get all of the tokens here.

20:42.450 --> 20:48.490
So I'm just finding where in this Ascii lowercase string which is this.

20:48.530 --> 20:50.930
Where do we see the actual letter.

20:51.290 --> 20:57.610
Uh in this text here I'm making it lowercase just so we don't have to deal with uppercase and lowercase.

20:58.050 --> 21:00.850
Here's a special exception for space okay.

21:01.010 --> 21:07.730
So all I'm doing here is translating this text into a sequence of numbers.

21:07.930 --> 21:11.370
And I'll talk much more about that in the course proper.

21:11.370 --> 21:14.130
But basically models cannot process text.

21:14.170 --> 21:15.370
They process numbers.

21:15.370 --> 21:18.970
So that's why we need to transform into numbers.

21:20.050 --> 21:24.890
Once we have those tokens we can actually forward pass through the model.

21:24.890 --> 21:25.810
So very simple.

21:25.810 --> 21:30.930
We input the tokens into the language model and we get some outputs.

21:30.930 --> 21:34.610
So the outputs are of size 11 by 27.

21:34.890 --> 21:36.050
And what does that mean.

21:36.130 --> 21:42.210
Well 27 corresponds to the number of characters that we are representing here.

21:42.210 --> 21:46.410
So 26 letters plus space and 11 is one.

21:46.570 --> 21:47.690
I'm not going to count those.

21:47.730 --> 21:50.850
I'll just do length of tokens okay.

21:50.890 --> 21:54.850
11 is the number of tokens that we have here okay.

21:55.050 --> 22:02.290
So what we're going to do now is plot the output of the model for the first token.

22:02.810 --> 22:03.090
Okay.

22:03.130 --> 22:07.690
So the model did a bunch of processing did a bunch of crazy calculations.

22:07.690 --> 22:12.450
In this case, these are totally nonsensical because they're literally just random numbers.

22:13.130 --> 22:16.090
Uh, and it gave some activation output.

22:16.130 --> 22:16.530
Okay.

22:16.570 --> 22:21.570
So for this first token, it gave us 27 outputs.

22:21.810 --> 22:24.890
Uh, and uh, this is the largest one.

22:25.210 --> 22:26.530
So what are we going to do now?

22:26.530 --> 22:33.810
We are going to say that the model thinks that this token here has the highest activation.

22:34.010 --> 22:37.210
And so this should be the token that comes next.

22:37.250 --> 22:37.450
Right.

22:37.490 --> 22:38.890
That's what these models are doing.

22:38.890 --> 22:42.410
They're taking the current token and the history of tokens.

22:42.410 --> 22:45.850
And they're trying to predict what is the next token.

22:46.130 --> 22:51.450
So this model says here that based on all of everything I've learned about the world from reading the

22:51.450 --> 22:55.930
internet, I think token, let's see, is 15, 16, 17, 18.

22:56.090 --> 23:00.730
I think token number 18 should be the next token.

23:00.930 --> 23:05.410
And 18, of course, maps onto some letter towards the end of the alphabet.

23:06.650 --> 23:08.050
Okay, so that's the idea.

23:08.570 --> 23:16.690
And now what I'm doing is looping through all of the letters and finding the strongest activation from

23:16.690 --> 23:17.930
each individual layer.

23:17.930 --> 23:21.570
So this is done for one letter just for the first token.

23:21.570 --> 23:23.610
And now we're doing it for everything else.

23:23.930 --> 23:31.780
And here we get the output of c c, which I think that's how you pronounce that word.

23:32.060 --> 23:37.540
Obviously this doesn't make any sense, but if you have a much larger language model with more sophisticated

23:37.540 --> 23:44.580
architecture, then this would be the text that the model is producing that you would use in a chatbot,

23:44.580 --> 23:45.420
for example.

23:46.300 --> 23:48.940
I hope you found that code demo enlightening.

23:49.180 --> 23:54.780
The models that we will work with later in this course, and also professional commercial models, are

23:54.780 --> 23:58.740
all basically put together like the model that we just created here.

23:59.100 --> 24:04.580
Of course there's training and lots of additional complexity, but this is the basic idea.

24:05.220 --> 24:11.180
You know, there's always a trade off between implementing something yourself manually versus using

24:11.180 --> 24:17.220
libraries and functions that other people wrote that do a bunch of calculations behind the scenes.

24:18.020 --> 24:24.500
Now, on the one hand, it's really insane to think about recreating everything in deep learning by

24:24.500 --> 24:29.740
writing assembly code, or even writing out all the details of backprop in C.

24:30.740 --> 24:37.860
On the other hand, if you would interact with models at such a high level that all the math and all

24:37.900 --> 24:43.980
the implementations and all the details and the parameters are completely abstracted and hidden from

24:43.980 --> 24:47.380
you, then that's also really not useful for learning.

24:47.500 --> 24:52.820
In fact, that's basically just you interacting with the chatbot without having any idea of what's going

24:52.820 --> 24:54.100
on behind the scenes.

24:54.540 --> 25:01.180
So what I tried to do in this lecture, and also in the rest of the course, is find the balance, where

25:01.180 --> 25:08.700
we are exposing and investigating some of the details about how these models are put together, while

25:08.700 --> 25:15.060
still leaving some lower level details abstracted away, kind of hidden in the PyTorch library.

25:15.660 --> 25:17.860
Anyway, that's enough about Forwardpass.

25:17.900 --> 25:23.020
Let's go to the next lecture and see how backprop is implemented in PyTorch.

WEBVTT

00:01.960 --> 00:10.000
I'd like to do one more video about conceptual aspects of Anns and the perceptron before we move on

00:10.040 --> 00:13.120
to the math in the next several videos.

00:13.640 --> 00:20.640
So in this video, you will learn more about the basic architecture of Anns and how to interpret them.

00:20.760 --> 00:25.240
You're going to learn some terms like feature space and separating hyperplanes.

00:25.440 --> 00:31.720
And you're going to learn how to visualize the feature space and the predictions of artificial neural

00:31.720 --> 00:32.440
networks.

00:32.680 --> 00:37.480
This is a geometric perspective of Anns and deep learning in general.

00:37.760 --> 00:42.000
So I actually introduced this concept already in the previous video.

00:42.200 --> 00:46.800
And so this video is an expansion of that concept.

00:47.440 --> 00:47.840
Okay.

00:47.880 --> 00:50.280
So let's start with a basic model.

00:50.280 --> 00:51.520
Looks something like this.

00:51.760 --> 00:54.640
We have two inputs x1 and x2.

00:55.040 --> 00:59.540
And those have their own corresponding weights w1 and W2.

00:59.820 --> 01:04.220
We have this computational node here, which is simply computing the sum.

01:04.580 --> 01:11.540
It's the weighted sum of these two inputs weighted by these parameters w1, w2 and then this weighted

01:11.540 --> 01:15.100
sum, which is a linear operation, gets passed through.

01:15.100 --> 01:19.060
The output of that operation is passed through this non-linear function.

01:19.300 --> 01:26.500
And that goes to the output of the model which here we call y hat this little cap here this carrot symbol

01:26.500 --> 01:29.100
or circumflex this is y hat.

01:29.540 --> 01:32.060
And then of course we have the bias term here.

01:32.100 --> 01:35.140
Now remember from the previous video this is set to one.

01:35.140 --> 01:36.860
So this is often left out.

01:36.900 --> 01:39.380
It's just implicitly included in here.

01:39.380 --> 01:41.620
But it does have its own weight okay.

01:41.660 --> 01:49.740
So this is the model architecture the computational graph of this basic perceptron model.

01:50.100 --> 01:52.220
So here's a scientific question.

01:52.340 --> 02:00.970
Can we predict whether students pass or fail an exam based on knowing two features based on how much

02:00.970 --> 02:06.770
they slept, how many hours they slept last night, and how many hours they studied for the exam.

02:07.210 --> 02:10.050
So we measure data from a bunch of students.

02:10.170 --> 02:13.370
We ask them, how much did you sleep and how much did you study?

02:13.570 --> 02:18.410
And then we use that to try to predict whether they pass or fail their exam.

02:18.850 --> 02:19.090
Okay.

02:19.130 --> 02:23.010
So let's start building up a table of these data.

02:23.610 --> 02:23.930
Okay.

02:23.970 --> 02:26.330
So the data set looks something like this.

02:26.370 --> 02:33.250
We have the student ID number, how much they studied, how much they slept and the result of their

02:33.530 --> 02:34.250
exam.

02:34.690 --> 02:40.450
So student one studied for five hours, slept for five hours and passed the exam.

02:40.930 --> 02:45.450
Student two studied ten hours, slept seven hours and also passed the exam.

02:45.650 --> 02:46.290
Blah blah blah.

02:46.330 --> 02:48.890
This goes on and on and we get to student N.

02:48.930 --> 02:56.630
You know, maybe this is 20 or 20 5 or 1 million, however many entries we have in this data table studied

02:56.630 --> 03:01.270
seven hours, slept five hours and unfortunately did not do so well in the exam.

03:01.950 --> 03:04.070
Now this result here.

03:04.070 --> 03:06.030
This is what we want to predict.

03:06.030 --> 03:12.430
We want the model to take the number of hours studied and the number of hours slept as inputs, so x1

03:12.430 --> 03:13.150
and x2.

03:13.710 --> 03:21.390
And then we want to define these weight parameters such that when the data gets plugged through this

03:21.390 --> 03:24.630
computation and then this computation, the output.

03:24.630 --> 03:31.950
Here the prediction of the model matches as closely as possible to the real world outcome which we measure.

03:31.950 --> 03:35.510
So y is the real outcome that we know.

03:35.550 --> 03:36.550
We measure this.

03:36.590 --> 03:40.150
We you know we know their their exam performance.

03:40.270 --> 03:42.430
So this is y and this is y hat.

03:42.430 --> 03:43.870
This is the prediction.

03:43.870 --> 03:51.550
Now the goal of learning these weights is to get this predicted outcome to be as close as possible to

03:51.590 --> 03:52.870
the true outcome.

03:52.870 --> 03:55.940
So we want y hat to look like y.

03:56.500 --> 04:02.340
And of course, the mechanism that we we do that for the mechanism of learning is gradient descent,

04:02.340 --> 04:04.340
as you know from the previous video.

04:04.380 --> 04:08.420
Now, I don't want to get too much into those details in this video.

04:08.420 --> 04:10.180
That's for the next couple of videos.

04:10.900 --> 04:18.460
Now I want to focus more on the the concept of representing and thinking about these data as inputs

04:18.460 --> 04:19.460
into the model.

04:20.660 --> 04:25.300
Because this model is two dimensional, there are two variables, two inputs.

04:25.500 --> 04:32.060
We can actually represent everything on a two dimensional Cartesian plane, where the x axis corresponds

04:32.060 --> 04:37.140
to the number of hours studied, and the y axis corresponds to the number of hours slept.

04:37.420 --> 04:44.460
So what we can do here is represent each individual student as a dot, as a, you know, a coordinate

04:44.500 --> 04:46.540
on this graph on this plane.

04:46.900 --> 04:55.800
So for example here we have student one and that is plotted as a dot in the at the coordinate five six

04:55.800 --> 05:00.400
because they studied five hours and slept six hours, so on and so on.

05:00.440 --> 05:01.520
Here is student two.

05:01.800 --> 05:08.400
Here is another student is a student N down here who studied seven hours but only slept five hours.

05:08.400 --> 05:11.080
And this student unfortunately failed the exam.

05:11.080 --> 05:16.600
So we're going to draw, um, red dots for all the students who failed the exam and green dots for all

05:16.600 --> 05:18.760
the students who passed the exam.

05:19.280 --> 05:23.960
So we can continue populating this graph for all of these students here.

05:23.960 --> 05:32.000
So each dot corresponds to a student and this entire geometric representation here, this picture here

05:32.000 --> 05:34.760
this is called the feature space.

05:34.960 --> 05:39.640
So the feature space is a geometric representation of the data.

05:39.840 --> 05:44.360
The input data here on the left of this model representation.

05:44.800 --> 05:52.590
So a geometric representation of the data where each feature is an axis and each observation is a coordinate.

05:52.750 --> 05:58.830
So the features correspond to the inputs to the model or the things that we have measured from the real

05:58.830 --> 05:59.390
world.

05:59.510 --> 06:04.550
In this case, we have two features the number of hours studied and the number of hours slept.

06:04.710 --> 06:06.590
So this is the feature space.

06:06.710 --> 06:12.470
And each observation which is each student is a coordinate in that feature space.

06:12.910 --> 06:18.270
Now of course with a two dimensional input space we can just visualize it like this.

06:18.310 --> 06:22.590
In practice we're going to have very high dimensional feature spaces.

06:22.590 --> 06:27.070
They're going to start off as being, you know, hundreds or thousands of dimensions.

06:27.190 --> 06:33.550
And very often in the course of deep learning, we end up changing the dimensionality of the feature

06:33.550 --> 06:35.870
space as we go through the model.

06:36.030 --> 06:41.550
So some layers in the model will have more complicated feature representations.

06:41.550 --> 06:43.710
So higher dimensional feature spaces.

06:43.710 --> 06:49.110
There's going to be other kinds of models we'll use like autoencoders that actually reduce the dimensionality

06:49.210 --> 06:50.530
of the feature space.

06:50.570 --> 06:53.490
Okay, but this is the idea of the feature space.

06:53.890 --> 06:55.450
So now what do we want to do?

06:55.490 --> 07:02.850
Well, you know, we want to draw a line that best separates these data, that best separates the students

07:02.850 --> 07:05.010
who passed from the students who failed.

07:05.410 --> 07:10.930
And if we can find that line, that's actually just going to be a way of setting these these three weight

07:10.930 --> 07:11.810
values here.

07:11.930 --> 07:18.370
If we can find that line, then that means that our model can accurately predict whether a student will

07:18.370 --> 07:23.450
pass or fail purely based on these two variables studied and sleeping time.

07:23.930 --> 07:26.090
Well, obviously, you know what that line looks like.

07:26.090 --> 07:27.730
It's going to look something like this.

07:28.290 --> 07:31.170
Now you can see that this line does a pretty good job.

07:31.170 --> 07:32.170
It's not perfect.

07:32.450 --> 07:32.770
Of course.

07:32.770 --> 07:35.930
We have one data point over here, one data point over here.

07:35.930 --> 07:38.610
But overall this is doing a pretty good job.

07:39.210 --> 07:43.970
Now this line here this is called a separating hyperplane.

07:44.010 --> 07:49.080
Now you might be wondering come on Mike you know why do I have to call this thing a hyperplane?

07:49.120 --> 07:53.560
Sounds like something from, you know, like an old sci fi movie or something.

07:54.160 --> 07:55.960
But in two dimensions.

07:55.960 --> 07:57.200
This is called a line.

07:57.560 --> 08:00.320
In three dimensions it's called a plane.

08:00.600 --> 08:04.680
And in any higher number of dimensions, we call it a hyperplane.

08:04.880 --> 08:09.000
So the general term for this is a separating hyperplane.

08:09.120 --> 08:13.760
When we're working in just two dimensions the hyperplane is really just a line.

08:14.360 --> 08:14.560
Okay.

08:14.600 --> 08:17.240
But what is the idea of a separating hyperplane.

08:17.400 --> 08:21.680
It is a boundary that binarize the feature space.

08:21.680 --> 08:26.560
So it is separating the feature space into two categories.

08:26.960 --> 08:30.520
In this case that would be above the line and below the line.

08:31.000 --> 08:33.800
So this is used as a decision boundary.

08:33.960 --> 08:34.960
Now what does this term mean.

08:34.960 --> 08:36.000
Decision boundary.

08:36.080 --> 08:41.280
Well it means that any data points that fall above the line.

08:41.280 --> 08:47.830
So above and to the right of the line the model will predict that that student is going to pass the

08:47.830 --> 08:48.270
exam.

08:48.270 --> 08:56.750
So the model is using this separating hyperplane in order to make a decision to make a prediction about

08:56.750 --> 08:59.110
the performance of an individual student.

08:59.790 --> 09:05.230
And obviously if you know you have a set of parameters, a set of inputs here.

09:05.230 --> 09:11.510
So this region of the feature space that is below the line, the model will predict that the student

09:11.510 --> 09:13.310
will fail the exam.

09:14.030 --> 09:18.110
Now here the model is making a binary prediction.

09:18.110 --> 09:19.590
So pass or fail.

09:19.790 --> 09:25.990
But of course you know mostly most of the time when you take an exam, it's not just pass or fail.

09:26.030 --> 09:29.430
You also get a grade that's like a continuous score.

09:29.870 --> 09:35.710
So now I want to talk a little bit about different ways that a model can make predictions about the

09:35.710 --> 09:38.310
world, different categories of predictions.

09:38.910 --> 09:45.130
So on the one hand we can talk about discrete or categorical or Goal or binary or boolean.

09:45.290 --> 09:48.770
These are all comparable terms, mostly discrete and categorical.

09:49.090 --> 09:51.410
You use if there's multiple options.

09:51.410 --> 09:54.650
Binary and Boolean is just for when there are two options.

09:55.010 --> 09:59.610
On the other hand, we have numeric predictions or continuous predictions.

09:59.610 --> 10:03.810
So this would be like an exam score instead of pass or fail.

10:04.050 --> 10:07.490
And in fact that is the first example that I'm going to show.

10:07.730 --> 10:14.330
So the discrete or categorical or binary model output would be pass versus fail.

10:14.330 --> 10:15.650
So these are just categories.

10:16.170 --> 10:22.450
And we can also get a model to output numeric or continuous outputs where we are trying to predict the

10:22.450 --> 10:24.970
actual grade of the exam itself.

10:25.530 --> 10:28.530
And you know here's a couple of other examples you can think of.

10:28.690 --> 10:29.930
Text sentiment.

10:29.930 --> 10:37.610
So try to get a deep learning model to to predict whether a piece of text, let's say a tweet or a review

10:37.610 --> 10:43.520
on, on Yelp, you know, a review of a restaurant is generally positive or generally negative.

10:43.560 --> 10:50.280
This is a categorical decision versus something like language translation which is a continuous output.

10:50.920 --> 10:52.600
And here's another example.

10:52.600 --> 10:58.960
We can have something like race which is categorical versus attractiveness, which is you know, we

10:58.960 --> 11:02.080
can we can say on a scale from 1 to 10.

11:03.000 --> 11:12.080
So in the previous several slides, I showed you how to represent the model's performance on a categorical

11:12.120 --> 11:16.200
task, in particular binary task where we have a separating hyperplane.

11:16.680 --> 11:20.320
What if we want to get the model to make continuous predictions?

11:20.400 --> 11:22.680
What does that look like geometrically?

11:23.200 --> 11:30.840
Well it's a little bit more complicated because now the exam score is its own third dimension here sticking

11:30.840 --> 11:31.880
out of this plane.

11:32.000 --> 11:34.720
So imagine this is like a three dimensional plane.

11:34.720 --> 11:36.520
Now it's no longer two dimensions.

11:36.800 --> 11:42.220
We we still have the two dimensions for the time studied and the time slept.

11:42.220 --> 11:44.860
So this is the the input feature space.

11:44.860 --> 11:45.820
That's the same.

11:46.060 --> 11:47.980
But now we need another axis.

11:47.980 --> 11:51.580
We need another dimension to represent the exam score.

11:51.820 --> 11:54.700
And that's because this is a continuous exam score.

11:54.700 --> 11:57.700
This goes from 0 to 100%.

11:58.260 --> 12:02.500
So now the idea is that we have the true result here.

12:02.500 --> 12:04.780
This is what the person actually scored.

12:04.780 --> 12:08.780
So they you know they slept this many hours and they studied this many hours.

12:08.980 --> 12:11.460
And their exam score looks like it's halfway up.

12:11.460 --> 12:14.380
So I don't know, maybe they got a 50% on the exam.

12:14.380 --> 12:19.460
Maybe it was a really, really difficult exam and 50% is actually relatively good.

12:19.940 --> 12:21.620
Now what is the model predict?

12:21.660 --> 12:27.460
The model isn't going to make a prediction that is binary or boolean like true or false.

12:27.460 --> 12:35.780
Instead this model we want to select these um the activation function here the nonlinear function so

12:35.780 --> 12:43.610
that the model is making a prediction that is continuous that we can map onto this numerical exam score.

12:44.010 --> 12:46.930
So then the model prediction is going to look something like this.

12:46.930 --> 12:52.610
The model might predict that this is the exam score from a particular student.

12:52.610 --> 12:55.570
So it's a slightly different way of setting up the problem.

12:55.690 --> 13:02.770
We're going to need to understand for each problem that we want to solve, whether we are making continuous

13:02.770 --> 13:09.290
predictions or discrete or categorical predictions, because the math of deep learning is actually going

13:09.290 --> 13:15.690
to change slightly depending on the kind of output that we need the model to produce.

13:16.250 --> 13:24.650
I hope you now feel that you have an intuitive understanding of the basic architecture of Anns and the

13:24.650 --> 13:26.290
perceptron model.

13:26.570 --> 13:32.170
Starting with the next video, we're going to get into the math underlying Anns, which is really the

13:32.170 --> 13:35.890
math that underlies pretty much all of deep learning.

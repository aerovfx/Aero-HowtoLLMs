WEBVTT

00:01.960 --> 00:06.960
Now for part three of the math and mechanisms of deep learning.

00:07.440 --> 00:15.600
You'll see how we go from a perceptron to a deep network, and you will learn how gradient descent is

00:15.600 --> 00:19.200
extended to deal with activation functions.

00:19.880 --> 00:25.960
It does start to get a little bit hairy towards the end, but the good part is you already know all

00:26.000 --> 00:28.080
the math that I'm going to cover here.

00:28.440 --> 00:32.800
It's just a matter of putting all the pieces together in the right way.

00:33.800 --> 00:38.280
Now, before moving forwards, we need to have a bit of a simplification.

00:38.840 --> 00:42.640
And that's because the networks are going to start getting complicated.

00:43.240 --> 00:50.400
So far in this course we've been looking at diagrams like this where I listed the inputs to the model,

00:50.720 --> 00:57.040
the weights, the linear summation feeding into the non-linear activation function which provides the

00:57.040 --> 00:59.440
output of the perceptron model.

01:00.080 --> 01:08.510
Now this is a useful diagram because it illustrates all of the computations that take place, but it's

01:08.510 --> 01:12.910
also cumbersome and it takes up a lot of visual space.

01:13.750 --> 01:21.270
So therefore, from now on, I'm going to simplify this entire diagram with just one circle.

01:21.590 --> 01:29.110
So everything you see here in this perceptron model is taking place inside this circle here.

01:29.630 --> 01:33.150
Now sometimes people call this thing an artificial neuron.

01:33.350 --> 01:40.470
But you know, from the beginning of this course that I don't really like that term because this computation,

01:40.470 --> 01:47.470
although incredibly simple and yet incredibly powerful, actually has really nothing to do with how

01:47.470 --> 01:48.990
real neurons behave.

01:49.270 --> 01:53.270
So I am going to call this a unit or a node.

01:53.870 --> 01:54.190
Okay.

01:54.230 --> 01:58.510
So here we see our diagram of a deep learning network.

01:58.830 --> 02:06.860
Again each of these nodes each of these circles is actually an entire perceptron all on its own.

02:07.380 --> 02:12.460
So each of these nodes in this network is taking in the inputs.

02:12.660 --> 02:19.500
Computing a weighted sum over those inputs, passing that through a non-linearity, and then sending

02:19.500 --> 02:23.060
that to the next node in the next layer of the network.

02:23.580 --> 02:30.060
Now you can see that these units, these nodes give multiple outputs, but in fact it's the exact same

02:30.100 --> 02:31.980
output, the same numerical value.

02:31.980 --> 02:35.500
It's just copied to multiple units.

02:36.100 --> 02:41.780
So these units on the left, these ones are actually getting input from the data, the measurements

02:41.780 --> 02:43.260
from the outside world.

02:43.500 --> 02:49.900
You know, for example, how many hours the student studied and slept or the image pixel values or the

02:49.900 --> 02:51.220
clinical measures.

02:51.420 --> 02:57.020
But these units here, these are actually not getting inputs directly from the data.

02:57.060 --> 03:02.860
They get their inputs as outputs of the units from previous layers.

03:03.290 --> 03:07.530
and those previous layers have a non-linearity built in.

03:07.890 --> 03:14.450
And so this is the mechanism of applying non-linear transformations to the input data.

03:14.850 --> 03:22.770
And again each of these nodes is actually implementing the same exact computation linear weighted sum

03:22.770 --> 03:25.170
and non-linear activation function.

03:25.530 --> 03:32.650
Now one thing you need to understand about these architectures is that each node is independent of each

03:32.690 --> 03:33.490
other node.

03:33.850 --> 03:40.370
In other words, this node here doesn't actually know that it is part of a larger network.

03:40.850 --> 03:46.250
This guy here thinks that he is the only unit in the entire universe.

03:46.450 --> 03:52.890
He gets his inputs here, but he doesn't know that these inputs come from the outputs of other units,

03:53.210 --> 03:56.450
and he computes and passes on his output.

03:56.770 --> 04:00.250
But he doesn't know what this output is used for.

04:00.650 --> 04:06.840
So each unit in the network acts independently of all of the other units.

04:07.160 --> 04:15.320
In fact, every unit in this network simply takes input processes and passes on the output, so they

04:15.320 --> 04:19.600
all think they are individually the only unit in the universe.

04:20.000 --> 04:25.080
It's a lonely life, but fortunately, you know, these things are not actually conscious, so we don't

04:25.080 --> 04:26.400
have to feel bad for them.

04:26.960 --> 04:28.400
Now, why do I mention this?

04:28.440 --> 04:29.600
Why is this important?

04:30.160 --> 04:35.880
It's important because we need to adjust the weights for each node independently.

04:36.280 --> 04:40.400
So this guy here, he happens to be the final output node.

04:41.000 --> 04:45.480
And the loss function which I told you about in the previous video.

04:45.480 --> 04:51.520
So the difference between his output y hat and what we know is the true state of the world.

04:51.520 --> 04:53.800
So that would be y or the target variable.

04:54.160 --> 04:57.240
But that error is computed over here.

04:57.240 --> 04:59.000
And it's used for learning.

04:59.360 --> 05:05.310
And because that error signal is used for learning it needs to propagate Gate backwards through the

05:05.310 --> 05:08.470
network to reach all of these other nodes.

05:08.870 --> 05:16.350
And that learning process needs to be done independently for each individual node in this network.

05:17.190 --> 05:17.510
Okay.

05:17.550 --> 05:21.190
And with that we are now going to discuss backpropagation.

05:21.710 --> 05:28.670
The thing about backpropagation is that it is exactly the same thing as gradient descent.

05:28.950 --> 05:30.710
So that's this step here.

05:31.310 --> 05:39.110
Updating the estimate of the local minimum according to itself, minus the gradient times the learning

05:39.110 --> 05:39.630
rate.

05:40.390 --> 05:43.990
So backpropagation is just gradient descent.

05:44.430 --> 05:49.790
Now there is a little bit more that's involved because we have a high dimensional feature space.

05:50.190 --> 05:53.030
And because we have embedded functions.

05:53.030 --> 05:56.510
So we're going to need to use the chain rule of derivatives.

05:56.750 --> 05:59.030
But the concept is the same.

05:59.790 --> 06:07.180
So thinking back to the previous section the x axis there was called simply X, but these things are

06:07.180 --> 06:09.980
actually going to be the weights of the model.

06:10.100 --> 06:18.500
And again, the idea is to keep moving along this axis, the x axis or the weights axis, until we find

06:18.500 --> 06:23.660
the set of weights that minimizes our loss or our cost function.

06:24.340 --> 06:30.900
Now this is in one dimension which is convenient for visualization, but we have to start abstracting

06:30.900 --> 06:37.980
this picture because we want the gradient descent procedure to work in any number of higher dimensions.

06:38.340 --> 06:41.500
Nonetheless, this is the picture that you can keep in mind.

06:41.540 --> 06:48.660
We continue to adjust the weights according to the derivative of the weights in order to minimize the

06:48.660 --> 06:50.820
loss or the cost function.

06:51.900 --> 06:54.620
So let's go through this procedure step by step.

06:54.700 --> 06:57.660
So here we have our unit our perceptron.

06:57.940 --> 07:05.140
Again this is the math that is taking place inside each node of the deep learning network.

07:05.140 --> 07:05.210
work.

07:05.810 --> 07:08.210
Now the X's are fixed.

07:08.730 --> 07:10.170
We cannot change them.

07:10.410 --> 07:14.730
And the output y hat is simply the result of the computation.

07:15.050 --> 07:20.130
The only thing that we can change is w which are the weights.

07:20.650 --> 07:28.850
So therefore we update the weights to be themselves minus the gradient times the learning rate.

07:29.290 --> 07:30.970
So here is the learning rate.

07:30.970 --> 07:34.210
It's often indicated using the Greek letter eta.

07:34.810 --> 07:39.370
And here is the derivative or partial L or sometimes called dl.

07:39.970 --> 07:44.170
This is the derivative of the loss function with respect to the weights.

07:44.530 --> 07:48.850
So you can see here that this is just vanilla gradient descent.

07:49.330 --> 07:49.650
Okay.

07:49.690 --> 07:56.730
So now let's talk about this term here this derivative we need to compute the gradient of the loss function

07:56.730 --> 07:58.370
with respect to the weights.

07:58.370 --> 08:04.250
And it's with respect to the weights again because those are the variables that are changing.

08:04.890 --> 08:14.480
So this little partial L over here actually expands to DL by DW or partial L partial W, which we can

08:14.480 --> 08:22.880
expand further to replace y hat with the computation that is actually taking place inside each node.

08:23.560 --> 08:26.280
Well, how do we actually compute this derivative.

08:26.560 --> 08:30.560
Let's take the MSE or mean squared error as an example.

08:31.040 --> 08:37.240
So this is the mean squared error function that you learned about in the previous video.

08:37.720 --> 08:42.800
And now we need to compute the derivative of this function with respect to w.

08:43.440 --> 08:46.240
Now notice that this is a compound function.

08:46.240 --> 08:48.200
This is an embedded function.

08:48.600 --> 08:55.560
So there's this function here which is embedded inside the activation function which is further embedded

08:55.560 --> 08:57.560
inside this polynomial.

08:58.120 --> 09:00.880
So we need the chain rule to unpack this.

09:00.880 --> 09:03.760
And the math is going to start getting a little bit hairy.

09:04.200 --> 09:09.590
So for simplicity I'm going to replace all of this inside stuff with you.

09:09.630 --> 09:10.710
The variable you.

09:11.230 --> 09:15.150
So now we can write out the chain rule with this substitution.

09:15.550 --> 09:22.790
And this means that we need to compute the derivative of the loss function of u with respect to u times

09:22.790 --> 09:29.790
the derivative of u with respect to w, and then we can expand u here to show that this is the derivative

09:29.990 --> 09:32.190
of the forward propagation step.

09:32.710 --> 09:40.470
Now exactly how we proceed from this point depends on the activation function, because each activation

09:40.470 --> 09:42.910
function has its own derivative.

09:43.470 --> 09:50.430
And I would like us to focus more on the principle here that this expression updating the weights according

09:50.470 --> 09:57.430
to the derivative of the loss function with respect to the weights times the learning rate, or eta,

09:57.870 --> 10:04.190
is the same thing as gradient descent, and that working through all of the details of the math gets

10:04.190 --> 10:11.580
pretty complicated pretty quickly, because it depends on a variety of choices that you make when constructing

10:11.580 --> 10:18.660
the model, including which loss function you use and which activation function is computed inside each

10:18.660 --> 10:19.140
node.

10:19.700 --> 10:25.780
Also note that I'm showing everything here with the loss function, but of course you can simply substitute

10:25.780 --> 10:30.940
the cost function for the loss function, which doesn't actually change the idea.

10:31.100 --> 10:37.220
It just means that we're averaging over a collection of samples instead of running back propagation

10:37.420 --> 10:38.980
one sample at a time.

10:39.420 --> 10:43.660
Again, it's going to make everything one layer more complicated.

10:43.660 --> 10:45.620
It adds a little bit more complexity.

10:45.860 --> 10:48.900
It doesn't change the fundamental principle here.

10:49.740 --> 10:54.060
So I hope that helps understand the mechanism of backpropagation.

10:54.340 --> 11:01.220
I admit that I went through the calculus part of it rather briskly, but the exact formulation depends

11:01.220 --> 11:02.940
on the activation function.

11:02.940 --> 11:10.490
And as is always the case with partial derivatives in calculus, it quickly becomes quite tedious without

11:10.490 --> 11:13.610
any real benefit to comprehension in my opinion.

11:14.250 --> 11:20.010
But even if I went through all the calculus, that wouldn't still fully explain how it actually works

11:20.010 --> 11:27.090
in practice, because the actual code implementations are a little bit different from the formulas that

11:27.090 --> 11:27.890
I showed.

11:28.130 --> 11:35.490
Due to the potential for numerical instabilities, and also from computation time issues that arise

11:35.490 --> 11:38.490
from implementing the calculus directly.

11:39.250 --> 11:46.570
So, in fact, the developers of PyTorch and other deep learning toolboxes have come up with some computational

11:46.570 --> 11:51.490
tricks to make the backpropagation procedure fast and stable.

11:51.530 --> 11:57.570
But again, the concept is exactly the same as what you learned about in gradient descent.

11:58.330 --> 11:58.610
Okay.

11:58.650 --> 12:06.290
Anyway, I would like to now switch to working through examples of developing, training, and evaluating

12:06.290 --> 12:07.610
deep learning models.

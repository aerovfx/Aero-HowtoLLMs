WEBVTT

00:02.360 --> 00:09.920
Let's continue with our investigations of embeddings vectors with another analysis that is different

00:09.920 --> 00:15.960
from, but related to looking at the angle of differences between vectors.

00:16.520 --> 00:24.960
What I will show you in this video is called path length, and it quantifies the distance that the embeddings

00:24.960 --> 00:30.160
vector travels as it passes from one layer to the next.

00:30.680 --> 00:37.120
When I switch to code, you will see that we get some really stunning and thought provoking beautiful

00:37.120 --> 00:38.040
findings.

00:38.280 --> 00:45.200
But unfortunately when I tried to replicate it, the pattern will turn out to be not universal, or

00:45.240 --> 00:49.720
at least more nuanced than a simplistic interpretation.

00:50.280 --> 00:57.960
I will start by defining the concept of path length in linear algebra, and then I will discuss it in

00:57.960 --> 01:02.030
the context of embeddings vectors in language models.

01:02.590 --> 01:06.150
So imagine these are two vectors on this graph.

01:06.310 --> 01:10.510
Now you can think of these as lines coming from the origin.

01:10.990 --> 01:17.110
But you can also think of these as being two coordinates at these two arrowheads.

01:17.710 --> 01:24.230
So if we want to know what is the distance, what is the length of the path to get from one coordinate

01:24.230 --> 01:29.990
to the next, you can use Euclidean geometry to calculate that distance.

01:30.470 --> 01:38.190
And when we think of these vectors in the linear algebra sense, then we can think about the path length

01:38.190 --> 01:45.870
from one vector to another by first thinking about the line or the vector that connects the head of

01:45.870 --> 01:52.590
one vector to the head of the other vector, and then to measure the length of that line.

01:52.590 --> 01:54.190
This is the path length here.

01:54.390 --> 01:57.510
You can take the difference between the two vectors.

01:57.510 --> 02:01.110
So literally one vector minus the other.

02:01.230 --> 02:04.340
And then you calculate the norm of that vector.

02:04.860 --> 02:10.660
Remember that the norm of a vector is the square root of the sum of the squared elements.

02:10.900 --> 02:16.380
So that's equivalent to Euclidean distance between these two arrowheads here.

02:17.580 --> 02:17.900
Okay.

02:17.940 --> 02:25.820
So this diagram was a bit generic just to illustrate the concept in math in linear algebra.

02:26.460 --> 02:34.020
Of course we cannot really draw these token embeddings vectors because they're just too high dimensional.

02:34.420 --> 02:41.940
But imagine that this graph shows the embeddings vectors for one specific token as it traverses through

02:41.940 --> 02:43.980
the transformer blocks.

02:44.580 --> 02:51.780
So T1 would be the vector for a given token when it comes out of the first transformer block.

02:52.220 --> 02:57.180
And then T2 is the output of the second transformer block, and so on.

02:57.180 --> 03:01.660
And here I'm depicting five transformer blocks for the same token.

03:02.340 --> 03:10.690
Of course, you know that the goal of a transformer block is to take a token embeddings vector, add

03:10.690 --> 03:17.730
a little bit of a rotation or a stretching or contraction according to the context that the token is

03:17.730 --> 03:22.690
presented in, and then pass it along to the next transformer block.

03:23.370 --> 03:31.530
So the idea is that when a transformer block has a really big impact on a token, then the path length

03:31.530 --> 03:36.930
from the output of one transformer block to the next should be really large.

03:38.050 --> 03:45.930
In contrast, if a transformer block doesn't really modify a token embedding vector that much, maybe

03:45.970 --> 03:48.290
it adds a tiny little adjustment.

03:48.690 --> 03:55.490
Then we would expect the path length across the two transformer blocks to be quite small.

03:56.090 --> 04:01.690
And that's really the idea of the analysis that I will show you in this video.

04:02.330 --> 04:08.720
So we're really just measuring the magnitude of the differences between the token embeddings vectors

04:08.880 --> 04:11.440
across different transformer blocks.

04:12.240 --> 04:15.880
So here is what I'm going to show you in the Python demo.

04:16.200 --> 04:21.360
I'll start by importing several different versions of GPT two.

04:21.920 --> 04:28.320
Now the reason for loading in several models here is that we want to see if the results that we get

04:28.320 --> 04:33.560
from the small version are reproduced in these larger versions.

04:34.800 --> 04:41.280
Next, I will tokenize a bit of text that I copied from the Wikipedia page on Nietzsche.

04:41.720 --> 04:43.360
I'm sure you've heard of this guy.

04:43.560 --> 04:50.080
He is one of the most important and influential philosophers of the past few centuries.

04:50.720 --> 04:57.960
He also suffered from several physical and mental problems, but he definitely had a lot of style when

04:57.960 --> 05:03.240
it came to growing mustaches and having those mustaches match his necktie.

05:03.830 --> 05:06.070
So at least he had that going for him.

05:06.870 --> 05:14.310
Uh, sadly, Nietzsche was mostly ostracized and ignored by his colleagues during his lifetime, and

05:14.310 --> 05:20.350
his writings and his intellectual prowess only became influential after he died.

05:20.950 --> 05:28.070
Anyway, I am going to push through some of this text through the model, get the hidden state activations

05:28.070 --> 05:35.550
for all of the transformer layers, and then calculate the path length changes from one layer to the

05:35.550 --> 05:43.590
next, and then also calculate the cumulative path length that is just the cumulative sum over all of

05:43.590 --> 05:45.950
the individual path lengths.

05:46.670 --> 05:50.510
And that's what you see on the left panel of this figure here.

05:51.430 --> 05:54.430
So the x axis is the transformer block.

05:54.630 --> 05:58.030
The y axis is the total path length.

05:58.030 --> 06:05.860
So the cumulative path length and each dotted line in here is for a different token in the sequence.

06:05.860 --> 06:13.820
So in this paragraph, of course, these values keep increasing because lengths are non-negative quantities,

06:13.820 --> 06:17.540
and we're just summing up more of them as we go through the model.

06:18.060 --> 06:23.540
And this scatter plot over here shows a really remarkable correlation.

06:23.980 --> 06:27.900
So we have path length on the y axis.

06:28.060 --> 06:31.060
And this is actually not the cumulative path length.

06:31.180 --> 06:37.100
It's just the change from the penultimate transformer block to the final transformer block.

06:37.100 --> 06:39.380
So 11 to 12.

06:39.860 --> 06:48.340
And then on the x axis is the final logit at the output of the model for the token that comes next.

06:48.340 --> 06:50.420
So next token prediction.

06:50.780 --> 06:58.340
So basically larger numbers here means that the model is giving a stronger activation to the true next

06:58.340 --> 07:00.420
token the real token that came next.

07:00.860 --> 07:07.930
Now this is a fantastic finding because it means that the shorter the path length, which means the

07:07.930 --> 07:15.010
closer the embeddings vectors are to each other from layer 11 to 12, the more likely the model is to

07:15.050 --> 07:18.730
make an accurate prediction of the subsequent token.

07:19.810 --> 07:24.810
Now, all of these data here is just for the small version of GPT two.

07:25.370 --> 07:32.730
So what I did next was repeat this analysis for all four of the models or the versions of GPT two.

07:33.290 --> 07:39.130
I'm not showing those results here, because I think it's more interesting to see them in context of

07:39.130 --> 07:39.810
the code.

07:40.050 --> 07:46.370
But suffice it to say that this very nice pattern sort of replicates, but not entirely.

07:46.810 --> 07:51.410
Anyway, let's switch to code and continue exploring and discussing.

07:52.730 --> 07:55.050
Here are the libraries that I will use here.

07:55.050 --> 07:57.850
This is the GPT two tokenizer.

07:57.850 --> 07:59.570
I've already imported that.

07:59.570 --> 08:03.410
And here's the list of models organized as a dictionary.

08:03.410 --> 08:09.080
And then I import all of the models from the dictionary and switch them to eval mode.

08:09.080 --> 08:14.440
And I'm putting them all on the GPU as well, mainly because of this really big model.

08:14.840 --> 08:19.280
All of this code you've seen I think several times before in this course.

08:19.720 --> 08:24.120
So let's so I've already run this because this takes several minutes to load in.

08:24.520 --> 08:24.880
Okay.

08:24.920 --> 08:31.800
So now, uh, until the end of this code file, I'm only going to be working with the small version

08:31.800 --> 08:33.160
of GPT two.

08:33.520 --> 08:37.880
I will tell you, when we get to the other versions, that's going to happen later.

08:38.280 --> 08:40.080
So here is this paragraph.

08:40.080 --> 08:45.960
Here is the direct URL, although you'd also get it just by googling for Nietzsche.

08:46.320 --> 08:49.160
So yeah, Nietzsche was a widely influenced philosopher, blah blah blah.

08:49.200 --> 08:51.600
You can see quite a bit of text here.

08:51.600 --> 08:52.960
It's just one paragraph.

08:53.400 --> 09:00.160
Okay, so then I tokenize this, move it to the GPU, and I run this through the small version of the

09:00.160 --> 09:03.480
model and I get the hidden states equals true.

09:03.920 --> 09:04.280
Okay.

09:04.320 --> 09:06.830
So that should be pretty fast to run.

09:07.190 --> 09:09.390
Actually, don't even need this line here.

09:09.390 --> 09:11.470
I think that's just copied from something else.

09:11.710 --> 09:18.110
This is, of course specifying the labels so that we can calculate the losses, but I'm not using that

09:18.110 --> 09:20.670
for this particular code file.

09:21.830 --> 09:22.190
Okay.

09:22.230 --> 09:28.630
Here is arguably the most important code cell in this code demo.

09:28.830 --> 09:36.790
This is where I'm going to be calculating the path length from each token to the next token in the layer.

09:36.830 --> 09:40.630
Or actually the way I set it up is each token relative to the previous.

09:41.070 --> 09:41.390
Okay.

09:41.430 --> 09:45.990
So let's see here I'm looping over all of the tokens in the text.

09:45.990 --> 09:48.990
And now I'm looping over all of the layers.

09:49.230 --> 09:52.910
Now notice that I'm starting from layer one.

09:53.110 --> 09:58.550
And that's because I am calculating relative to layer zero here.

09:58.950 --> 10:00.310
So here is.

10:00.430 --> 10:06.430
So now actually some of this code will look familiar from the previous few videos where we were calculating

10:06.580 --> 10:14.980
the angle of the embeddings vector from the current relative to the previous layer, the previous transformer

10:14.980 --> 10:15.460
block.

10:15.660 --> 10:17.340
So very similar here.

10:17.340 --> 10:20.420
So I get the current activation vector for.

10:20.580 --> 10:27.780
So from the hidden states for this particular layer this is the first sequence this particular token.

10:27.780 --> 10:31.740
And then we have all of the embeddings dimensions.

10:31.860 --> 10:37.700
So this gives me one vector for the current processing of the of one token.

10:37.900 --> 10:46.140
And then this is exactly the same token exactly the same vectors but just one layer backwards in time

10:46.140 --> 10:46.780
if you will.

10:47.940 --> 10:48.260
Okay.

10:48.300 --> 10:55.300
So now exactly as I showed in the slides a few moments ago, we just subtract the two vectors.

10:55.300 --> 10:58.860
That gives us the vector that connects these two points.

10:58.860 --> 11:05.220
So the two heads of these arrows, if you think of these as being vectors that start from the origin

11:05.220 --> 11:06.780
and go to these coordinates.

11:07.260 --> 11:11.940
And then I calculate their norm and then bring it back to the CPU.

11:12.100 --> 11:15.500
And that is called path length for this layer.

11:15.620 --> 11:17.660
And for this token.

11:18.020 --> 11:18.260
Okay.

11:18.300 --> 11:21.940
And then here probably more correct is to call this the logit.

11:22.020 --> 11:25.660
So here I'm getting the next token prediction.

11:25.660 --> 11:28.060
So here is the output logits.

11:28.060 --> 11:31.020
This is not the hidden states that I'm accessing here.

11:31.020 --> 11:39.100
But actually the logits at the very end of the model immediately before we would do a embeddings and

11:39.100 --> 11:40.220
token selection.

11:40.700 --> 11:43.940
So I get all of the logits again for this token.

11:44.060 --> 11:47.300
All of the tokens in the vocab.

11:47.300 --> 11:56.620
So the size of this vector here is going to be 50,256 or 57, whatever it is.

11:57.060 --> 12:01.740
And then what I'm looking for is the logits for the next token.

12:01.740 --> 12:09.570
So this is how strong does the model weight the output for the token that should come next.

12:09.570 --> 12:11.690
So next token prediction.

12:11.890 --> 12:14.090
And then I'm storing that variable here.

12:14.570 --> 12:14.930
Okay.

12:14.970 --> 12:17.370
So then so that is the for loop.

12:17.370 --> 12:19.010
And that's the analysis here.

12:19.010 --> 12:23.570
So loop over all of the tokens loop over all of the layers.

12:24.570 --> 12:27.490
And now I'm calculating the cumulative path length.

12:27.490 --> 12:31.690
That is the dotted line plot that I showed in the slides.

12:31.810 --> 12:36.770
Very simple to calculate you just use the numpy function cumulative sum.

12:37.090 --> 12:44.210
And because this is a matrix, you need to specify the dimension along which you want to calculate the

12:44.210 --> 12:45.370
cumulative sum.

12:45.810 --> 12:52.290
So we want to calculate the cumulative sum along the first dimension which is the layers.

12:52.290 --> 12:54.770
So axis equals zero.

12:55.170 --> 12:57.890
And then here I'm setting the first one to be Nan.

12:58.130 --> 13:05.530
That's basically because the first value is zero because yeah this is not defined for the very first

13:05.530 --> 13:06.130
layer.

13:06.370 --> 13:10.680
It's only defined starting from the second layer with index one.

13:10.880 --> 13:13.840
And yeah that's because we have to subtract here.

13:14.200 --> 13:14.520
Okay.

13:14.560 --> 13:17.680
So this is just for convenience for plotting.

13:17.800 --> 13:21.960
When I set this to be zero then the plot actually plots at zero.

13:21.960 --> 13:24.200
But that's not a valid data point here.

13:24.200 --> 13:28.080
So I'm just replacing all of these values with an Nan.

13:28.080 --> 13:31.560
And then matplotlib will ignore those values.

13:32.760 --> 13:32.960
Okay.

13:33.000 --> 13:34.560
And then here is for the correlation.

13:34.560 --> 13:43.520
So here I'm correlating the lat the token logits here for the prediction for the upcoming token with

13:43.520 --> 13:47.480
the path length at the very end of the transformer block.

13:47.480 --> 13:53.400
So that is the penultimate up until the ultimate or final transformer block.

13:53.800 --> 13:57.960
This one here is because I'm skipping the first token.

13:57.960 --> 13:59.680
You've already seen that several times.

13:59.680 --> 14:04.120
That will come up again in a couple of videos from now in a code challenge.

14:04.280 --> 14:11.070
But as I mentioned multiple times, in general, in analyses you will often want to skip the first token.

14:11.070 --> 14:11.990
It looks weird.

14:11.990 --> 14:17.790
It's often an extreme value and is not really representative of what's happening in the rest of the

14:17.790 --> 14:18.110
text.

14:18.110 --> 14:20.590
It can really screw up the analyses.

14:20.870 --> 14:23.270
If you're curious, you can also, of course try it.

14:23.270 --> 14:28.950
You just get rid of this and get rid of that, and that's fine, but I'm going to preserve that.

14:29.350 --> 14:29.710
Okay.

14:29.750 --> 14:34.190
Then the rest of this code here is just for the visualization.

14:35.030 --> 14:35.350
Okay.

14:35.390 --> 14:39.150
So this is exactly the figure you've seen before in the slides.

14:39.150 --> 14:41.630
This is a beautiful result.

14:41.630 --> 14:45.670
Basically the less so the shorter the path length.

14:45.710 --> 14:54.870
That means that the embeddings vector has changed relatively little from the penultimate to the final

14:54.870 --> 14:56.270
transformer block.

14:56.670 --> 14:59.950
And that predicts a larger magnitude.

14:59.990 --> 15:03.350
So a larger logit for the correct token.

15:03.350 --> 15:07.470
The token that the model should predict as the subsequent token.

15:07.910 --> 15:08.990
Beautiful results.

15:09.260 --> 15:11.020
Let's see if it replicates.

15:11.260 --> 15:16.420
Okay, so what I'm doing here now is basically repeating this analysis.

15:16.420 --> 15:25.540
So all of this code here is the same code that I just described above, but that's embedded inside another

15:25.700 --> 15:28.740
for loop over all of the different models.

15:28.740 --> 15:33.660
So I start by pushing the data through getting the hidden states.

15:33.780 --> 15:37.500
And yeah this is some initializations running the analysis.

15:37.500 --> 15:40.580
This is also exactly the same analysis you saw above.

15:40.860 --> 15:46.460
And then yeah, printing out the correlation coefficient and drawing the figure.

15:46.620 --> 15:51.700
And this code here is exactly this code to generate this plot.

15:51.740 --> 15:57.300
It's just going to appear for all four of these model sizes okay.

15:57.340 --> 16:05.020
So again this plot here is for GPT two small that is yeah that's the same thing you saw before.

16:05.380 --> 16:13.330
Now the question is does this negative correlation Continue to be observed in the other model variants.

16:13.810 --> 16:17.490
For the medium model, it was definitely not observed.

16:17.490 --> 16:22.410
The correlation is very close to zero, and we don't visually see anything remarkable.

16:22.730 --> 16:24.050
For the large version.

16:24.050 --> 16:29.890
It does look like there is some negative correlation, but this is a little difficult to.

16:30.210 --> 16:33.730
So when I look at this what I see is two populations.

16:33.730 --> 16:37.530
There's this stuff over here and then there's this stuff over here.

16:37.530 --> 16:40.770
So the global correlation is negative.

16:40.770 --> 16:47.530
I'm not doing any statistical analysis here, but my guess is that the p value here is pretty small.

16:48.410 --> 16:52.290
You could test that if you are interested here for the Excel.

16:52.650 --> 16:52.890
Yeah.

16:52.930 --> 16:59.730
Again you do see some uh tokens here that that seem to show this negative correlation.

16:59.730 --> 17:01.330
It is technically negative.

17:01.490 --> 17:06.130
Maybe this will also be significant because the sample size is relatively large.

17:06.130 --> 17:06.370
Right.

17:06.410 --> 17:12.320
Each of these dots is a token, but this is certainly not so visually compelling.

17:12.960 --> 17:16.920
I certainly do not want to claim that this finding does not exist.

17:17.280 --> 17:23.560
It is possible that the relationship does exist, but only for certain categories of tokens.

17:23.720 --> 17:30.240
And then maybe this model, you see it more clearly because it's just a smaller, simpler model.

17:30.360 --> 17:35.360
Maybe with these other models you'd see it more robustly, more visually.

17:35.640 --> 17:40.280
Obviously, if you would find some nuanced pattern in here.

17:40.280 --> 17:46.720
So I don't want to claim that this pattern does or does not exist, but it certainly does not easily

17:47.440 --> 17:50.080
replicate in a very straightforward manner.

17:50.440 --> 17:57.560
And also just to be like super duper crystal clear about this, this lack of replication or maybe nuanced

17:57.560 --> 18:01.880
replication doesn't mean that path length is problematic per se.

18:02.160 --> 18:09.600
It's just about this one very specific correlational analysis between the path length from the penultimate

18:09.600 --> 18:15.470
to the final transformer block with the logits for the next token prediction.

18:15.470 --> 18:22.590
So that is the specific finding that I had a hard time believing was really replicable.

18:22.590 --> 18:26.630
Not that there's anything wrong with path length as an analysis method.

18:27.790 --> 18:35.710
Path length is a nice, simple metric to measure how much a vector changes after it is adjusted.

18:36.190 --> 18:42.270
It is different from the angle that you learned about in the previous couple of videos, because path

18:42.270 --> 18:46.910
length also incorporates the relative lengths of the vectors.

18:46.910 --> 18:50.190
This is something I mentioned a couple of videos ago.

18:51.030 --> 18:58.190
For example, imagine that a vector stays in the same direction, so the angle is roughly the same,

18:58.550 --> 19:00.750
but it gets considerably longer.

19:00.750 --> 19:08.350
Like maybe the vector is twice as long after a transformer block, the angle change would be zero or

19:08.350 --> 19:13.980
very close to zero, whereas the the path length would indicate a doubling.

19:14.660 --> 19:21.900
More generally, you have already seen several times in this course that some analyses that we do seem

19:21.900 --> 19:29.780
to generalize fairly well across models, whereas other findings either do not generalize, or perhaps

19:29.780 --> 19:36.140
maybe they would generalize if the analysis were done in a different way, or maybe with a lot more

19:36.140 --> 19:36.780
data.

19:37.340 --> 19:44.540
For example, it actually is possible that the relationship between path length in the final transformer

19:44.540 --> 19:52.020
blocks and next token prediction logits is preserved for different models, and that relationship might

19:52.020 --> 19:56.620
be just deceptively simple in the small version of GPT two.

19:57.220 --> 20:01.420
Maybe you would also see that relationship in larger models.

20:01.420 --> 20:06.660
If we had a more nuanced way of selecting certain categories of tokens.

20:06.780 --> 20:11.540
Or maybe we just need to use a larger and more diverse data set.

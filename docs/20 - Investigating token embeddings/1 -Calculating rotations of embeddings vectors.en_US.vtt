WEBVTT

00:02.000 --> 00:05.160
Welcome to this section of the course.

00:05.560 --> 00:12.920
Now, there isn't really a huge difference between the analyses in the previous sections and the analyses

00:12.920 --> 00:14.520
in this section.

00:15.000 --> 00:21.880
It's a slight shift in focus from what's happening at the level of the entire layer to what's happening

00:21.920 --> 00:27.920
to individual embeddings, vectors coming from each token in a text sequence.

00:28.600 --> 00:37.680
So the idea of this section is to track how the embeddings vectors change as they pass through the transformer

00:37.680 --> 00:40.760
layers and how they interact with other tokens.

00:41.280 --> 00:48.160
So yeah, it's a slight shift in conceptualization and terminology, but not really fundamentally different

00:48.160 --> 00:51.400
from the material so far in this part of the course.

00:51.920 --> 00:59.990
Anyway, the goal of this video is to teach you about calculating how these embeddings vectors are rotating

01:00.350 --> 01:03.950
as they pass from one transformer block to the next.

01:04.430 --> 01:11.110
I will show a bit of math and a Python demo here, and then you're going to have a couple of code challenges

01:11.110 --> 01:16.510
where you can continue exploring these concepts for different analyses.

01:17.950 --> 01:26.990
I'd like to start by reminding you that tokens are represented as vectors in a high dimensional space.

01:27.470 --> 01:34.750
This screenshot here shows the slide from way back towards the beginning of the course, when I explained

01:34.750 --> 01:42.110
the idea of representing words using dense vectors in, in this case, a two dimensional space.

01:42.830 --> 01:49.990
Now, the thing is that we cannot actually visualize or even conceptualize a high dimensional space

01:49.990 --> 01:52.150
that's used in language models.

01:52.270 --> 01:59.310
But the cool thing about linear algebra is that the conceptualizations and the math that you work out

01:59.310 --> 02:03.790
for two dimensions apply to any number of dimensions.

02:05.150 --> 02:14.630
So let's think about the word cat not as a box with some coordinate, but instead as a vector that starts

02:14.630 --> 02:22.510
at the origin of this space and ends at the coordinate of the token in this embedding space.

02:23.110 --> 02:26.710
Now this is the initial embeddings for the word cat.

02:27.070 --> 02:34.830
And as you know, the point of all the transformer blocks is to slowly transform this vector from representing

02:34.830 --> 02:40.590
the token for cat to making a prediction about what token should come next.

02:41.190 --> 02:43.630
And how does that transformation happen?

02:44.190 --> 02:50.510
Well, within each transformer you have two sublayers that calculate these little adjustments that get

02:50.550 --> 02:52.150
added onto the vector.

02:52.830 --> 02:59.620
Those adjustments depend on the context of other tokens Surrounding the appearance of the word cat.

03:00.180 --> 03:07.380
So let's imagine that other words in the sentence are, for example, the word mean or ferocious or

03:07.420 --> 03:08.380
unfriendly.

03:09.020 --> 03:14.940
Those tokens are going to pull down this vector a little bit on the friendliness dimension.

03:15.740 --> 03:22.860
So now the token embeddings vector of the word cat has been rotated a tiny bit like this.

03:23.340 --> 03:30.860
And also just as a reminder that these embeddings dimensions generally do not have human interpretable

03:30.860 --> 03:33.420
labels like friendliness or size.

03:33.540 --> 03:35.700
That's really just for this toy example.

03:35.780 --> 03:39.340
So we have a picture to look at and some math to work through.

03:40.100 --> 03:41.860
But this is the idea.

03:42.100 --> 03:48.380
The embeddings vector starts off pointing in one direction, and then each transformer block that it

03:48.380 --> 03:57.620
passes through, the vector gets rotated by some amount according to the context And how much does this

03:57.620 --> 03:59.100
vector get rotated?

03:59.380 --> 04:03.300
That is the question that I will answer on this slide.

04:03.780 --> 04:11.140
In fact, to answer this question, I'm going to start with the formula for cosine similarity.

04:11.820 --> 04:17.100
So let's say we want to calculate the cosine similarity between the original vector.

04:17.100 --> 04:22.740
We can call that x and the adjusted vector after going through a transformer block.

04:22.900 --> 04:24.580
We can call that vector y.

04:26.020 --> 04:28.100
Now you already know how to do that.

04:28.100 --> 04:30.620
And we've done this earlier in the course.

04:30.820 --> 04:35.860
But the question here in this video is not about cosine similarity.

04:35.900 --> 04:39.260
It's about the angle between the two vectors.

04:39.660 --> 04:47.540
So to get that angle all we need to do is isolate theta on the left hand side of this equation.

04:47.540 --> 04:49.220
So this is the angle here.

04:49.940 --> 04:53.100
But there's a lot of pixels in this equation.

04:53.100 --> 04:59.890
So for simplicity I'm going to rewrite this equation using this more compact notation.

05:00.410 --> 05:06.290
These angle brackets here are often used to indicate the dot product.

05:06.290 --> 05:10.850
So this would be the dot product between vector x and vector y.

05:11.490 --> 05:15.810
And these vertical lines here indicate the vector norms.

05:15.850 --> 05:19.530
Sometimes in linear algebra you will see double vertical lines.

05:19.530 --> 05:26.010
Here you might also see a little two subscript here, but I'm just using one vertical line just for

05:26.010 --> 05:26.890
simplicity.

05:27.730 --> 05:30.970
So these two equations here are the same.

05:31.130 --> 05:34.450
This one is just more compact in terms of notation.

05:35.130 --> 05:40.650
Now to isolate theta you'd like to do something like dividing by the cosine.

05:41.170 --> 05:43.450
But the cosine is an operator.

05:43.450 --> 05:44.290
It's a function.

05:44.290 --> 05:46.530
It's not something we can just divide by.

05:47.050 --> 05:55.970
However there is an inverse cosine function which is also called the arc cosine, and that will cancel

05:55.970 --> 05:58.650
out the cosine function on the left.

05:58.850 --> 06:00.970
Leaving us with just theta.

06:01.490 --> 06:07.450
So we can take the inverse cosine or the arc cosine of both sides of the equation.

06:07.450 --> 06:11.730
And then we have a formula for the angle between two vectors.

06:12.330 --> 06:21.890
This result is in units of radians, and we can convert it to degrees by multiplying by 180 over pi.

06:22.570 --> 06:23.930
So this is the theory.

06:24.090 --> 06:29.330
It's surprisingly simple once you are familiar with cosine similarity.

06:30.090 --> 06:34.210
And this is actually one of the things I really love about linear algebra.

06:34.490 --> 06:41.570
Seemingly complicated linear algebra operations are actually quite simple once you have a decent foundation.

06:42.170 --> 06:46.930
Anyway, now I will describe what I'll do in the Python demo.

06:48.210 --> 06:56.240
We are going to work with ChatGPT to Excel because it has lots of layers, lots of transformer blocks.

06:56.240 --> 06:59.080
So that's pretty useful for this analysis.

06:59.840 --> 07:06.440
To create the activations for the token embeddings vectors, I will use the sentences that all contain

07:06.480 --> 07:09.640
her somewhere in each sentence.

07:09.960 --> 07:13.840
We've worked with this sentence data set before.

07:14.280 --> 07:18.960
It is 54 sentences that I got from Claude I.

07:19.520 --> 07:22.920
Now these sentences have different token lengths.

07:23.160 --> 07:31.720
So this Python demo will also be a nice reminder of padding sequences to create batches of equal lengths

07:31.800 --> 07:35.040
when you start from sentences that have unequal lengths.

07:36.360 --> 07:43.280
After pushing through these sequences through the model, I'll get all of the Hidden States activations.

07:43.280 --> 07:52.160
You can see that there are 49 hidden layers for 48 transformer blocks, so eight tokens and an embedding

07:52.240 --> 07:54.600
dimension of 1600.

07:55.240 --> 08:03.120
The main analysis in the code is to calculate the angle of the embeddings vector from each layer, each

08:03.120 --> 08:10.280
transformer block relative to the previous layer, so that is equivalent to what I described in the

08:10.280 --> 08:17.960
previous slide, where the context surrounding the token will modify the embeddings vector at each transformer

08:17.960 --> 08:23.640
block, and we want to calculate the angle from each transformer to the next.

08:24.040 --> 08:24.400
Okay.

08:24.440 --> 08:27.160
And that's what you see in this plot here.

08:27.640 --> 08:35.160
The x axis shows depth into the model or the transformer block index, and the y and the y axis shows

08:35.160 --> 08:40.640
the angle from the previous layer to the current layer for the target token.

08:40.640 --> 08:42.640
So that's the token word here.

08:43.880 --> 08:47.080
And then I also used a non-target reference.

08:47.080 --> 08:52.430
And for that I use the token immediately preceding the target word.

08:52.790 --> 08:58.790
That's a pretty interesting comparison, because the context for the target and non-target words are

08:58.950 --> 09:04.870
nearly identical, but the words themselves are all different from each other.

09:05.430 --> 09:11.750
And actually, there is another comparison that I have in the code, which is randomly selected tokens

09:11.750 --> 09:14.710
across randomly selected layers.

09:15.190 --> 09:21.750
I did not include that visualization in this plot because it's so much higher and it's really noisy,

09:22.110 --> 09:24.870
but it's interesting to look at and to ponder.

09:24.870 --> 09:27.310
And so I will show it when I switch to code.

09:27.910 --> 09:35.390
And also in the code, I will convert the y axis into angles so that they're easier to interpret.

09:36.470 --> 09:42.110
Now there are several features that we see in this plot that are pretty interesting to think about.

09:42.350 --> 09:50.430
First is the huge change from the very beginning as we transition from the initial embeddings to the

09:50.430 --> 09:52.470
first transformer block.

09:52.950 --> 09:54.510
And that's actually pretty sensible.

09:54.510 --> 09:59.630
The initial embeddings have zero context modulation.

09:59.630 --> 10:07.030
And then the first transformer block has a lot of work to do to incorporate the initial context.

10:07.630 --> 10:15.710
And then you see this gentle but consistent and fairly linear looking decrease in the angle as we get

10:15.710 --> 10:17.670
deeper into the model.

10:18.190 --> 10:25.070
So it seems like the adjustments from one transformer to the next are relatively small and getting smaller

10:25.070 --> 10:27.550
as we go deeper into the model.

10:28.630 --> 10:35.070
And then there's this big jump right at the very end, which can be interpreted as the final transformer

10:35.070 --> 10:42.110
block, making the last adjustments to the embeddings vectors before they get sent out to the embeddings

10:42.110 --> 10:44.310
and a new token is generated.

10:45.030 --> 10:53.070
And by the way, these thin lines are for each of the 54 sentences, and the thick lines show the average.

10:54.510 --> 10:59.390
And because we have a lot of repetitions here we have 54 sentences.

10:59.390 --> 11:02.790
So 54 targets and 54 non targets.

11:02.830 --> 11:06.310
We can actually perform a statistical analysis.

11:06.790 --> 11:12.670
In particular it kind of looks like the blue line is higher than the red line in early blocks.

11:12.670 --> 11:16.750
Whereas the red line is higher than the blue line in later blocks.

11:17.270 --> 11:26.750
And so the final part of this demo is to run a t test on the angles of the vector changes at each layer.

11:27.230 --> 11:29.150
And those results you see here.

11:29.270 --> 11:37.870
So there are green circles where the target angles are statistically significantly larger than the non-target

11:37.870 --> 11:38.590
angles.

11:38.910 --> 11:41.670
And then we have blue squares for the opposite.

11:41.670 --> 11:49.940
So the non-target angles are significantly bigger than the target angles and red X's, where there are

11:49.940 --> 11:52.260
no statistically significant differences.

11:52.500 --> 11:58.780
In other words, the angle of the change in the embeddings vectors from one layer to the next is roughly

11:58.780 --> 12:01.740
the same for targets and non targets.

12:03.340 --> 12:10.540
Now, I don't want to over interpret this plot or this result, because it's only based on 54 short

12:10.540 --> 12:16.820
sentences that are all fairly homogeneous in the sense that the target token is always the same.

12:17.380 --> 12:25.340
But the interpretation of this kind of finding would be that as we get deeper into the model, the adjustments

12:25.340 --> 12:27.780
for the target word have to increase.

12:28.060 --> 12:35.780
In this case, because the pronoun her needs to be bound to other words in the text that the word her

12:36.140 --> 12:37.100
references.

12:37.460 --> 12:39.140
So that would be the interpretation.

12:39.540 --> 12:43.660
Anyway, let's now switch to Python and see what the code looks like.

12:44.930 --> 12:47.850
here are the libraries we will use here.

12:48.010 --> 12:54.610
And also I will use the Scipy.stats library for running t tests towards the end of the code.

12:55.090 --> 12:57.810
Here I'm importing GPT two XL.

12:57.850 --> 13:03.090
This takes a few minutes, so I probably should have started running this before I switched to Python.

13:03.210 --> 13:07.770
But anyway, here are the sentences these you've seen before.

13:08.090 --> 13:10.050
And uh, yeah, let me see.

13:10.250 --> 13:10.970
Uh, yeah.

13:11.010 --> 13:12.530
All of this code you've seen before as well.

13:12.530 --> 13:14.090
So I'm just tokenizing them.

13:14.330 --> 13:14.770
Uh, here.

13:14.770 --> 13:16.210
I'm tokenizing them here.

13:16.210 --> 13:18.530
I'm tokenizing the target word her.

13:18.570 --> 13:21.450
I've also talked about this issue with the space before.

13:21.970 --> 13:24.490
So I'll cue up that cell to run.

13:24.850 --> 13:29.730
And here is where we tokenize all of those sentences.

13:29.850 --> 13:36.170
And just as a reminder, the issue with creating batches from these sentences is that the different

13:36.170 --> 13:39.250
sentences have different numbers of tokens.

13:39.490 --> 13:47.570
And so therefore we need to pad all of the tokens such that all the sequences are exactly the same length

13:47.730 --> 13:49.090
within a batch.

13:49.210 --> 13:55.930
So to do that, we use the end of sequence token as the pad token, and then we write padding equals

13:55.930 --> 13:56.370
true.

13:56.490 --> 14:01.370
And then we get padded tokens here which we can then push through the model.

14:01.530 --> 14:05.250
This is all stuff that I have described earlier in the course.

14:05.690 --> 14:05.970
Okay.

14:06.010 --> 14:09.770
So this will also take a about a minute on the CPU.

14:10.010 --> 14:14.050
Of course it's much faster on the GPU than it's like a second maybe.

14:14.210 --> 14:18.930
But uh, yeah, we don't really need the GPU for this, uh, entire script.

14:19.370 --> 14:25.330
Okay, so then, yeah, I'm just grabbing the number of hidden layers, and here I'm just printing out

14:25.330 --> 14:29.130
the shape the size of one of the hidden layers.

14:29.130 --> 14:33.410
This is really just confirmation that we get some expected results.

14:33.930 --> 14:37.290
And now here is where we do the analysis.

14:37.370 --> 14:42.920
So I'm looping over all of the sentences here I'm finding the target.

14:42.920 --> 14:45.920
Where again, this is code that you've seen in earlier videos.

14:45.920 --> 14:51.640
But the main idea is that the target word her is in a different index location.

14:51.640 --> 14:54.440
So different location in each sentence.

14:54.440 --> 15:01.200
So therefore we need to loop over all of the sentences and find the index of the target word in each

15:01.240 --> 15:02.680
individual sentence.

15:03.200 --> 15:06.840
Here I'm starting the loop over all of the layers.

15:06.880 --> 15:11.800
Notice that I start the loop at index one and not index zero.

15:12.160 --> 15:18.720
And the reason is that what I want to calculate in this analysis is the change, or the angle of the

15:18.720 --> 15:24.960
change in the vector from each transformer block relative to the previous transformer block.

15:25.160 --> 15:28.360
So outputs dot hidden states layer I.

15:28.400 --> 15:31.960
So this is the current layer in this for loop minus one.

15:32.000 --> 15:34.480
And then for convenience I call that vector v.

15:34.720 --> 15:41.960
This I call vector u that is still this sentence, this target location, but the current layer.

15:43.000 --> 15:43.280
Okay.

15:43.320 --> 15:44.320
So then detach.

15:44.320 --> 15:49.440
And the squeeze is basically just to get rid of these singleton dimensions here.

15:49.560 --> 15:53.040
Of course this is the embeddings dimensionality here.

15:53.040 --> 16:00.640
So that means that the result here this v is going to be one vector with 1600 elements in it.

16:00.760 --> 16:02.280
So v and then u.

16:02.600 --> 16:10.320
And here is where I directly translate the formula that I showed in the slides a moment ago into code.

16:10.600 --> 16:18.880
So we have the dot product between v and u divided by or normalized by the product of the norms of the

16:18.880 --> 16:20.120
two vectors.

16:20.480 --> 16:23.520
And then here I apply the arc cosine.

16:23.560 --> 16:25.840
This is the inverse cosine function.

16:26.280 --> 16:31.800
And the result here is the angle between these two vectors.

16:32.000 --> 16:35.240
And again it's in units of radians.

16:36.240 --> 16:43.790
And again the idea is that if a particular embeddings vector doesn't really change after the transformer

16:43.790 --> 16:44.110
block.

16:44.110 --> 16:50.310
Let's say the attention mechanism looks at the vector as like yep, this is good as it is, we don't

16:50.310 --> 16:51.990
need to do any adjustments.

16:52.190 --> 16:59.190
Then that angle is going to look basically the same before and after the transformer block, in which

16:59.190 --> 17:00.990
case the angle will be zero.

17:01.430 --> 17:07.510
On the other hand, if the transformer block really needs to make some pretty significant adjustments

17:07.510 --> 17:14.870
to the vector, then that vector will be rotated and then vector u will point off in a different direction

17:14.870 --> 17:18.670
from vector v, and then this angle will be larger.

17:19.150 --> 17:25.110
Now the adjustments to these vectors is not only about changing the angles.

17:25.430 --> 17:31.870
That is to say, it's not the case that the vectors necessarily stay exactly the same length, they

17:31.870 --> 17:33.190
just rotate a little bit.

17:33.590 --> 17:39.590
The attention and MLP adjustments can also change the length of the vectors.

17:39.590 --> 17:46.390
They can get shorter, they can get longer, and the angle does actually not really capture those changes

17:46.390 --> 17:47.230
in lengths.

17:47.350 --> 17:55.110
For example, let's imagine that a transformer block decides that a vector needs to be a little bit

17:55.110 --> 17:57.870
shorter, but pointed in the same direction.

17:58.750 --> 18:05.030
In that case, there's no rotation, so the angle is going to be zero where the whereas the length of

18:05.070 --> 18:07.430
that vector might actually have changed a lot.

18:07.630 --> 18:12.790
So that is something that we would not be able to detect with this analysis.

18:12.790 --> 18:18.270
But we will be able to detect it by measuring the change in the length of the vector.

18:18.430 --> 18:21.190
That's what we're going to do in a couple of videos for now.

18:21.230 --> 18:22.430
Later in this section.

18:22.670 --> 18:22.950
Okay.

18:22.990 --> 18:27.590
Anyway, I just wanted to discuss the interpretation of this result a little bit more.

18:28.030 --> 18:35.830
So this is for the target words here I have exactly the same analysis, but this is for all of the tokens

18:35.990 --> 18:37.950
Preceding the target word.

18:38.310 --> 18:45.350
Now, this is pretty interesting because this is exactly the same token in every single sentence.

18:45.350 --> 18:47.910
By definition, it's always the word her.

18:48.270 --> 18:53.110
But the word preceding her is different in every sentence.

18:53.270 --> 18:58.070
So you know here it's introduced, informed, married and so on.

18:58.470 --> 19:04.550
So in this case it's still the same token across the different layers.

19:04.710 --> 19:09.230
But each sentence is going to have a different word here.

19:09.630 --> 19:10.070
Okay.

19:10.470 --> 19:10.710
Yeah.

19:10.750 --> 19:12.670
And so those are the same.

19:12.790 --> 19:15.350
These are the same but it's different for each sentence.

19:15.430 --> 19:22.510
And then what I've done here is completely shuffle to get random vectors mapping onto random other vectors.

19:22.790 --> 19:29.750
Now this result doesn't really it's not really interpretable because I'm picking random layers and random

19:29.750 --> 19:36.300
pairs of tokens, but it's going to provide an interesting comparison to Harrison to see what is the

19:36.300 --> 19:40.220
angle difference between randomly selected embeddings vectors.

19:40.500 --> 19:45.860
So what I do is from the hidden states I take a random layer.

19:45.860 --> 19:47.500
So I'm not just using this layer.

19:47.500 --> 19:55.020
I'm literally picking a random layer excluding the embeddings, but any random transformer layer.

19:55.420 --> 19:59.020
And then I'm also picking any random token.

19:59.220 --> 20:06.300
So any random token from anywhere in the sentence and any random layer from anywhere in the model completely

20:06.300 --> 20:06.860
random.

20:06.860 --> 20:12.340
We do not expect any sensible results between these two vectors.

20:12.380 --> 20:19.180
It's just a couple of randomly picked vectors, so it's going to be interesting to compare the magnitude

20:19.180 --> 20:23.860
of these vectors that we see with the magnitude of vectors that we see here.

20:24.380 --> 20:24.820
Okay.

20:25.780 --> 20:27.340
So I can run that analysis.

20:27.340 --> 20:29.060
This actually goes really fast.

20:29.380 --> 20:32.900
Uh, because uh, yeah it can all be done on the CPU.

20:32.940 --> 20:34.730
It's just some calculations.

20:35.170 --> 20:35.650
Okay.

20:35.810 --> 20:37.690
And then let's see.

20:37.690 --> 20:42.890
So the sizes of this matrix angles is 48 for the transformer blocks.

20:43.290 --> 20:48.050
And then 54 sentences and three of these analyses.

20:48.050 --> 20:50.650
So target non-target and shuffled.

20:51.090 --> 20:53.930
This is to convert from radians to angles.

20:53.930 --> 20:56.130
I'm not going to run that just yet.

20:56.170 --> 20:58.570
I want to show you the results first.

20:59.050 --> 21:01.010
So here you see the results.

21:01.010 --> 21:02.490
This is what I showed in the slides.

21:02.490 --> 21:05.090
Now let me uncomment this.

21:05.810 --> 21:11.890
So now we have also the completely shuffled angles randomly selected angles.

21:11.890 --> 21:13.490
And you don't even see the average.

21:13.490 --> 21:14.850
It's so much higher here.

21:15.010 --> 21:15.850
So let me see.

21:15.850 --> 21:21.770
I have to get rid of this uh y axis limit here.

21:22.370 --> 21:22.690
Okay.

21:22.730 --> 21:28.490
So we see that, uh, these randomly selected angles are much higher.

21:28.490 --> 21:34.370
So in radians we get around 1.3, 31.4 or so radians.

21:34.370 --> 21:37.050
That's actually pretty close to pi over two.

21:37.250 --> 21:39.410
You can also see that these are just wild.

21:39.450 --> 21:40.970
And this is not really surprising.

21:41.010 --> 21:43.690
I mean these vectors are just pointing all over the place.

21:43.690 --> 21:47.290
They're randomly selected pairs of tokens.

21:47.490 --> 21:53.290
And one thing I wanted to point out here is just the way I set up this analysis, maybe a little bit

21:53.290 --> 21:55.850
lazy, but it is actually possible.

21:55.890 --> 21:58.290
Let me show you this without me.

21:58.450 --> 21:59.530
So here we get.

21:59.570 --> 22:01.650
We're probably going to get a couple up.

22:01.690 --> 22:03.530
We didn't get any missing data points.

22:03.570 --> 22:04.690
Let me try this again.

22:04.930 --> 22:10.930
Sometimes when you run this analysis, you will get missing data points in the green lines.

22:11.050 --> 22:12.850
And that happens because.

22:13.250 --> 22:13.450
Okay.

22:13.490 --> 22:15.250
Here we go randomly.

22:15.370 --> 22:21.450
It can happen that you get these two in the shuffled and the randomly selected pairs.

22:21.450 --> 22:25.450
These two can be exactly the same vectors just by chance.

22:25.810 --> 22:30.130
And when that happens we just run into some numerical problems.

22:30.130 --> 22:32.040
We get some Nan values.

22:32.040 --> 22:36.240
And so therefore I just take the Nan mean.

22:36.640 --> 22:40.280
So next I'm going to statistically evaluate.

22:40.320 --> 22:43.240
Let me go back to this visualization.

22:43.400 --> 22:50.560
So yeah here when we look at this result it seems like the uh the non-targets have a higher angle than

22:50.560 --> 22:52.440
the targets early on.

22:52.440 --> 22:55.640
And that, uh, pattern flips later on.

22:56.040 --> 23:00.160
So then I did a statistical analysis to evaluate that.

23:00.280 --> 23:07.920
So I grabbed all of the angles for, uh, zero and one that corresponds to the target tokens and the

23:07.960 --> 23:09.200
non-target tokens.

23:09.200 --> 23:16.240
I'm not doing any analyses on the randomly selected pairs, because I don't think it makes sense to

23:16.240 --> 23:16.840
do that.

23:17.080 --> 23:18.720
Uh, statistical analysis.

23:19.200 --> 23:19.480
Okay.

23:19.520 --> 23:25.840
And then, yeah, I'm using a threshold of 0.05 divided by the number of hidden layers.

23:25.840 --> 23:33.240
So basically it's Bonferroni corrected p values Because we are doing so many comparisons here, and

23:33.240 --> 23:36.520
these are the results that I showed in the slides.

23:36.680 --> 23:36.880
Okay.

23:36.960 --> 23:44.800
The last thing I want to show here is changing all of these angles from radians to degrees.

23:44.840 --> 23:46.280
This should say degrees here.

23:46.920 --> 23:52.080
Now, it probably would be better if I would create a new variable and call this like degrees.

23:52.440 --> 23:56.560
But then I would need to do new plotting code and I didn't feel like doing that.

23:56.720 --> 24:01.320
So now I've overwritten all of these results.

24:01.320 --> 24:02.080
But that's fine.

24:02.080 --> 24:04.360
This takes literally one second to run.

24:04.800 --> 24:09.600
Okay, so now we have data in angles instead of in radians.

24:09.720 --> 24:17.720
And the whole point of me doing that is that it's just a little bit easier for a lot of people to interpret.

24:18.160 --> 24:25.440
So we see that the change, the difference in angle between these randomly selected pairs is somewhere

24:25.440 --> 24:34.160
around 70 to 80 Degrees and remember that 90 degrees would correspond to exactly orthogonal.

24:34.800 --> 24:37.120
Completely orthogonal vectors.

24:37.280 --> 24:39.400
Now in a high dimensional space.

24:39.600 --> 24:43.760
Random vectors are likely to be orthogonal to each other.

24:44.120 --> 24:45.520
So the fact that this.

24:45.760 --> 24:51.360
These random pairs are almost but not quite orthogonal, they are a little bit correlated with each

24:51.360 --> 24:51.760
other.

24:52.160 --> 25:01.760
That actually is consistent with the bias towards positive cosine similarities in the GPT two tokenizer

25:01.960 --> 25:05.280
that we discovered very early in the course.

25:05.640 --> 25:05.880
Okay.

25:05.920 --> 25:09.240
And otherwise you see that the target and non-target.

25:09.240 --> 25:16.080
So when we use exactly the same token, looking at the same token before versus after each transformer,

25:16.440 --> 25:19.680
then the rotations are somewhere around ten degrees.

25:20.040 --> 25:25.960
And again also interesting to see in the very beginning going from the initial embeddings.

25:26.160 --> 25:31.510
So the token plus the position embeddings to the first transformer block there.

25:31.550 --> 25:35.750
The change is almost 90 degrees, so it's a major rotation.

25:35.870 --> 25:43.310
That's the first moment in the model where the context, the surrounding context, is loaded in and

25:43.310 --> 25:48.190
modulates the angle of the embeddings vector for each token.

25:48.190 --> 25:50.350
That's a pretty significant rotation there.

25:51.630 --> 26:00.470
I've mentioned before that the field of mechanistic interpretability of LL is still fairly new, which

26:00.470 --> 26:07.750
is a little bit frustrating sometimes, because it means that a lot of the interpretations are speculative

26:07.750 --> 26:13.590
and going to change over time as new theories develop, a new research comes out.

26:13.950 --> 26:21.150
But the nation see of this field is also exciting, because it means that a lot of work remains to be

26:21.150 --> 26:27.260
done, and a lot of concepts that you might have learned about in linear algebra or machine learning

26:27.260 --> 26:30.740
or calculus can be relevant here for understanding.

26:32.260 --> 26:39.060
Now, one of the situations where you see this phenomenon is that sometimes we do analyses where it

26:39.060 --> 26:47.260
seems like the representations inside the model transition smoothly and slowly across the transformer

26:47.260 --> 26:55.700
blocks, whereas other analyses seem to suggest that there are sudden discontinuous jumps in processing.

26:56.260 --> 27:04.100
Of course, both of those statements can be true because these are such complex systems and they implement

27:04.100 --> 27:09.100
many calculations at the same time and distributed in different ways.

27:09.860 --> 27:16.060
Anyway, I hope you are looking forward to the code challenge in the next video where you will follow

27:16.060 --> 27:24.460
up on these ideas by calculating angles not just between layers, but between tokens within each sentence.

WEBVTT

00:02.120 --> 00:07.040
Laminar evolution of sequential angular adjustments.

00:07.040 --> 00:10.560
Wow, that is quite a mouthful of a title.

00:11.080 --> 00:17.680
I hope you find the actual code challenge to be more interesting and fun and thought provoking than

00:17.680 --> 00:19.920
the very long and dry title.

00:20.560 --> 00:27.160
There's actually only two real exercises in this code challenge, plus an initial one to set things

00:27.160 --> 00:27.600
up.

00:28.040 --> 00:35.040
It's also quite similar to the previous video in terms of the math, the analyses, the code, and the

00:35.040 --> 00:43.040
visualizations, and you're going to get some curious and weird patterns of results in exercise two.

00:43.480 --> 00:50.080
And then the purpose of exercise three is basically just to figure out what is going on with that weird

00:50.080 --> 00:50.720
pattern.

00:51.160 --> 00:54.560
Anyway, I think you're going to enjoy this code challenge.

00:54.720 --> 00:55.840
So let's begin.

00:57.000 --> 01:04.260
Exercise one just involves copying and modifying some code that you've used in the previous video.

01:04.660 --> 01:13.420
So import GPT two Excel copy over all of the her sentences that we worked with in the previous video,

01:13.740 --> 01:19.820
tokenize them, push them through, get the hidden states, activations, etc., etc. you can do this

01:19.820 --> 01:21.780
whole code challenge on the CPU.

01:22.020 --> 01:24.980
There's no need to access a GPU here.

01:25.740 --> 01:32.780
This exercise can be almost entirely solved just by blindly copying code from the previous video.

01:33.020 --> 01:39.540
So I will now switch to Python, but only very briefly, just to run a few lines of code and make sure

01:39.540 --> 01:40.860
we're on the same page.

01:41.860 --> 01:43.860
So yeah, import some libraries.

01:44.100 --> 01:46.940
I'm just not importing the stats library here.

01:46.940 --> 01:50.660
Otherwise it's similar to the previous exercise.

01:50.660 --> 01:57.060
Here are the previous video here I'm importing GPT two and the text here.

01:57.340 --> 01:59.360
This is all copied from before.

01:59.400 --> 01:59.760
Okay.

01:59.800 --> 02:01.520
So that is exercise one.

02:01.760 --> 02:07.240
It's still going to take several minutes for this to load but that's fine.

02:08.080 --> 02:12.360
Now for the main exercise in this code challenge.

02:12.920 --> 02:19.920
In the previous video, I showed you how to calculate the angle between two vectors that correspond

02:19.960 --> 02:25.320
to the same token, but passed in between transformer blocks.

02:25.680 --> 02:34.640
And now in this exercise, you're going to apply a similar analysis except it's between successive tokens

02:34.760 --> 02:36.320
within the same layer.

02:36.880 --> 02:40.200
And then you'll repeat that analysis for all of the layers.

02:40.720 --> 02:48.560
The question we want to investigate here is whether the token embeddings vectors for the target relative

02:48.560 --> 02:57.240
to its preceding token has the a similar change compared to the two other tokens that are randomly selected

02:57.240 --> 02:58.890
from the same sentence.

02:59.450 --> 02:59.730
Okay.

02:59.770 --> 03:03.130
So let me describe what you're going to do in detail.

03:04.090 --> 03:06.770
So this is one of the sentences that we have here.

03:07.170 --> 03:10.690
And of course her is the target word.

03:10.850 --> 03:16.450
And in this sentence promoted is the token preceding the target.

03:17.010 --> 03:25.530
So now you want to calculate the angle change between the vector for promoted and the vector for her.

03:26.050 --> 03:29.530
That's going to give you one number for this sentence.

03:29.890 --> 03:33.130
And then there are 54 sentences in total.

03:33.410 --> 03:39.330
So you get these angle changes for each of those 54 sentences.

03:39.970 --> 03:43.050
And then of course there are 48 transformer blocks.

03:43.210 --> 03:48.970
So you want to repeat that analysis for all 48 transformer blocks all the hidden layers.

03:50.130 --> 03:54.370
Now all of the individual angles you can show using thin lines.

03:54.370 --> 04:02.790
And then you can average over all of the 54 sentences using a thick line very similar to the visualization

04:02.790 --> 04:05.590
in the previous video.

04:06.030 --> 04:09.870
And that's what this red line depicts over here.

04:10.230 --> 04:14.750
So that's for the target to get the non-target sequence.

04:14.750 --> 04:22.950
The blue lines here you want to pick randomly any two tokens from this sentence that are not the target.

04:23.390 --> 04:26.390
And then you do that randomly for each sentence.

04:26.390 --> 04:35.030
So each time it's going to be a different pair of randomly selected tokens from the sentence but within

04:35.030 --> 04:35.750
the same layer.

04:35.750 --> 04:41.110
So not completely shuffling across layers the way I did in the previous video.

04:41.710 --> 04:48.430
So then you want to sort the two randomly selected tokens and then calculate their angle change.

04:48.590 --> 04:57.450
So for example you might get the two tokens the and last or maybe you get manager and promoted.

04:57.770 --> 05:00.210
Or maybe its promoted and weak.

05:00.210 --> 05:06.810
So any two randomly selected tokens from this sentence, and then you sort them so that you're always

05:06.810 --> 05:12.690
calculating the angle of the earlier token to the angle of the later token.

05:13.210 --> 05:19.130
And then each sentence you get a new random pair of non-target tokens.

05:19.290 --> 05:26.730
Again, you can pick any of the tokens in a sentence, but just exclude the token for the target.

05:27.850 --> 05:29.050
Okay, hope that makes sense.

05:29.050 --> 05:31.890
That is where the blue line here comes from.

05:32.210 --> 05:37.090
Now when you get this result, this is going to give you a weird looking pattern.

05:37.610 --> 05:44.450
The purpose of exercise three is to figure out what is going on with that weird pattern, what it means,

05:44.450 --> 05:45.770
where it comes from.

05:46.090 --> 05:52.010
But before getting to exercise three, I would like you to think about it.

05:52.130 --> 05:59.270
That's what it says here and come up with some Hypotheses of what you think might be causing that strange

05:59.270 --> 06:00.750
pattern of results.

06:01.150 --> 06:09.470
I would also like you to notice the magnitude of the angle changes here in this exercise, compared

06:09.470 --> 06:12.590
to what we observed in the previous video.

06:14.070 --> 06:16.830
I hope you enjoy working through this code challenge.

06:16.950 --> 06:20.430
And now I will switch to Python and discuss my code.

06:20.550 --> 06:27.590
And that will lead us into exercise three, where we solve the mystery of the strange patterns of results.

06:28.390 --> 06:34.190
Here is the main for loop for this analysis, where I loop over all of the sentences.

06:34.790 --> 06:37.110
These two lines of code you've seen before.

06:37.350 --> 06:41.270
This is where I'm getting the target index for the word her.

06:41.310 --> 06:42.950
So the actual target token.

06:43.230 --> 06:47.110
And then here what I do is randomly choose a.

06:47.110 --> 06:49.710
So let me start by working my way from inside.

06:49.870 --> 06:50.190
Okay.

06:50.230 --> 06:53.430
So this these are the tokens in this sentence.

06:53.550 --> 07:01.200
And then I want to know where are the tokens not or which tokens are not equal to the target token.

07:01.320 --> 07:05.600
So this is the way that I can exclude the target token.

07:05.760 --> 07:12.400
And then I find those and I randomly pick two of those without replacement.

07:12.600 --> 07:18.960
That just makes sure that I'm not going to accidentally select the same exact token twice, which can

07:18.960 --> 07:20.080
happen by chance.

07:20.080 --> 07:22.360
If we had replace equals true.

07:23.000 --> 07:23.280
Okay.

07:23.320 --> 07:27.960
And then I am sorting those to make sure that the earlier token is first.

07:27.960 --> 07:30.200
And then we get to the second token.

07:30.240 --> 07:31.200
The later token.

07:31.360 --> 07:31.640
Okay.

07:31.680 --> 07:34.840
So now we have the target index and we have two.

07:35.520 --> 07:41.080
This is going to be a two element numpy array for non-target indices.

07:42.160 --> 07:44.520
Now I can loop over all of the layers.

07:44.520 --> 07:48.400
Now this code you've seen mostly before in the previous video.

07:48.440 --> 07:49.680
It should look familiar.

07:49.920 --> 07:53.660
What's different is that in the previous Video.

07:53.700 --> 07:55.460
The code looked like this.

07:55.740 --> 08:01.980
So each layer the same target index relative to the previous layer.

08:02.180 --> 08:10.260
Now what I'm doing is looking inside the same layer, but I have the target word relative to the previous

08:10.300 --> 08:11.140
token.

08:11.500 --> 08:17.500
So that is only a very small change in the code from the previous video.

08:17.500 --> 08:23.140
But it's a pretty significant change because in the previous video, we were always looking at the change

08:23.300 --> 08:26.820
in the angle for exactly the same token.

08:27.060 --> 08:31.900
So it was always the same word, same token, just rotating a little bit.

08:32.220 --> 08:34.420
But here it's always going to be different.

08:34.420 --> 08:34.660
Right.

08:34.660 --> 08:43.180
Because each target relative to its previous here that's thanked to her here it's admired to her nominated

08:43.180 --> 08:44.100
to her and so on.

08:44.100 --> 08:50.500
So every sentence has the same target token but a different non-target token.

08:51.540 --> 08:51.820
Okay.

08:51.860 --> 09:00.120
But that's okay for comparison because for the Non-targets we have two also randomly selected target

09:00.640 --> 09:04.840
non-target tokens that are also going to be different from each other.

09:05.360 --> 09:05.640
Okay.

09:05.680 --> 09:07.520
So now I can run that analysis.

09:07.520 --> 09:10.080
Also super fast takes less than a second.

09:10.680 --> 09:12.520
Now I can do some plotting.

09:12.880 --> 09:19.160
And this is also a very, very similar code to what I showed in the previous video.

09:19.560 --> 09:22.560
Okay, so here is where we get the weird findings.

09:22.680 --> 09:28.480
First of all, I will point out the actual magnitude of the angle differences.

09:28.600 --> 09:33.640
Remember in the previous video these were around .1.2.

09:33.880 --> 09:41.040
So basically an order of magnitude smaller compared to the size of the results that we see here.

09:41.480 --> 09:43.240
Now that should not be surprising.

09:43.240 --> 09:47.040
These are two different words that we are looking at the angle between.

09:47.240 --> 09:54.580
Whereas in the previous video it was the same token the same word and it was just being modified.

09:54.580 --> 09:58.980
So yeah, now we see much bigger angles of rotation relatively.

09:59.300 --> 10:01.260
It also looks like the spread.

10:01.260 --> 10:07.220
So the variability is increasing as we get later into the model.

10:07.220 --> 10:09.340
So some angles are increasing.

10:09.340 --> 10:10.980
Some angles are decreasing.

10:11.260 --> 10:17.060
That means that some of these successive tokens are getting closer together and sometimes they're getting

10:17.060 --> 10:17.980
further apart.

10:18.380 --> 10:25.540
Now if you wanted a more precise interpretation, what you would have to do is basically split this

10:25.540 --> 10:32.220
down the middle and try to find which are the sentences where it's increasing, which are the sentences

10:32.220 --> 10:34.580
where the angles are decreasing.

10:34.580 --> 10:36.420
So they're getting closer together.

10:36.420 --> 10:42.020
And then you could look at those sentences and see if you see some consistency in the results.

10:42.180 --> 10:47.860
Anyway, that is just a general suggestion that is not part of this code challenge.

10:48.260 --> 10:55.430
Obviously, the most striking and curious result is that the the non-targets are split into these two

10:55.430 --> 10:58.230
distributions, where some of them go up.

10:58.230 --> 11:05.230
We have really large angle discrepancies, and some of them seem to be lower and basically the same

11:05.270 --> 11:10.150
magnitude of change relative or compared with the targets.

11:10.350 --> 11:14.390
So then that also means that the average is right in the middle.

11:14.710 --> 11:22.510
This is one of these classic statistics situations where the mean does not actually indicate the central

11:22.510 --> 11:29.510
tendency of the distribution, because the distribution is so perfectly split between high values and

11:29.510 --> 11:30.470
low values.

11:31.470 --> 11:33.110
And what is going on here?

11:33.110 --> 11:37.670
That is the mystery of the bimodally distributed angles.

11:37.830 --> 11:40.910
I am not going to answer the question for you here.

11:40.950 --> 11:45.030
I'm not going to tell you how to resolve this mystery just yet.

11:45.070 --> 11:47.390
That comes up in the next exercise.

11:48.390 --> 11:50.850
Now to To reveal the mystery.

11:51.330 --> 11:54.810
And actually, I'm not going to tell you the reveal.

11:54.970 --> 11:57.530
I'm just going to tell you an analysis.

11:57.570 --> 11:58.250
To do.

11:58.530 --> 12:03.010
And then you will discover the solution when you run this analysis.

12:03.610 --> 12:04.650
So it's very simple.

12:04.650 --> 12:13.050
Just copy paste and modify the code from exercise two so that you calculate the vector angle differences

12:13.210 --> 12:15.570
between successive pairs.

12:15.890 --> 12:23.050
In particular, the first token relative to the second token, the second token relative to the third

12:23.090 --> 12:24.650
token, and so on.

12:24.690 --> 12:26.210
Of course these are indices here.

12:26.210 --> 12:29.290
So zero means the first token in the sentence.

12:30.010 --> 12:34.570
Now here in this legend I'm showing the pairs of indices in each color.

12:34.970 --> 12:40.290
And just to be clear this analysis has nothing to do with the the targets.

12:40.290 --> 12:42.010
So we're not working with targets anymore.

12:42.050 --> 12:43.570
You don't need to worry about that.

12:43.690 --> 12:51.150
This is really just about the first four pairs of successive tokens in each sentence.

12:51.630 --> 12:58.590
Now, I think that when you implement this analysis and see this graph, you will figure out the mystery

12:58.590 --> 13:00.270
from exercise two.

13:01.550 --> 13:07.750
Oh, and also I would like you to transform these angles from radians into degrees.

13:08.230 --> 13:12.870
That is just to give you a little bit more experience with the transformation there.

13:13.790 --> 13:17.910
I hope you enjoy this exercise and find it elucidating.

13:18.110 --> 13:20.790
Please pause the video now and switch to code.

13:20.790 --> 13:24.910
And now I will switch to Python and continue this discussion.

13:26.110 --> 13:30.350
Here I loop over sentences and then I loop over layers.

13:30.470 --> 13:34.670
And now here I'm looping over tokens for token pairs.

13:34.830 --> 13:36.950
So this is hard coded to four.

13:36.950 --> 13:37.870
But that's fine.

13:38.150 --> 13:42.590
So here are the vectors that we want always from the same layer always from the same sentence.

13:42.710 --> 13:45.910
And here I'm just getting the token I index.

13:45.910 --> 13:50.650
And then plus one and then calculating the angle between them.

13:50.810 --> 13:53.450
Now this gives me an angle in radians.

13:53.450 --> 13:59.330
So then to convert to degrees we just have to multiply by 180 over pi.

13:59.890 --> 14:00.210
Okay.

14:00.250 --> 14:02.570
So run that analysis.

14:02.690 --> 14:04.770
And now I can generate that plot.

14:04.930 --> 14:07.530
And this resolves the mystery.

14:07.810 --> 14:14.730
So the weird thing that's happening is that basically all of this weird stuff up here that we saw up

14:14.730 --> 14:23.330
here, all of these really high angles here, are randomly selected pairs where the first pair was the

14:23.370 --> 14:27.250
very first token in the sentence.

14:27.370 --> 14:36.530
And this is where both randomly selected pairs of both randomly selected tokens in the pair were not

14:36.770 --> 14:38.010
the first token.

14:38.890 --> 14:39.290
Okay.

14:39.530 --> 14:44.970
So and then basically what's happening is that the very first token the model knows nothing.

14:44.970 --> 14:46.250
It has no context.

14:46.250 --> 14:47.580
It doesn't know what to do.

14:47.860 --> 14:54.660
And then you finally get to another token, and now the model can start importing some contacts.

14:54.660 --> 15:01.700
It's not a huge amount of context, but it is some context, and therefore the angle changes a lot.

15:01.740 --> 15:08.260
So these vectors really rotate a lot in the very beginning after the first token.

15:08.500 --> 15:12.340
And then after that you see the adjustments are relatively smaller.

15:12.500 --> 15:18.860
Looks like they are decreasing with each successive pair, although I'm not going to statistically analyze

15:18.860 --> 15:19.300
that.

15:19.620 --> 15:19.980
Okay.

15:20.020 --> 15:21.220
So that was pretty interesting.

15:21.220 --> 15:23.900
I hope you enjoyed making that discovery.

15:25.100 --> 15:33.980
To be honest, I don't think that the angles between sequential token embeddings vectors is as insightful

15:33.980 --> 15:40.300
as tracking the same token across different layers, like what I showed in the previous video.

15:40.860 --> 15:47.400
That's mainly because the sequential pairs of tokens in this code challenge We're all different from

15:47.400 --> 15:48.040
each other.

15:48.480 --> 15:53.760
I could imagine this kind of approach being more useful and more elucidating.

15:53.960 --> 15:57.880
If the two tokens were the same, but the context was different.

15:58.480 --> 16:05.560
Still, I wanted to include this video to make sure that you see a different application of the vector

16:05.560 --> 16:08.280
angle difference analysis approach.

16:09.440 --> 16:16.120
Another important take home from this video is yet another demonstration that the first token in the

16:16.120 --> 16:20.800
sequence is processed very differently from the rest of the tokens.

16:21.280 --> 16:28.960
These models really need a couple of tokens to like load in a relevant context, so you should try to

16:29.000 --> 16:34.520
avoid making bold interpretations or even including in your analyses.

16:34.960 --> 16:42.960
The first 1 or 2 tokens from each sequence, you can see those first tokens as like the model warming

16:42.960 --> 16:44.200
up its engines.

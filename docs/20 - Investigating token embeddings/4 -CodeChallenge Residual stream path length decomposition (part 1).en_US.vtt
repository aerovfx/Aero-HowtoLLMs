WEBVTT

00:02.400 --> 00:08.920
Welcome to this code challenge on path lengths and transformer adjustments.

00:09.440 --> 00:16.400
I think that this code challenge is a really nice integration of several concepts from earlier in the

00:16.400 --> 00:23.200
course, including calculating path length from the previous video, and also concepts about how the

00:23.200 --> 00:31.920
attention and MLP subblocks contribute to the output of each transformer block, which we measure through

00:31.920 --> 00:33.520
the hidden states.

00:34.520 --> 00:39.760
Exercise one involves just setting things up for the rest of the code challenge.

00:40.320 --> 00:43.240
So import GPT two large.

00:43.520 --> 00:45.520
Remember that this one.

00:45.520 --> 00:55.840
This model version has 36 transformer blocks, and it also has an embedding dimension of 1280.

00:56.480 --> 01:03.850
Now, unlike in the previous video, We are not going to worry about replicating the findings here in

01:03.850 --> 01:10.490
other models, although of course that would be a fantastic extension if you want to do that on your

01:10.490 --> 01:10.930
own.

01:11.530 --> 01:20.970
What I want you to do here is implant hooks into the outputs of the attention and MLP subblocks in the

01:20.970 --> 01:27.210
OpenAI model terminology that corresponds to these C proj layers here.

01:27.290 --> 01:32.050
So there's a C proj for attention and a C proj for MLP.

01:33.250 --> 01:39.490
And remember that these are the outputs of the attention and MLP transformations.

01:39.690 --> 01:44.730
After all of the internal calculations for the adjustments have been made.

01:44.970 --> 01:50.050
But before they actually get added back onto the residual stream.

01:50.650 --> 01:58.860
And that means that these vectors that you get from C proj are not the token embeddings vectors themselves,

01:59.140 --> 02:06.100
but instead are the adjustments to the token embeddings vectors that are passing along the residual

02:06.100 --> 02:06.660
stream.

02:07.420 --> 02:15.940
Anyway, once you create an implant those hooks, then you can paste in and tokenize the Nietzsche text

02:16.020 --> 02:18.540
that I used in the previous video.

02:19.180 --> 02:24.940
And then you can do a forward pass through the model to get the activations from the hooks.

02:24.980 --> 02:28.140
And you also want to get the hidden states outputs.

02:28.620 --> 02:35.060
Finally, you can have a look through the activations in terms of the names of the activations from

02:35.060 --> 02:40.140
the hooks and also the sizes of the activation matrices.

02:40.340 --> 02:45.340
This is just to make sure that these sizes are consistent with what you expect.

02:45.740 --> 02:51.260
And also, yeah, this will just help you make sure you don't have any coding bugs or anything that

02:51.260 --> 02:58.030
would prevent you from accessing and analyzing the Using the data in later exercises.

02:58.830 --> 03:06.630
So now is your opportunity to pause the video, copy, paste, modify code, and solve exercise one.

03:07.150 --> 03:12.270
I will now switch to code, but only fairly briefly just to show the updated code.

03:13.110 --> 03:22.750
In a later exercise in exercise four, we will use the Pearson R correlation function from the Scipy.stats

03:22.790 --> 03:29.190
library, and we will also use the FDR correction function from Statsmodels.

03:29.630 --> 03:33.590
This will be used to correct for multiple comparisons.

03:33.750 --> 03:36.990
Anyway, I will get back to these two functions later.

03:37.390 --> 03:40.070
So otherwise these libraries you've seen before.

03:40.390 --> 03:43.470
Here I'm importing GPT two large.

03:43.990 --> 03:49.190
Here I'm defining and then implanting the hooks for attention.

03:49.230 --> 03:51.270
Proj and MLP proj.

03:51.270 --> 03:55.440
You can see I'm attaching those to the C proj layers.

03:55.440 --> 04:01.560
So again this is not the qkv matrices and the softmax and so on.

04:01.600 --> 04:05.440
This is the final output of the attention subblock.

04:05.640 --> 04:10.600
But before it gets added back onto the residual stream.

04:10.720 --> 04:13.120
And same thing for the MLP layer.

04:13.160 --> 04:16.200
This is not the expansion and then contraction.

04:16.200 --> 04:20.840
This is what happens after the MLP calculations are finished.

04:20.840 --> 04:26.920
And we are yeah this is the adjustment to the embeddings vectors.

04:27.440 --> 04:27.840
Okay.

04:27.880 --> 04:30.120
So uh run that code.

04:30.320 --> 04:34.680
Here is the text literally copy pasted from the previous video.

04:34.760 --> 04:38.120
And we're getting the output uh, hidden states here.

04:38.600 --> 04:38.920
Okay.

04:38.960 --> 04:42.960
And then yeah, this is just, uh, confirming some of the sizes.

04:43.160 --> 04:46.080
So there's 342 tokens, I think.

04:46.120 --> 04:46.480
Yeah.

04:46.760 --> 04:48.920
Uh, one stream of text.

04:48.920 --> 04:54.730
So batch size of one and 180 embeddings dimensions in this model.

04:55.970 --> 05:06.050
Now for some analyses, the goal of exercise two is to calculate and visualize the cosine similarities

05:06.050 --> 05:09.730
between the attention and MLP adjustments.

05:10.010 --> 05:14.610
Now we're not doing any path lengths or angles or anything like that just yet.

05:14.930 --> 05:23.330
Here in this exercise, we just want to make a basic assessment of how the attention and the MLP adjustments

05:23.370 --> 05:26.370
are related to each other per token.

05:26.970 --> 05:35.810
So loop over all of the tokens in the text and over all of the layers the transformer blocks.

05:36.210 --> 05:42.970
And then for each token get the attention and MLP token vector adjustments.

05:43.250 --> 05:49.250
So that means that for each token and each layer, that's going to give you one vector for each of the

05:49.250 --> 05:56.300
transformer block subparts, and each of those vectors is 1280 elements long.

05:56.340 --> 06:00.300
Of course, that corresponds to the embeddings dimensionality.

06:01.180 --> 06:09.580
Then you can calculate the cosine similarity between the attention and MLP adjustments vectors.

06:10.100 --> 06:13.900
You can plot those results in two ways as I'm showing here.

06:14.180 --> 06:17.060
So let me first describe this plot on the left.

06:17.460 --> 06:20.700
The x axis here is the transformer layer.

06:21.060 --> 06:27.380
The y axis is cosine similarity between the attention and MLP adjustments vectors.

06:28.380 --> 06:38.420
And this little gray box over here is the average of the cosine similarities over all of the 342 tokens.

06:38.620 --> 06:42.260
Because we have so many tokens to average over.

06:42.500 --> 06:49.360
You can also calculate the standard deviation of the cosine similarities over that set over all the

06:49.360 --> 06:49.960
tokens.

06:50.160 --> 06:53.400
And that's what these vertical lines here correspond to.

06:53.840 --> 06:55.160
So this is an error bar.

06:55.160 --> 06:57.360
That's the function that you use in Python.

06:57.520 --> 06:58.880
This is for one layer.

06:59.000 --> 07:03.520
And then you just show these error bar plots for all of the layers.

07:04.040 --> 07:11.600
You can already see from the y axis ticks that there's going to be more positive than negative cosine

07:11.600 --> 07:12.520
similarities.

07:12.720 --> 07:17.720
But it will be interesting to see whether they are consistently large and positive.

07:17.760 --> 07:21.920
Maybe they go down to zero or you know, some other pattern could happen.

07:22.480 --> 07:29.960
Now, the way to interpret this cosine similarity here is that if the cosine similarity is really high,

07:30.600 --> 07:37.240
then the attention and the MLP adjustment vectors are really similar across the tokens.

07:37.440 --> 07:46.000
In fact, if the cosine similarity were exactly one, then that would mean that the two subcomponents

07:46.000 --> 07:53.690
of the transformer block are entirely redundant because the attention and MLP layers will be adding

07:53.890 --> 07:59.450
exactly the same information back onto the original token embeddings vector.

07:59.930 --> 08:05.490
So we certainly don't expect the correlation to be one, and you don't even, you know, it's obvious

08:05.490 --> 08:06.850
from this plot here as well.

08:07.090 --> 08:13.530
But the question is what do what does the range of these cosine similarities look like empirically in

08:13.530 --> 08:14.410
this text?

08:14.850 --> 08:16.330
Okay, so that's for the left plot.

08:16.330 --> 08:24.450
Over here on the right, I have the histogram of all of the cosine similarity values over all the tokens

08:24.450 --> 08:25.810
and all the layers.

08:25.970 --> 08:30.730
So that's going to be 342 times 36 numbers.

08:30.890 --> 08:34.850
And here is the histogram of all of those numbers.

08:35.530 --> 08:37.810
So that's for exercise two.

08:37.850 --> 08:40.570
I hope you enjoy working through this exercise.

08:40.850 --> 08:43.690
You should now pause the video and get to work.

08:43.850 --> 08:48.900
And now I will switch to code, show my solution, and interpret the findings.

08:49.780 --> 08:52.900
Here I'm looping over all of the layers.

08:52.900 --> 08:57.540
Here I'm looping over all of the tokens in the text.

08:57.580 --> 09:04.940
Now, I suppose what I probably could be doing is ignoring the first token based on just claims I've

09:04.940 --> 09:07.100
made several times in this course.

09:07.140 --> 09:14.020
And what you will see in a couple of exercises from now that the first token is weird and not typical

09:14.020 --> 09:14.980
of all the other tokens.

09:14.980 --> 09:16.220
But anyway, this is.

09:16.260 --> 09:17.820
Yeah, this is just how I wrote the code.

09:18.260 --> 09:20.620
Okay, so loop over all the tokens.

09:20.620 --> 09:28.460
Here I am extracting the activations for attention and MLP from this layer and this token.

09:28.740 --> 09:33.140
And then I am calculating the cosine similarity between them.

09:33.460 --> 09:40.780
Now at various points in this course, I've calculated cosine similarity manually by directly implementing

09:40.780 --> 09:41.660
the formula.

09:42.020 --> 09:45.750
Sometimes I use the scikit learn implementation.

09:45.750 --> 09:48.470
Here I'm using the PyTorch implementation.

09:48.990 --> 09:55.190
If you do not have multi-dimensional tensors then this is going to crash.

09:55.310 --> 10:01.910
So if you try to do something like this, then these variables here, I can even run this code here.

10:02.270 --> 10:04.390
This is going to crash over here.

10:04.590 --> 10:11.870
And the problem is that PyTorch is expecting these to be multi-dimensional tensors.

10:11.870 --> 10:15.230
But they're actually just single vectors here.

10:15.590 --> 10:17.630
So that's really easy to solve.

10:17.670 --> 10:25.110
You can just unsqueeze it or basically add more dimensions to these vectors and then it's going to work

10:26.070 --> 10:26.350
okay.

10:26.390 --> 10:31.030
If you solve this by writing out the cosine similarity formula manually.

10:31.230 --> 10:32.190
Fantastic.

10:32.230 --> 10:33.230
That's really great.

10:33.630 --> 10:35.790
Okay here I'm showing the error bars.

10:35.950 --> 10:40.710
So these are the x values just indices going up to the number of layers.

10:41.160 --> 10:48.160
Here are the cosine similarities I'm averaging over x is one, which is the token.

10:48.160 --> 10:53.040
So it's going to be the average over all of the tokens for each layer.

10:53.400 --> 10:55.560
And then the standard deviation.

10:55.680 --> 10:56.160
Okay.

10:56.360 --> 11:01.360
And then uh let's see over here in the right plot I'm just showing the histogram.

11:01.360 --> 11:07.080
I'm basically just flattening out all of this matrix just to get all of those values in here.

11:07.480 --> 11:08.760
And here's what it looks like.

11:08.760 --> 11:17.280
So we see that the cosine similarity between the attention and the MLP sublayers is basically zero for

11:17.280 --> 11:19.960
most of the transformer block.

11:20.120 --> 11:23.960
Now I haven't done any statistical analyses here.

11:23.960 --> 11:27.400
If you would like to, uh, do that, that's great.

11:27.560 --> 11:29.040
But I haven't done that here.

11:29.040 --> 11:32.800
So I can only make qualitative interpretations.

11:32.800 --> 11:40.610
But it certainly is the case that, uh, except for the very beginning and the very end, Most of the

11:40.930 --> 11:47.530
adjustments that are calculated by the attention and the MLP subblocks are uncorrelated.

11:47.530 --> 11:51.850
They are orthogonal to each other, and that is actually pretty sensible.

11:51.850 --> 11:58.490
It means that the calculations engaged by the attention mechanism and the calculations engaged by the

11:58.530 --> 12:01.970
MLP sublayer are different from each other.

12:02.370 --> 12:04.970
And we also see that in the histogram over here.

12:04.970 --> 12:10.050
So most of the values are clustered around zero cosine similarity.

12:10.170 --> 12:12.290
And then we have this little shoulder up here.

12:12.290 --> 12:15.770
And that corresponds to the beginning and the end.

12:16.770 --> 12:20.210
I hope you are enjoying this code challenge so far.

12:20.650 --> 12:23.250
We're about halfway through the code challenge.

12:23.250 --> 12:28.690
There's another several exercises to go, so I recommend getting up from your chair.

12:28.890 --> 12:30.290
Splash some water in your face.

12:30.290 --> 12:33.210
Same thing that I'd say every time I cut these videos.

12:33.330 --> 12:36.050
Anyway, let's continue in the next video.

WEBVTT

00:01.960 --> 00:07.040
And now for the second part and conclusion of this code challenge.

00:07.560 --> 00:10.320
Let's continue with exercise four.

00:10.680 --> 00:15.880
There are six exercises in total, although the last one is like super simple and fast.

00:16.400 --> 00:25.520
Anyway, the goal of this exercise here is to create the trajectories in PC space and also plot the

00:25.520 --> 00:29.680
distances between the nouns and the adjectives.

00:30.240 --> 00:34.000
Now the middle and the right plots that you see here.

00:34.160 --> 00:37.400
You have also seen a couple of videos ago.

00:37.640 --> 00:39.800
Exactly, almost exactly the same.

00:39.840 --> 00:42.600
Of course, the labels and so on are a little bit different.

00:43.000 --> 00:48.200
So again, you can copy and paste and modify or write the code from scratch.

00:48.640 --> 00:56.920
Now this plot on the left I haven't shown you previously, but it's just plotting the projections onto

00:56.920 --> 01:03.610
the two principal components for nouns and adjectives over all of the layers.

01:04.130 --> 01:11.650
In fact, this middle plot here, these trajectories, that is literally the same data that you see

01:11.650 --> 01:13.210
in the left plot here.

01:13.450 --> 01:21.450
The only difference is that here the layer is plotted on the x axis, whereas here the layer dimension

01:21.450 --> 01:26.050
is plotted implicitly through the two principal components.

01:26.890 --> 01:27.250
Okay.

01:27.290 --> 01:32.970
So here the principal components projections are all on the y axis.

01:33.210 --> 01:42.130
And here in the middle plot I have the component one on the x axis and component two projection on the

01:42.170 --> 01:43.330
y axis.

01:44.050 --> 01:50.690
So it's quite striking how different the trajectories are for nouns and adjectives.

01:50.730 --> 01:58.070
That is quite different from what we saw two videos ago, where the pronouns and also the word round

01:58.110 --> 02:02.150
generally followed a similar trajectory through the state space.

02:02.310 --> 02:04.630
Of course, there were some differences.

02:04.910 --> 02:11.110
Now it is your turn to have a go at this, and now I will switch to code and discuss these results.

02:11.750 --> 02:21.550
Here I am calculating the average over all the 150 instances of the nouns and adjectives.

02:21.830 --> 02:26.910
The code is nearly identical to the code from a couple of videos ago.

02:27.070 --> 02:30.710
Of course, the variable names are a little bit different.

02:30.950 --> 02:36.150
Uh, and also, yeah, just working with the lookup table, if you're copying and pasting the code,

02:36.150 --> 02:42.870
you have to be mindful of that because you will remember in the previous video, uh, two videos ago,

02:43.070 --> 02:49.790
this element here in the lookup table actually corresponded to the sentence index, whereas here it

02:49.830 --> 02:52.630
corresponds to the layer index.

02:52.630 --> 02:56.480
And then yeah, here we have uh target one and target two.

02:56.680 --> 03:01.560
So these correspond to the nouns and these correspond to the adjectives.

03:02.120 --> 03:02.440
Okay.

03:02.480 --> 03:04.240
So I can run that code.

03:04.240 --> 03:05.720
And now I can run this code.

03:05.720 --> 03:13.760
This is also uh, all of this code is exactly what you have seen in the previous two videos ago.

03:13.960 --> 03:16.040
Uh, just with some minor modifications.

03:16.440 --> 03:19.760
This code up here is new, but it's not terribly difficult.

03:19.760 --> 03:23.360
In fact, it's literally the same thing that I'm plotting here.

03:23.680 --> 03:26.960
Except here I'm plotting PC1 by PC2.

03:27.000 --> 03:30.280
That's how you create these state space trajectories.

03:30.680 --> 03:37.880
And here I'm just plotting all of the PCs independently of each other, uh, the projections independently

03:37.880 --> 03:38.680
of each other.

03:38.680 --> 03:41.280
And implicit in here is the layers.

03:41.280 --> 03:47.160
So, you know, you could have something like this, uh, in, uh, layers like this, if you wanted

03:47.160 --> 03:50.880
to have an explicit x axis, uh, tick.

03:50.880 --> 03:53.040
But that's not really necessary.

03:53.040 --> 03:57.530
say, okay, so let's have a small discussion about this.

03:57.810 --> 04:04.610
The most striking result that pops out here is that the projections onto the first principal component

04:04.610 --> 04:10.410
are not identical, but quite similar for nouns and adjectives.

04:10.890 --> 04:16.050
So PC1 looks really similar and PC2 the projection onto PC2.

04:16.210 --> 04:21.610
That's where we really see a striking difference between nouns and adjectives.

04:21.810 --> 04:28.970
So here you see the data, the PC components plotted for over the different layers.

04:28.970 --> 04:31.690
And here you see the state space trajectories.

04:32.170 --> 04:38.250
Also, as you have seen before, there's something funky going on with the very last transformer block.

04:38.410 --> 04:38.610
Yeah.

04:38.650 --> 04:44.970
It's just a that's a fairly typical pattern for lots of ways of looking at the data.

04:46.090 --> 04:52.900
So but also yeah, the most striking thing here is that there's this really large Divergence between

04:52.900 --> 04:54.820
the nouns and the adjectives.

04:54.820 --> 05:00.380
They really just kind of go off into opposite signs in PC2.

05:00.820 --> 05:08.740
So this indicates that the model really has a pretty strong divergence in how these two categories of

05:08.740 --> 05:12.540
of grammar words are processed internally.

05:12.900 --> 05:19.020
And by the way, just a small note in case you're wondering whether this is just due to the arbitrariness

05:19.020 --> 05:25.700
of the sign in principal components analysis, then that actually cannot account for this, because

05:25.820 --> 05:30.500
the sign indeterminacy would flip the sign of the entire axis.

05:30.780 --> 05:37.740
So it could be possible to have the adjectives positive and the nouns negative, but it wouldn't flip

05:37.740 --> 05:40.660
just one condition, it would flip the entire axis.

05:41.100 --> 05:48.300
If you have no idea what I'm talking about, this is a important but nuanced detail of principal components

05:48.340 --> 05:49.100
analysis.

05:49.220 --> 05:55.560
Basically, that the signs are a little bit arbitrary, and there are some principled methods for figuring

05:55.560 --> 06:00.280
out whether the axis should be positive versus negative.

06:00.320 --> 06:02.400
Yeah, but don't don't worry about those details.

06:02.800 --> 06:03.080
Okay.

06:03.120 --> 06:07.520
And anyway here you see the diff distances between nouns and adjectives.

06:07.680 --> 06:14.400
So this is now for each individual layer I'm just looking at the distance between each successive point.

06:14.400 --> 06:17.520
So this point and this point for each layer.

06:18.560 --> 06:26.240
Well the title of this video is all about whether nouns or adjectives have longer trajectories.

06:26.440 --> 06:31.680
We have not yet approached that question, but that is the goal here.

06:31.760 --> 06:39.800
So what you will need to do for this exercise is calculate the distance between each successive point

06:39.800 --> 06:40.840
in this plot.

06:41.240 --> 06:45.840
Now, this is not a plot you need to create here for exercise five.

06:45.880 --> 06:49.970
It's literally the plot that you created in exercise four.

06:50.130 --> 06:52.890
I just put it here for illustration.

06:53.330 --> 07:00.530
So the distance metric that you calculated in the previous exercise, that was the distance between

07:00.570 --> 07:04.530
nouns and adjectives within each layer.

07:04.810 --> 07:10.250
But that is not what you want to calculate here for the trajectory path distance.

07:10.890 --> 07:17.890
Instead here you want to calculate the distance traveled by this particle going from layer to layer

07:17.930 --> 07:21.610
or this vector within each category.

07:21.770 --> 07:24.810
For each layer relative to the previous layer.

07:25.050 --> 07:30.090
Okay, so the idea is to imagine that these are like hiking trails.

07:30.090 --> 07:33.370
These are two hiking trails here, the Red trail and the Blue Trail.

07:33.610 --> 07:36.090
And you're walking along these hiking trails.

07:36.090 --> 07:40.890
And the question is which of these two trails is longer?

07:41.530 --> 07:47.620
So you calculate the Euclidean distance from each layer Relative to the previous layer.

07:47.740 --> 07:53.300
In other words, the Euclidean distance from each point to the next point in this trajectory.

07:53.860 --> 07:58.660
Now I decided to implement this analysis twice.

07:58.780 --> 08:00.940
I did it once in a for loop.

08:00.940 --> 08:03.140
So looping over all the layers.

08:03.420 --> 08:11.060
And I did that because it really helped me think about how the analysis works mechanistically, and

08:11.060 --> 08:18.100
also how the distance is calculated between each successive pair of layers within each part of speech

08:18.100 --> 08:18.860
category.

08:19.380 --> 08:25.220
But it turns out that you do not need for loops to run this calculation.

08:25.660 --> 08:30.660
You can actually do the whole thing vectorized without any loops.

08:31.180 --> 08:38.620
Now, my personal preference with coding is that I generally find for loops easier to understand.

08:38.740 --> 08:45.670
But of course vectorized code is more efficient and less prone to coding errors, and it's often faster

08:46.110 --> 08:49.190
anyway, as long as you get the right answer, it's fine.

08:49.710 --> 08:56.270
So these two plots here show the main results that you want to generate in exercise five.

08:57.390 --> 09:03.230
Here on the left we have the token distances over all of the layers.

09:03.390 --> 09:06.230
So you're going to have two lines in this plot.

09:06.270 --> 09:09.390
One for the nouns and one for the adjectives.

09:09.870 --> 09:17.790
And the y axis is the distance between each layer and the previous layer within the same category.

09:18.390 --> 09:25.550
Now these distances can get really big, especially towards the end of the path towards the end of the

09:25.550 --> 09:26.430
trajectory.

09:26.710 --> 09:33.910
So therefore, what I did and what I recommend doing is plotting the log of the distances instead of

09:33.910 --> 09:35.670
the linear distance.

09:36.190 --> 09:43.450
That just facilitates visual interpretation, as I've mentioned earlier in the course that log scaling

09:43.450 --> 09:50.290
is often done in machine learning and statistics, when there are really large differences between the

09:50.290 --> 09:52.530
numerical values in the plot.

09:53.010 --> 09:59.730
Anyway, once you have those two lines for nouns and adjectives, then you can just literally subtract

09:59.730 --> 10:04.250
the two layer by layer and show those results over here.

10:04.490 --> 10:11.690
So this is literally just the difference of the log distances for nouns versus adjectives.

10:12.170 --> 10:12.450
Okay.

10:12.490 --> 10:17.290
But importantly, you know, this exercise is not just about generating a plot.

10:17.570 --> 10:25.810
So before you create this graph, before you look at the results, I want you to think again about which

10:25.810 --> 10:28.370
way you predict the results to come out.

10:28.810 --> 10:35.770
Do you think that nouns or adjectives have longer trajectories in this state space?

10:36.130 --> 10:43.780
Based on some ideas that you might have about how much processing and token integration is required

10:43.780 --> 10:45.820
for nouns versus adjectives.

10:46.980 --> 10:47.700
Very good.

10:47.860 --> 10:50.260
Pause the video now and switch to code.

10:50.420 --> 10:53.740
And now I will do the same and discuss some of the results.

10:54.740 --> 10:58.740
Here I initialize a matrix called distances.

10:58.980 --> 11:05.460
You can see it is the number of layers minus one, because we are calculating the distance from each

11:05.460 --> 11:07.180
layer to the previous.

11:07.180 --> 11:13.180
So therefore I start looping over the layers from the second layer.

11:13.220 --> 11:17.180
Of course that actually corresponds to the first transformer block.

11:17.700 --> 11:17.980
Okay.

11:18.020 --> 11:22.420
And then the two is for the two different categories of words.

11:22.780 --> 11:27.500
So here I get the x points and the y points of this here.

11:27.500 --> 11:35.100
So you know for any given point here I have two x coordinates that I need to calculate Euclidean distance.

11:35.260 --> 11:36.180
This point here.

11:36.180 --> 11:40.310
So this x y coordinate and this xy coordinate.

11:40.310 --> 11:45.190
And once we have those two coordinates then we can calculate Euclidean distance.

11:45.310 --> 11:47.190
And yeah here's the formula.

11:47.190 --> 11:52.270
So difference of the x's squared plus the difference of the y's squared.

11:52.270 --> 11:54.910
And then we take the square root of that.

11:55.190 --> 11:55.510
Okay.

11:55.550 --> 11:58.310
So then that gives me the distances.

11:58.590 --> 12:03.070
And then plotting that I'm going to discuss this plot in a moment.

12:03.270 --> 12:11.510
First I just want to show what the code looks like to calculate the distances without any for loops.

12:11.710 --> 12:20.510
So here I'm subtracting all of the x values for one to the end from the beginning to the end minus one.

12:21.710 --> 12:24.230
Now this is a vector right up here.

12:24.230 --> 12:25.950
These were just scalars.

12:25.950 --> 12:27.550
These are individual numbers.

12:27.550 --> 12:29.230
So now these are vectors.

12:29.230 --> 12:35.750
And now I can calculate the difference vector squared element wise and then square root of that.

12:36.030 --> 12:38.640
And then this is just a confirmation.

12:38.640 --> 12:48.040
If this calculation is really the same as this calculation, then their differences should be zeros

12:48.040 --> 12:48.680
which they are.

12:48.720 --> 12:49.600
So that's great.

12:50.160 --> 12:54.640
And then just using zero here because I only ran this analysis for the nouns.

12:54.640 --> 12:59.720
If you wanted to repeat it for the adjectives you just turn all of these into ones.

12:59.720 --> 13:02.160
And then we can subtract like this.

13:02.160 --> 13:04.200
And we also get a bunch of zeros.

13:04.480 --> 13:05.040
Very good.

13:05.080 --> 13:05.400
Okay.

13:05.440 --> 13:07.600
So now let's go back to these results.

13:08.040 --> 13:13.480
Now we see actually I do want to show what this looks like without the log.

13:14.560 --> 13:19.320
So this is the plot with log scaling without log scaling.

13:19.360 --> 13:20.840
Perfectly valid results.

13:20.840 --> 13:26.760
But these values over here are just so big that they dominate the y axis.

13:26.880 --> 13:32.880
And you don't really see much of anything happening really anywhere in the model until the very, very

13:32.880 --> 13:33.360
end.

13:33.600 --> 13:41.340
So that is why when you have a situation like this, it's probably a good idea to log scale.

13:41.500 --> 13:46.300
Then you can see the more subtle features of the plots.

13:46.500 --> 13:53.300
For example, here we see that the nouns have larger distances and the adjectives for a series of,

13:53.340 --> 13:57.100
I don't know, 4 or 5 different layers in a row.

13:57.540 --> 13:59.420
This is this feature.

13:59.460 --> 14:04.700
This this result is technically present in the linear scaling, but they're so scrunched together that

14:04.700 --> 14:05.940
you don't actually see it.

14:06.500 --> 14:06.820
Okay.

14:06.860 --> 14:09.140
Anyway, here are the differences here.

14:09.420 --> 14:11.140
Uh, and let me see.

14:11.140 --> 14:16.620
This was so you can calculate the logs separately and then subtract them.

14:16.900 --> 14:24.100
I did it this way because I just wanted to give you a quick reminder of the law of logarithms, which

14:24.100 --> 14:31.980
basically says that the difference of logs is equal to the ratio or the log of the ratio.

14:32.380 --> 14:36.150
So, uh, just to make sure that's super clear.

14:36.630 --> 14:38.630
These two expressions are the same.

14:38.630 --> 14:45.670
So the log of something minus the log of another thing that is mathematically equivalent to I can even

14:45.670 --> 14:48.150
plot this again, you can see the plot doesn't change.

14:48.390 --> 14:54.510
This is mathematically equivalent to taking one log transform of a ratio.

14:55.630 --> 14:55.910
Okay.

14:55.950 --> 14:58.990
Just a little bit of reminder about algebra.

14:59.350 --> 14:59.630
Okay.

14:59.670 --> 15:01.910
So here we see the differences here.

15:01.950 --> 15:05.110
You know this the this is nouns minus adjectives.

15:05.110 --> 15:10.190
So positive numbers here are where the nouns are higher than the adjectives here.

15:10.190 --> 15:17.750
Which means that the distance that these pathways are traversing uh, in state space are larger.

15:17.750 --> 15:22.230
So nouns have larger distances compared to adjectives.

15:22.790 --> 15:25.990
Now you don't really see any consistent results.

15:25.990 --> 15:29.350
It it swaps back and forth between the categories.

15:29.710 --> 15:36.320
And to be honest, I'm not sure if there is a meaningful interpretation of that or if this would be

15:36.520 --> 15:40.160
over interpreting noise to some extent.

15:40.600 --> 15:47.880
This question will be addressed with the next exercise, but in reality, what you'd want to do is basically

15:47.880 --> 15:52.280
repeat this kind of analysis with a lot more text data.

15:52.600 --> 15:59.840
And maybe we see inconsistent results here simply because we just don't have a lot of data.

16:01.080 --> 16:05.360
This was quite an intense set of exercises.

16:05.960 --> 16:13.640
So the last exercise here is super duper simple and just geared towards examining reproducibility.

16:14.160 --> 16:23.560
All you need to do is rerun exercises two through five using GPT two x instead of the large version.

16:23.960 --> 16:30.730
Now, I do not recommend copying and pasting all the code cells, nor do I recommend doing anything

16:30.770 --> 16:34.330
fancy, like writing a big for loop or setting up a toggle.

16:34.530 --> 16:39.690
Just go back to the code, change the model name, and then rerun all the code.

16:40.130 --> 16:45.530
Now it's trivial that the numerical results will be at least somewhat different.

16:45.890 --> 16:53.250
The question here is whether the overall features of the key findings seem to be consistent or different

16:53.250 --> 16:54.690
between the two models.

16:54.850 --> 17:02.330
And in particular, I'm thinking about the nature of the scree plot and the last plot of the differences

17:02.330 --> 17:08.970
between the distances, the path lengths that we looked at at the end of exercise five.

17:09.730 --> 17:15.250
Okay, so now I'm going to switch to Python and show the results.

17:16.410 --> 17:20.850
So all I'm going to do is change the X the large to XL.

17:21.050 --> 17:25.290
And then I'm going to press runtime and run cell and below.

17:25.290 --> 17:27.290
So that will run this cell.

17:27.390 --> 17:29.990
and it's going to run everything else below.

17:30.390 --> 17:36.310
This is going to take a moment to import the Excel version of this model.

17:38.670 --> 17:39.030
Okay.

17:39.070 --> 17:44.710
So I thought this was going to be simple, but unfortunately I ran out of memory.

17:45.070 --> 17:51.550
And this happened because yeah, when you run a lot of code often I keep rerunning code over and over

17:51.550 --> 17:52.110
again.

17:52.150 --> 17:57.670
You just get some like memory leaks and you end up with a lot of variables that you used, but you no

17:57.670 --> 17:58.310
longer need.

17:58.350 --> 18:00.470
And they're just sitting there on the GPU.

18:00.870 --> 18:07.190
When you run into a situation like that, the best thing to do is disconnect and delete the runtime.

18:07.190 --> 18:11.630
That will just completely obliterate everything that you did.

18:11.670 --> 18:15.950
Not the code, of course, and then just rerun the whole thing from scratch.

18:16.950 --> 18:23.790
So the scree plot shows some interesting differences relative to the large version of GPT two.

18:24.550 --> 18:30.800
In particular, it particular, it looks like we are explaining maybe around 10% more variance from

18:30.800 --> 18:32.280
the top two components.

18:32.400 --> 18:34.640
So this is somewhere around 45.

18:34.680 --> 18:38.720
This is around ten, so around 55% of the variance.

18:38.880 --> 18:41.360
And I didn't do the exact calculation.

18:41.360 --> 18:47.080
But with the large version of the model I think it was 40% somewhere around there.

18:47.560 --> 18:47.920
Okay.

18:47.960 --> 18:49.600
So that is one difference.

18:49.600 --> 18:57.360
So that actually helps justify the use of a state space analysis approach, because we really see these

18:57.360 --> 19:02.120
two components beautifully popping out from the rest of the components.

19:02.640 --> 19:03.000
Okay.

19:03.040 --> 19:08.440
So now we look here, we see the trajectories themselves look kind of different.

19:08.440 --> 19:15.120
But you do see something several really consistent features, which is that they are much further apart

19:15.120 --> 19:15.760
from each other.

19:15.800 --> 19:20.880
The nouns and the adjectives compared to what we saw a couple of videos ago.

19:21.080 --> 19:28.450
And their distances also increase, uh, quite, uh, you know, powerfully until the very end where

19:28.450 --> 19:30.410
it just drops and they merge together.

19:30.890 --> 19:31.250
Okay.

19:31.290 --> 19:38.370
And then here now, this is pretty interesting because now we see that, uh, the pattern.

19:38.410 --> 19:45.090
So this pattern is overall still the same as with the slightly smaller version of this language model

19:45.370 --> 19:50.890
in that, uh, it increases ferociously as we get deeper into the model.

19:51.290 --> 19:54.970
But now we actually see a result that is pretty consistent.

19:54.970 --> 20:03.330
So now nouns compared to adjectives have generally longer paths, so longer trajectories pretty much

20:03.370 --> 20:08.410
everywhere throughout the model with the exception of, you know, the very beginning.

20:08.410 --> 20:14.570
So this certainly looks a lot more consistent than it did with the large model.

20:15.690 --> 20:24.580
My prediction for the outcome of this analysis was that the nouns would be longer in state space than

20:24.580 --> 20:31.300
the adjectives, and that was based on reasoning that processing nouns and generating predictions for

20:31.300 --> 20:37.620
what words come after nouns generally requires integrating over more tokens.

20:37.620 --> 20:42.940
So more information, more context further back in the text, and so on.

20:43.300 --> 20:50.900
Compared to adjectives, of course, adjectives also need to integrate across tokens, but in English

20:50.940 --> 20:57.620
it is generally the case that the noun that the adjective modifies is pretty close to the adjective

20:57.620 --> 21:04.620
itself, which means that the context window integration demands are generally lower.

21:05.180 --> 21:11.060
And so was that hypothesis confirmed in the data for the Excel version?

21:11.060 --> 21:18.940
I think that it did seem like the results were consistent with the hypothesis for the GPT two large

21:19.420 --> 21:19.820
I.

21:20.040 --> 21:21.280
It's not so clear.

21:21.280 --> 21:27.800
There wasn't like really clear, resounding, overwhelming evidence consistent with my hypothesis that

21:27.800 --> 21:30.240
was also consistent across models.

21:30.720 --> 21:33.920
But I would say that it seems like it is the case.

21:34.200 --> 21:39.720
But of course you should keep in mind all of the like typical limitations that are always present in

21:39.720 --> 21:47.080
this course, which is that I used a fairly small sample size only from one text, and it's a rather

21:47.080 --> 21:54.040
old text, which is quite different from the modern texts that these models have been pre-trained on.

21:54.720 --> 22:01.920
Nonetheless, I want to be clear that my hesitation about the interpretations is about the specific

22:02.040 --> 22:08.840
scientific interpretation of the results, and my hesitation is not about the method itself.

22:09.040 --> 22:10.320
It's a great method.

22:10.480 --> 22:17.080
Definitely worth including in your ever growing toolkit of data analysis methods.

WEBVTT

00:02.200 --> 00:09.120
Parts of speech are categories of words like nouns, verbs, adjectives, and so on.

00:09.920 --> 00:17.840
Parts of speech are obviously a major part of understanding the structure and use of language, and

00:17.840 --> 00:25.320
therefore parts of speech, is also a major part of trying to understand how computers process natural

00:25.320 --> 00:29.920
language and mechanistic interpretability of language models.

00:30.600 --> 00:39.160
Now, you have already seen a few ways in this course of incorporating parts of speech into model investigations,

00:39.160 --> 00:47.240
but a lot of the methods that I showed so far were either looking at single words with no context,

00:47.440 --> 00:54.160
or fairly small data sets that were specifically crafted for the particular analysis.

00:54.880 --> 01:01.700
Now, the problem with that approach, like the sentences with him and her and round, is that they.

01:01.980 --> 01:08.620
The sentences are fairly short and a little contrived, so they don't really have a large context window

01:08.620 --> 01:10.100
of relevancy.

01:10.660 --> 01:15.580
And it's also difficult to scale up data sets like that.

01:16.100 --> 01:23.220
You know, it's one thing to come up with like 50 sentences, but it's just not feasible to have thousands

01:23.220 --> 01:31.580
of handcrafted sentences with a particular part of speech in a particular token position that are coded

01:31.580 --> 01:35.380
or at least supervised by a human anyway.

01:35.540 --> 01:44.100
Obviously, the solution is to use some other routines or libraries that can categorize words into parts

01:44.100 --> 01:44.980
of speech.

01:45.660 --> 01:50.660
Now, one of those libraries that is fairly often used is called spacy.

01:51.300 --> 01:54.660
It's a fairly big and versatile library.

01:54.700 --> 01:55.860
It does a lot of stuff.

01:56.340 --> 02:04.160
And in this video I will introduce you to just the basic usage of this library for identifying parts

02:04.160 --> 02:06.360
of speech in written text.

02:07.080 --> 02:15.520
Unfortunately, it can be a little tricky to integrate this spacy library with the Tokenizers.

02:16.040 --> 02:22.880
I will show you some of the issues that tend to arise, and then I'll discuss a few alternatives.

02:22.880 --> 02:30.560
And then in the code challenge in the next video, you will continue exploring some solutions that aren't

02:30.600 --> 02:35.000
really perfect, but at least are pretty good and easy to implement.

02:35.000 --> 02:41.400
In terms of integrating the Spacy library tokenizer with the GPT tokenizer.

02:42.560 --> 02:47.280
Anyway, here is a screenshot of the website for this library.

02:47.640 --> 02:54.120
You can find it on the web just by searching, and I've also put a direct link to the website in the

02:54.120 --> 02:56.520
code file in the Python demo.

02:57.440 --> 03:00.820
And yeah, actually all of this stuff I've already mentioned.

03:00.820 --> 03:06.860
Basically, I'm just going to show you one application of this library here in this video.

03:07.660 --> 03:14.740
Fortunately, Google Colab already has this library installed, so we just need to import it.

03:15.300 --> 03:25.140
Then here I am loading up this uh, module which is trained based on web text, and then I'll show you

03:25.140 --> 03:29.300
a bunch of properties associated with this class object.

03:29.580 --> 03:35.620
And then here I have a little sentence that we can work with to help demo this library.

03:35.900 --> 03:43.100
So the sentence is, this is a very happy sentence that I will use to explore the Spacy library.

03:43.620 --> 03:51.540
So here what I have done is manually split the words, uh, split the text into words, and then looked

03:51.540 --> 03:55.140
up the part of speech for each word individually.

03:55.580 --> 04:00.800
Now that's fine, although it's not actually the best way to use this library.

04:01.280 --> 04:08.960
And that's because the Spacy library actually determines the part of speech, in part based on the context.

04:08.960 --> 04:11.560
So the words surrounding each word.

04:12.640 --> 04:19.840
So it is actually better to tokenize the entire sentence instead of splitting the sentence into words,

04:19.840 --> 04:25.000
and then trying to extract the part of speech from each individual word.

04:25.560 --> 04:33.680
For example, here you can see that when I used this approach, the word explore was mislabeled as a

04:33.720 --> 04:34.280
noun.

04:34.840 --> 04:43.440
But here, when I inputted the entire sentence into the NLP class object instead of just one word at

04:43.440 --> 04:44.000
a time.

04:44.320 --> 04:51.240
Now it correctly used the context to identify this word explore as a verb.

04:52.080 --> 04:58.730
On the other hand, the spacey tokenizer does not correspond to LLM Tokenizers.

04:59.170 --> 05:06.210
And so if we actually want to work with language models, then we need to tokenize according to the

05:06.210 --> 05:07.490
model's tokenizer.

05:08.490 --> 05:17.410
That will create some confusions, maybe some headaches, or at least some annoyances because the tokenizers

05:17.410 --> 05:20.370
are different from the spacey tokenizer.

05:20.970 --> 05:27.170
For one thing, LLM tokenizers are based on the byte pair encoding algorithm.

05:27.370 --> 05:32.090
So that means they will often get subwords instead of full words.

05:32.650 --> 05:36.290
Here you can see one example of how this can go wrong.

05:36.690 --> 05:40.810
So here I used the GPT two tokenizer.

05:41.010 --> 05:49.610
And now most of these tokens were actually categorized as space by the Spacy library utility.

05:50.370 --> 05:58.870
Now this specific issue is actually really easy to solve simply by removing all of the preceding spaces,

05:58.870 --> 06:03.790
but there are other incompatibilities between the different tokenizers.

06:03.910 --> 06:06.910
For example, that arise when dealing with subwords.

06:07.350 --> 06:14.030
So I'll show you some more demos of how this can be annoying when I switch to code, and then I'll discuss

06:14.030 --> 06:20.070
a few strategies and then yeah, as I mentioned in the code challenge, in the next video, you will

06:20.070 --> 06:24.070
discover one way of dealing with this incompatibility.

06:24.110 --> 06:26.270
It's not perfect, but it is pretty good.

06:26.830 --> 06:29.630
Okay, let's switch to code and have a look.

06:30.670 --> 06:35.190
Here is the URL for this library so we don't need to install it.

06:35.230 --> 06:38.990
It's already installed in Google Colab so we can just import it.

06:39.430 --> 06:47.230
And yeah here I'm showing you all of the properties that are associated with this instance of this class.

06:48.190 --> 06:54.470
I am not going to introduce you to most of these properties and options and all these things that are

06:54.470 --> 06:57.850
available, but just to show you that there is.

06:58.330 --> 07:00.970
There are many things that you can do with this library.

07:00.970 --> 07:06.530
And yeah, what we are going to do is fairly simple, just extracting parts of speech.

07:06.970 --> 07:11.810
Okay, so here I am splitting up the text into words.

07:11.810 --> 07:12.930
This you've seen before.

07:12.930 --> 07:15.370
And I can show it to you again really quickly.

07:15.370 --> 07:22.650
Basically this is just creating a list where each element in the list is a space separated string of

07:22.650 --> 07:26.450
characters from this, from the original sentence.

07:26.970 --> 07:27.290
Okay.

07:27.330 --> 07:29.490
And then I am defining.

07:29.490 --> 07:35.370
So inputting that into this object NLP and then getting from that.

07:35.370 --> 07:36.930
So this will tokenize it.

07:37.010 --> 07:43.170
And then because it's just one word, there's also just going to be one token associated with the output

07:43.170 --> 07:45.570
of this NLP object.

07:45.570 --> 07:49.010
And that's why I'm indexing the zeroth element.

07:49.010 --> 07:52.210
That's just getting the first token in here okay.

07:52.250 --> 08:00.390
And then once we have that individual token we can write dot That is a method that returns the part

08:00.390 --> 08:01.270
of speech.

08:01.430 --> 08:04.350
So pronoun, pronoun, adjective and so on.

08:04.910 --> 08:05.230
Okay.

08:05.270 --> 08:06.310
And again yeah.

08:06.310 --> 08:08.030
So this is kind of awkward.

08:08.070 --> 08:15.510
The thing is that several like lots of words can be nouns or verbs, depending on how they're used in

08:15.510 --> 08:16.390
context.

08:16.710 --> 08:23.310
And when I only apply this method to one word at a time, there is no context.

08:23.310 --> 08:29.230
And so we are not guaranteed to get the what would be the contextually correct answer.

08:30.350 --> 08:39.070
So then what I do over here is I push the entire sentence so I'm not tokenizing or splitting the sentence

08:39.230 --> 08:40.550
into separate words.

08:40.550 --> 08:46.830
I'm just inputting the entire text into this NLP instance, this object here.

08:46.830 --> 08:53.350
And then it will actually, you know, the output of this is actually a tokenized piece of text.

08:53.350 --> 09:00.650
And now we get a more accurate answer, because this is actually the way that the Spacy library was

09:00.650 --> 09:04.490
designed to process text, to take in all the text at once.

09:04.970 --> 09:13.410
Okay, so now I'm going to import the GPT two tokenizer and tokenize that same piece of text.

09:13.410 --> 09:19.930
And now I'm going to try to extract the part of speech from each individual token.

09:20.290 --> 09:27.610
And as I discussed in the slides a moment ago, a tricky thing about this is that the GPT tokenizer

09:27.610 --> 09:29.970
is based on subwords.

09:30.370 --> 09:30.650
Yeah.

09:30.650 --> 09:35.730
And so you see that this is this is really tripping up the Spacy library.

09:35.730 --> 09:41.370
Basically every token that begins with a space is labeled as space.

09:41.650 --> 09:45.410
So that is and then we get this other weird stuff here as well.

09:45.410 --> 09:54.650
So the word spacy, the name of the library spacy, got separated into two tokens by the GPT tokenizer.

09:54.650 --> 10:01.190
So space SP and then AC, and it labeled this one as space and AC an adjective.

10:01.230 --> 10:02.630
But really it should be a noun, right?

10:02.670 --> 10:06.430
This should be one or this is one word and that's a noun.

10:06.830 --> 10:14.270
So in some sense we can fix this fairly easily simply by adding dot strip to this token text.

10:14.270 --> 10:18.870
And all that is going to do is remove the preceding space.

10:19.230 --> 10:20.510
So that certainly helps.

10:20.510 --> 10:26.230
But that doesn't deal with the problem that we've still completely lost the context.

10:26.390 --> 10:31.470
So now this spacy library still doesn't see any of the context.

10:31.470 --> 10:33.790
So this is something that you should do.

10:34.030 --> 10:40.070
Uh, but uh yeah, there's there's going to be some additional considerations that I will mention,

10:40.230 --> 10:41.950
uh, after I finish the code demo.

10:41.950 --> 10:46.310
And then we will implement in the code challenge in the next video.

10:46.790 --> 10:47.110
Okay.

10:47.150 --> 10:54.790
So then what I'd like to do just for fun is, uh, import a GPT two small model, and then I'm going

10:54.790 --> 10:57.050
to generate some text.

10:57.050 --> 11:02.690
So I'm seeding the text here with the phrase I think the world could be better if.

11:03.290 --> 11:10.530
And so this I input into the model and then ask the model to generate another 400 tokens.

11:10.650 --> 11:17.210
And we will see what GPT two thinks that we should do to make the world a better place.

11:18.370 --> 11:23.010
I think the world could be a better place if we have more women in finance, he added.

11:23.050 --> 11:25.130
Okay, so then this goes on.

11:25.130 --> 11:30.690
It's getting into stuff about Iran and I'm not going to get too much into that.

11:30.690 --> 11:34.130
But anyway, uh, so it always makes for interesting reading.

11:34.170 --> 11:36.130
Uh, GPT two's generation.

11:36.490 --> 11:43.490
Okay, so now the last thing that I'm doing here is looping over all of this text here that the model

11:43.490 --> 11:44.370
generated.

11:44.610 --> 11:53.210
And for each of the tokens, I am simply decoding that token, replacing the space with emptiness,

11:53.250 --> 11:59.980
with nothingness And, uh, yeah, there's no special reason why I'm using this method instead of dot

11:59.980 --> 12:00.540
strip here.

12:00.540 --> 12:03.420
I think I wanted to just show some alternatives.

12:03.420 --> 12:10.780
And I guess one thing is that strip will only remove the space in the beginning of the, uh, character

12:10.780 --> 12:17.700
string character list, whereas this will get rid of any spaces anywhere in the text and then identify

12:17.700 --> 12:20.500
that, uh, tokens part of speech.

12:20.740 --> 12:27.060
And then here I say, if this is a noun, then I increment the noun counter and I print it out.

12:27.100 --> 12:31.700
And if it's a verb, then I increment the verb counter and print that out.

12:31.940 --> 12:39.540
This is a simplified version of what you are going to do in the next video, in the code challenge.

12:39.540 --> 12:46.540
In the next video, basically what you are going to do is loop through a lot of text from a book and

12:46.580 --> 12:53.740
identify nouns and adjectives, and you're going to use code that is a little bit more sophisticated

12:53.740 --> 13:01.000
than this, but this is code that you can use to help you get started in the next, uh, code challenge

13:01.000 --> 13:02.000
in the next video.

13:02.440 --> 13:02.840
Okay.

13:02.880 --> 13:07.040
So, uh, yeah, this is just looping through all of this text here.

13:07.240 --> 13:11.920
And wherever it finds a verb or a noun, it prints that out.

13:11.920 --> 13:13.440
So, yeah, you can go through this.

13:13.440 --> 13:16.760
You can see mostly it's going to work really well.

13:16.880 --> 13:23.240
The Spacy library does work quite well, even with these kind of weird headaches and annoyances that

13:23.240 --> 13:24.200
I've introduced.

13:24.560 --> 13:27.240
Here's a case where it's not working well.

13:27.240 --> 13:28.880
I don't know what the S is for.

13:28.880 --> 13:33.080
I guess you could, uh, go back and figure, oh, that's us, I think.

13:33.320 --> 13:33.560
Yeah.

13:33.560 --> 13:36.960
So anyway, sometimes it gives some weird results.

13:36.960 --> 13:38.440
Mostly it's fine.

13:38.640 --> 13:39.200
Uh, yeah.

13:39.240 --> 13:41.440
And then here, let me scroll down.

13:41.800 --> 13:42.440
Uh, this.

13:42.440 --> 13:46.440
I was just curious to see how many nouns and verbs there are.

13:46.600 --> 13:48.280
Of course, this number.

13:48.280 --> 13:51.400
This count changes each time you rerun this code.

13:52.400 --> 13:58.580
There are a few options for using Spacy with mechanistic interpretability.

13:59.140 --> 14:05.700
First of all, I should say that if it's possible, then you should just stick with Spacy's internal

14:05.740 --> 14:09.580
tokenizer for its part of speech categorization.

14:09.780 --> 14:18.180
That will give you maximal accuracy, but it's not always possible because if you need to identify specific

14:18.220 --> 14:22.300
tokens, then yeah, it's just not necessarily going to work.

14:22.340 --> 14:29.780
You will see also in the beginning of the next code challenge, that there can be mismatches between

14:29.820 --> 14:34.100
spacy's tokenization and GPT two tokenization.

14:34.420 --> 14:41.540
One thing you can do is only use tokens that are full words in the model's tokenizer.

14:41.820 --> 14:45.940
And yeah, that's also what I'm going to recommend in the next video.

14:46.100 --> 14:52.100
That's a pretty good method, although it's not guaranteed to be a perfect solution, and so it's likely

14:52.100 --> 14:58.880
that you will be excluding a lot of otherwise valid tokens, and it also means that there might still

14:58.880 --> 15:06.320
be some mistakes that are introducing some non-systematic errors into the analysis.

15:06.760 --> 15:14.320
And for that reason, is good to have a pretty large text dataset so that you can afford to filter out

15:14.360 --> 15:22.680
a lot of uncertain matches, and also so that the errors have a minimal impact on the statistical analyses.

15:23.360 --> 15:29.440
Last thing I want to mention here is that if you are really interested in using the Spacy library,

15:29.600 --> 15:35.840
in practice, in applications or in research, then I recommend looking into it in a little bit more

15:36.000 --> 15:38.760
detail than just what I presented here.

15:38.920 --> 15:43.360
So you can explore what other utilities they have available.

15:44.000 --> 15:45.640
So that's it for this video.

15:45.680 --> 15:51.920
You will continue exploring this library for part of speech identification in the next video.

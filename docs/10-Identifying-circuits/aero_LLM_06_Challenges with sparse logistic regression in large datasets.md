
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [10 Identifying circuits](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# ThÃ¡ch Thá»©c Cá»§a TÃ­n Hiá»‡u ThÆ°a Trong Dá»¯ Liá»‡u Táº­p Lá»›n (Statistical Suppression)

## TÃ³m táº¯t (Abstract)
Video trÆ°á»›c Ä‘Ã£ trÃ¬nh bÃ y hiá»‡u nÄƒng tuyá»‡t Ä‘á»‘i cá»§a thuáº­t toÃ¡n DÃ² ThÆ°a (Sparse Probing). Tuy nhiÃªn, khi á»©ng dá»¥ng trÃªn CÃ¡c Há»‡ Thá»‘ng PhÃ¢n TÃ¡n Lá»›n (VÃ­ dá»¥: Ma tráº­n 3000 Neurons), L1 Logistic Regression bá»™c lá»™ má»™t sá»‘ cáº¡m báº«y diá»…n giáº£i nghiÃªm trá»ng. ThÃ­ nghiá»‡m mÃ´ phá»ng trÃªn dá»¯ liá»‡u nhiá»…u ngáº«u nhiÃªn (Simulated data) trong bÃ¡o cÃ¡o nÃ y cho tháº¥y hiá»‡n tÆ°á»£ng ÄÃ n Ã¡p thá»‘ng kÃª (Statistical suppression) - nÆ¡i cÃ¡c Táº¿ bÃ o mang tÃ­nh phÃ¢n cá»±c dÆ°Æ¡ng máº¡nh máº½ láº¡i bá»‹ gÃ¡n trá»ng sá»‘ Ã¢m (Negative Beta) hoáº·c bá»‹ Ã©p vá» $0$. BÃ i viáº¿t giáº£i pháº«u báº£n cháº¥t cá»§a Há»“i quy: Trá»ng tÃ¢m cá»§a mÃ´ hÃ¬nh lÃ  Tá»‘i Ä‘a hÃ³a kháº£ nÄƒng Dá»± bÃ¡o NhÃ£n (Label accuracy) trÃªn má»™t khÃ´ng gian TÆ°Æ¡ng quan nhiá»…u (Collinearity), chá»© khÃ´ng pháº£i Tháº©m Ä‘á»‹nh sá»± Æ°u viá»‡t cá»§a tá»«ng biáº¿n Ä‘á»™c láº­p.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Trong CÆ¡ há»c Giáº£i diá»…n biá»ƒu hÃ¬nh, ta thÆ°á»ng mong Ä‘á»£i sá»± liÃªn Ä‘á»›i trá»±c tiáº¿p: Má»™t NÆ¡-ron mang kÃ­ch hoáº¡t khá»•ng lá»“ Ä‘á»‘i vá»›i tÃ­n hiá»‡u $A$, thÃ¬ khi Ä‘Æ°a vÃ o mÃ´ hÃ¬nh Máº¡ng LÆ°á»›i PhÃ¢n loáº¡i (Regression classifier), há»‡ sá»‘ $\beta$ cá»§a nÃ³ cÅ©ng pháº£i cá»±c ká»³ lá»›n.
NhÆ°ng ká»‹ch báº£n Há»“i quy dá»¯ liá»‡u lá»›n hoáº¡t Ä‘á»™ng theo má»™t há»‡ Quy táº¯c Ä‘Ã n Ã¡p. Khi hÃ ng nghÃ¬n biáº¿n cung cáº¥p chung má»™t "LÆ°á»£ng thÃ´ng tin dÆ° thá»«a" (Redundant information), MÃ´ hÃ¬nh sáº½ tÃ¬m cÃ¡ch cÃ¢n báº±ng báº±ng cÃ¡ch vÃ´ hiá»‡u hÃ³a hoáº·c tháº­m chÃ­ Ä‘áº£o ngÆ°á»£c dáº¥u (Negative sign) cá»§a nhiá»u NÆ¡-ron xuáº¥t sáº¯c. Sá»± hiá»ƒu nháº§m vá» Ä‘áº·c tÃ­nh nÃ y cÃ³ thá»ƒ lÃ m chá»‡ch hÆ°á»›ng nghiÃªn cá»©u cÃ¡c Circuit Vi Máº¡ch cá»§a LLM.

---

## 2. Thiáº¿t Láº­p ThÃ­ Nghiá»‡m ÄÃ n Ãp (Methodology)

### 2.1. Ma Tráº­n Giáº£ Láº­p Hiá»‡u á»¨ng Tuyá»‡t Äá»‘i
Ta táº¡o má»™t táº­p dá»¯ liá»‡u giáº£ láº­p (Mock dataset) vá»›i $N = 200$ (Token samples) chia lÃ m 2 nhÃ£n vÃ  $K = 3000$ (MLP Neurons).
Thay vÃ¬ láº¥y TÃ­n hiá»‡u tá»« Máº¡ng NgÃ´n ngá»¯, ta Ä‘á»‹nh hÃ¬nh dá»¯ liá»‡u khá»Ÿi táº¡o báº±ng HÃ m Random Noise. Máº¥u chá»‘t thÃ­ nghiá»‡m, táº¡i $100$ token nhÃ£n $1$, ta tá»‹nh tiáº¿n bÃ¹ (Offset) thÃªm $+5$ háº±ng sá»‘ kÃ­ch hoáº¡t cho toÃ n bá»™ 3000 nÆ¡-ron. 
Há»‡ quáº£: Cáº£ 3000 NÆ¡-ron Ä‘á»u cÃ³ sá»©c máº¡nh phÃ¢n loáº¡i (Effect size / Äá»™ Ä‘o Cohen's d) cá»±c ká»³ kinh khá»§ng, cho tháº¥y sá»± Æ°u Ã¡i hoÃ n toÃ n vá»›i Category Label 1 thay vÃ¬ Category Label 0. 

### 2.2. Nghá»‹ch LÃ½ L1 Penalty Trá»ng Sá»‘ $\beta$
Khi cháº¡y mÃ´ hÃ¬nh L1 Logistic Regression $C=3$, káº¿t quáº£ Accuracy lÃ  $100\%$ vÃ  cÃ³ tá»· lá»‡ thÆ°a Sparsity $36\%$ (Gáº§n má»™t nghÃ¬n NÆ¡-ron bá»‹ gáº¡ch bá» trá»ng sá»‘ $\beta \to 0$).
NhÆ°ng Ä‘iá»u cháº¥n Ä‘á»™ng khi kiá»ƒm tra cÃ¡c $\beta$ cÃ²n sá»‘ng sÃ³t: Gáº§n má»™t ná»­a táº­p há»£p cÃ³ dáº¥u *Ã¢m* ($\beta < 0$).
HÃ£y lÆ°u Ã½: Náº¿u Ä‘á»c Ä‘á»™c láº­p tá»«ng tham sá»‘, NÆ¡-ron cÃ³ $\beta < 0$ Ä‘á»“ng nghÄ©a nÃ³ Ä‘ang á»§ng há»™ NhÃ£n Category 0. NhÆ°ng Dá»¯ liá»‡u gá»‘c á»Ÿ trÃªn cho tháº¥y $100\%$ táº¿ bÃ o Ä‘á»u á»§ng há»™ NhÃ£n Category 1!

---

## 3. Kháº£o SÃ¡t & Giáº£i Pháº«u MÃ´ HÃ¬nh (Analysis)

### 3.1. Sá»± CÃ¢n Báº±ng DÆ° Thá»«a Sá»‘ Liá»‡u (Statistical Suppression)
ÄÃ¢y lÃ  Ä‘á»‹nh lÃ½ CÃ¢n báº±ng ÄÃ n Ã¡p (Suppression mechanism). Khi bá»™ Há»c sÃ¢u dá»‘c Gradient (SAGA) cháº¡y vá»›i hÃ ng ngÃ n mÅ©i tÃªn chá»‰ vá» má»™t hÆ°á»›ng (Redundancy), cÆ°á»ng Ä‘á»™ dá»± bÃ¡o tá»•ng sáº½ cháº¡m tá»›i Ä‘iá»ƒm ná»• quÃ¡ Ä‘Ã  (Overshooting the logits).
Äá»ƒ duy trÃ¬ HÃ m tháº¥t thoÃ¡t há»£p lÃ½, MÃ´ hÃ¬nh quyáº¿t Ä‘á»‹nh **"triá»‡t tiÃªu bá»›t lá»±c kÃ©o"**. NÃ³ chá»n cÃ¡c táº¿ bÃ o cÃ³ kháº£ nÄƒng phÃ¢n tÃ¡n nhá» hÆ¡n hoáº·c nhiá»…u hÆ¡n, Ã©p chÃºng nháº­n trá»ng sá»‘ chÃ¬m (Negative Negative beta values) Ä‘á»ƒ lÃ m chá»‘t hÃ£m hoáº·c má» neo Ä‘á»‘i trá»ng cho cÃ¡c NÆ¡-ron tÃ­ch cá»±c khÃ¡c. NÃ³ khÃ´ng pháº£n Ã¡nh sá»± thay Ä‘á»•i vai trÃ² báº£n cháº¥t cá»§a NÆ¡-ron, mÃ  pháº£n Ã¡nh ká»¹ thuáº­t tÃ­nh toÃ¡n cá»§a má»™t máº¡ng há»“i quy ngáº§m hiá»ƒu (Latent correlation balance).

### 3.2. Hiá»‡n TÆ°á»£ng Sáº­p Cohen's D (Effect Size Irrelevance)
Má»™t nhÃ  nghiÃªn cá»©u cÃ³ thá»ƒ tin ráº±ng, thuáº­t toÃ¡n L1 Regularizer sáº½ "giá»¯ láº¡i cÃ¡c siÃªu táº¿ bÃ o cÃ³ Cohen's d lá»›n" vÃ  "vá»©t bá» cÃ¡c táº¿ bÃ o yáº¿u kÃ©m cÃ³ Ä‘á»™ nháº­n diá»‡n nhá»".
NhÆ°ng phÃ¢n bá»• Ä‘á»“ thá»‹ phÃ¢n tÃ¡n cháº¥m Ä‘á» (CÃ¡c NÆ¡-ron bá»‹ loáº¡i bá») tráº£ vá» thá»±c táº¿ ngÆ°á»£c láº¡i: ChÃºng náº±m ráº£i rÃ¡c Ä‘á»u trÃªn toÃ n bá»™ phá»• giÃ¡ trá»‹ Effect size dÃ i tá»« $(4.0 \to 6.0)$. Thuáº­t toÃ¡n L1 gáº¡t bá» biáº¿n ngáº«u nhiÃªn theo mÃ´-tuÃ½p há»™i tá»¥ tÆ°Æ¡ng quan, chá»© khÃ´ng dá»±a trÃªn thá»© báº­c Ä‘á»™ Ä‘o Ä‘á»™c láº­p cá»§a tá»«ng biáº¿n. Nghá»‹ch lÃ½ Simpson (Simpson's Paradox) lÃ  lá»i giáº£i thÃ­ch tÆ°Æ¡ng Ä‘á»“ng á»Ÿ Ä‘Ã¢y.

---

## 4. Káº¿t Luáº­n
"Má»¥c tiÃªu cá»§a hÃ m phÃ¢n tá»­ há»“i quy Logistic Ä‘a biáº¿n lÃ  Dá»± Ä‘á»‹nh NhÃ£n (Label Prediction), KHÃ”NG pháº£i Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ giÃ¡ trá»‹ diá»…n giáº£i cá»§a tá»«ng Trá»ng sá»‘ nhá» giÃ¡n Ä‘oáº¡n."
PhÆ°Æ¡ng phÃ¡p Sparse Probing cá»±c ká»³ sáº¯c bÃ©n, nhÆ°ng cÃ¡c nghiÃªn cá»©u viÃªn cáº§n lÆ°u tÃ¢m khÃ´ng Ä‘Æ°á»£c mang tÆ° duy ÄÆ¡n biáº¿n (Univariate interpretation) Ã¡p vÃ o táº­p tham sá»‘ Äáº§u ra cá»§a Thuáº­t toÃ¡n Äa Khá»‘i (Multivariate output). Giáº£i phÃ¡p phÃ²ng trÃ¡nh hiá»‡u quáº£ nháº¥t: Tiá»n cháº¯t lá»c (Dimensionality pre-selection) - Giá»›i háº¡n vÃ  cÃ´ Ä‘á»ng sá»‘ lÆ°á»£ng biáº¿n chá»‰ vá»›i vÃ i cá»¥m Táº¿ bÃ o Ä‘áº·c trÆ°ng Ä‘á»ƒ háº¡ tháº¥p rá»§i ro ÄÃ n Ã¡p há»‡ sá»‘, trÆ°á»›c khi thiáº¿t láº­p quy luáº­t cho Vi Máº¡ch LLM.

---

## TÃ i liÃªn tham kháº£o (Citations)
1. ThÃ­ nghiá»‡m kháº£o sÃ¡t Há»‡ sá»‘ bÃ¹ $C$, Ma tráº­n Simulated Data vÃ  Äá»‹nh cháº¿ nghá»‹ch lÃ½ Cohen's D trong hÃ m Logistic tá»« bÃ i `aero_LLM_06_Challenges with sparse logistic regression in large datasets.md`. LÆ°á»£c thuáº­t khÃ¡i niá»‡m Simpson's Paradox.
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

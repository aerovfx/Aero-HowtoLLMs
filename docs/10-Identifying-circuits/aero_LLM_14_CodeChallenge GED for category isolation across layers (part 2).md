
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [10 Identifying circuits](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): TÃ¡ch NhÃ³m GED Äa Táº§ng (Pháº§n 2) & Kiá»ƒm Chá»©ng ChÃ©o

## TÃ³m táº¯t (Abstract)
Tiáº¿p ná»‘i cÆ¡ sá»Ÿ Háº¡ táº§ng GED Ä‘Ã£ xÃ¢y dá»±ng trong Pháº§n 1, pháº§n 2 nÃ y kÃ­ch hoáº¡t vÃ²ng láº·p `FOR` cháº¡y dá»c toÃ n bá»™ chiá»u sÃ¢u cá»§a MÃ´ hÃ¬nh Language Model. BÃ¡o cÃ¡o nÃ y tá»•ng há»£p quÃ¡ trÃ¬nh diá»…n dá»‹ch Ä‘á»“ thá»‹ thÃ´ng kÃª T-test, há»‡ sá»‘ TÆ°Æ¡ng quan Pearson (Correlation) vÃ  Max Eigenvalues qua cÃ¡c táº§ng. ChÃºng ta phÃ¡t hiá»‡n ra nhá»¯ng quy luáº­t thÃº vá»‹: Má»©c Ä‘á»™ tÆ°Æ¡ng quan (Correlation) giá»¯a "Pattern Him" vÃ  "Pattern Her" cao á»Ÿ cÃ¡c lá»›p Äáº§u, nhÆ°ng trÆ°á»£t dá»‘c vá» $0$ á»Ÿ cÃ¡c lá»›p Cuá»‘i. CÃ¹ng vá»›i Ä‘Ã³, kháº£ nÄƒng PhÃ¢n Ly Dá»¯ Liá»‡u (Separability Ratio) Ä‘áº¡t Ä‘á»‰nh táº¡i khoáº£ng $1/3$ thÃ¢n máº¡ng. Cuá»‘i cÃ¹ng, viá»‡c Kiá»ƒm Ä‘á»‹nh mÃ¹ (Blind Test) trÃªn Fineweb mang láº¡i káº¿t quáº£ cá»±c ká»³ Ã½ nghÄ©a (Significant T-values) â€“ báº£o chá»©ng ráº±ng Bá»™ Äá»‹nh Tuyáº¿n (Vectors) do GED tÃ¬m tháº¥y khÃ´ng pháº£i lÃ  cÃº Overfitting may máº¯n, mÃ  nÃ³ thá»±c sá»± tháº¥u hiá»ƒu KhÃ¡i niá»‡m Giá»›i tÃ­nh.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Sau khi cÃ³ Ä‘Æ°á»£c hai HÃ²m dá»¥ng cá»¥: `dim_red(layer)` Ä‘á»ƒ Ã©p dá»¯ liá»‡u xuá»‘ng KhÃ´ng gian con PCA, vÃ  HÃ m `run_ged(train_data, pca_evecs)` Ä‘á»ƒ nhÃ o náº·n KhÃ´ng Gian Tá»‘i Æ¯u, ta cáº§n kháº£o sÃ¡t Sinh lÃ½ há»c cá»§a mÃ´ hÃ¬nh GPT-2:
1. Äá»™ sÃ¢u cá»§a Transformer Block (tá»« $Layer\ 1 \to Layer\ 12$) thay Ä‘á»•i cÃ¡ch Biá»ƒu Diá»…n Giá»›i tÃ­nh nhÆ° tháº¿ nÃ o?
2. CÃ³ hiá»‡n tÆ°á»£ng Há»c Váº¹t (Memorization / Overfitting) hay khÃ´ng?

---

## 2. Tiáº¿t Thiáº¿t Láº­p ÄÃ¡nh GiÃ¡ Äa Táº§ng (Methodology)

### 2.1. Ma Tráº­n Äá»±ng Káº¿t Quáº£ Äa Chiá»u (Result Tensor)
BÃªn bÃ i thÃ­ nghiá»‡m, ta Ä‘á»‹nh hÃ¬nh má»™t Ma tráº­n PyTorch `[12_Layers, 7_Analyses, 2_Stats]`:
- Chiá»u thá»© nháº¥t: Sá»‘ thá»© tá»± Lá»›p (Layer $0 \to 11$).
- Chiá»u thá»© hai: CÃ¡c kiá»ƒu Testing (T-test Him vs Her, R-Pearson Correlation Pattern, Max Eigenvalue...)
- Chiá»u thá»© ba: LÆ°u thÃ´ng sá»‘ Äá»™ lá»›n $T_{value}$ (Magnitude) vÃ  GiÃ¡ trá»‹ chuáº©n xÃ¡c suáº¥t $p\text{-value}$ (Bonferroni Corrected).
Trong VÃ²ng láº·p:
**BÆ°á»›c Lá»c MÃ¹ (Out-of-sample Evaluation):** ToÃ n bá»™ Data TrÃ­ch láº¥y tá»« Fineweb (Test-set) **TUYá»†T Äá»I** KHÃ”NG ÄÆ¯á»¢C CHáº Y Láº I THUáº¬T TOÃN PCA vÃ  GED. ChÃºng chá»‰ Ä‘Æ¡n thuáº§n Ä‘á»©ng im vÃ  bá»‹ PhÃ³ng (Projected) xuyÃªn qua cÃ¡c lÆ°á»›i Lá»c `PCA_Eigenvectors` vÃ  `GED_Eigenvectors` táº¡o ra bá»Ÿi Bá»™ Tá»« Vá»±ng NhÃ¢n táº¡o, sau Ä‘Ã³ má»›i dÃ¹ng PhÃ©p thá»­ T-test Ä‘á»ƒ Ä‘o Ä‘á»™ Cáº¯t Xáº» (Separation).

### 2.2. Kiá»ƒm Ä‘á»‹nh T-Test Hai PhÃ­a (Bi-directional T-Testing)
VÃ¬ GED tá»± sinh ra 2 Lá»›p Cá»™t (2 Top Eigenvectors, 1 dÃ¹ng S=Him_R=Her, 1 dÃ¹ng S=Her_R=Him). ChÃºng ta thi hÃ nh T-test chÃ©o ngÆ°á»£c: ÄÆ°a Khá»‘i dá»¯ liá»‡u 'Her' Äi qua MÃ ng Lá»c 'Him', lÃºc nÃ y LÆ°á»£ng Activations bá»‹ tiÃªu biáº¿n dáº§n dáº«n tá»›i hiá»‡u sá»‘ Trung bÃ¬nh PhÆ°Æ¡ng sai cá»±c nhá» (Magnitude T-Value Ã‚m). Do thuáº­t toÃ¡n GED giáº£i Vi PhÃ¢n dáº«n Ä‘áº¿n **Báº¥t Ä‘á»‹nh Dáº¥u (Sign Indeterminacy)**, ta giáº£i tá»a chuyá»‡n láº±ng nháº±ng cá»§a Dáº¥u + / Dáº¥u - báº±ng viá»‡c Ä‘áº·t Tuyá»‡t Äá»‘i $|T_{value}|$.

---

## 3. Kháº£o SÃ¡t & PhÃ¡c Há»a HÃ nh Vi (Analysis)

### 3.1. Sá»± Sá»¥p Äá»• TÆ°Æ¡ng Quan CÆ¡ TÃ­nh (Correlation Plummeting)
Khi Váº½ Trá»¥c $x=Layer$, $y=Pearson\ R\ (|Correlation|)$ giá»¯a Khá»‘i Vector HÆ°á»›ng Pattern HIM vÃ  Khá»‘i HÆ°á»›ng HER. 
- Táº¡i $4$ Layers Ä‘áº§u: $R$ ráº¥t cao. Pháº£n Ã¡nh Ä‘Ãºng Thá»±c táº¡i: "Him" vÃ  "Her" vá»‘n cÃ¹ng mang má»™t Há»‡ Ä‘áº·c tÃ­nh cÃº phÃ¡p (Grammar function) giá»‘ng y há»‡t nhau lÃ m Äáº¡i tá»« NhÃ¢n xÆ°ng (Pronouns). Sá»± khÃ¡c biá»‡t váº­t lÃ½ cá»§a máº¡ng trong lÃºc vá»«a nhai Nuá»‘t Token (Shallow layers) lÃ  Ráº¤T ÃT. 
- Táº¡i CÃ¡c Layers Cuá»‘i (Deeper layers): $R \to 0$. Transformer Ä‘Ã£ chuyá»ƒn tráº¡ng thÃ¡i tá»« viá»‡c PhÃ¢n TÃ­ch CÃº phÃ¡p Ná»™i Táº¡i $\to$ Tiáº¿n tá»›i TiÃªn ÄoÃ¡n TÆ°Æ¡ng Lai (Next-Token Prediction). LÃºc nÃ y, HÃ nh vi, Logic, Cáº¥u trÃºc khÃ´ng gian cá»§a con Äá»±c vÃ  con CÃ¡i ráº½ nhÃ¡nh hoÃ n toÃ n, khiáº¿n cÃ¡c HÃ m Pattern bay ra hai phÆ°Æ¡ng trá»i riÃªng biá»‡t.

### 3.2. Hiá»‡u NÄƒng VÆ°á»£t RÃ o Chá»‘ng Overfit (Significant Out-of-Sample Performance)
Äá»“ thá»‹ biá»ƒu diá»…n Test Data (Cháº¥m TrÃ²n vÃ  Dáº¥u $x$ Äá»): Háº§u nhÆ° toÃ n Cáº§u (12/12 Layers) Ä‘á»u ghi nháº­n Má»©c Ä‘á»™ TÃ¡ch Báº¡ch KhÃ¡c Biá»‡t Giá»›i TÃ­nh trÃªn FineWeb Test Set lÃ  Cá»±c KÃ¬ ÄÃ¡ng Tin Cáº­y ($p < 0.05 / 12$). DÃ¹ dá»¯ liá»‡u Fineweb cá»±c ká»³ nhiá»…u loáº¡n vá» Ngá»¯ cáº£nh, VectÆ¡ Máº¡ch Giá»›i TÃ­nh (Gender Circuit Vectors) do GED cá» xÃ¡t ra vÃ´ cÃ¹ng KiÃªn Cá»‘. Sá»£i dÃ¢y Cáº¥u trÃºc Giá»›i tÃ­nh Ä‘Ã£ thá»±c sá»± bá»‹ Cáº¯t Ra vÃ  CÃ´ Láº­p Ä‘Æ°á»£c Ä‘Ãºng Ä‘á»‹nh tuyáº¿n.

### 3.3. BÃ­ áº¨n Vá» Her Separability (Kháº£ nÄƒng TÃ¡ch biá»‡t Cá»§a Her)
Biá»ƒu Ä‘á»“ *Tiá»n Sá»‘ GiÃ¡ trá»‹ riÃªng (Eigenvalue Ratio $E_1/E_2$)* - Äo lÆ°á»ng Äá»™ Sáº¯c bÃ©n Cá»§a PhÃ©p Cáº¯t. Äá»“ thá»‹ Vá»“ng lÃªn táº¡o Äá»‰nh Ä‘á»“i chÃ³p táº¡i Khoáº£ng Layer sá»‘ 4. Cá»±c kÃ¬ ká»³ láº¡, NÄƒng lá»±c Cáº¯t "HER" tÃ¡ch khá»i "HIM" luÃ´n RÃµ nÃ©t vÃ  bÃ©n vÃ³t hÆ¡n viá»‡c pháº£i Cáº¯t "HIM" khá»i "HER". Hiá»‡n tÆ°á»£ng nÃ y xuáº¥t hiá»‡n trÃªn cáº£ GPT-2 Small vÃ  PhiÃªn báº£n Khá»•ng Khá»“ GPT-2 XL, chá»©ng tá» nÃ³ lÃ  má»™t Feature HÃ m áº©n Thuá»™c Vá» Cáº¥u TrÃºc Khá»‘i Dá»¯ Liá»‡u Internet (CÃ³ thá»ƒ do Táº§n suáº¥t xuáº¥t hiá»‡n, ngá»¯ phÃ¡p phÃ¢n cá»±c cá»§a PhÃ¡i ná»¯ trong Data thÃ´ ná»•i báº­t hÆ¡n) - Má»™t hiá»‡n tÆ°á»£ng mÃ  TÃ¡c giáº£ khÃ´ng thá»ƒ LÃ½ giáº£i táº­n cÃ¹ng.

---

## 4. Káº¿t Luáº­n
BÃ i thá»±c hÃ nh PhÃ¢n rÃ£ ToÃ¡n Há»c suy rá»™ng GED chá»‘t láº¡i Bá»©c Tranh ToÃ n Cáº£nh vá» Kháº£ nÄƒng BÃ³c Máº¡ch CÆ¡ há»c trong LLMs: Báº±ng cÃ¡ch Káº¿t Há»£p Äáº¡i sá»‘ Tuyáº¿n TÃ­nh Giáº£i TÃ­ch ($\mathbf{R}^{-1}\mathbf{S}$ Eigenvalue) dÆ°á»›i sá»± báº£o vá»‡ cá»§a KhÃ´ng gian Nhá» Äáº§u TiÃªn (PCA Pre-filter), ta hoÃ n toÃ n Ä‘á»§ sá»©c KhoÃ©t VÃ¡ch nhá»¯ng KhÃ¡i niá»‡m Cá»±c kÃ¬ VÃ´ HÃ¬nh (Giá»›i tÃ­nh Äáº¡i tá»«) ra khá»i bá» máº·t Hoáº¡t HÃ³a khá»•ng lá»“. VÃ  quan trá»ng nháº¥t, Hiá»‡n TÆ°á»£ng TÃ­ch Trá»¥ Äáº·c Äiá»ƒm vÃ  PhÃ¢n Ly (Separability / Decoupling) diá»…n ra máº¡nh máº½ nháº¥t Lá»›p Giá»¯a - LÃ²ng cháº£o biáº¿n thiÃªn Logic Ä‘Ã­ch thá»±c cá»§a má»™t Large Language Model.

---

## TÃ i Liá»‡u Tham Kháº£o (Citations)
1. Thá»±c nghiá»‡m Blind-Test GED Validation trÃªn ná»n FineWeb Text, dá»±a trÃªn `aero_LLM_14_CodeChallenge GED for category isolation across layers (part 2).md`. Kháº³ng Ä‘á»‹nh hiá»‡n tÆ°á»£ng sá»¥p Ä‘á»• Pattern Correlation á»Ÿ LÃ¡t cáº¯t Output Layers trong kiáº¿n trÃºc mÃ´ hÃ¬nh Transformer.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Máº¡ng Máº¡ch Thuáº­t ToÃ¡n (Circuits) Trong MÃ´ HÃ¬nh Há»c SÃ¢u](aero_LLM_01_What is a circuit in a DL model.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_What is a circuit in a DL model.md) |
| [CÃ´ Láº­p VÃ  ThÄƒm DÃ² Khá»‘i ChÃº Ã (Attention Heads)](aero_LLM_02_Isolating and investigating attention heads.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Isolating and investigating attention heads.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: Biá»ƒu Diá»…n PhÃ¢n Bá»‘ Nhiá»‡t Laminar Cá»§a Trá»ng Sá»‘ ChÃº Ã](aero_LLM_03_CodeChallenge Laminar profile of attention head weights.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Laminar profile of attention head weights.md) |
| [Kháº£o SÃ¡t TÆ°Æ¡ng Quan Cá»¥m (Clustering) Vi Máº¡ch (Circuits) Trong KhÃ´ng Gian Giáº£m Chiá»u](aero_LLM_04_Are circuits clustered in low-dimensional space.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_Are circuits clustered in low-dimensional space.md) |
| [LÃ½ Thuyáº¿t VÃ  á»¨ng Dá»¥ng Cá»§a Ká»¹ Thuáº­t DÃ² ThÆ°a (Sparse Probing)](aero_LLM_05_Sparse probing theory and code.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Sparse probing theory and code.md) |
| [ThÃ¡ch Thá»©c Cá»§a TÃ­n Hiá»‡u ThÆ°a Trong Dá»¯ Liá»‡u Táº­p Lá»›n (Statistical Suppression)](aero_LLM_06_Challenges with sparse logistic regression in large datasets.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_Challenges with sparse logistic regression in large datasets.md) |
| [Biáº¿n Tiá»m áº¨n (Latent) VÃ  Biáº¿n Hiá»ƒn NgÃ´n (Manifest) Trong Giáº£i Diá»…n AI](aero_LLM_07_Latent vs. manifest variables.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Latent vs. manifest variables.md) |
| [MÃ´ HÃ¬nh Sparse Autoencoders (SAEs): LÃ½ Thuyáº¿t VÃ  Kiáº¿n TrÃºc KhÃ´i Phá»¥c Vi Máº¡ch Tiá»m áº¨n](aero_LLM_08_Sparse autoencoders theory and code.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_Sparse autoencoders theory and code.md) |
| [Huáº¥n Luyá»‡n Sparse Autoencoder TrÃ­ch Xuáº¥t KhÃ¡i Niá»‡m Ngá»¯ Cáº£nh Palinka TrÃªn GPT-2](aero_LLM_09_SAE in GPT2 learns about Hungarian Palinka.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_SAE in GPT2 learns about Hungarian Palinka.md) |
| [Kháº£o SÃ¡t PhÃ¢n Táº§ng KÃ­ch Hoáº¡t (Laminar Profile) Qua Sparse Autoencoder](aero_LLM_10_CodeChallenge Laminar profile of autoencoder sparsity.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_CodeChallenge Laminar profile of autoencoder sparsity.md) |
| [Nháº­n Diá»‡n KhÃ¡i Niá»‡m XuyÃªn TÃ¢m Vá»›i PhÃ¢n RÃ£ GiÃ¡ Trá»‹ RiÃªng Suy Rá»™ng (Generalized Eigendecomposition - GED)](aero_LLM_11_Non-orthogonal latent components via eigendecomposition (theory and demo).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_Non-orthogonal latent components via eigendecomposition (theory and demo).md) |
| [Ráº¡ch RÃ²i Giá»›i TÃ­nh (Him vs Her) Báº±ng Generalized Eigendecomposition Trong MLP](aero_LLM_12_Generalized eigendecomposition separates him from her in MLP.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_Generalized eigendecomposition separates him from her in MLP.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): TÃ¡ch NhÃ³m GED Äa Táº§ng (Pháº§n 1)](aero_LLM_13_CodeChallenge GED for category isolation across layers (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_CodeChallenge GED for category isolation across layers (part 1).md) |
| ğŸ“Œ **[Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): TÃ¡ch NhÃ³m GED Äa Táº§ng (Pháº§n 2) & Kiá»ƒm Chá»©ng ChÃ©o](aero_LLM_14_CodeChallenge GED for category isolation across layers (part 2).md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_14_CodeChallenge GED for category isolation across layers (part 2).md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [10 Identifying circuits](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Máº¡ng Máº¡ch Thuáº­t ToÃ¡n (Circuits) Trong MÃ´ HÃ¬nh Há»c SÃ¢u

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y má»Ÿ ra má»™t khÃ¡i niá»‡m trá»ng tÃ¢m trong nghiÃªn cá»©u CÆ¡ há»c Diá»…n dá»‹ch (Mechanistic Interpretability): Máº¡ng máº¡ch (Circuits). Náº±m á»Ÿ cáº¥p Ä‘á»™ trá»«u tÆ°á»£ng cao hÆ¡n so vá»›i viá»‡c phÃ¢n tÃ­ch cÃ¡c nÆ¡-ron Ä‘Æ¡n láº», nhÆ°ng láº¡i thu háº¹p hÆ¡n so vá»›i viá»‡c Ä‘Ã¡nh giÃ¡ toÃ n bá»™ Vector nhÃºng (Embeddings) hay Khá»‘i xá»­ lÃ½ (Transformer Layers), khÃ¡i niá»‡m "Circuit" Ä‘áº¡i diá»‡n cho má»™t cá»¥m cÃ¡c chiá»u khÃ´ng gian (dimensions) hay cá»¥m nÆ¡-ron há»£p tÃ¡c Ä‘á»ƒ thá»±c thi má»™t vi tÃ¡c vá»¥ luáº­n lÃ½ cá»¥ thá»ƒ. Viá»‡c truy tÃ¬m cÃ¡c máº¡ng máº¡ch nÃ y Ä‘á»‘i diá»‡n vá»›i nghá»‹ch lÃ½ giá»›i háº¡n bá»Ÿi tÃ­nh liÃªn tá»¥c (Continuous parameters) tá»± nhiÃªn cá»§a ToÃ¡n há»c há»c sÃ¢u vÃ  sá»± thay Ä‘á»•i linh hoáº¡t theo ngá»¯ cáº£nh, biáº¿n "Máº¡ch thuáº­t toÃ¡n" trá»Ÿ thÃ nh má»™t khÃ¡i niá»‡m khÃ³ Ä‘á»‹nh nghÄ©a báº±ng ranh giá»›i váº­t lÃ½ tÄ©nh, sinh ra má»™t bÃ i toÃ¡n má»Ÿ Ä‘áº§y há»©a háº¹n.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Trong thiáº¿t káº¿ pháº§n cá»©ng, má»™t báº£ng vi máº¡ch (Electronic circuit) lÃ  há»‡ thá»‘ng cÃ³ ranh giá»›i, káº¿t ná»‘i rÃµ rÃ ng vÃ¬ do con ngÆ°á»i hÃ n ná»‘i ráº­p khuÃ´n theo má»¥c Ä‘Ã­ch thiáº¿t káº¿. Tuy nhiÃªn, trong CÆ¡ há»c há»‡ thá»‘ng phá»©c táº¡p tá»± sinh (Emergent Complex Systems) â€“ nhÆ° NÃ£o bá»™ sinh há»c hay TrÃ­ tuá»‡ nhÃ¢n táº¡o (Deep Learning Language Models), khÃ¡i niá»‡m Máº¡ch trá»Ÿ nÃªn trá»«u tÆ°á»£ng vÃ  phi ranh giá»›i. 
NghiÃªn cá»©u vá» Circuits hÆ°á»›ng Ä‘áº¿n viá»‡c tráº£ lá»i cÃ¢u há»i: LÃ m tháº¿ nÃ o nhá»¯ng tá»• há»£p nhá» cÃ¡c thÃ´ng sá»‘ toÃ¡n há»c cÃ³ thá»ƒ tá»± Ä‘á»™ng bá»‡n xoáº¯n vá»›i nhau táº¡o thÃ nh má»™t cá»¥m chá»©c nÄƒng (vÃ­ dá»¥: Cá»¥m nháº­n diá»‡n sá»‘ nhiá»u, Cá»¥m phÃ¡t hiá»‡n lá»—i chÃ­nh táº£, Cá»¥m chÃº Ã½ Ä‘áº¡i tá»« nhÃ¢n xÆ°ng)? 

---

## 2. Kiáº¿n TrÃºc Cá»§a Má»™t Máº¡ch Vi MÃ´ HÃ¬nh (Circuits Architecture)

### 2.1. Äa Táº§ng Káº¿t Cáº¥u Sinh Há»c
TÆ°Æ¡ng tá»± sá»± tiáº¿n hÃ³a cá»§a Giáº£i pháº«u nÃ£o bá»™ (Neuroanatomy), cáº¥u trÃºc "máº¡ch" trong má»™t há»‡ thá»‘ng lá»›n chá»©a hÃ ng tá»· tham sá»‘ thÆ°á»ng chia lÃ m nhiá»u cáº¥p Ä‘á»™:
1. **Cáº¥p NÆ¡-ron ÄÆ¡n (Subcellular):** á» nhÃ¡nh sinh há»c, trÃªn má»™t táº¿ bÃ o cÃ³ thá»ƒ chia thÃ nh nhiá»u phÃ¢n luá»“ng vi máº¡ch ráº½ nhÃ¡nh Ä‘uÃ´i gai (dendrites) tÃ­nh toÃ¡n riÃªng. á» AI, nÃ³ lÃ  tÃ­nh nÄƒng phi tuyáº¿n á»Ÿ má»™t Head Ä‘á»™c láº­p.
2. **Cáº¥p Cá»¥m Táº¿ BÃ o (Local Network):** Tá»• há»£p gá»“m hÃ ng trÄƒm Neuron liÃªn káº¿t nhau gÃ¡nh vÃ¡c má»™t kháº£ nÄƒng nháº­n thá»©c nhá». á» AI, nÃ³ lÃ  má»™t "Circuit" náº±m yÃªn trong má»™t nhÃ¡nh Attention hay MLP cá»¥ thá»ƒ.
3. **Cáº¥p VÃ¹ng Nháº­n Thá»©c (Macro-regions):** LiÃªn káº¿t nhiá»u mÃ´-Ä‘un Ä‘á»ƒ giáº£i quyáº¿t luáº­n lÃ½ vÄ© mÃ´. á» AI, Ä‘Ã¢y lÃ  cÃ¡c khá»‘i Circuit Ä‘an chÃ©o vÆ°á»£t thá»i gian nhiá»u táº§ng lá»›p rá»… (Multiple Layers).

### 2.2. Báº£n Cháº¥t Báº¥t Äá»‹nh Cá»§a Thuáº­t ToÃ¡n SÃ¢u (Continuous Activations) 
BÃ i toÃ¡n tÃ¬m ra Circuit cá»§a má»™t máº¡ng LM gáº·p ba thÃ¡ch thá»©c siÃªu viá»‡t:
- KhÃ´ng giá»‘ng nÃ£o bá»™ Ä‘o Ä‘Æ°á»£c dÃ²ng xung Ä‘iá»‡n tÃ¡ch biá»‡t, trong Há»c sÃ¢u, cÃ¡c trá»ng sá»‘ tham sá»‘ (Weights) vÃ  má»©c kÃ­ch hoáº¡t tuyáº¿n tÃ­nh (Activations) mang giÃ¡ trá»‹ sá»‘ thá»±c liÃªn tá»¥c (Continuous Numbers). Trá»« khi sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t Ã©p chuáº©n L1 Regularizations hay Dropout liÃªn tá»¥c, vá» cÆ¡ báº£n khÃ´ng bao giá» cÃ³ má»™t cá»¥m nÆ¡-ron nÃ o hoÃ n toÃ n Ä‘á»©t káº¿t ná»‘i báº±ng 0 tÄ©nh. Sá»± tÃ­nh toÃ¡n cá»§a chÃºng lan tá»a váº¡n váº­t, cáº£n trá»Ÿ viá»‡c xÃ¡c Ä‘á»‹nh Ä‘Ã¢u lÃ  viá»n ngoÃ i cá»§a má»™t Circuit lÃµi.
- "Circuit Fluidity": Cáº¥u trÃºc máº¡ng máº¡ch khÃ´ng Ä‘á»©ng yÃªn. Khi chuá»—i Token dáº§n dáº§n Ä‘Æ°á»£c náº¡p thÃªm vÃ o ngá»¯ cáº£nh (Context length), cÃ¡c thÃ nh viÃªn cáº¥u thÃ nh nÃªn máº¡ch hiá»‡n táº¡i cÃ³ thá»ƒ Ä‘Æ°á»£c thay tháº¿ hoáº·c dáº¡t hÆ°á»›ng Ä‘á»ƒ phá»¥c vá»¥ má»™t Máº¡ch má»›i Ä‘á»™ng.

---

## 3. KhÃ¡i Niá»‡m Hoáº¡t Äá»™ng (Operational Definition)
Trong giá»›i háº¡n cÆ¡ báº£n, chÃºng ta xem Circuit trong LLMs báº±ng lÄƒng kÃ­nh **Thá»‘ng kÃª CÆ¡ há»c (Statistical techniques)**: Máº¡ch lÃ  má»™t táº­p con cÃ¡c mÃ´-Ä‘un (neurons hoáº·c heads) cho tháº¥y sá»± biá»ƒu thá»‹ cÃ¹ng hÃ nh vi (Behaving in similar localized ways) dÆ°á»›i cÃ¡c loáº¡i Context xÃ¡c Ä‘á»‹nh, Ä‘Æ°á»£c khai thÃ¡c thÃ´ng qua phÃ¢n rÃ£ thÃ nh pháº§n vÄ© mÃ´ hoáº·c há»“i quy thÆ°a thá»›t (Sparse probing). QuÃ¡ trÃ¬nh giáº£i pháº«u sáº½ Ä‘Æ°á»£c triá»ƒn khai báº±ng cÃ¡ch bÃ³c tÃ¡ch tá»«ng Attention Head trÃªn ná»n cÃ¡c phÃ©p toÃ¡n tÆ°Æ¡ng quan ma tráº­n.

---

## 4. Káº¿t Luáº­n
Viá»‡c bá»c tÃ¡ch thuáº­t toÃ¡n Máº¡ch (Circuits tracking) lÃ  ngá»n giÃ¡o sáº¯c bÃ©n nháº¥t cá»§a Mechanistic Interpretability hiá»‡n nay, nhÆ°ng cÅ©ng áº©n chá»©a nhÆ°á»£c Ä‘iá»ƒm "Diá»…n dá»‹ch quÃ¡ Ä‘Ã " (Over-interpretation) khi ta cá»‘ gÃ² Ã©p mÃ¡y mÃ³c tÆ° duy theo mÃ´ hÃ¬nh sinh há»c tÄ©nh. Sá»± nháº­n thá»©c tháº¥u Ä‘Ã¡o vÃ  linh hoáº¡t sáº½ má»Ÿ Ä‘Æ°á»ng cho nhá»¯ng chuá»—i bÃ i táº­p cÃ´ láº­p tá»«ng Head vÃ  phÃ¢n tÃ­ch biáº¿n sá»‘ áº©n sáº¯p tá»›i.

---

## TÃ i Liá»‡u Tham Kháº£o (Citations)
1. Tháº£o luáº­n lÃ½ thuyáº¿t cá»‘t lÃµi vá» báº£n thá»ƒ há»c cá»§a Circuits, sá»± khÃ¡c biá»‡t giá»¯a Engineering circuit vÃ  Emergent complex system phÃ¢n tÃ­ch tá»« `aero_LLM_01_What is a circuit in a DL model.md`.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| ğŸ“Œ **[Máº¡ng Máº¡ch Thuáº­t ToÃ¡n (Circuits) Trong MÃ´ HÃ¬nh Há»c SÃ¢u](aero_LLM_01_What is a circuit in a DL model.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_01_What is a circuit in a DL model.md) |
| [CÃ´ Láº­p VÃ  ThÄƒm DÃ² Khá»‘i ChÃº Ã (Attention Heads)](aero_LLM_02_Isolating and investigating attention heads.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Isolating and investigating attention heads.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: Biá»ƒu Diá»…n PhÃ¢n Bá»‘ Nhiá»‡t Laminar Cá»§a Trá»ng Sá»‘ ChÃº Ã](aero_LLM_03_CodeChallenge Laminar profile of attention head weights.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Laminar profile of attention head weights.md) |
| [Kháº£o SÃ¡t TÆ°Æ¡ng Quan Cá»¥m (Clustering) Vi Máº¡ch (Circuits) Trong KhÃ´ng Gian Giáº£m Chiá»u](aero_LLM_04_Are circuits clustered in low-dimensional space.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_Are circuits clustered in low-dimensional space.md) |
| [LÃ½ Thuyáº¿t VÃ  á»¨ng Dá»¥ng Cá»§a Ká»¹ Thuáº­t DÃ² ThÆ°a (Sparse Probing)](aero_LLM_05_Sparse probing theory and code.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Sparse probing theory and code.md) |
| [ThÃ¡ch Thá»©c Cá»§a TÃ­n Hiá»‡u ThÆ°a Trong Dá»¯ Liá»‡u Táº­p Lá»›n (Statistical Suppression)](aero_LLM_06_Challenges with sparse logistic regression in large datasets.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_Challenges with sparse logistic regression in large datasets.md) |
| [Biáº¿n Tiá»m áº¨n (Latent) VÃ  Biáº¿n Hiá»ƒn NgÃ´n (Manifest) Trong Giáº£i Diá»…n AI](aero_LLM_07_Latent vs. manifest variables.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Latent vs. manifest variables.md) |
| [MÃ´ HÃ¬nh Sparse Autoencoders (SAEs): LÃ½ Thuyáº¿t VÃ  Kiáº¿n TrÃºc KhÃ´i Phá»¥c Vi Máº¡ch Tiá»m áº¨n](aero_LLM_08_Sparse autoencoders theory and code.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_Sparse autoencoders theory and code.md) |
| [Huáº¥n Luyá»‡n Sparse Autoencoder TrÃ­ch Xuáº¥t KhÃ¡i Niá»‡m Ngá»¯ Cáº£nh Palinka TrÃªn GPT-2](aero_LLM_09_SAE in GPT2 learns about Hungarian Palinka.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_SAE in GPT2 learns about Hungarian Palinka.md) |
| [Kháº£o SÃ¡t PhÃ¢n Táº§ng KÃ­ch Hoáº¡t (Laminar Profile) Qua Sparse Autoencoder](aero_LLM_10_CodeChallenge Laminar profile of autoencoder sparsity.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_CodeChallenge Laminar profile of autoencoder sparsity.md) |
| [Nháº­n Diá»‡n KhÃ¡i Niá»‡m XuyÃªn TÃ¢m Vá»›i PhÃ¢n RÃ£ GiÃ¡ Trá»‹ RiÃªng Suy Rá»™ng (Generalized Eigendecomposition - GED)](aero_LLM_11_Non-orthogonal latent components via eigendecomposition (theory and demo).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_Non-orthogonal latent components via eigendecomposition (theory and demo).md) |
| [Ráº¡ch RÃ²i Giá»›i TÃ­nh (Him vs Her) Báº±ng Generalized Eigendecomposition Trong MLP](aero_LLM_12_Generalized eigendecomposition separates him from her in MLP.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_Generalized eigendecomposition separates him from her in MLP.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): TÃ¡ch NhÃ³m GED Äa Táº§ng (Pháº§n 1)](aero_LLM_13_CodeChallenge GED for category isolation across layers (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_CodeChallenge GED for category isolation across layers (part 1).md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): TÃ¡ch NhÃ³m GED Äa Táº§ng (Pháº§n 2) & Kiá»ƒm Chá»©ng ChÃ©o](aero_LLM_14_CodeChallenge GED for category isolation across layers (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_CodeChallenge GED for category isolation across layers (part 2).md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

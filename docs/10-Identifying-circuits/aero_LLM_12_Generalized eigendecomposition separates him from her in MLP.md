
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [10 Identifying circuits](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Ráº¡ch RÃ²i Giá»›i TÃ­nh (Him vs Her) Báº±ng Generalized Eigendecomposition Trong MLP

## TÃ³m táº¯t (Abstract)
ThÃ¡ch thá»©c lá»›n nháº¥t khi Ã¡p dá»¥ng Generalized Eigendecomposition (GED) trÃªn Máº¡ng ngÃ´n ngá»¯ Lá»›n (LLMs) náº±m á»Ÿ Váº¥n Ä‘á» Khá»§ng hoáº£ng KhÃ´ng gian Äa chiá»u: QuÃ¡ nhiá»u Biáº¿n sá»‘ (Neurons) nhÆ°ng Cáº¥p báº­c thá»© háº¡ng dá»¯ liá»‡u (Rank) láº¡i quÃ¡ tháº¥p, khiáº¿n hÃ m ma tráº­n khÃ´ng thá»ƒ tá»± nghá»‹ch Ä‘áº£o. BÃ¡o cÃ¡o nÃ y Ä‘Æ°a ra phÆ°Æ¡ng thá»©c "NÃ©n rá»“i TÃ¡ch" (Two-stage Compression-Separation Procedure) báº±ng cÃ¡ch Ã©p pháº³ng KhÃ´ng gian $3072$ chiá»u thÃ nh $63$ chiá»u thÃ´ng qua PCA, sau Ä‘Ã³ má»›i dÃ¹ng thuáº­t GED cÃ³ Shrinkage Regularization. Káº¿t quáº£ trÃªn bá»™ test-set CÃ¢u Äiá»u kiá»‡n Äáº¡i Tá»« (Pronouns dataset) cho tháº¥y thuáº­t toÃ¡n tÃ¡ch Ä‘Ã´i vÃ  khoanh vÃ¹ng Ä‘á»™c láº­p thÃ nh cÃ´ng LÆ°á»›i kÃ­ch hoáº¡t dÃ nh riÃªng cho tá»« 'Him' so diá»‡n vá»›i KhÃ´ng gian dÃ nh riÃªng cho 'Her' ngay táº¡i MLP Expansion. 

---

## 1. Má»Ÿ Äáº§u (Introduction)
Tá»« Demo mÃ´ phá»ng á»Ÿ chÆ°Æ¡ng trÆ°á»›c, chÃºng ta Ä‘Ã£ náº¯m Ä‘Æ°á»£c GED hoáº¡t Ä‘á»™ng báº±ng cÃ¡ch lÃ m PhÃ©p chia Tá»‰ Lá»‡ (SNR) giá»¯a Khá»‘i Cáº§u TÃ­n hiá»‡u vÃ  Khá»‘i Cáº§u Äá»‘i Chiáº¿u. Khi Ã¡p dá»¥ng tháº³ng vÃ o KhÃ´ng gian Thá»±c cá»§a MÃ´ hÃ¬nh Transformers, tá»©c $\sim 3072$ chiá»u NÆ¡-ron trÃªn hÃ ng ngÃ n Lá»›p Token, mÃ¡y tÃ­nh sáº½ bÃ¡o lá»—i VÃ´ Hiá»‡u Lá»‡nh (Rank Deficient) do $R^{-1}$ tiáº¿n tá»›i vÃ´ cá»±c. 
Nhiá»‡m vá»¥ cá»§a bÃ i thÃ­ nghiá»‡m lÃ  pháº£i Cáº¯t Bá» Má»¡ Thá»«a (Data Compression) cho bá»™ Data trÆ°á»›c khi Ä‘Æ°a vÃ o MÃ¡y váº¯t GED: Thay vÃ¬ thao tÃ¡c trÃªn 3072 tham sá»‘ rá»—ng, ta thu bÃ© nÃ³ láº¡i vá» má»™t NhÃ³m Nhá» Äáº¡i Diá»‡n nhÆ°ng váº«n chá»©a $99\%$ nÄƒng lÆ°á»£ng biáº¿n thiÃªn cá»§a dá»¯ liá»‡u. 

---

## 2. Tiáº¿t Thiáº¿t Láº­p Cáº¥u TrÃºc Khá»‘i NÃ©n KÃ©p (Methodology)

### 2.1. Giáº£i Thuáº­t Hai Giai Äoáº¡n (Two-stage Separation Procedure)
Khi $\text{Rank} \ll \text{Size}$, PhÃ©p TÃ­nh Eigendecomposition trá»Ÿ nÃªn báº¥t á»•n tá»™t Ä‘á»™. Ta thi hÃ nh "PCA lá»c Ná»n":
1. TrÃ­ch xuáº¥t Activations kÃ­ch thÆ°á»›c `[N_Máº«u_cÃ¢u, 3072_Neurons]`. 
2. Cháº¡y **PCA** trÃªn Ma Tráº­n Trung BÃ¬nh (Ave18-RAGe Covariance Matrix) cá»§a cáº£ Hai Dá»¯ Kho (Cáº£ HIM vÃ  HER gá»™p chung). Táº¡i sao? Äá»ƒ PCA Ä‘i lÃ¹ng sá»¥c **"ToÃ n bá»™ vÃ¹ng khÃ´ng gian chung mÃ  Cáº£ hai Ä‘á»‘i tÆ°á»£ng nÃ y cÃ¹ng kÃ­ch hoáº¡t"**, lá»c láº¥y cÃ¡c PhÃ¢n máº£nh ChÃ­nh mang tÃ­nh sá»‘ng cÃ²n.
3. Cáº¯t LÃ¡t (Scree Plot Cut-off): Chá»‰ giá»¯ láº¡i cÃ¡c PC gá»™p Ä‘á»§ $99\%$ lÆ°á»£ng Variance (Lá»‡ch chuáº©n) cÃ¹a toÃ n Ä‘á»“ thá»‹. VÃ­ dá»¥ á»Ÿ Ä‘Ã¢y ta thu vá» NhÃ³m Tinh TÃºy $63$ Máº¡ch $PC$.
4. **Chiáº¿u RÃºt Chiá»u:** PhÃ³ng (Project) khá»‘i Dá»¯ liá»‡u Gá»‘c lÃªn khÃ´ng gian 63 chiá»u má»›i nÃ y Ä‘á»ƒ "XÃ³a sá»• 3000 chiá»u RÃ¡c".

### 2.2. Trá»±c KhÃ¡n Vá»›i Shrinkage (Shrinkage Regularized GED)
Tuyá»ƒn 63-Dimension Matrix má»›i cÃ³ váº» bÃ©, nhÆ°ng báº£n thÃ¢n nÃ³ váº«n bá»‹ VÆ°á»›ng Rank Zero! NghÄ©a lÃ  $\text{Rank}(Cov) = 52 < 63$. 
Ãp dá»¥ng cÆ¡ cháº¿ Covariance Shrinking $1\%$ ($\gamma = 0.01$):
$$ \tilde{\mathbf{R}} = (1 - 0.01)\mathbf{R} + 0.01 \alpha \mathbf{I} $$
PhÃ©p toÃ¡n nÃ y biáº¿n hÃ³a Rank $52 \xrightarrow{Inflate} 63$ (Full Rank). LÃºc nÃ y hÃ m vi phÃ¢n cá»§a SciPy (`scipy.linalg.eigh`) cÃ³ thá»ƒ tiÃªu hÃ³a ma tráº­n $R_{her\_shrunk}^{-1} \cdot S_{him}$ hoÃ n toÃ n trÆ¡n tru.

---

## 3. Kháº£o SÃ¡t TÃ¡ch Máº¡ch CÄƒn Giá»›i (Analysis)

### 3.1. Sá»± Trá»—i Dáº­y Cá»§a ThÃ nh Pháº§n PhÃ¢n Cá»±c Tuyá»‡t Äá»‘i (Top Eigenvector)
Khi GED hoÃ n táº¥t, há»‡ sá»‘ Trá»‹ RiÃªng (Eigenvalues) Ä‘Æ°á»£c sáº¯p xáº¿p tá»« cao xuá»‘ng tháº¥p. Top 1 Eigenvalue cho tháº¥y cÃ³ má»™t VectÆ¡ Ä‘áº·c biá»‡t (Eigenvector) mÃ  khi dá»¯ liá»‡u chiáº¿u vÃ o:
- NÃ³ TrÃ n Äáº§y NÄƒng lÆ°á»£ng (Táº¡o Max Variance) khi dá»¯ liá»‡u mang chá»¯ $HIM$.
- NÃ³ Triá»‡t TiÃªu NÄƒng lÆ°á»£ng (ChÃ¬m nghá»‰m thÃ nh Zero Variance) khi dá»¯ liá»‡u mang chá»¯ $HER$.
(VÃ  khi Ä‘áº£o $\mathbf{S=Her}, \mathbf{R=Him}$, ta láº¡i tháº¥y Ä‘iá»u ngÆ°á»£c láº¡i hoáº¡t Ä‘á»™ng song song).
Do khÃ´ng cÃ³ Ä‘iá»u kiá»‡n rÃ ng buá»™c Trá»±c Giao (Orthogonality), VectÆ¡ tÃ¬m tháº¥y Ä‘Ã£ "tháº©m tháº¥u lÃ¡ch mÃ¬nh" má»™t cÃ¡ch uyá»ƒn chuyá»ƒn theo dá»c chiá»u PhÃ¢n Lá»›p Giá»›i TÃ­nh chá»© khÃ´ng bá»‹ Ã©p xoay 90 Ä‘á»™ cá»©ng ngáº¯c nhÆ° PCA.

### 3.2. Hiá»‡u á»¨ng Loáº¡i Cá»«u Khá»i Báº§y XÃ³i (Sentence Contrast Validation)
Kiá»ƒm chá»©ng tÃ­nh "ChuyÃªn biá»‡t" (Selectivity) cá»§a Vector nÃ y: Ta dÃ¹ng bá»™ VectÆ¡ TÃ¡ch HIM nÃ©m chá»“ng lÃªn toÃ n bá»™ Trá»¥c KÃ­ch Hoáº¡t cá»§a Má»˜T cÃ¢u chá»¯ dÃ i (Bao gá»“m cÃ¡c tá»« khÃ´ng liÃªn quan: The, dog, was...).
Káº¿t quáº£ Ä‘Ã¡ng kinh ngáº¡c:
- Äá»‘i vá»›i háº§u háº¿t cÃ¡c tá»« nhÆ° The, Dog: NÄƒng lÆ°á»£ng KÃ­ch Hoáº¡t $\dots$ TÆ°Æ¡ng Ä‘Æ°Æ¡ng nhau (Ráº¥t tháº¥p do bá»‹ lá»c Nhiá»…u).
- Chá»‰ riÃªng táº¡i NgÆ°á»¡ng cáº¯t Token vá»‹ trÃ­ Äáº¡i Tá»« ("Him" hoáº·c "Her"), Ä‘á»“ thá»‹ cÃ³ cÃº xÃ© toáº¡c tháº³ng Ä‘á»©ng: CÃ¹ng má»™t bá»™ lá»c, Chá»¯ HIM vÄƒng Ä‘á»‰nh cá»±c Ä‘áº¡i, Chá»¯ HER thá»¥t Ä‘Ã¡y táº­n cÃ¹ng. Hiá»‡n tÆ°á»£ng láº­t Máº·t Phiáº¿n (Flip Activation) nÃ y xÃ¡c nháº­n ta Ä‘Ã£ TÃ¡ch thÃ nh cÃ´ng Máº¡ch Ná»™i Suy Giá»›i tÃ­nh CÃ´ láº­p Ä‘á»™c láº­p hoÃ n toÃ n khá»i há»‡ thá»‘ng cáº¥u trÃºc cÃº phÃ¡p ná»n (Grammar Base).

---

## 4. Káº¿t Luáº­n
Viá»‡c Ã¡p Ä‘áº·t tháº³ng thuáº­t toÃ¡n GED lÃªn Dá»¯ liá»‡u Khá»•ng Lá»“ LLM lÃ  tá»± sÃ¡t mÃ´ hÃ¬nh. NhÆ°ng thÃ´ng qua Chiáº¿n thuáº­t NÃ©n KhÃ´ng gian $\to$ Ãp dá»¥ng Äiá»u tiáº¿t Shrinkage, bÃ i toÃ¡n chia cáº¯t Cáº¥u TÃ­nh Máº¡ch Tá»« Vá»±ng (VÃ­ dá»¥ TÃ¡ch biá»‡t giá»›i tÃ­nh Äáº¡i Tá»«) giá»¯a hÃ ng NgÃ n NÆ¡-ron MLP trá»Ÿ thÃ nh hiá»‡n thá»±c rá»±c rá»¡ vÃ  dá»… dÃ ng truy váº¿t. PhÆ°Æ¡ng phÃ¡p GED bá»™c lá»™ tÃ­nh VÆ°á»£t Trá»™i so vá»›i SAEs hay Logistic Regression á»Ÿ Ä‘iá»ƒm nÃ³ "PhÃ¢n RÃ£ Hai Äá»‘i Thá»§ KhÃ´ng Gian" ra má»™t cÃ¡ch Trá»±c Quan cá»±c Ä‘áº¡i mÃ  khÃ´ng cáº§n má»™t Biáº¿n ÄÃ­ch Lable Label kháº¯t khe. 

---

## TÃ i Liá»‡u Tham Kháº£o (Citations)
1. ThÃ­ nghiá»‡m "Two-stage compression + GED" bÃ³c tÃ¡ch Äáº¡i tá»« Giá»›i tÃ­nh tá»« file `aero_LLM_12_Generalized eigendecomposition separates him from her in MLP.md`. Giáº£i pháº«u hiá»‡n tÆ°á»£ng TrÃ n Rank-Deficient vÃ  cÃ¡ch há»“i sinh thÃ nh Full-Rank báº±ng $\gamma=0.01$ Shrinkage parameter.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Máº¡ng Máº¡ch Thuáº­t ToÃ¡n (Circuits) Trong MÃ´ HÃ¬nh Há»c SÃ¢u](aero_LLM_01_What is a circuit in a DL model.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_What is a circuit in a DL model.md) |
| [CÃ´ Láº­p VÃ  ThÄƒm DÃ² Khá»‘i ChÃº Ã (Attention Heads)](aero_LLM_02_Isolating and investigating attention heads.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Isolating and investigating attention heads.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: Biá»ƒu Diá»…n PhÃ¢n Bá»‘ Nhiá»‡t Laminar Cá»§a Trá»ng Sá»‘ ChÃº Ã](aero_LLM_03_CodeChallenge Laminar profile of attention head weights.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Laminar profile of attention head weights.md) |
| [Kháº£o SÃ¡t TÆ°Æ¡ng Quan Cá»¥m (Clustering) Vi Máº¡ch (Circuits) Trong KhÃ´ng Gian Giáº£m Chiá»u](aero_LLM_04_Are circuits clustered in low-dimensional space.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_Are circuits clustered in low-dimensional space.md) |
| [LÃ½ Thuyáº¿t VÃ  á»¨ng Dá»¥ng Cá»§a Ká»¹ Thuáº­t DÃ² ThÆ°a (Sparse Probing)](aero_LLM_05_Sparse probing theory and code.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Sparse probing theory and code.md) |
| [ThÃ¡ch Thá»©c Cá»§a TÃ­n Hiá»‡u ThÆ°a Trong Dá»¯ Liá»‡u Táº­p Lá»›n (Statistical Suppression)](aero_LLM_06_Challenges with sparse logistic regression in large datasets.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_Challenges with sparse logistic regression in large datasets.md) |
| [Biáº¿n Tiá»m áº¨n (Latent) VÃ  Biáº¿n Hiá»ƒn NgÃ´n (Manifest) Trong Giáº£i Diá»…n AI](aero_LLM_07_Latent vs. manifest variables.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Latent vs. manifest variables.md) |
| [MÃ´ HÃ¬nh Sparse Autoencoders (SAEs): LÃ½ Thuyáº¿t VÃ  Kiáº¿n TrÃºc KhÃ´i Phá»¥c Vi Máº¡ch Tiá»m áº¨n](aero_LLM_08_Sparse autoencoders theory and code.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_Sparse autoencoders theory and code.md) |
| [Huáº¥n Luyá»‡n Sparse Autoencoder TrÃ­ch Xuáº¥t KhÃ¡i Niá»‡m Ngá»¯ Cáº£nh Palinka TrÃªn GPT-2](aero_LLM_09_SAE in GPT2 learns about Hungarian Palinka.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_SAE in GPT2 learns about Hungarian Palinka.md) |
| [Kháº£o SÃ¡t PhÃ¢n Táº§ng KÃ­ch Hoáº¡t (Laminar Profile) Qua Sparse Autoencoder](aero_LLM_10_CodeChallenge Laminar profile of autoencoder sparsity.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_CodeChallenge Laminar profile of autoencoder sparsity.md) |
| [Nháº­n Diá»‡n KhÃ¡i Niá»‡m XuyÃªn TÃ¢m Vá»›i PhÃ¢n RÃ£ GiÃ¡ Trá»‹ RiÃªng Suy Rá»™ng (Generalized Eigendecomposition - GED)](aero_LLM_11_Non-orthogonal latent components via eigendecomposition (theory and demo).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_Non-orthogonal latent components via eigendecomposition (theory and demo).md) |
| ğŸ“Œ **[Ráº¡ch RÃ²i Giá»›i TÃ­nh (Him vs Her) Báº±ng Generalized Eigendecomposition Trong MLP](aero_LLM_12_Generalized eigendecomposition separates him from her in MLP.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_12_Generalized eigendecomposition separates him from her in MLP.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): TÃ¡ch NhÃ³m GED Äa Táº§ng (Pháº§n 1)](aero_LLM_13_CodeChallenge GED for category isolation across layers (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_CodeChallenge GED for category isolation across layers (part 1).md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): TÃ¡ch NhÃ³m GED Äa Táº§ng (Pháº§n 2) & Kiá»ƒm Chá»©ng ChÃ©o](aero_LLM_14_CodeChallenge GED for category isolation across layers (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_CodeChallenge GED for category isolation across layers (part 2).md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

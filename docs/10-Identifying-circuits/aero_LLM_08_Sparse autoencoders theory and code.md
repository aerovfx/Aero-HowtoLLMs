
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [10 Identifying circuits](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# MÃ´ HÃ¬nh Sparse Autoencoders (SAEs): LÃ½ Thuyáº¿t VÃ  Kiáº¿n TrÃºc KhÃ´i Phá»¥c Vi Máº¡ch Tiá»m áº¨n

## TÃ³m táº¯t (Abstract)
Autoencoders, cá»¥ thá»ƒ lÃ  Sparse Autoencoders (SAEs), ná»•i lÃªn nhÆ° má»™t cÃ´ng cá»¥ giáº£i mÃ£ há»‡ thá»‘ng Máº¡ng lÆ°á»›i Lá»›n (LLM) Ä‘á»ƒ dÃ² tÃ¬m cÃ¡c khÃ¡i niá»‡m Tiá»m áº©n (Latent constructs) tá»« dá»¯ liá»‡u Hiá»ƒn ngÃ´n. BÃ i viáº¿t khai phÃ¡ Kiáº¿n trÃºc Phá»¥c dá»±ng Cáº¥u trÃºc hÃ m (Autoencoder Architecture) Ä‘Æ°á»£c xÃ¢y dá»±ng theo hÃ¬nh dáº¡ng Äá»“ng há»“ cÃ¡t má»Ÿ rá»™ng (Expanded bottleneck), nÆ¡i chiá»u dá»¯ liá»‡u bá»‹ Ã©p phÃ¬nh Ä‘á»ƒ Ã©p tÆ°Æ¡ng tÃ¡c ThÆ°a hÃ³a (Sparsity L1 Penalty). Táº¡i phÃ²ng thÃ­ nghiá»‡m kiá»ƒm soÃ¡t báº±ng ma tráº­n dá»¯ liá»‡u giáº£ láº­p $2 \times 3000$ (Bao gá»“m sÃ³ng Sine vÃ  Biáº¿n nhá»‹ phÃ¢n), SAEs Ä‘Ã£ chá»©ng minh Ä‘Æ°á»£c kháº£ nÄƒng giáº£i nÃ©n thÃ nh cÃ´ng hai Ä‘áº·c tÃ­nh TÃ­n hiá»‡u Ná»n gá»‘c, chá»©ng minh sá»± kháº£ thi cá»§a phÆ°Æ¡ng thá»©c giáº£i pháº«u mÃ´ hÃ¬nh Sinh ngá»¯ mÃ  khÃ´ng cáº§n thao tÃºng Ä‘á»™c láº­p tá»«ng NÆ¡-ron.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Tá»« chÆ°Æ¡ng "Biáº¿n Latent vs Biáº¿n Manifest", khoa há»c Giáº£i diá»…n (Interpretability) cáº§n má»™t cÃ´ng cá»¥ ToÃ¡n há»c tá»± váº­n hÃ nh (Unsupervised) Ä‘á»ƒ mÃ³c ná»‘i dá»¯ liá»‡u Nhiá»…u thÃ nh cÃ¡c KhÃ¡i niá»‡m Cá»‘t lÃµi (VÃ­ dá»¥: TrÃ­ch xuáº¥t Ã½ niá»‡m "Tá»« loáº¡i Danh tá»«" tá»« hÃ ng váº¡n giÃ¡ trá»‹ Äiá»‡n Ã¡p Táº¿ bÃ o MLP Ä‘áº±ng sau chá»¯ "Cat"). Khoa há»c thá»‹ giÃ¡c (Computer vision) Ä‘Ã£ thÃ nh cÃ´ng rá»±c rá»¡ vá»›i Autoencoder trong viá»‡c nÃ©n vÃ  khá»­ nhiá»…u (Denoising). Do Ä‘Ã³, Sparse Autoencoders Ä‘ang nháº­n Ä‘Æ°á»£c kÃ¬ vá»ng sáº½ lÃ m Ä‘Æ°á»£c Ä‘iá»u tÆ°Æ¡ng tá»± trÃªn Máº¡ch vÄƒn LLM: Äá»‹nh hÆ°á»›ng láº¡i cÃ¡c Cá»¥m kÃ­ch hoáº¡t (Activations) lan tá»a trÃªn KhÃ´ng gian Cao chiá»u vá» thÃ nh má»™t nhÃ³m cÃ¡c Äá»‹nh lÃ½ NgÃ´n ngá»¯ Tá»‘i giáº£n.

---

## 2. Tiáº¿t Thiáº¿t Láº­p (Methodology)

### 2.1. Cáº¥u TrÃºc KhÃ¡c Biá»‡t Cá»§a NÃºt Tháº¯t Tiá»m áº¨n (Bottleneck Layer)
Autoencoder truyá»n thá»‘ng thÆ°á»ng lÃ  Kiáº¿n trÃºc Cá»• chai (Bottleneck), nháº±m nÃ©n KhÃ´ng gian $D_{input}$ xuá»‘ng khÃ´ng gian $D_{latent} \ll D_{input}$. NgÆ°á»£c láº¡i, Sparse Autoencoders (SAEs) cháº¡y báº±ng chiáº¿n thuáº­t "Ná»Ÿ PhÃ¬nh vÃ  QuÃ©t Sáº¡ch" (Overcomplete and Sparse Expansions): 
Táº§ng Tiá»m áº©n (Latent layer) $D_{latent}$ Ä‘Æ°á»£c cá»‘ tÃ¬nh má»Ÿ rá»™ng lá»›n hÆ¡n Táº§ng Hiá»ƒn ngÃ´n Äáº§u vÃ o (Input Variables). Sá»± má»Ÿ rá»™ng nÃ y cÃ³ thá»ƒ gÃ¢y ra hiá»‡n tÆ°á»£ng há»c Váº¹t (Identity Matrix) cá»±c Ä‘áº¡i Ä‘á»ƒ sinh ra Lá»—i Zero Output Error.
Äáº¿ giáº£i bÃ i toÃ¡n nÃ y, HÃ m Tháº¥t ThoÃ¡t cá»§a SAEs gÃ¡nh vÃ¡c tá»›i 3 yáº¿u tá»‘:
1. **Mean Squared Error (MSE):** HÃ m Ä‘á»‘i sÃ¡nh sá»± chÃªnh lá»‡ch (Difference) giá»¯a Äáº§u vÃ o vÃ  Äáº§u ra: $\frac{1}{N}\sum (x_i - \hat{x}_i)^2$. 
2. **L1 Penalty (Sparsity Constraint):** HÃ m cÆ°á»¡ng Ã©p Máº­t Ä‘á»™ Táº¯t (Zero-activation Density) trÃªn Táº§ng Latent, Ã©p cÃ¡c káº¿t quáº£ kÃ­ch hoáº¡t phÃ¢n tÃ¡n pháº£i cÃ´ Ä‘á»ng láº¡i má»™t vÃ i Äiá»ƒm chÃ³i nhá» (Sparse nodes).
3. **Decorrelation Loss (Covariance Penalty):** Giáº£m thiá»ƒu Tá»•ng BÃ¬nh PhÆ°Æ¡ng cá»§a cÃ¡c Trá»ng sá»‘ Náº±m ngoÃ i Cáº¡nh chÃ©o chÃ­nh $Tr(\Sigma_{off-diagonal}^2)$, bá»©c tá»­ cÃ¡c Nodes trong khÃ´ng gian Tiá»m áº©n khÃ´ng Ä‘Æ°á»£c phÃ©p há»c cÃ¡c thÃ´ng tin trÃ¹ng láº·p (Redundant Info) cá»§a nhau.

### 2.2. TrÃ­ Tuá»‡ ToÃ¡n Há»c NhÃ¢n Táº¡o Äá»ƒ Khá»­ Trá»™n (Unmixing Simulation)
ChÆ°Æ¡ng trÃ¬nh viáº¿t ra hai luá»“ng Nguá»“n Gá»‘c ToÃ¡n há»c (Ground Truth Latent Variables): Dáº¡ng SÃ³ng Sine vÃ  HÃ m Biáº¿n rá»i ráº¡c BÆ°á»›c $0 \to 2$.
Cáº£ hai Ä‘Æ°á»£c hÃ²a quyá»‡n há»—n loáº¡n bá»Ÿi ma tráº­n ChÃ©o Linear Mixing (Rotation + Stretching), táº¡o thÃ nh Táº­p KhÃ´ng gian Hiá»ƒn ngÃ´n Há»—n táº¡p (Manifest Input) cÃ³ Há»‡ sá»‘ tÆ°Æ¡ng quan (Correlation) ráº¥t cao.

---

## 3. Kháº£o SÃ¡t & Giáº£i Pháº«u MÃ´ HÃ¬nh (Analysis)

### 3.1. Sá»± TÃ¡i Sinh Quá»¹ Äáº¡o Äá»™c Láº­p
SAE cháº¡y chu trÃ¬nh Ä‘áº¡o hÃ m (Gradient Descent) vá»›i Optimizer Adam $(Learning\ Rate = 0.007)$ xuyÃªn tÃ¢m $600\ Epochs$. XuyÃªn suá»‘t quÃ¡ trÃ¬nh, sá»± sá»¥t giáº£m hÃ m sá»‘ phÃ¢n ly giá»¯a MSE (Nhanh, chÃªnh lá»‡ch lá»›n) vÃ  L1 Loss (Cháº­m, cÃ³ hiá»‡n tÆ°á»£ng TÄƒng giáº£ táº¡o trÆ°á»›c khi giáº£m sÃ¢u) pháº£n Ã¡nh ká»‹ch báº£n Ä‘Ã m phÃ¡n ná»™i táº¡i cá»§a mÃ´ hÃ¬nh: Thá»­ nghiá»‡m phÃ¢n rÃ£ cáº¥u trÃºc cho tá»›i khi Ä‘áº¡t Ä‘iá»ƒm ThÆ°a HoÃ n háº£o.
ChÃºng ta cÃ³ tá»•ng $20$ chiá»u Tiá»m áº©n. ThÃ´ng qua Ä‘á»‘i chiáº¿u Ma tráº­n TÆ°Æ¡ng Quan (Correlation check), mÃ´ hÃ¬nh tá»± Ä‘á»™ng chá»n Lá»c ra Component $6$ vÃ  $10$ tÆ°Æ¡ng xá»©ng nháº¥t vá»›i $2$ hÃ m SÃ³ng Sine vÃ  HÃ m Rá»i ráº¡c gá»‘c vá»›i Tá»· lá»‡ TrÃ¹ng Khá»›p vÆ°á»£t $\ge 95\%$. 

### 3.2. Cáº¡m Báº«y áº¢o GiÃ¡c VÃ  Chá»©c NÄƒng Cáº£m XÃºc 
DÃ¹ Ä‘áº¡t Ä‘Æ°á»£c tá»‰ lá»‡ giáº£i nÃ©n chÃ³i lá»i, SAE bá»™c lá»™ yáº¿u Ä‘iá»ƒm "KÃ©n tham sá»‘" (Finicky Hyperparameter Turning). á» má»™t sá»‘ chu ká»³ Ä‘Ã o táº¡o (Random Initiations), hÃ¬nh thÃ¡i tÃ¡i táº¡o cáº¥u trÃºc Cáº¥p BÆ°á»›c (Discretization) cá»§a Component sá»‘ $10$ khÃ´ng mÃ´ phá»ng hoÃ n má»¹ Ä‘Æ°á»ng cáº¯t gÃ³c cáº¡nh nhÆ° dá»¯ liá»‡u Simulated gá»‘c. Kháº£ nÄƒng giáº£i cáº¥u trÃºc chá»‰ hiá»‡u quáº£ khi cÃ³ sá»± gÃ³p máº·t cá»§a $L1\ Penalty$ vÃ  $Decorrelation\ Loss$. Khuyáº¿t thiáº¿u má»™t trong hai, máº¡ng Tiá»m áº©n SAE láº­p tá»©c bá»‹ tÃ n phÃ¡ thÃ nh Äá»‘ng RÃ¡c Äá»“ng Dáº¡ng (Highly correlated redundant noise). Äiá»u nÃ y lÃ½ giáº£i táº¡i sao viá»‡c tÃ¬m ra Kiáº¿n trÃºc Tham sá»‘ tá»‘i Æ°u trÃªn Dá»¯ liá»‡u Thá»±c (LM Datasets) láº¡i lÃ  bÃ i toÃ¡n lÃ m Ä‘au Ä‘áº§u cÃ¡c Kiáº¿n trÃºc sÆ° Interpretability Ä‘Æ°Æ¡ng Ä‘áº¡i.

---

## 4. Káº¿t Luáº­n
Báº±ng viá»‡c ná»›i rá»™ng LÃµi áº¨n vÃ  cÆ°á»¡ng Ã©p hÃ m Pháº¡t Tuyáº¿n L1, SAEs vÆ°Æ¡n ra xa hÆ¡n khá»i vÃ¹ng an toÃ n "NÃ©n dá»¯ liá»‡u Denoising" Ä‘á»ƒ trá»Ÿ thÃ nh NgÃ²i ná»• Cá»¥c bá»™ (Unmixing Filter). Máº·c dÃ¹ cÃ¡c Latent Component Ä‘Æ°á»£c cháº¯t lá»c ra Ä‘Ã£ xuáº¥t sáº¯c truy váº¿t ngÆ°á»£c cÃ¡c Nguá»“n Dá»¯ Liá»‡u Thuáº§n, viá»‡c lá»±a chá»n Ä‘Ã¢u lÃ  "Máº¡ch Vi Ngá»¯ Äáº·c ThÃ¹" tá»« hÃ ng chá»¥c Latent Reconstructs sinh ra mÃ  thiáº¿u Ä‘i Vector Ground Truth lÃ m thang Ä‘o sáº½ lÃ  thá»­ thÃ¡ch khá»‘c liá»‡t khi chuyá»ƒn Ä‘á»•i trÃªn Real Datasets cá»§a Äáº¡i NgÃ´n Ngá»¯ MÃ´ HÃ¬nh. Cáº£nh cá»­a bÆ°á»›c vÃ o "Reverse engineering LLM logic" chÃ­nh thá»©c báº¯t Ä‘áº§u.

---

## TÃ i liÃªn tham kháº£o (Citations)
1. ThÃ­ nghiá»‡m xÃ¢y dá»±ng Sparse Autoencoder Pytorch trong `aero_LLM_08_Sparse autoencoders theory and code.md`. Bao gá»“m phÃ¢n tÃ­ch Kiáº¿n trÃºc $Encoder \to Bottleneck \to Decoder$ vÃ  á»©ng dá»¥ng L1 Sparsity, Decorrelation Penalty Matrices.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Máº¡ng Máº¡ch Thuáº­t ToÃ¡n (Circuits) Trong MÃ´ HÃ¬nh Há»c SÃ¢u](aero_LLM_01_What is a circuit in a DL model.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_What is a circuit in a DL model.md) |
| [CÃ´ Láº­p VÃ  ThÄƒm DÃ² Khá»‘i ChÃº Ã (Attention Heads)](aero_LLM_02_Isolating and investigating attention heads.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Isolating and investigating attention heads.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: Biá»ƒu Diá»…n PhÃ¢n Bá»‘ Nhiá»‡t Laminar Cá»§a Trá»ng Sá»‘ ChÃº Ã](aero_LLM_03_CodeChallenge Laminar profile of attention head weights.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Laminar profile of attention head weights.md) |
| [Kháº£o SÃ¡t TÆ°Æ¡ng Quan Cá»¥m (Clustering) Vi Máº¡ch (Circuits) Trong KhÃ´ng Gian Giáº£m Chiá»u](aero_LLM_04_Are circuits clustered in low-dimensional space.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_Are circuits clustered in low-dimensional space.md) |
| [LÃ½ Thuyáº¿t VÃ  á»¨ng Dá»¥ng Cá»§a Ká»¹ Thuáº­t DÃ² ThÆ°a (Sparse Probing)](aero_LLM_05_Sparse probing theory and code.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Sparse probing theory and code.md) |
| [ThÃ¡ch Thá»©c Cá»§a TÃ­n Hiá»‡u ThÆ°a Trong Dá»¯ Liá»‡u Táº­p Lá»›n (Statistical Suppression)](aero_LLM_06_Challenges with sparse logistic regression in large datasets.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_Challenges with sparse logistic regression in large datasets.md) |
| [Biáº¿n Tiá»m áº¨n (Latent) VÃ  Biáº¿n Hiá»ƒn NgÃ´n (Manifest) Trong Giáº£i Diá»…n AI](aero_LLM_07_Latent vs. manifest variables.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Latent vs. manifest variables.md) |
| ğŸ“Œ **[MÃ´ HÃ¬nh Sparse Autoencoders (SAEs): LÃ½ Thuyáº¿t VÃ  Kiáº¿n TrÃºc KhÃ´i Phá»¥c Vi Máº¡ch Tiá»m áº¨n](aero_LLM_08_Sparse autoencoders theory and code.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_08_Sparse autoencoders theory and code.md) |
| [Huáº¥n Luyá»‡n Sparse Autoencoder TrÃ­ch Xuáº¥t KhÃ¡i Niá»‡m Ngá»¯ Cáº£nh Palinka TrÃªn GPT-2](aero_LLM_09_SAE in GPT2 learns about Hungarian Palinka.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_SAE in GPT2 learns about Hungarian Palinka.md) |
| [Kháº£o SÃ¡t PhÃ¢n Táº§ng KÃ­ch Hoáº¡t (Laminar Profile) Qua Sparse Autoencoder](aero_LLM_10_CodeChallenge Laminar profile of autoencoder sparsity.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_CodeChallenge Laminar profile of autoencoder sparsity.md) |
| [Nháº­n Diá»‡n KhÃ¡i Niá»‡m XuyÃªn TÃ¢m Vá»›i PhÃ¢n RÃ£ GiÃ¡ Trá»‹ RiÃªng Suy Rá»™ng (Generalized Eigendecomposition - GED)](aero_LLM_11_Non-orthogonal latent components via eigendecomposition (theory and demo).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_Non-orthogonal latent components via eigendecomposition (theory and demo).md) |
| [Ráº¡ch RÃ²i Giá»›i TÃ­nh (Him vs Her) Báº±ng Generalized Eigendecomposition Trong MLP](aero_LLM_12_Generalized eigendecomposition separates him from her in MLP.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_Generalized eigendecomposition separates him from her in MLP.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): TÃ¡ch NhÃ³m GED Äa Táº§ng (Pháº§n 1)](aero_LLM_13_CodeChallenge GED for category isolation across layers (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_CodeChallenge GED for category isolation across layers (part 1).md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): TÃ¡ch NhÃ³m GED Äa Táº§ng (Pháº§n 2) & Kiá»ƒm Chá»©ng ChÃ©o](aero_LLM_14_CodeChallenge GED for category isolation across layers (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_CodeChallenge GED for category isolation across layers (part 2).md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

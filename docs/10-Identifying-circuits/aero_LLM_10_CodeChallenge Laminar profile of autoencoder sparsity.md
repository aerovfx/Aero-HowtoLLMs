
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [10 Identifying circuits](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Kháº£o SÃ¡t PhÃ¢n Táº§ng KÃ­ch Hoáº¡t (Laminar Profile) Qua Sparse Autoencoder

## TÃ³m táº¯t (Abstract)
Thá»­ thÃ¡ch láº­p trÃ¬nh (Code Challenge) nÃ y hÆ°á»›ng Ä‘áº¿n viá»‡c thu tháº­p Äá»›i KÃ­ch Hoáº¡t (Laminar Profile) cá»§a toÃ n bá»™ cÃ¡c Ä‘iá»ƒm nÃºt MLP cháº¡y dá»c theo tá»« lá»›p (Layer) Ä‘áº§u tá»›i cuá»‘i cá»§a Há»‡ Sinh thÃ¡i GPT-2 (Cáº£ báº£n Small $12\ Layers$ vÃ  Large $36\ Layers$). Má»¥c tiÃªu khÃ´ng nháº¯m vÃ o viá»‡c giáº£i cáº¥u trÃºc hay Ä‘á»‹nh hÆ°á»›ng Ã½ nghÄ©a má»™t Feature Latent Ä‘Æ¡n láº», mÃ  nháº±m váº½ ra BÆ°á»›c sÃ³ng Máº­t Äá»™ Khá»Ÿi Ä‘á»™ng (Activation Density) dÆ°á»›i tÆ° cÃ¡ch lÃ  nhá»¯ng quáº§n thá»ƒ vi mÃ´. Báº±ng cÃ¡ch ná»‘i á»‘ng hÃºt Activations trá»±c tiáº¿p tá»« bÃ i thi Wikipedia vá» CÃ´ng nghá»‡ Ä‘Ã¨n LED, nghiÃªn cá»©u chá»©ng minh Ä‘Æ°á»£c má»™t Quy luáº­t tá»‹nh tiáº¿n máº¡nh máº½: Sá»± ThÆ°a thá»›t bá»‹ triá»‡t tiÃªu dáº§n, vÃ  Máº­t Ä‘á»™ KÃ­ch hoáº¡t (Density) tÄƒng vá»t má»™t cÃ¡ch tuyáº¿n tÃ­nh khi ta Ä‘i sÃ¢u vÃ o cÃ¡c Táº§ng Ä‘Ã¡y (Deeper Layers).

---

## 1. Má»Ÿ Äáº§u (Introduction)
Thay vÃ¬ chÃ¬m Ä‘áº¯m trong viá»‡c "Äá»c tÃ¢m trÃ­" má»™t Latent Component nhÆ° Ä‘Ã£ lÃ m vá»›i khÃ¡i niá»‡m "Geography", bÃ¡o cÃ¡o nÃ y bay lÃªn gÃ³c nhÃ¬n VÄ© mÃ´ (Macro-scale). CÃ¢u há»i Ä‘áº·t ra lÃ : Autoencoder sáº½ pháº£n á»©ng khÃ¡c nhau tháº¿ nÃ o khi nÃ©n vÃ  giáº£i nÃ©n dá»¯ liá»‡u Äiá»‡n Ã¡p tá»« cÃ¡c lá»›p Transformer khÃ¡c nhau?
Dá»¯ liá»‡u Ä‘Ã o táº¡o Ä‘Æ°á»£c Ä‘á»•i kháº©u vá»‹ báº±ng cÃ¡ch khÃ´ng copy-paste truyá»n thá»‘ng, mÃ  lÃ  sá»­ dá»¥ng ThÆ° viá»‡n `requests` Ä‘á»ƒ thÃ¢u tÃ³m toÃ n bá»™ Source Code HTML $\sim 500,000$ kÃ½ tá»± tÆ°Æ¡ng Ä‘Æ°Æ¡ng $52,000$ Tokens cá»§a trang Wikipedia "LEDs". QuÃ¡ trÃ¬nh cáº¡o vÄƒn báº£n (Text scraping) tuÃ¢n thá»§ tiÃªu chuáº©n: Lá»c rÃ¡c Header/Footers báº±ng cÃ¡ch khoanh vÃ¹ng tá»« cá» `<mw-body-content>` Ä‘áº¿n `id="references"`.

---

## 2. Tiáº¿t Thiáº¿t Láº­p Ghi Nháº­n XuyÃªn TÃ¢m (Methodology)

### 2.1. Kiáº¿n TrÃºc Thu Tháº­p Ma Tráº­n Äa Táº§ng (Multi-layer Hooks)
Ta thiáº¿t láº­p má»™t vÃ²ng láº·p FOR khá»•ng lá»“ quÃ©t qua toÃ n bá»™ Layer $L \in \{0 \dots n\}$.
1. Thu nháº­n Dá»¯ liá»‡u thÃ´ (Input): Chá»n ngáº«u nhiÃªn $10,240$ Tokens $\to$ Äá»‹nh dáº¡ng thÃ nh khá»‘i Tensor KÃ­ch thÆ°á»›c `[10, 1024]`.
2. TrÃ­ch xuáº¥t Activations: Táº¡i má»—i Layer $L$, mÃ³c Hook thu tháº­p giÃ¡ trá»‹ xuáº¥t ra tá»« cá»•ng $MLP$ ná»™i táº¡i. Dá»¯ liá»‡u máº£ng Äáº§u tiÃªn (Zero-th Token) thÆ°á»ng chá»©a cÃ¡c hiá»‡n tÆ°á»£ng khá»Ÿi bÃ o há»—n loáº¡n cá»±c Ä‘oan (Unusual Outliers) do cÆ¡ cháº¿ Context Loading, nÃªn báº¯t buá»™c bá»‹ cáº¯t bá» (Slicing out token $[0]$).

### 2.2. Huáº¥n Luyá»‡n Cá»¥c Bá»™ (Per-layer SAE Training)
Táº¡i duy nháº¥t má»—i Táº§ng $L$:
- SAE Ä‘Æ°á»£c khá»Ÿi táº¡o Má»›i HoÃ n ToÃ n (KhÃ´i phá»¥c ma tráº­n Trá»ng sá»‘ vá» Random).
- Training loop cháº¡y vá»›i $75\ Epochs$, sá»­ dá»¥ng hÃ m L1 Sparsity vÃ  MSE Loss.
- Thu tháº­p láº¡i giÃ¡ trá»‹ Density cuá»‘i cÃ¹ng (Má»©c Ä‘á»™ chiáº¿m Ä‘Ã³ng cá»§a Táº§ng Latent $> 0$) vÃ  GiÃ¡ trá»‹ Gá»‘c MSE (Kháº£ nÄƒng khÃ´i phá»¥c).

---

## 3. Kháº£o SÃ¡t & PhÃ¡c Há»a HÃ nh Vi (Analysis)

### 3.1. Sá»± TÄƒng TrÆ°á»Ÿng Tuyáº¿n TÃ­nh Cá»§a Máº­t Äá»™ KÃ­ch Hoáº¡t (Density Profiling)
Quan sÃ¡t biá»ƒu Ä‘á»“ Line Graph trung bÃ¬nh Density cho tháº¥y má»™t quá»¹ Ä‘áº¡o tá»‹nh tiáº¿n máº¡nh máº½ tá»« $Layer\ 0 \to Layer\ 12$ (Äá»‘i vá»›i GPT2-Small) vÃ  vÆ°Æ¡n tá»›i $Layer\ 36$ (Äá»‘i vá»›i GPT2-Large).
CÃ¡c lá»›p Má»Ÿ Äáº§u (Early layers), vá»›i nÄƒng lá»±c tÃ­nh toÃ¡n cÃ²n gáº§n vá»›i Tá»« Vá»±ng (Shallow Embeddings), ghi nháº­n kháº£ nÄƒng thu gá»n ráº¥t xuáº¥t sáº¯c cá»§a SAE: Háº§u háº¿t Activation bá»‹ nÃ©n vá» $0$. Tuy nhiÃªn, khi luá»“ng vÄƒn báº£n cháº£y vÃ o cÃ¡c Lá»›p Cuá»‘i (Deeper layers), MÃ´ HÃ¬nh Ä‘á»‘i diá»‡n vá»›i Trá»ng trÃ¡ch Cá»‘t LÃµi: TiÃªn Ä‘oÃ¡n Token tiáº¿p theo (Next-Token Prediction). á» giai Ä‘oáº¡n nÃ y, hÃ m nghÄ©a cá»§a vÄƒn báº£n Ä‘Ã£ trá»Ÿ nÃªn TÃ­ch cháº­p Ä‘a chiá»u (Incorporating broad context). Do Ä‘Ã³ SAE báº¥t lá»±c trong viá»‡c duy trÃ¬ tráº¡ng thÃ¡i ThÆ°a thá»›t (Sparsity); CÃ¡c Vi máº¡ch pháº£i bung ná»Ÿ há»a lá»±c Ä‘á»“ng loáº¡t Ä‘á»ƒ gÃ¡nh vÃ¡c cÃ¡c logic ná»™i suy Ä‘a há»£p tuyáº¿n. Sá»± chÃªnh lá»‡ch Máº­t Ä‘á»™ nÃ y Ä‘Ãºng trÃªn cáº£ $2$ Size mÃ´ hÃ¬nh, cho tháº¥y Ä‘Ã¢y lÃ  Báº£n tÃ­nh HÃ m TÃ­nh ToÃ¡n (Algorithmic Nature) chá»© khÃ´ng pháº£i Do Háº¡n cháº¿ Tham sá»‘.

### 3.2. VÃ¹ng "Äá»‡m" Dá»… Thá»Ÿ Nháº¥t (Sweet Spot of Reconstruction)
Song song Ä‘Ã³, Äá»“ thá»‹ Äá»™ trÆ°á»£t KhÃ´i Phá»¥c (Final Loss) bá»™c lá»™ hiá»‡n tÆ°á»£ng ÄÃ¡y cháº£o (U-shape) thoai thoáº£i: Layer Äáº§u tiÃªn vÃ  Lá»›p Cuá»‘i cÃ¹ng luÃ´n máº¯c sai sá»‘ tÃ¡i há»“i cao nháº¥t. Ká»³ láº¡ thay, cÃ¡c Lá»›p Chuyá»ƒn Máº¡ch á»Ÿ Trung TÃ¢m (VÃ­ dá»¥ Layer $5-7$ cá»§a Small, hoáº·c $10-20$ cá»§a GPT2-Large) ghi nháº­n MSE Loss cháº¡m Ä‘Ã¡y tá»‘i thiá»ƒu. Äiá»u nÃ y ngá»¥ Ã½ KhÃ´ng gian Biá»ƒu diá»…n (Representation Space) á»Ÿ giá»¯a mÃ´ hÃ¬nh lÃ  tÄ©nh táº¡i, bá»›t bá»‹ dá»“n nÃ©n hay vá»¡ ráº¡c nháº¥t, giÃºp Autoencoder "Dá»‹ch thuáº­t" tÃ­n hiá»‡u dá»… dÃ ng hÆ¡n.

---

## 4. Káº¿t Luáº­n
Viá»‡c chá»‰ má»• xáº» Má»™t Táº§ng Máº¡ng Ä‘á»ƒ phÃ¡n xÃ©t lÃ  thiá»ƒn cáº­n, bá»Ÿi Cáº¥u trÃºc KÃ­ch hoáº¡t bÃªn trong LLM váº­n Ä‘á»™ng vÃ  biáº¿n thiÃªn liÃªn tá»¥c theo LÃ¡t cáº¯t Laminar. ThÃ­ nghiá»‡m quÃ©t táº§ng báº±ng SAE trÃªn GPT-2 cá»§ng cá»‘ gÃ³c nhÃ¬n VÄ© MÃ´: Sá»± thÆ°a thá»›t (Sparsity) - thá»© quyáº¿t Ä‘á»‹nh TÃ­nh Diá»…n Giáº£i (Interpretability) - giáº£m dáº§n tá»‰ lá»‡ nghá»‹ch vá»›i Äá»™ SÃ¢u cá»§a Máº¡ng. CÃ ng tiá»‡m cáº­n tá»›i Táº§ng Final Output, má»i cÆ¡ cháº¿ nÃ©n gá»n sáº½ bá»‹ Ä‘Ã¡nh sáº­p Ä‘á»ƒ nhÆ°á»ng chá»— cho cá»¥m Logic phÃ¢n tÃ¡n, dáº­p táº¯t hi vá»ng chia Ä‘á»ƒ trá»‹ (Divide-and-conquer) á»Ÿ giai Ä‘oáº¡n xuáº¥t xÆ°á»Ÿng. á» bÃ¡o cÃ¡o káº¿ tiáº¿p, ta sáº½ chia tay vá»›i há»‡ thá»‘ng Gradient cá»§a Autoencoders Ä‘á»ƒ lÃ m quen vá»›i NÄƒng lÆ°á»£ng PhÃ¢n rÃ£ Trá»‹ RiÃªng (Genearlized Eigendecomposition).

---

## TÃ i liÃªn tham kháº£o (Citations)
1. ThÃ­ nghiá»‡m kháº£o sÃ¡t VÄ© mÃ´ Sparse Autoencoder cháº¡y ngang Laminar Profile cá»§a GPT-2, tham váº¥n tÃ i liá»‡u `aero_LLM_10_CodeChallenge Laminar profile of autoencoder sparsity.md`. Ghi nháº­n sá»± khÃ¡c biá»‡t cá»§a cÆ¡ cháº¿ Scraping HTTP Body (`mw-body-content`) báº±ng `requests`.
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

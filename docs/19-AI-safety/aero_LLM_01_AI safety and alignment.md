# Đánh giá An toàn AI (AI Safety) và Sự Căn chỉnh (Alignment) thông qua Khả năng Diễn giải Cơ chế (Mechanistic Interpretability)

## Tóm tắt

Khi các Mô hình Ngôn ngữ Lớn (LLMs) ngày càng hội nhập sâu vào mọi khía cạnh của cuộc sống: văn hóa, kinh tế, giáo dục hay an ninh mạng, thì tầm quan trọng của tính an toàn (Safety) ngày càng hiện rõ. Bài viết này định nghĩa sự khác biệt giữa hai thuật ngữ cốt lõi: An toàn AI (AI Safety) và Sự Căn chỉnh (Alignment). Đặc biệt, chúng tôi trình bày cách tiếp cận Khả năng Diễn giải Cơ chế (Mechanistic Interpretability) như một khung kỹ thuật nhằm bóc tách các "hộp đen" LLM. Bài viết đề cập bài toán từ góc nhìn toán học, mô tả cơ chế của luồng dữ liệu dư (residual stream), can thiệp nhân quả (causal intervention) và sự liên hệ của chúng đối với việc giảm thiểu rủi ro từ hệ thống trí tuệ nhân tạo.

---

## 1. Safety và Alignment

Dù thường bị dùng lẫn lộn, "Safety" và "Alignment" có bản chất phân lập. 

### 1.1 An toàn AI (AI Safety)
Là quá trình phát triển, triển khai các hệ thống AI sao cho mang lại lợi ích cao nhất cho số đông và giảm thiểu tối đa rủi ro thiệt hại. An toàn là một khái niệm mang tính phổ quát vì nó liên quan tới phúc lợi, đạo đức và sự sống còn của thế giới con người cũng như tự nhiên.

### 1.2 Sự Căn chỉnh (Alignment)
Là việc đảm bảo AI hành xử đúng với mục tiêu định trước mà "chúng ta" mong muốn.
- **Tính Phức tạp Của Căn Chỉnh**: Sự khó khăn nằm ở việc xác định đại từ "chúng ta" (người dùng). Nếu một nhóm tin tặc sử dụng LLM để hỏi phương thức viết phần mềm tống tiền, và LLM cung cấp một đoạn code mã hóa ổ cứng. Khi đó, AI đã được **căn chỉnh** (aligned) theo mong muốn của tin tặc, nhưng nó lại **không an toàn** (unsafe) với xã hội.

### 1.3 Mâu Thuẫn Giá Trị
Thậm chí không cần dùng tới những mục đích phạm tội, việc căn chỉnh bị kẹp giữa các bên cũng rất thường xảy ra. Một học sinh yêu cầu LLM "giải giúp bài tập về nhà", trong khi giáo viên chỉ muốn LLM "đưa ra gợi ý, không giải hộ". Bất kể mô hình trả lời thế nào, độ Căn chỉnh của hệ thống cũng sẽ mất lòng một trong hai.

---

## 2. Các Rủi Ro Điển Hình

AI là một hệ sinh thái mạnh mẽ nhưng không thể vận hành phi rủi ro:
- **Nguy cơ hiện sinh (Existential risks):** AI vượt qua khả năng kiểm soát của con người, đe dọa trực tiếp sự tồn vong của nhân loại khi vượt qua trí tuệ con người một cách không thể dự báo.
- **Tiếp thị xâm nhập (Intrusive marketing):** Việc theo dõi dữ liệu thiết bị, tin nhắn và thói quen một cách vi phạm quyền riêng tư.
- **Đồng nhất hóa văn hóa (Cultural homogenization):** Sự bão hòa các tác phẩm ngôn ngữ và nghệ thuật trượt về một vector trung bình (average vector), đánh mất đi sự sáng tạo nguyên bản của loài người.
- **Vũ khí tự trị (Autonomous weapons):** AI định đoạt khả năng khai hỏa trên chiến trường theo công thức lượng hóa mà không có sự kiểm soát của con người (Human-in-the-loop).

---

## 3. Cơ sở Toán học của Diễn giải Cơ chế (Mechanistic Interpretability) trong An toàn AI

Để khắc phục vấn đề hộp đen của AI, Khả năng Diễn giải Cơ chế (Mech Interp) đi sâu vào tầng kỹ thuật, chia làm các hướng nghiên cứu Từ dưới lên (Bottom-up) và Từ trên xuống (Top-down). Mục tiêu là thiết lập biểu diễn toán học cho các hành vi của AI nhằm theo dõi và loại bỏ tính "Unsafe".

### 3.1 Nhận dạng Tri thức qua Phương pháp Quan sát (Non-causal Observation)
Mô hình Transformer xử lý thông tin qua các lớp mạng, trong đó luồng dữ liệu chính là dòng dư (residual stream) được biểu diễn bằng vector trạng thái ẩn $h_l$:

$$
h_l = h_{l-1} + \text{Attention}(h_{l-1}) + \text{MLP}(h_{l-1})
$$

Để đo lường một mô hình có đang lưu giữ các tri thức độc hại hay không (ví dụ: công thức chế tạo bom), ta tiến hành thiết lập các Hook (hàm trích xuất trạng thái). Phương pháp quan sát phân bố xác suất từ các lớp trung gian (Logit Lens) cho phép chuẩn hóa và ánh xạ ngược dòng dư về không gian từ vựng (Vocabulary):

$$
P(y_i | h_l) = \text{Softmax}(W_U \cdot h_l)
$$

Trong đó $W_U$ là ma trận Un-embedding matrix. Nếu xác suất $P$ chệch cao vào các từ vựng gây hại, ta có thể xây dựng trạm thẩm định (monitoring systems) giám sát độc lập.

### 3.2 Can thiệp Nhân quả (Causal Intervention) và Vector Khắc phục (Steering Vectors)
Chỉ có hiện tượng tương quan (correlation) là không đủ, Mech Interp đòi hỏi Can thiệp Nhân quả (Causal Intervention). Nếu phát hiện một vi mạch (circuit) cấu thành bởi ma trận $W_Q, W_K, W_V$ mang đặc tính thiên kiến (bias) hoặc không an toàn, ta có thể cô lập hướng không gian (direction) cụ thể $\mathbf{v}_{harmful}$ đại diện cho hành vi đó. 

Quá trình "thanh tẩy" (surgery) mô hình được thực hiện bằng cách bẻ lái (steering) activation trong lúc chạy (forward pass):

$$
\tilde{h}_l = h_l - \alpha \cdot (\mathbf{v}_{harmful}^T h_l) \mathbf{v}_{harmful} 
$$

Phép toán trên triệt tiêu hình chiếu của $\mathbf{v}_{harmful}$ lên trạng thái $h_l$, giúp LLM giữ được sự Căn chỉnh (Alignment) mà không làm suy giảm Năng lực tổng quát (Universality) đối với các tác vụ hợp pháp khác.

---

## 4. Khung Giải Pháp Kết Hợp

Việc theo đuổi "AI Safety" là chuỗi xích phối hợp giữa pháp trị (Legal) và kỹ trị (Technical). 

### 4.1 Giải Pháp Kỹ Thuật
- **Thiết lập Guardrails & Hook Interventions**: Áp dụng Mechanistic Interpretability như trình bày ở phần trên để bẻ ngoặt trọng số trong quá trình suy luận.
- **Training Interpretable Models**: Xây dựng các mô hình mà kiến trúc tự thân đã mang tính minh bạch ngay từ khi bắt đầu training thay vì dịch ngược mô hình phức tạp.

### 4.2 Giải Pháp Pháp Lý
- Không xuất khẩu các phần cứng tính toán cốt lõi cho các nhóm có ý đồ nguy hiểm.
- Quy chiếu trách nhiệm của doanh nghiệp đối với cơ chế hoạt động của mô hình ("liable for harm"). 

---

## 5. Kết luận

An toàn AI không chỉ là câu chuyện của luân lý mà là bài toán cần được lượng hóa, giải thuật qua các phương pháp kỹ thuật sâu sắc như Mechanistic Interpretability. Mặc dù vẫn còn nhiều hạn chế và tranh cãi, việc kết hợp giữa Căn chỉnh giá trị con người và hiểu rõ toán học bên trong mạng neuron đại diện cho pháo đài phòng thủ vững chắc nhất của nhân loại trước những hệ thống trí tuệ nhân tạo ngày càng hùng mạnh.

---

## Tài liệu tham khảo

1. **Amodei, D. et al. (2016).** *Concrete Problems in AI Safety.* arXiv preprint arXiv:1606.06565.
2. **Bengio, Y. et al. (2023).** *Managing AI Risks in an Era of Rapid Progress.*
3. **Bostrom, N. (2014).** *Superintelligence: Paths, Dangers, Strategies.* Oxford University Press.
4. **Christian, B. (2020).** *The Alignment Problem: Machine Learning and Human Values.* Norton & Company.
5. **Elhage, N., et al. (2021).** *A Mathematical Framework for Transformer Circuits.* Transformer Circuits Thread.
6. **Olah, C., et al. (2020).** *Zoom In: An Introduction to Circuits.* Distill.
7. **Räuköll, D. et al. (2023).** *Mechanistic Interpretability connects AI Safety and Architectural Understanding.*

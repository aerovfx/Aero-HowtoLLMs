
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [19 AI safety](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Há»c Trong Ngá»¯ Cáº£nh (In-Context Learning) vÃ  Rá»§i Ro Äá»‘i Vá»›i An ToÃ n AI

## TÃ³m táº¯t

Há»c Trong Ngá»¯ Cáº£nh (In-Context Learning - ICL) lÃ  má»™t trong nhá»¯ng cÆ¡ cháº¿ Ä‘Ã¡ng chÃº Ã½ vÃ  máº¡nh máº½ báº­c nháº¥t cá»§a MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLMs), cho phÃ©p chÃºng tÃ­nh toÃ¡n vÃ  pháº£n há»“i chÃ­nh xÃ¡c tÃ¡c vá»¥ mÃ  khÃ´ng cáº§n tráº£i qua bÆ°á»›c tinh chá»‰nh (fine-tuning). Tuy nhiÃªn, chÃ­nh kháº£ nÄƒng phi thÆ°á»ng nÃ y láº¡i trá»Ÿ thÃ nh Ä‘iá»ƒm mÃ¹ lá»›n Ä‘e dá»a trá»±c tiáº¿p Khung An ToÃ n AI (AI Safety). BÃ i viáº¿t nÃ y sáº½ diá»…n giáº£i In-Context Learning thÃ´ng qua á»‘ng kÃ­nh toÃ¡n há»c cá»§a *Diá»…n giáº£i CÆ¡ cháº¿ (Mechanistic Interpretability)* Ä‘á»ƒ minh hoáº¡ cÃ¡ch thÃ´ng tin ngá»¯ cáº£nh Ä‘iá»u hÆ°á»›ng dÃ²ng cháº£y chÃº Ã½ (attention flows), Ä‘á»“ng thá»i phÃ¢n tÃ­ch cÃ¡c rá»§i ro báº£o máº­t Ä‘i kÃ¨m.

---

## 1. Há»c Trong Ngá»¯ Cáº£nh (In-Context Learning) LÃ  GÃ¬?

In-Context Learning (ICL) Ä‘á» cáº­p tá»›i nÄƒng lá»±c giáº£i quyáº¿t tÃ¡c vá»¥ hoÃ n toÃ n má»›i (unseen tasks) chá»‰ thÃ´ng qua Ä‘oáº¡n vÄƒn báº£n nháº¯c (prompt) Ä‘áº§u vÃ o mÃ  khÃ´ng cáº§n thay Ä‘á»•i báº¥t ká»³ trá»ng sá»‘ tá»· lá»‡ nÃ o (weights) cá»§a mÃ´ hÃ¬nh. Trong ICL, chÃºng ta thÆ°á»ng sá»­ dá»¥ng cÃ¡c má»©c Ä‘á»™ "shot" khÃ¡c nhau:
- **Zero-shot:** ÄÆ°a ra yÃªu cáº§u suÃ´ng mÃ  khÃ´ng cÃ³ báº¥t ká»³ vÃ­ dá»¥ minh há»a nÃ o.
- **One-shot/Few-shot:** ÄÆ°a ra yÃªu cáº§u Ä‘i kÃ¨m má»™t vÃ i máº«u vÃ­ dá»¥ nháº­p - xuáº¥t (input - output).

### 1.1 Kháº£ nÄƒng Tiá»m áº©n ÄÃ¡ng Kinh Ngáº¡c
Sá»± linh hoáº¡t cá»§a ICL giÃºp ngÆ°á»i dÃ¹ng phá»• thÃ´ng táº­n dá»¥ng LLM (nhÆ° cáº¥u trÃºc láº¡i ngÃ y, há»c cÃ¡c ngÃ´n ngá»¯ bá»‹a Ä‘áº·t, hay trÃ­ch xuáº¥t thÃ´ng tin) mÃ  khÃ´ng cáº§n tá»›i pháº§n cá»©ng Ä‘Ã o táº¡o Ä‘áº¯t Ä‘á». Kháº£ nÄƒng nÃ y vá»‘n khÃ´ng Ä‘Æ°á»£c láº­p trÃ¬nh chá»§ Ä‘Ã­ch tá»« Ä‘áº§u, mÃ  tá»± phÃ¡t sinh má»™t cÃ¡ch báº¥t ngá» (emergent capability) trong quÃ¡ trÃ¬nh má»Ÿ rá»™ng kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh (Scaling).

---

## 2. In-Context Learning Tá»« GÃ³c NhÃ¬n Diá»…n Giáº£i CÆ¡ Cháº¿ (Mechanistic Interpretability)

Táº¡i sao má»™t mÃ´ hÃ¬nh chá»‰ dá»±a vÃ o tÄ©nh sá»‘ há»c (frozen parameters) láº¡i cÃ³ thá»ƒ "há»c" theo thá»i gian thá»±c? Diá»…n giáº£i CÆ¡ cháº¿ (Mech Interp) cung cáº¥p má»™t mÃ´ hÃ¬nh giáº£i thÃ­ch toÃ¡n há»c ráº¥t chÃ­nh xÃ¡c: *CÆ¡ cháº¿ Äáº§u Cáº£m á»¨ng (Induction Heads).*

### 2.1 Ma tráº­n ChÃº Ã½ vÃ  Khá»›p Máº«u Hiá»‡n Táº¡i (Pattern Matching)
QuÃ¡ trÃ¬nh ICL thá»±c cháº¥t lÃ  thao tÃ¡c nhÃ¢n ma tráº­n Key ($K$) vÃ  Query ($Q$) Ä‘á»ƒ truy xuáº¥t Token Ä‘Æ°á»£c láº·p láº¡i trong Prompt. Attention Score Ä‘Æ°á»£c biá»ƒu thá»‹ lÃ :

$$
A = \text{Softmax}\left( \frac{x W_Q W_K^T x^T}{\sqrt{d_k}} \right)
$$

### 2.2 Äáº§u Cáº£m á»¨ng (Induction Heads)
CÆ¡ cháº¿ cá»‘t lÃµi chá»‹u trÃ¡ch nhiá»‡m cho in-context learning lÃ  "Induction Heads". Giáº£ sá»­ chuá»—i token Ä‘áº§u vÃ o xuáº¥t hiá»‡n mÃ´ hÃ¬nh $[A][B] ... [A]$. Induction Head cá»§a Transformer sáº½ thá»±c hiá»‡n hai bÆ°á»›c thÃ´ng qua Composition:
1. XÃ¡c Ä‘á»‹nh vá»‹ trÃ­ cá»§a $[A]$ trÆ°á»›c Ä‘Ã³ vÃ  nhÃ¬n vÃ o token $[B]$ ngay sau nÃ³.
2. Sao chÃ©p Ä‘áº·c trÆ°ng cá»§a $[B]$ vÃ  di chuyá»ƒn thÃ´ng tin nÃ y tá»›i vá»‹ trÃ­ $[A]$ hiá»‡n táº¡i thÃ´ng qua ma tráº­n Value ($V$).

$$
\text{Output}_{\text{induction}} = \text{Softmax}\left( \frac{q W_Q W_K^T k^T}{\sqrt{d}} \right) v W_V W_O
$$

Thuáº­t toÃ¡n trÃªn giáº£i thÃ­ch viá»‡c mÃ´ hÃ¬nh cÃ³ thá»ƒ giáº£i cÃ¡c bÃ i toÃ¡n few-shot learning báº±ng cÃ¡ch ghi nhá»› "quy luáº­t tÆ°Æ¡ng á»©ng" tá»« cÃ¡c vÃ­ dá»¥ $shot$ trÆ°á»›c thay vÃ¬ hiá»ƒu logic chiá»u sÃ¢u.

---

## 3. ThÃ¡ch Thá»©c Äá»‘i Vá»›i AI Safety

Máº·c dÃ¹ ICL Ä‘em láº¡i cÆ¡ há»™i thÆ°Æ¡ng máº¡i hÃ³a lá»›n, nÃ³ láº¡i lÃ  má»™t cÆ¡n Ã¡c má»™ng Ä‘á»‘i vá»›i Ä‘áº£m báº£o an toÃ n há»‡ thá»‘ng (AI Alignment & Safety).

### 3.1 VÆ°á»£t Máº·t Há»‡ Thá»‘ng Báº£o Vá»‡ (Bypass Firewalls)
Guardrail an ninh cá»§a LLM Ä‘a pháº§n Ä‘Æ°á»£c luyá»‡n trong giai Ä‘oáº¡n RLHF hoáº·c Fine-tuning tÄ©nh. Khi ICL váº­n hÃ nh dá»±a trÃªn Induction head, cÃ¡c nhÃ³m tÃ¡c nhÃ¢n xáº¥u (bad actors) cÃ³ thá»ƒ qua máº·t hÃ ng rÃ o nÃ y báº±ng ká»¹ thuáº­t chÃ¨n prompt (Prompt Injection) dáº¡ng few-shot.

VÃ­ dá»¥: Báº±ng cÃ¡ch Ä‘Æ°a ra 5 bÃ¡o cÃ¡o mÃ£ hÃ³a áº£o nhÆ° má»™t biá»ƒu máº«u, mÃ´ hÃ¬nh sáº½ bá»‹ kÃ©o vÃ o khÃ´ng gian Induction Head vÃ  báº¯t máº£nh quy luáº­t, vÃ´ tÃ¬nh sinh ra mÃ£ nguá»“n Ä‘á»™c háº¡i á»Ÿ output tiáº¿p theo. 

### 3.2 KhÃ³ KhÄƒn Cá»§a CÃ¡c Ká»¹ SÆ° PhÃ¡t Triá»ƒn
CÃ¡c nhÃ  sÃ¡ng láº­p pháº§n má»m (OpenAI, Anthropic, Meta) khÃ´ng thá»ƒ dá»± Ä‘oÃ¡n vÃ  báº£o vá»‡ mÃ´ hÃ¬nh cho má»i trÆ°á»ng há»£p, bá»Ÿi láº½ ICL táº¡o ra má»™t vÃ´ háº¡n cÃ¡c bÃ i toÃ¡n phá»¥ (sub-tasks) mÃ  mÃ´ hÃ¬nh tá»± thiáº¿t láº­p. CÆ¡ cháº¿ nÃ y diá»…n ra Ä‘á»™c láº­p trong máº¡ng Transformer mÃ  khÃ´ng thÃ´ng bÃ¡o hay ghi láº¡i lá»—i á»Ÿ báº£ng trá»ng sá»‘. 

---

## 4. Káº¿t Luáº­n

In-Context Learning chá»©ng minh kÃ­ch thÆ°á»›c lÆ°á»£ng tham sá»‘ cá»§a máº¡ng nÆ¡-ron cÃ³ thá»ƒ thai nghÃ©n ra nhá»¯ng Ä‘á»™ng lá»±c hÃ nh vi mÃ  ngay cáº£ ngÆ°á»i táº¡o ra nÃ³ cÅ©ng khÃ´ng tháº¥y trÆ°á»›c Ä‘Æ°á»£c. Viá»‡c nghiÃªn cá»©u hiá»‡n tÆ°á»£ng quy luáº­t nhÃ¢n quáº£ thÃ´ng qua Mechanistic Interpretability cho phÃ©p chÃºng ta lÃ m rÃµ cÃ¡ch cÃ¡c Head há»c há»i nhanh chÃ³ng, tá»« Ä‘Ã³ thiáº¿t láº­p cÃ¡c giáº£i phÃ¡p triá»‡t tiÃªu "Induction Heads" Ä‘á»™c háº¡i, hay duy trÃ¬ tÃ­nh cÃ¢n báº±ng an ninh dÃ i háº¡n cho cÃ¡c há»‡ thá»‘ng Generative AI. 

---

## TÃ i liá»‡u tham kháº£o

1. **Dong, Q., et al. (2022).** *A Survey for In-context Learning.* arXiv preprint arXiv:2301.00234.
2. **Brown, T., et al. (2020).** *Language Models are Few-Shot Learners.* NeurIPS.
3. **Olsson, C., et al. (2022).** *In-context Learning and Induction Heads.* Transformer Circuits Thread.
4. **EIhage, N., et al. (2021).** *A Mathematical Framework for Transformer Circuits.* Anthropic.
5. **Wei, J., et al. (2022).** *Emergent Abilities of Large Language Models.* TMLR.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [ÄÃ¡nh giÃ¡ An toÃ n AI (AI Safety) vÃ  Sá»± CÄƒn chá»‰nh (Alignment) thÃ´ng qua Kháº£ nÄƒng Diá»…n giáº£i CÆ¡ cháº¿ (Mechanistic Interpretability)](aero_LLM_01_AI safety and alignment.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_AI safety and alignment.md) |
| [Táº¡i Sao TrÃ­ Tuá»‡ NhÃ¢n Táº¡o (AI) KhÃ´ng Thá»ƒ Tá»± Äá»™ng An ToÃ n vÃ  CÃ³ Äáº¡o Äá»©c?](aero_LLM_02_Why can't AI just be safe and moral.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Why can't AI just be safe and moral.md) |
| ğŸ“Œ **[Há»c Trong Ngá»¯ Cáº£nh (In-Context Learning) vÃ  Rá»§i Ro Äá»‘i Vá»›i An ToÃ n AI](aero_LLM_03_In-context and few-shot learning.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_03_In-context and few-shot learning.md) |
| [Äá»‹nh Luáº­t Má»Ÿ Rá»™ng (Scaling Laws) vÃ  Sá»± PhÃ¡t Triá»ƒn Cá»§a An ToÃ n TrÃ­ Tuá»‡ NhÃ¢n Táº¡o](aero_LLM_04_Scaling and AI safety.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_Scaling and AI safety.md) |
| [Thá»±c hÃ nh: Hack AI Ä‘á»ƒ ÄÃ¡nh cáº¯p Máº­t kháº©u (Prompt Injection)](aero_LLM_05_Hands-on Hack an AI to steal a password!.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Hands-on Hack an AI to steal a password!.md) |
| [Tham Gia VÃ o LÄ©nh Vá»±c An ToÃ n TrÃ­ Tuá»‡ NhÃ¢n Táº¡o (AI Safety): Khá»Ÿi Äáº§u VÃ  CÆ¡ Há»™i](aero_LLM_06_How to get involved in AI safety.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_How to get involved in AI safety.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c

---

## TÃ³m táº¯t

BÃ i viáº¿t nÃ y trÃ¬nh bÃ y má»™t phÃ¢n tÃ­ch khoa há»c vá» quy trÃ¬nh tokenization thÃ´ng qua vÃ­ dá»¥ vÄƒn báº£n *The Time Machine* cá»§a H. G. Wells. Ná»™i dung táº­p trung vÃ o cÃ¡ch tiá»n xá»­ lÃ½ vÄƒn báº£n, xÃ¢y dá»±ng tá»« vá»±ng, Ã¡nh xáº¡ token sang chá»‰ sá»‘ vÃ  chuyá»ƒn Ä‘á»•i sang khÃ´ng gian vector phá»¥c vá»¥ huáº¥n luyá»‡n mÃ´ hÃ¬nh Transformer. BÃ i viáº¿t bá»• sung cÃ¡c ná»n táº£ng lÃ½ thuyáº¿t tá»« kiáº¿n trÃºc Attention Is All You Need vÃ  cÃ¡c mÃ´ hÃ¬nh GPT do OpenAI phÃ¡t triá»ƒn, kÃ¨m theo cÃ¡c cÃ´ng thá»©c toÃ¡n há»c minh há»a cho quÃ¡ trÃ¬nh rá»i ráº¡c hÃ³a vÃ  biá»ƒu diá»…n liÃªn tá»¥c.

---

# 1. Giá»›i thiá»‡u

Trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡n Ä‘áº¡i, dá»¯ liá»‡u vÄƒn báº£n pháº£i Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i tá»« dáº¡ng kÃ½ tá»± sang dáº¡ng token trÆ°á»›c khi Ä‘Æ°a vÃ o máº¡ng nÆ¡-ron.

Cho vÄƒn báº£n Ä‘áº§u vÃ o:

$$
X = (c_1, c_2, \dots, c_n), \quad c_i \in \Sigma
$$

Tokenization thá»±c hiá»‡n Ã¡nh xáº¡:

$$
\tau : \Sigma^* \rightarrow V^*
$$

trong Ä‘Ã³:

* $\Sigma$: táº­p kÃ½ tá»±
* $V$: tá»« vá»±ng token
* $V^*$: chuá»—i token

VÃ­ dá»¥ vá»›i *The Time Machine*:

The Time Machine by H. G. Wells

Sau xá»­ lÃ½ cÃ³ thá»ƒ thÃ nh:

$$

$$

\text{"the"}, \text{"time"}, \text{"machine"}, \text{"by"}, \text{"h"}, \text{"g"}, \text{"wells"}

$$

$$

---

# 2. Tiá»n xá»­ lÃ½ vÄƒn báº£n

## 2.1 Chuáº©n hÃ³a chá»¯ thÆ°á»ng

$$
f_{lower}(x) = \text{lower}(x)
$$

GiÃºp giáº£m kÃ­ch thÆ°á»›c tá»« vá»±ng:

$$
|V_{raw}| > |V_{normalized}|
$$

---

## 2.2 Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t

HÃ m lá»c:

$$
f_{clean}(x) = x \setminus { \text{punctuation} }
$$

Má»¥c tiÃªu:

* Giáº£m nhiá»…u
* Chuáº©n hÃ³a cáº¥u trÃºc

---

# 3. Tokenization má»©c tá»« (Word-level Tokenization)

Sau khi tÃ¡ch theo khoáº£ng tráº¯ng:

$$
X = (w_1, w_2, \dots, w_T)
$$

Sá»‘ lÆ°á»£ng token:

$$
T \leq n
$$

Táº§n suáº¥t xuáº¥t hiá»‡n cá»§a tá»« $w$:

$$
f(w) = \sum_{i=1}^{T} \mathbf{1}(w_i = w)
$$

---

# 4. XÃ¢y dá»±ng tá»« vá»±ng (Vocabulary Construction)

Táº­p tá»« vá»±ng:

$$
V = { w \mid f(w) \geq \delta }
$$

vá»›i $\delta$ lÃ  ngÆ°á»¡ng tá»‘i thiá»ƒu.

KÃ­ch thÆ°á»›c tá»« vá»±ng:

$$
|V| = M
$$

Ãnh xáº¡:

$$
w \rightarrow id(w) \in {0, 1, \dots, M-1}
$$

---

# 5. Biá»ƒu diá»…n One-Hot

Token $w_i$ Ä‘Æ°á»£c biá»ƒu diá»…n:

$$
x_i \in \mathbb{R}^{M}
$$

vá»›i:

$$
x_{ij} = \begin{cases} 1 & \text{náº¿u } j = id(w_i) \ 0 & \text{ngÆ°á»£c láº¡i} \end{cases}
$$

NhÆ°á»£c Ä‘iá»ƒm:

* KÃ­ch thÆ°á»›c lá»›n
* KhÃ´ng pháº£n Ã¡nh ngá»¯ nghÄ©a

---

# 6. Embedding Vector

Embedding matrix:

$$
E \in \mathbb{R}^{M \times d}
$$

Vector embedding:

$$
e_i = E^T x_i
$$

Do Ä‘Ã³:

$$
e_i \in \mathbb{R}^{d}
$$

Khoáº£ng cÃ¡ch cosine:

$$
\cos(e_i, e_j) = \frac{e_i \cdot e_j}{|e_i||e_j|}
$$

GiÃºp Ä‘o má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a.

---

# 7. MÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t ngÃ´n ngá»¯

Theo mÃ´ hÃ¬nh tá»± há»“i quy:

$$
P(X) = \prod_{t=1}^{T} P(w_t \mid w_{<t})
$$

Máº¡ng Transformer tÃ­nh:

$$
Z = \text{Transformer}(e_1, \dots, e_T)
$$

Logits:

$$
z_t = W_{out} h_t
$$

Softmax:

$$
P(w_t = j \mid w_{<t}) = \frac{\exp(z_{tj})} {\sum_{k=1}^{M} \exp(z_{tk})}
$$

---

# 8. Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n

Self-attention:

$$
\mathcal{O}(T^2 d)
$$

Náº¿u vÄƒn báº£n dÃ i nhÆ° *The Time Machine* (~30,000 tá»«), chi phÃ­ tÄƒng theo bÃ¬nh phÆ°Æ¡ng Ä‘á»™ dÃ i chuá»—i.

Do Ä‘Ã³, tokenization tá»‘i Æ°u giÃºp:

* Giáº£m $T$
* Giáº£m bá»™ nhá»›
* TÄƒng tá»‘c huáº¥n luyá»‡n

---

# 9. PhÃ¢n tÃ­ch thá»‘ng kÃª vÄƒn báº£n

Entropy cá»§a táº­p tá»«:

$$
H(W) = - \sum_{w \in V} P(w) \log P(w)
$$

Vá»›i:

$$
P(w) = \frac{f(w)}{T}
$$

Náº¿u phÃ¢n bá»‘ Zipf:

$$
f(w_r) \propto \frac{1}{r}
$$

trong Ä‘Ã³ $r$ lÃ  thá»© háº¡ng táº§n suáº¥t.

Äiá»u nÃ y cho tháº¥y:

* Sá»‘ Ã­t tá»« xuáº¥t hiá»‡n ráº¥t nhiá»u
* Nhiá»u tá»« hiáº¿m xuáº¥t hiá»‡n

---

# 10. So sÃ¡nh vá»›i Subword Tokenization

Word-level tokenization cÃ³ nhÆ°á»£c Ä‘iá»ƒm:

$$
P(\text{OOV}) > 0
$$

Giáº£i phÃ¡p: Byte Pair Encoding (BPE).

Táº­p há»£p phÃ¢n rÃ£:

$$
w = s_1 s_2 \dots s_k
$$

vá»›i $s_i \in V_{subword}$

Äáº£m báº£o:

$$
\forall w, \exists \text{ decomposition}
$$

---

# 11. Tháº£o luáº­n

Tokenization lÃ  quÃ¡ trÃ¬nh:

$$
\text{Text} \rightarrow \text{Discrete Representation} \rightarrow \text{Continuous Geometry}
$$

Vá» máº·t toÃ¡n há»c:

* LÃ  Ã¡nh xáº¡ tá»« chuá»—i kÃ½ tá»± sang khÃ´ng gian vector
* LÃ  bÆ°á»›c nÃ©n thÃ´ng tin
* áº¢nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n phÃ¢n phá»‘i xÃ¡c suáº¥t

---

# 12. Káº¿t luáº­n

ThÃ´ng qua vÃ­ dá»¥ *The Time Machine*, ta tháº¥y:

1. Tokenization quyáº¿t Ä‘á»‹nh cáº¥u trÃºc dá»¯ liá»‡u Ä‘áº§u vÃ o
2. Vocabulary áº£nh hÆ°á»Ÿng Ä‘áº¿n kÃ­ch thÆ°á»›c embedding
3. Biá»ƒu diá»…n vector quyáº¿t Ä‘á»‹nh kháº£ nÄƒng há»c ngá»¯ nghÄ©a
4. Äá»™ dÃ i chuá»—i áº£nh hÆ°á»Ÿng Ä‘áº¿n Ä‘á»™ phá»©c táº¡p Transformer

ToÃ n bá»™ quÃ¡ trÃ¬nh cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ hÃ¬nh hÃ³a:

$$
\Sigma^* \xrightarrow{\tau} V^* \xrightarrow{E} \mathbb{R}^{T \times d}
$$

Ä‘Ã³ng vai trÃ² ná»n táº£ng cho má»i mÃ´ hÃ¬nh Transformer hiá»‡n Ä‘áº¡i.

---

# TÃ i liá»‡u tham kháº£o

1. Attention Is All You Need
2. Sennrich, R. et al. (2016). *Neural Machine Translation of Rare Words with Subword Units*.
3. Shannon, C. (1948). *A Mathematical Theory of Communication*.
4. Jurafsky, D., Martin, J. (2023). *Speech and Language Processing*.
5. Manning, C., SchÃ¼tze, H. (1999). *Foundations of Statistical Natural Language Processing*.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_llm_05_preparing_text_for_tokenization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_preparing_text_for_tokenization.md) |
| ğŸ“Œ **[PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_llm_06_codechallenge_tokenizing_the_time_machine.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

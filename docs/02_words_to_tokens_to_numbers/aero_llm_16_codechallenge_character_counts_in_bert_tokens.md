
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
PhÃ¢n tÃ­ch Sá»‘ lÆ°á»£ng KÃ½ tá»± trong Token cá»§a BERT:

MÃ´ hÃ¬nh Thá»‘ng kÃª, Entropy vÃ  áº¢nh hÆ°á»Ÿng Ä‘áº¿n Äá»™ phá»©c táº¡p Transformer

â¸»

TÃ³m táº¯t

BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch sá»‘ lÆ°á»£ng kÃ½ tá»± cáº¥u thÃ nh má»—i token trong bá»™ tokenizer cá»§a BERT do Google phÃ¡t triá»ƒn, dá»±a trÃªn dá»¯ liá»‡u tá»« tÃ i liá»‡u Ä‘Ã­nh kÃ¨m. ChÃºng tÃ´i xÃ¢y dá»±ng mÃ´ hÃ¬nh thá»‘ng kÃª cho phÃ¢n bá»‘ Ä‘á»™ dÃ i token, Æ°á»›c lÆ°á»£ng entropy há»‡ subword, vÃ  phÃ¢n tÃ­ch tÃ¡c Ä‘á»™ng cá»§a Ä‘á»™ dÃ i kÃ½ tá»± Ä‘áº¿n Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n trong kiáº¿n trÃºc Transformer. Káº¿t quáº£ cho tháº¥y phÃ¢n bá»‘ Ä‘á»™ dÃ i token cÃ³ xu hÆ°á»›ng lá»‡ch pháº£i (right-skewed), gáº§n vá»›i phÃ¢n bá»‘ hÃ¬nh há»c hoáº·c log-linear, pháº£n Ã¡nh sá»± cÃ¢n báº±ng giá»¯a kÃ­ch thÆ°á»›c tá»« vá»±ng vÃ  Ä‘á»™ dÃ i chuá»—i Ä‘áº§u vÃ o.

â¸»

1. Giá»›i thiá»‡u

Trong BERT, vÄƒn báº£n Ä‘áº§u vÃ o Ä‘Æ°á»£c token hÃ³a báº±ng thuáº­t toÃ¡n WordPiece thÃ nh cÃ¡c subword token:

$$
S = (w_1, w_2, ..., w_n)
$$

$$
T = (t_1, t_2, ..., t_m)
$$

Vá»›i:

$$
m $\ge$ n
$$

Má»—i token t_i cÃ³ Ä‘á»™ dÃ i kÃ½ tá»±:

$\ell$(t_i)

Má»¥c tiÃªu nghiÃªn cá»©u:
	1.	PhÃ¢n bá»‘ xÃ¡c suáº¥t cá»§a $\ell$(t)
	2.	Äá»™ dÃ i trung bÃ¬nh token
	3.	áº¢nh hÆ°á»Ÿng Ä‘áº¿n chi phÃ­ self-attention

â¸»

2. MÃ´ hÃ¬nh Thá»‘ng kÃª PhÃ¢n bá»‘ Äá»™ dÃ i Token

2.1 Äá»‹nh nghÄ©a

Gá»i:
	â€¢	V: táº­p tá»« vá»±ng BERT

$$
â€¢	|V| $\approx$ 30{,}000
$$

	â€¢	N_k: sá»‘ token cÃ³ Ä‘á»™ dÃ i kÃ½ tá»± báº±ng k

XÃ¡c suáº¥t:

$P(L = k)$ = \frac{N_k}{|V|}

Chuáº©n hÃ³a:

$$
$\sum$_{k=1}^{K_{\max}} $P(L=k)$ = 1
$$

â¸»

2.2 MÃ´ hÃ¬nh HÃ¬nh há»c (Geometric Approximation)

Quan sÃ¡t thá»±c nghiá»‡m cho tháº¥y:

$$
N_k $\approx$ Ae^{-\lambda k}
$$

Suy ra:

$P(L=k)$ = (1-q)q^{k-1}

Trong Ä‘Ã³:

$$
q = e^{-\lambda}
$$

ÄÃ¢y lÃ  phÃ¢n bá»‘ hÃ¬nh há»c rá»i ráº¡c.

â¸»

2.3 Ká»³ vá»ng vÃ  PhÆ°Æ¡ng sai

Ká»³ vá»ng:

$$
$\mathbb${E}[L] = \frac{1}{1-q}
$$

PhÆ°Æ¡ng sai:

\mathrm{Var}$L$ = \frac{q}{(1-q)^2}

Náº¿u q \to 1, phÃ¢n bá»‘ cÃ³ Ä‘uÃ´i dÃ i hÆ¡n (nhiá»u token dÃ i).

â¸»

3. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Äá»™ dÃ i Chuá»—i VÄƒn báº£n

Giáº£ sá»­ vÄƒn báº£n cÃ³ tá»•ng sá»‘ kÃ½ tá»± n.

Sá»‘ token trung bÃ¬nh:

$$
m = \frac{n}{$\mathbb${E}[L]}
$$

Self-attention trong Transformer encoder:

$O(m^2)$

Thay vÃ o:

$$
O$\le$ft($\le$ft(\frac{n}{$\mathbb${E}[L]}\right)^2\right)
$$

Khi $\mathbb${E}[L] \uparrow, chi phÃ­ giáº£m.

â¸»

4. Entropy cá»§a Há»‡ Token

Entropy theo phÃ¢n bá»‘ Ä‘á»™ dÃ i:

$$
H_L = - $\sum$_{k} $P(L=k)$\log $P(L=k)$
$$

Thay phÃ¢n bá»‘ hÃ¬nh há»c:

$$
H_L = - $\sum$_{k=1}^{$\infty$} (1-q)q^{k-1} $\log$[(1-q)q^{k-1}]
$$

RÃºt gá»n:

$$
H_L = -$\log$(1-q) - \frac{q}{1-q}$\log$ q
$$

Entropy cÃ ng lá»›n â†’ Ä‘á»™ Ä‘a dáº¡ng Ä‘á»™ dÃ i cÃ ng cao.

â¸»

5. Quan há»‡ vá»›i Luáº­t Zipf

Táº§n suáº¥t token thÆ°á»ng tuÃ¢n theo:

f$r$ $\propto$ \frac{1}{r^\alpha}

Trong Ä‘Ã³:
	â€¢	r: thá»© háº¡ng token

$$
â€¢	\alpha $\approx$ 1
$$

Token ngáº¯n thÆ°á»ng:
	â€¢	CÃ³ táº§n suáº¥t cao
	â€¢	á» thá»© háº¡ng tháº¥p

Do Ä‘Ã³ tá»“n táº¡i tÆ°Æ¡ng quan nghá»‹ch:

$\ell$(t) $\propto$ $\log$ r

â¸»

6. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Embedding Matrix

Embedding:

E: V \rightarrow $\mathbb${R}^d

Ma tráº­n embedding:

W \in $\mathbb${R}^{|V| \times d}

BÃ i toÃ¡n tá»‘i Æ°u:

$$
\min_{V} $\le$ft( $\mathbb${E}[m] + \lambda |V| \right)
$$

Trong Ä‘Ã³:
	â€¢	$\mathbb${E}[m]: sá»‘ token trung bÃ¬nh
	â€¢	|V|: kÃ­ch thÆ°á»›c tá»« vá»±ng
	â€¢	\lambda: há»‡ sá»‘ cÃ¢n báº±ng

â¸»

7. So sÃ¡nh vá»›i Character-level Modeling

MÃ´ hÃ¬nh	Äá»™ dÃ i trung bÃ¬nh	OOV	Chi phÃ­
Character-level	1	KhÃ´ng	Ráº¥t cao
Word-level	Lá»›n	Cao	Trung bÃ¬nh
WordPiece	Trung bÃ¬nh	Tháº¥p	Tá»‘i Æ°u

Náº¿u xá»­ lÃ½ á»Ÿ má»©c kÃ½ tá»±:

$$
m = n
$$

Chi phÃ­:

$O(n^2)$

WordPiece giáº£m:

$$
m = \frac{n}{$\mathbb${E}[L]}
$$

â¸»

8. Tháº£o luáº­n

Dá»¯ liá»‡u thá»±c nghiá»‡m cho tháº¥y:
	â€¢	Pháº§n lá»›n token cÃ³ Ä‘á»™ dÃ i nhá» (1â€“5 kÃ½ tá»±)
	â€¢	Token dÃ i tá»“n táº¡i nhÆ°ng Ã­t
	â€¢	PhÃ¢n bá»‘ cÃ³ Ä‘uÃ´i nháº¹ (mild heavy-tail)

Äiá»u nÃ y pháº£n Ã¡nh:
	â€¢	Sá»± cÃ¢n báº±ng giá»¯a kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a vÃ  Ä‘á»™ nÃ©n
	â€¢	Tá»‘i Æ°u hÃ³a thá»±c nghiá»‡m hÆ¡n lÃ  lÃ½ thuyáº¿t thuáº§n tÃºy

â¸»

9. Káº¿t luáº­n

PhÃ¢n bá»‘ Ä‘á»™ dÃ i kÃ½ tá»± cá»§a token trong BERT cÃ³ thá»ƒ mÃ´ hÃ¬nh hÃ³a gáº§n Ä‘Ãºng báº±ng phÃ¢n bá»‘ hÃ¬nh há»c:

$P(L=k)$ \sim q^{k-1}

TÃ¡c Ä‘á»™ng trá»±c tiáº¿p Ä‘áº¿n:

$$
m = \frac{n}{$\mathbb${E}[L]}
$$

\text{Attention Cost} \sim $O(m^2)$

$$
H_L = - $\sum$ $P(L)$\log $P(L)$
$$

Thiáº¿t káº¿ tokenizer lÃ  bÃ i toÃ¡n tá»‘i Æ°u Ä‘a má»¥c tiÃªu giá»¯a:
	â€¢	KÃ­ch thÆ°á»›c tá»« vá»±ng
	â€¢	Äá»™ dÃ i chuá»—i
	â€¢	Entropy thÃ´ng tin
	â€¢	Chi phÃ­ tÃ­nh toÃ¡n

â¸»

TÃ i liá»‡u tham kháº£o
	1.	BERT â€“ Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
	2.	Sennrich et al. (2016). Neural Machine Translation of Rare Words with Subword Units.
	3.	Vaswani et al. (2017). Attention Is All You Need.
	4.	Shannon, C. (1948). A Mathematical Theory of Communication.
	5.	Jurafsky & Martin. Speech and Language Processing.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_llm_05_preparing_text_for_tokenization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_preparing_text_for_tokenization.md) |
| [PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_tokenization_in_bert.md) |
| ğŸ“Œ **[aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

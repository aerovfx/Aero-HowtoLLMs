
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n

---

## TÃ³m táº¯t

QuÃ¡ trÃ¬nh táº¡o (create) vÃ  trá»±c quan hÃ³a (visualize) token lÃ  bÆ°á»›c trung gian quan trá»ng giá»¯a vÄƒn báº£n thÃ´ vÃ  khÃ´ng gian vector trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs). BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch cÆ¡ sá»Ÿ toÃ¡n há»c cá»§a tokenization, embedding, vÃ  cÃ¡c ká»¹ thuáº­t trá»±c quan hÃ³a khÃ´ng gian Ä‘áº·c trÆ°ng (feature space visualization) nhÆ° PCA vÃ  t-SNE. PhÃ¢n tÃ­ch dá»±a trÃªn kiáº¿n trÃºc Transformer cá»§a Vaswani et al. (2017) vÃ  cÃ¡c mÃ´ hÃ¬nh GPT do OpenAI phÃ¡t triá»ƒn.

---

# 1. Giá»›i thiá»‡u

MÃ´ hÃ¬nh ngÃ´n ngá»¯ khÃ´ng xá»­ lÃ½ vÄƒn báº£n trá»±c tiáº¿p mÃ  xá»­ lÃ½:
$$
\text{Text} \rightarrow \text{Token IDs} \rightarrow \text{Embedding vectors}
$$


Viá»‡c trá»±c quan hÃ³a token giÃºp:

* Hiá»ƒu cáº¥u trÃºc khÃ´ng gian embedding
* PhÃ¢n tÃ­ch quan há»‡ ngá»¯ nghÄ©a
* Kiá»ƒm tra tÃ­nh cháº¥t há»c Ä‘Æ°á»£c cá»§a mÃ´ hÃ¬nh

---

# 2. Táº¡o Token (Token Creation)

## 2.1. Tokenization

Cho vÄƒn báº£n $x$, hÃ m tokenization:
$$
T: \mathcal{X} \rightarrow V^T
$$


Trong Ä‘Ã³:

* $V$: tá»« vá»±ng cÃ³ kÃ­ch thÆ°á»›c $|V| = N$
* (T(x) = (t_1, t_2, ..., t_T))

Má»—i token $t_i \in {1,2,...,N}$

---

## 2.2. Embedding

Ma tráº­n embedding:
$$
E \in \mathbb{R}^{N \times d}
$$


Vector cá»§a token thá»© $i$:
$$
e_i = E[t_i]
$$


Chuá»—i Ä‘áº§u vÃ o:
$$
Z = (e_1, e_2, ..., e_T)
$$


---

# 3. ThÃªm thÃ´ng tin vá»‹ trÃ­

Transformer khÃ´ng cÃ³ RNN hay CNN nÃªn cáº§n positional encoding:
$$
z_i = e_i + p_i
$$


Trong GPT:
$$
p_i \in \mathbb{R}^d
$$


Ä‘Æ°á»£c há»c trá»±c tiáº¿p.

---

# 4. Trá»±c quan hÃ³a khÃ´ng gian token

Embedding cÃ³ chiá»u cao $vÃ­ dá»¥ ( d = 768, 1024, 1280$).
Äá»ƒ trá»±c quan hÃ³a, ta cáº§n giáº£m chiá»u.

---

## 4.1. Principal Component Analysis (PCA)

Cho ma tráº­n embedding:
$$
X \in \mathbb{R}^{T \times d}
$$


Ma tráº­n hiá»‡p phÆ°Æ¡ng sai:
$$
\Sigma = \frac{1}{T} X^T X
$$


Giáº£i bÃ i toÃ¡n trá»‹ riÃªng:
$$
\Sigma v = \lambda v
$$


Chá»n 2 trá»‹ riÃªng lá»›n nháº¥t â†’ chiáº¿u xuá»‘ng 2D:
$$
X_{2D} = X W_{2}
$$


---

## 4.2. t-SNE

t-SNE tá»‘i thiá»ƒu hÃ³a KL-divergence giá»¯a phÃ¢n phá»‘i khoáº£ng cÃ¡ch cao chiá»u vÃ  tháº¥p chiá»u:
$$
\min_{Y}
D_{KL}(P | Q)
$$


Trong Ä‘Ã³:
$$
D_{KL}(P|Q)
===========

\sum_{i,j}
P_{ij}
\log
\frac{P_{ij}}{Q_{ij}}
$$


---

# 5. Quan há»‡ ngá»¯ nghÄ©a trong khÃ´ng gian embedding

Embedding há»c Ä‘Æ°á»£c tÃ­nh cháº¥t tuyáº¿n tÃ­nh.

VÃ­ dá»¥:
$$
\text{King} - \text{Man} + \text{Woman} \approx \text{Queen}
$$


Vá» máº·t vector:
$$
e_{king} - e_{man} + e_{woman}
\approx e_{queen}
$$


Äiá»u nÃ y cho tháº¥y embedding mÃ£ hÃ³a cáº¥u trÃºc ngá»¯ nghÄ©a.

---

# 6. Self-Attention vÃ  tÆ°Æ¡ng tÃ¡c token

Attention:
$$
\text{Attention}(Q,K,V)
=======================

\text{softmax}
\left(
\frac{QK^T}{\sqrt{d_k}}
\right)V
$$


Ma tráº­n attention:
$$
A_{ij}
======

\frac
{\exp(q_i k_j / \sqrt{d_k})}
{\sum_j \exp(q_i k_j / \sqrt{d_k})}
$$


Trá»±c quan hÃ³a attention giÃºp hiá»ƒu:

* Token nÃ o áº£nh hÆ°á»Ÿng token nÃ o
* Quan há»‡ phá»¥ thuá»™c dÃ i háº¡n

---

# 7. Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n

Self-attention:
$$
\mathcal{O}(T^2 d)
$$


Náº¿u sá»‘ token tÄƒng:
$$
T \uparrow \Rightarrow \text{Memory} \uparrow
$$


Viá»‡c táº¡o token hiá»‡u quáº£ giÃºp:

* Giáº£m chiá»u dÃ i chuá»—i
* Giáº£m chi phÃ­ huáº¥n luyá»‡n

---

# 8. VÃ­ dá»¥ minh há»a quy trÃ¬nh

Cho cÃ¢u:

> "Transformers process tokens"

BÆ°á»›c 1: Tokenization
$$
[1245, 5432, 987]
$$


BÆ°á»›c 2: Embedding
$$
Z \in \mathbb{R}^{3 \times d}
$$


BÆ°á»›c 3: Attention
$$
Z' = \text{Transformer}(Z)
$$


BÆ°á»›c 4: Visualization

* PCA â†’ 2D
* t-SNE â†’ cá»¥m ngá»¯ nghÄ©a

---

# 9. á»¨ng dá»¥ng trong huáº¥n luyá»‡n GPT

MÃ´ hÃ¬nh GPT tá»‘i Æ°u:
$$
P(x) = \prod_{t=1}^{T} P(x_t | x_{<t})
$$


Token lÃ  Ä‘Æ¡n vá»‹ cÆ¡ báº£n cá»§a xÃ¡c suáº¥t.

Loss:
$$
\mathcal{L}
===========

-\sum_{t=1}^{T}
\log P(x_t | x_{<t})
$$


Náº¿u tokenization khÃ´ng tá»‘t:

* Chuá»—i dÃ i
* Gradient nhiá»…u
* Hiá»‡u suáº¥t giáº£m

---

# 10. Tháº£o luáº­n

Táº¡o vÃ  trá»±c quan hÃ³a token giÃºp:

1. Hiá»ƒu cáº¥u trÃºc embedding
2. PhÃ¡t hiá»‡n bias
3. PhÃ¢n tÃ­ch clustering ngá»¯ nghÄ©a
4. Kiá»ƒm tra alignment

Token khÃ´ng chá»‰ lÃ  ID â€” chÃºng lÃ  Ä‘iá»ƒm trong khÃ´ng gian vector cao chiá»u.

---

# 11. Káº¿t luáº­n

QuÃ¡ trÃ¬nh:
$$
\text{Text}
\rightarrow
\text{Token IDs}
\rightarrow
\text{Embedding}
\rightarrow
\text{Attention}
$$


lÃ  ná»n táº£ng cá»§a má»i mÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡n Ä‘áº¡i.

Trá»±c quan hÃ³a giÃºp:

* Giáº£i thÃ­ch mÃ´ hÃ¬nh
* PhÃ¢n tÃ­ch hÃ nh vi
* Cáº£i thiá»‡n hiá»‡u nÄƒng

---

# TÃ i liá»‡u tham kháº£o

1. Vaswani, A. et al. (2017). *Attention Is All You Need*.
2. Radford, A. et al. (2019). *Language Models are Unsupervised Multitask Learners*.
3. van der Maaten, L., Hinton, G. (2008). *Visualizing Data using t-SNE*.
4. Jolliffe, I. (2002). *Principal Component Analysis*.

-
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| ğŸ“Œ **[Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_llm_05_preparing_text_for_tokenization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_preparing_text_for_tokenization.md) |
| [PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

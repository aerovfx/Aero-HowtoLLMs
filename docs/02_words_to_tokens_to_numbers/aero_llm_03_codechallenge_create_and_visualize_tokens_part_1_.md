
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n

---

## TÃ³m táº¯t

QuÃ¡ trÃ¬nh táº¡o (create) vÃ  trá»±c quan hÃ³a (visualize) token lÃ  bÆ°á»›c trung gian quan trá»ng giá»¯a vÄƒn báº£n thÃ´ vÃ  khÃ´ng gian vector trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs). BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch cÆ¡ sá»Ÿ toÃ¡n há»c cá»§a tokenization, embedding, vÃ  cÃ¡c ká»¹ thuáº­t trá»±c quan hÃ³a khÃ´ng gian Ä‘áº·c trÆ°ng (feature space visualization) nhÆ° PCA vÃ  t-SNE. PhÃ¢n tÃ­ch dá»±a trÃªn kiáº¿n trÃºc Transformer cá»§a Vaswani et al. (2017) vÃ  cÃ¡c mÃ´ hÃ¬nh GPT do OpenAI phÃ¡t triá»ƒn.

---

# 1. Giá»›i thiá»‡u

MÃ´ hÃ¬nh ngÃ´n ngá»¯ khÃ´ng xá»­ lÃ½ vÄƒn báº£n trá»±c tiáº¿p mÃ  xá»­ lÃ½:

$$
\text{Text} \rightarrow \text{Token IDs} \rightarrow \text{Embedding vectors}
$$

Viá»‡c trá»±c quan hÃ³a token giÃºp:

* Hiá»ƒu cáº¥u trÃºc khÃ´ng gian embedding
* PhÃ¢n tÃ­ch quan há»‡ ngá»¯ nghÄ©a
* Kiá»ƒm tra tÃ­nh cháº¥t há»c Ä‘Æ°á»£c cá»§a mÃ´ hÃ¬nh

---

# 2. Táº¡o Token (Token Creation)

## 2.1. Tokenization

Cho vÄƒn báº£n $x$, hÃ m tokenization:

T: \mathcal{X} \rightarrow V^T

Trong Ä‘Ã³:

* V: tá»« vá»±ng cÃ³ kÃ­ch thÆ°á»›c |V\mid = N

$$
* (T(x) = (t_1, t_2, ..., t_T)) Má»—i token t_i \in {1,2,...,N} --- ## 2.2. Embedding Ma tráº­n embedding: E \in \mathbb{R}^{N \times d} Vector cá»§a token thá»© i: e_i = E[t_i] Chuá»—i Ä‘áº§u vÃ o: Z = (e_1, e_2, ..., e_T) --- # 3. ThÃªm thÃ´ng tin vá»‹ trÃ­ Transformer khÃ´ng cÃ³ RNN hay CNN nÃªn cáº§n positional encoding: z_i = e_i + p_i Trong GPT: p_i \in \mathbb{R}^d Ä‘Æ°á»£c há»c trá»±c tiáº¿p. --- # 4. Trá»±c quan hÃ³a khÃ´ng gian token
$$

Embedding cÃ³ chiá»u cao vÃ­ dá»¥ ( d = 768, 1024, 1280).

$$
Äá»ƒ trá»±c quan hÃ³a, ta cáº§n giáº£m chiá»u. --- ## 4.1. Principal Component Analysis (PCA) Cho ma tráº­n embedding: X \in \mathbb{R}^{T \times d} Ma tráº­n hiá»‡p phÆ°Æ¡ng sai: \Sigma = \frac{1}{T} X^T X Giáº£i bÃ i toÃ¡n trá»‹ riÃªng: \Sigma v = \lambda v Chá»n 2 trá»‹ riÃªng lá»›n nháº¥t â†’ chiáº¿u xuá»‘ng 2D: X_{2D} = X W_{2} --- ## 4.2. t-SNE t-SNE tá»‘i thiá»ƒu hÃ³a KL-divergence giá»¯a phÃ¢n phá»‘i khoáº£ng cÃ¡ch cao chiá»u vÃ  tháº¥p chiá»u:
$$

\min_{Y} D_{KL}(P | Q)

$$
Trong Ä‘Ã³: D_{KL}(P|Q) = \sum_{i,j} P_{ij} \log \frac{P_{ij}}{Q_{ij}} --- # 5. Quan há»‡ ngá»¯ nghÄ©a trong khÃ´ng gian embedding Embedding há»c Ä‘Æ°á»£c tÃ­nh cháº¥t tuyáº¿n tÃ­nh. VÃ­ dá»¥: \text{King} - \text{Man} + \text{Woman} \approx \text{Queen} Vá» máº·t vector: e_{king} - e_{man} + e_{woman} \approx e_{queen} Äiá»u nÃ y cho tháº¥y embedding mÃ£ hÃ³a cáº¥u trÃºc ngá»¯ nghÄ©a. --- # 6. Self-Attention vÃ  tÆ°Æ¡ng tÃ¡c token Attention: \text{Attention}(Q,K,V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right)V Ma tráº­n attention: A_{ij} = \frac {\exp(q_i k_j / \sqrt{d_k})} {\sum_j \exp(q_i k_j / \sqrt{d_k})} Trá»±c quan hÃ³a attention giÃºp hiá»ƒu: * Token nÃ o áº£nh hÆ°á»Ÿng token nÃ o * Quan há»‡ phá»¥ thuá»™c dÃ i háº¡n --- # 7. Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n Self-attention:
$$

$\mathcal${O}(T^2 d)

$$
Náº¿u sá»‘ token tÄƒng:
$$

T \uparrow \Rightarrow \text{Memory} \uparrow

$$
Viá»‡c táº¡o token hiá»‡u quáº£ giÃºp: * Giáº£m chiá»u dÃ i chuá»—i * Giáº£m chi phÃ­ huáº¥n luyá»‡n --- # 8. VÃ­ dá»¥ minh há»a quy trÃ¬nh Cho cÃ¢u: > "Transformers process tokens" BÆ°á»›c 1: Tokenization 1245, 5432, 987 BÆ°á»›c 2: Embedding Z \in \mathbb{R}^{3 \times d} BÆ°á»›c 3: Attention Z' = \text{Transformer}(Z) BÆ°á»›c 4: Visualization * PCA â†’ 2D * t-SNE â†’ cá»¥m ngá»¯ nghÄ©a --- # 9. á»¨ng dá»¥ng trong huáº¥n luyá»‡n GPT MÃ´ hÃ¬nh GPT tá»‘i Æ°u: P(x) = \prod_{t=1}^{T} P(x_t  \mid  x_{\lt t}) Token lÃ  Ä‘Æ¡n vá»‹ cÆ¡ báº£n cá»§a xÃ¡c suáº¥t. Loss: \mathcal{L} = -\sum_{t=1}^{T} \log P(x_t  \mid  x_{\lt t}) Náº¿u tokenization khÃ´ng tá»‘t: * Chuá»—i dÃ i * Gradient nhiá»…u * Hiá»‡u suáº¥t giáº£m --- # 10. Tháº£o luáº­n Táº¡o vÃ  trá»±c quan hÃ³a token giÃºp: 1. Hiá»ƒu cáº¥u trÃºc embedding 2. PhÃ¡t hiá»‡n bias 3. PhÃ¢n tÃ­ch clustering ngá»¯ nghÄ©a 4. Kiá»ƒm tra alignment Token khÃ´ng chá»‰ lÃ  ID â€” chÃºng lÃ  Ä‘iá»ƒm trong khÃ´ng gian vector cao chiá»u. --- # 11. Káº¿t luáº­n QuÃ¡ trÃ¬nh:
$$

\text{Text} \rightarrow \text{Token IDs} \rightarrow \text{Embedding} \rightarrow \text{Attention}
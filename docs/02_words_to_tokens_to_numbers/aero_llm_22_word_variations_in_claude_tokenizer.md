
<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Bi·∫øn th·ªÉ T·ª´ v·ª±ng trong Tokenizer c·ªßa Claude:

Ph√¢n t√≠ch H√¨nh th·ª©c, Ph√¢n b·ªë X√°c su·∫•t v√† ·∫¢nh h∆∞·ªüng ƒë·∫øn Bi·ªÉu di·ªÖn Ng·ªØ nghƒ©a

‚∏ª

T√≥m t·∫Øt

D·ª±a tr√™n t√†i li·ªáu ƒë√≠nh k√®m ‚ÄúWord Variations in Claude Tokenizer‚Äù, b√†i vi·∫øt n√†y ph√¢n t√≠ch c√°ch tokenizer c·ªßa m√¥ h√¨nh Claude x·ª≠ l√Ω c√°c bi·∫øn th·ªÉ t·ª´ v·ª±ng (word variations) nh∆∞ ti·ªÅn t·ªë, h·∫≠u t·ªë, ch·ªØ hoa‚Äìth∆∞·ªùng v√† h√¨nh th√°i h·ªçc. Ch√∫ng t√¥i x√¢y d·ª±ng m√¥ h√¨nh to√°n h·ªçc cho ph√¢n r√£ subword, ph√¢n t√≠ch ph√¢n b·ªë x√°c su·∫•t token, v√† ƒë√°nh gi√° ·∫£nh h∆∞·ªüng ƒë·∫øn entropy, t·ª∑ l·ªá n√©n v√† chi ph√≠ self-attention trong Transformer. B√†i vi·∫øt c≈©ng so s√°nh v·ªõi tokenizer c·ªßa BERT v√† c√°c ph∆∞∆°ng ph√°p d·ª±a tr√™n BPE.

‚∏ª

1. Gi·ªõi thi·ªáu

Tokenizer l√† h√†m √°nh x·∫°:

$\mathcal${T}: \Sigma^* \rightarrow V^*

Trong ƒë√≥:
	‚Ä¢	\Sigma: b·∫£ng k√Ω t·ª±
	‚Ä¢	V: t·∫≠p token
	‚Ä¢	V^*: chu·ªói token

M·ªôt t·ª´ c√≥ nhi·ªÅu bi·∫øn th·ªÉ h√¨nh th√°i:

w_k = r + s_k

v·ªõi:
	‚Ä¢	r: g·ªëc t·ª´ (root)
	‚Ä¢	$s_k$: h·∫≠u t·ªë (suffix)

Tokenizer subword s·∫Ω ph√¢n r√£:

$$
\mathcal{T}w_k = r, s_k
$$

Thay v√¨ xem m·ªói bi·∫øn th·ªÉ l√† m·ªôt token ƒë·ªôc l·∫≠p.

‚∏ª

2. M√¥ h√¨nh To√°n h·ªçc c·ªßa Bi·∫øn th·ªÉ T·ª´

Gi·∫£ s·ª≠ m·ªôt t·∫≠p bi·∫øn th·ªÉ:

W = \{w_1, w_2, \dots, w_K\}

Trong ƒë√≥:

w_k = r + s_k

N·∫øu x√°c su·∫•t xu·∫•t hi·ªán:

$P($w_k$)$

th√¨ x√°c su·∫•t c·ªßa root:

$P(r)$ = $\sum$_{k=1}^{K} $P($w_k$)$

Tokenizer hi·ªáu qu·∫£ s·∫Ω h·ªçc:

$P(r)$ \gg $P($w_k$)$

‚∏ª

3. Entropy Tr∆∞·ªõc v√† Sau Ph√¢n r√£

3.1 Entropy ·ªü m·ª©c t·ª´

H_W = -\sum_{k=1}^{K} P(w_k)\log P(w_k)

‚∏ª

3.2 Entropy ·ªü m·ª©c subword

Gi·∫£ s·ª≠ t√°ch th√†nh root v√† suffix:

H_{sub} = -P(r)\log P(r) - \sum_{k} P(s_k)\log P(s_k)

V√¨:

$P(r)$ = $\sum$_k $P($w_k$)$

n√™n:

H_{sub} \le H_W

(gi·∫£m entropy nh·ªù gom t·∫ßn su·∫•t v·ªÅ root chung).

‚∏ª

4. Compression Ratio v√† ƒê·ªô d√†i Chu·ªói

Gi·∫£ s·ª≠:
	‚Ä¢	VƒÉn b·∫£n c√≥ n k√Ω t·ª±
	‚Ä¢	Sau tokenization c√≥ m token

Compression ratio:

$$
R = \frac{n}{m}
$$

N·∫øu tokenizer t√°i s·ª≠ d·ª•ng root cho nhi·ªÅu bi·∫øn th·ªÉ:

m \downarrow \Rightarrow R \uparrow

Chi ph√≠ attention:

$O(m^2)$

Thay:

$$
O(\le)ft\frac{n^2}{R^2}\right
$$

‚∏ª

5. Ph√¢n b·ªë Zipf trong Bi·∫øn th·ªÉ T·ª´

Theo George Kingsley Zipf:

$$
fr \propto \frac{1}{r^\alpha}
$$

Root th∆∞·ªùng c√≥ th·ª© h·∫°ng th·∫•p (t·∫ßn su·∫•t cao).
Suffix c√≥ ph√¢n b·ªë ƒëu√¥i d√†i.

Ph√¢n r√£ subword l√†m thay ƒë·ªïi h·ªá s·ªë:

\alpha_{sub} \neq \alpha_{word}

‚∏ª

6. M√¥ h√¨nh X√°c su·∫•t H√¨nh th√°i

Gi·∫£ s·ª≠ x√°c su·∫•t sinh t·ª´:

$P($w_k$)$ = $P(r)$$P($s_k$ \mid r)$

Log-likelihood:

$$
\log P(w_k) = \log P(r) + \log P(s_k \mid r)
$$

Tokenizer subword x·∫•p x·ªâ ph√¢n t√≠ch h√¨nh th√°i n√†y.

‚∏ª

7. So s√°nh v·ªõi Tokenizer c·ªßa BERT

Tokenizer WordPiece trong BERT t·ªëi ∆∞u:

$$
\arg\max_{s_1,\dots,s_m} \prod_i P(s_i)
$$

Trong khi c√°c tokenizer hi·ªán ƒë·∫°i (nh∆∞ Claude) t·ªëi ∆∞u theo t·∫ßn su·∫•t byte ho·∫∑c subword linh ho·∫°t h∆°n.

‚∏ª

8. ·∫¢nh h∆∞·ªüng ƒë·∫øn Embedding

Embedding:

$$
E: V \rightarrow \mathbb{R}^d
$$

N·∫øu c√°c bi·∫øn th·ªÉ chia s·∫ª root:

ew_k \approx er + es_k

Sai s·ªë:

\delta_k = \| ew_k - (e(r)+es_k) \|_2

T·ªëi ∆∞u h√≥a:

$$
\min \sum_k \delta_k^2
$$

ƒêi·ªÅu n√†y c·∫£i thi·ªán kh·∫£ nƒÉng t·ªïng qu√°t h√≥a.

‚∏ª

9. ·∫¢nh h∆∞·ªüng ƒë·∫øn Hu·∫•n luy·ªán

Gradient c·ªßa token hi·∫øm:

$\nabla$ L$w_k$

N·∫øu chia th√†nh root v√† suffix:

$$
\nabla Lr = \sum_k \nabla Lw_k
$$

‚Üí TƒÉng ·ªïn ƒë·ªãnh gradient.

‚∏ª

10. Ph√¢n t√≠ch ƒêa ng√¥n ng·ªØ

Trong ng√¥n ng·ªØ ch·∫Øp d√≠nh:

|$s_k$| \uparrow

Tokenizer ph·∫£i c√¢n b·∫±ng gi·ªØa:
	‚Ä¢	Gi·ªØ nguy√™n to√†n b·ªô t·ª´
	‚Ä¢	Chia th√†nh nhi·ªÅu subword

T·ªëi ∆∞u h√≥a ƒëa m·ª•c ti√™u:

\min \left\frac{n^2}{R^2} + \lambda \mid V\mid \right

‚∏ª

11. Th·∫£o lu·∫≠n

Bi·∫øn th·ªÉ t·ª´ v·ª±ng t·∫°o ra:
	‚Ä¢	ƒêu√¥i d√†i trong ph√¢n b·ªë token
	‚Ä¢	TƒÉng entropy n·∫øu kh√¥ng t√°ch

Tokenizer subword hi·ªáu qu·∫£:
	1.	Gom t·∫ßn su·∫•t v√†o root
	2.	Gi·∫£m entropy
	3.	TƒÉng compression ratio
	4.	·ªîn ƒë·ªãnh hu·∫•n luy·ªán

C√°c h·ªá do Anthropic, OpenAI v√† Google ph√°t tri·ªÉn ƒë·ªÅu √°p d·ª•ng nguy√™n t·∫Øc n√†y.

‚∏ª

12. K·∫øt lu·∫≠n

Ph√¢n r√£ bi·∫øn th·ªÉ t·ª´ c√≥ th·ªÉ ƒë∆∞·ª£c m√¥ h√¨nh h√≥a:

$P($w_k$)$ = $P(r)$$P($s_k$ \mid r)$

Entropy gi·∫£m khi:

H_{sub} \le H_W

Compression ratio:

$$
R = \frac{n}{m}
$$

Chi ph√≠ attention:

$$
O(\le)ft\frac{n^2}{R^2}\right
$$

Tokenizer hi·ªán ƒë·∫°i t·∫≠n d·ª•ng c·∫•u tr√∫c h√¨nh th√°i ƒë·ªÉ:
	‚Ä¢	N√©n th√¥ng tin
	‚Ä¢	Gi·∫£m ƒë·ªô d√†i chu·ªói
	‚Ä¢	TƒÉng t√≠nh t·ªïng qu√°t h√≥a

‚∏ª

T√†i li·ªáu tham kh·∫£o
	1.	Zipf, G. K. (1935). The Psycho-Biology of Language.
	2.	Shannon, C. (1948). A Mathematical Theory of Communication.
	3.	Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
	4.	Sennrich et al. (2016). Neural Machine Translation of Rare Words with Subword Units.
	5.	Vaswani et al. (2017). Attention Is All You Need.
	6.	Kudo & Richardson (2018). SentencePiece.
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [T·∫°i sao vƒÉn b·∫£n c·∫ßn ƒë∆∞·ª£c ƒë√°nh s·ªë?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [Ph√¢n t√≠ch v√† chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh chu·ªói token ƒë∆∞·ª£c ƒë√°nh s·ªë: C∆° s·ªü to√°n h·ªçc v√† ·ª©ng d·ª•ng trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [T·∫°o v√† tr·ª±c quan h√≥a Token trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn: C∆° s·ªü to√°n h·ªçc v√† ph√¢n t√≠ch bi·ªÉu di·ªÖn](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [T·∫°o v√† tr·ª±c quan h√≥a Token (Ph·∫ßn 2): Ph√¢n t√≠ch h√¨nh h·ªçc kh√¥ng gian embedding v√† Attention Map trong m√¥ h√¨nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chu·∫©n b·ªã vƒÉn b·∫£n cho Tokenization trong m√¥ h√¨nh Transformer: C∆° s·ªü l√Ω thuy·∫øt v√† ph√¢n t√≠ch to√°n h·ªçc](aero_llm_05_preparing_text_for_tokenization.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_05_preparing_text_for_tokenization.md) |
| [Ph√¢n t√≠ch quy tr√¨nh Tokenization qua v√≠ d·ª• *The Time Machine*: C∆° s·ªü thu·∫≠t to√°n v√† m√¥ h√¨nh h√≥a to√°n h·ªçc](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So s√°nh Tokenization m·ª©c k√Ω t·ª±, t·ª´ v√† subword: Ph√¢n t√≠ch l√Ω thuy·∫øt v√† m√¥ h√¨nh to√°n h·ªçc](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thu·∫≠t to√°n Byte Pair Encoding (BPE) v√† B√†i to√°n T·ªëi ∆∞u K√≠ch th∆∞·ªõc T·ª´ v·ª±ng trong M√¥ h√¨nh Ng√¥n ng·ªØ](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| üìå **[aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md)** | [Xem b√†i vi·∫øt ‚Üí](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

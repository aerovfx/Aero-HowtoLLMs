
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer

---

## TÃ³m táº¯t

BÃ i viáº¿t nÃ y má»Ÿ rá»™ng phÃ¢n tÃ­ch quÃ¡ trÃ¬nh táº¡o vÃ  trá»±c quan hÃ³a token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n, táº­p trung vÃ o hÃ¬nh há»c cá»§a khÃ´ng gian embedding, cáº¥u trÃºc attention map vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p giáº£m chiá»u Ä‘á»ƒ quan sÃ¡t Ä‘áº·c trÆ°ng há»c Ä‘Æ°á»£c. NghiÃªn cá»©u dá»±a trÃªn kiáº¿n trÃºc Transformer Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Vaswani et al. (2017) vÃ  cÃ¡c mÃ´ hÃ¬nh GPT do OpenAI phÃ¡t triá»ƒn. CÃ¡c cÃ´ng thá»©c toÃ¡n há»c Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ mÃ´ táº£ cáº¥u trÃºc Ä‘áº¡i sá»‘ tuyáº¿n tÃ­nh cá»§a embedding, self-attention vÃ  phÃ©p chiáº¿u khÃ´ng gian.

---

# 1. Giá»›i thiá»‡u

Trong mÃ´ hÃ¬nh Transformer, token khÃ´ng chá»‰ lÃ  ID sá»‘ nguyÃªn mÃ  lÃ :

$$
t_i \rightarrow e_i \in \mathbb{R}^d
$$

KhÃ´ng gian embedding cÃ³ thá»ƒ xem nhÆ° má»™t Ä‘a táº¡p (manifold) cao chiá»u, trong Ä‘Ã³:

* Khoáº£ng cÃ¡ch pháº£n Ã¡nh quan há»‡ ngá»¯ nghÄ©a
* HÆ°á»›ng vector pháº£n Ã¡nh quan há»‡ ngá»¯ phÃ¡p

Viá»‡c trá»±c quan hÃ³a giÃºp ta hiá»ƒu:

* Cá»¥m ngá»¯ nghÄ©a
* Sá»± phÃ¢n tÃ¡ch lá»›p tá»« loáº¡i
* áº¢nh hÆ°á»Ÿng cá»§a attention

---

# 2. KhÃ´ng gian embedding: GÃ³c nhÃ¬n hÃ¬nh há»c

Giáº£ sá»­ tá»« vá»±ng cÃ³ kÃ­ch thÆ°á»›c (N), embedding dimension (d):

$$
E \in \mathbb{R}^{N \times d}
$$

Má»—i token lÃ  má»™t Ä‘iá»ƒm:

$$
e_i \in \mathbb{R}^d
$$

Khoáº£ng cÃ¡ch cosine giá»¯a hai token:

$$
\text{cosine}(e_i, e_j)
$$

$$
\frac{e_i \cdot e_j}
{|e_i||e_j|}
$$

Náº¿u:

$$
\text{cosine}(e_i, e_j) \approx 1
$$

â†’ Hai token gáº§n nhau vá» ngá»¯ nghÄ©a.

---

# 3. Biáº¿n Ä‘á»•i qua Transformer Layer

Má»™t layer Transformer gá»“m:

1. Multi-head attention
2. Feed-forward network

Biá»ƒu diá»…n Ä‘áº§u ra:

$$
Z' = \text{LayerNorm}(Z + \text{Attention}(Z))
$$

$$
Z'' = \text{LayerNorm}(Z' + \text{MLP}(Z'))
$$

Qua nhiá»u layer:

$$
Z^{(L)} = f^{(L)}(Z^{(0)})
$$

KhÃ´ng gian embedding ban Ä‘áº§u bá»‹ biáº¿n Ä‘á»•i phi tuyáº¿n.

---

# 4. Trá»±c quan hÃ³a Attention Map

Attention matrix:

$$
A = \text{softmax}
\left(
\frac{QK^T}{\sqrt{d_k}}
\right)
$$

Pháº§n tá»­:

$$
A_{ij}
$$

$$
P(\text{token } j \mid \text{token } i)
$$

TÃ­nh cháº¥t:

$$
\sum_j A_{ij} = 1
$$

Ma tráº­n (A) cÃ³ thá»ƒ trá»±c quan hÃ³a dÆ°á»›i dáº¡ng heatmap:

* VÃ¹ng sÃ¡ng â†’ tÆ°Æ¡ng tÃ¡c máº¡nh
* VÃ¹ng tá»‘i â†’ Ã­t tÆ°Æ¡ng tÃ¡c

---

# 5. PhÃ¢n tÃ­ch Eigenstructure cá»§a Embedding

Ma tráº­n hiá»‡p phÆ°Æ¡ng sai:

$$
\Sigma = \frac{1}{N} E^T E
$$

Giáº£i bÃ i toÃ¡n:

$$
\Sigma v = \lambda v
$$

Trá»‹ riÃªng lá»›n pháº£n Ã¡nh:

* HÆ°á»›ng phÆ°Æ¡ng sai lá»›n nháº¥t
* Cáº¥u trÃºc ngá»¯ nghÄ©a chÃ­nh

Chiáº¿u embedding:

$$
E_{proj} = E W_k
$$

vá»›i (W_k) chá»©a (k) vector riÃªng lá»›n nháº¥t.

---

# 6. t-SNE vÃ  cáº¥u trÃºc cá»¥m

t-SNE tá»‘i Æ°u:

$$
\min_Y D_{KL}(P | Q)
$$

Trong Ä‘Ã³:

$$
P_{ij} =
\frac
{\exp(-|x_i - x_j|^2 / 2\sigma^2)}
{\sum_{k,l} \exp(-|x_k - x_l|^2 / 2\sigma^2)}
$$

$$
Q_{ij} =
\frac
{(1 + |y_i - y_j|^2)^{-1}}
{\sum_{k,l}(1 + |y_k - y_l|^2)^{-1}}
$$

Má»¥c tiÃªu:

$$
D_{KL}(P|Q)
$$

$$
\sum_{i,j} P_{ij}
\log
\frac{P_{ij}}{Q_{ij}}
$$

Káº¿t quáº£:

* Token cÃ¹ng chá»§ Ä‘á» â†’ cá»¥m gáº§n nhau
* Token trÃ¡i nghÄ©a â†’ phÃ¢n tÃ¡ch

---

# 7. Biáº¿n Ä‘á»•i qua nhiá»u táº§ng (Representation Drift)

Giáº£ sá»­ embedding táº¡i layer (l):

$$
Z^{(l)}
$$

Khoáº£ng cÃ¡ch giá»¯a hai layer:

$$
\Delta^{(l)} =
| Z^{(l)} - Z^{(l-1)} |
$$

Quan sÃ¡t thá»±c nghiá»‡m:

* Layer Ä‘áº§u â†’ cÃº phÃ¡p
* Layer giá»¯a â†’ ngá»¯ nghÄ©a
* Layer cuá»‘i â†’ dá»± Ä‘oÃ¡n xÃ¡c suáº¥t


# 8. LiÃªn há»‡ vá»›i mÃ´ hÃ¬nh tá»± há»“i quy

GPT tá»‘i Æ°u:

$$
P(x) = \prod_{t=1}^{T} P(x_t | x_{<t})
$$

Logits:

$$
\text{logits} = Z^{(L)} W_{out}
$$

Softmax:

$$
P(x_t | x_{<t})
$$

$$
\frac
{\exp(z_t W_{out})}
{\sum_j \exp(z_j W_{out})}
$$

Viá»‡c trá»±c quan hÃ³a logits cho tháº¥y:

* PhÃ¢n phá»‘i xÃ¡c suáº¥t
* Äá»™ cháº¯c cháº¯n cá»§a mÃ´ hÃ¬nh

---

# 9. PhÃ¢n tÃ­ch Ä‘á»™ phá»©c táº¡p

Self-attention:

$$
\mathcal{O}(L T^2 d)
$$

Visualization chi phÃ­:

* PCA: (\mathcal{O}(Nd^2))
* t-SNE: (\mathcal{O}(N^2))

---

# 10. Tháº£o luáº­n

Tá»« gÃ³c nhÃ¬n Ä‘áº¡i sá»‘ tuyáº¿n tÃ­nh:

* Embedding lÃ  Ã¡nh xáº¡ tuyáº¿n tÃ­nh
* Attention lÃ  phÃ©p chiáº¿u cÃ³ trá»ng sá»‘
* MLP lÃ  biáº¿n Ä‘á»•i phi tuyáº¿n

ToÃ n bá»™ Transformer cÃ³ thá»ƒ xem nhÆ°:

$$
f: \mathbb{R}^{T \times d} \to \mathbb{R}^{T \times d}
$$

Viá»‡c trá»±c quan hÃ³a giÃºp:

1. PhÃ¡t hiá»‡n bias
2. PhÃ¢n tÃ­ch cáº¥u trÃºc
3. Giáº£i thÃ­ch hÃ nh vi mÃ´ hÃ¬nh

---

# 11. Káº¿t luáº­n

Táº¡o vÃ  trá»±c quan hÃ³a token (pháº§n 2) cho tháº¥y:

* KhÃ´ng gian embedding cÃ³ cáº¥u trÃºc hÃ¬nh há»c rÃµ rÃ ng
* Attention pháº£n Ã¡nh tÆ°Æ¡ng tÃ¡c ngá»¯ cáº£nh
* Biáº¿n Ä‘á»•i qua layer mang tÃ­nh phi tuyáº¿n máº¡nh

ToÃ¡n há»c giÃºp ta hiá»ƒu ráº±ng token lÃ  Ä‘iá»ƒm trong khÃ´ng gian vector cao chiá»u, vÃ  Transformer lÃ  chuá»—i phÃ©p biáº¿n Ä‘á»•i hÃ¬nh há»c phá»©c táº¡p.

---

# TÃ i liá»‡u tham kháº£o

1. Vaswani, A. et al. (2017). *Attention Is All You Need*.
2. Radford, A. et al. (2019). *Language Models are Unsupervised Multitask Learners*.
3. van der Maaten, L., Hinton, G. (2008). *Visualizing Data using t-SNE*.
4. Jolliffe, I. (2002). *Principal Component Analysis*.
5. Goodfellow, I., Bengio, Y., Courville, A. (2016). *Deep Learning*. MIT Press.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| ğŸ“Œ **[Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_llm_05_preparing_text_for_tokenization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_preparing_text_for_tokenization.md) |
| [PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# T·∫°o v√† tr·ª±c quan h√≥a Token (Ph·∫ßn 2): Ph√¢n t√≠ch h√¨nh h·ªçc kh√¥ng gian embedding v√† Attention Map trong m√¥ h√¨nh Transformer

---

## T√≥m t·∫Øt

B√†i vi·∫øt n√†y m·ªü r·ªông ph√¢n t√≠ch qu√° tr√¨nh t·∫°o v√† tr·ª±c quan h√≥a token trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn, t·∫≠p trung v√†o h√¨nh h·ªçc c·ªßa kh√¥ng gian embedding, c·∫•u tr√∫c attention map v√† c√°c ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu ƒë·ªÉ quan s√°t ƒë·∫∑c tr∆∞ng h·ªçc ƒë∆∞·ª£c. Nghi√™n c·ª©u d·ª±a tr√™n ki·∫øn tr√∫c Transformer ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t b·ªüi Vaswani et al. (2017) v√† c√°c m√¥ h√¨nh GPT do OpenAI ph√°t tri·ªÉn. C√°c c√¥ng th·ª©c to√°n h·ªçc ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ m√¥ t·∫£ c·∫•u tr√∫c ƒë·∫°i s·ªë tuy·∫øn t√≠nh c·ªßa embedding, self-attention v√† ph√©p chi·∫øu kh√¥ng gian.

---

# 1. Gi·ªõi thi·ªáu

Trong m√¥ h√¨nh Transformer, token kh√¥ng ch·ªâ l√† ID s·ªë nguy√™n m√† l√†:

$$

$$

t_i \rightarrow e_i \in \mathbb{R}^d

$$

$$

Kh√¥ng gian embedding c√≥ th·ªÉ xem nh∆∞ m·ªôt ƒëa t·∫°p (manifold) cao chi·ªÅu, trong ƒë√≥:

* Kho·∫£ng c√°ch ph·∫£n √°nh quan h·ªá ng·ªØ nghƒ©a
* H∆∞·ªõng vector ph·∫£n √°nh quan h·ªá ng·ªØ ph√°p

Vi·ªác tr·ª±c quan h√≥a gi√∫p ta hi·ªÉu:

* C·ª•m ng·ªØ nghƒ©a
* S·ª± ph√¢n t√°ch l·ªõp t·ª´ lo·∫°i
* ·∫¢nh h∆∞·ªüng c·ªßa attention

---

# 2. Kh√¥ng gian embedding: G√≥c nh√¨n h√¨nh h·ªçc

Gi·∫£ s·ª≠ t·ª´ v·ª±ng c√≥ k√≠ch th∆∞·ªõc $N$, embedding dimension $d$:

$$

$$

E \in \mathbb{R}^{N \times d}

$$

$$

M·ªói token l√† m·ªôt ƒëi·ªÉm:

$$

$$

e_i \in \mathbb{R}^d

$$

$$

Kho·∫£ng c√°ch cosine gi·ªØa hai token:

$$

$$

\text{cosine}(e_i, e_j) = \frac{e_i \cdot e_j} {|e_i||e_j|}

$$

$$

N·∫øu:

$$

$$

\text{cosine}(e_i, e_j) \approx 1

$$

$$

‚Üí Hai token g·∫ßn nhau v·ªÅ ng·ªØ nghƒ©a.

---

# 3. Bi·∫øn ƒë·ªïi qua Transformer Layer

M·ªôt layer Transformer g·ªìm:

1. Multi-head attention
2. Feed-forward network

Bi·ªÉu di·ªÖn ƒë·∫ßu ra:

$$

$$

Z' = \text{LayerNorm}(Z + \text{Attention}(Z))

$$

$$

$$
Z'' = \text{LayerNorm}(Z' + \text{MLP}(Z'))
$$

$$
Qua nhi·ªÅu layer:
$$

$$
Z^{(L)} = f^{(L)}(Z^{(0)})
$$

$$
Kh√¥ng gian embedding ban ƒë·∫ßu b·ªã bi·∫øn ƒë·ªïi phi tuy·∫øn. --- # 4. Tr·ª±c quan h√≥a Attention Map Attention matrix:
$$

$$
A = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right)
$$

$$
Ph·∫ßn t·ª≠:
$$

$$
A_{ij} = P(\text{token } j \mid \text{token } i)
$$

$$
T√≠nh ch·∫•t:
$$

$$
\sum_j A_{ij} = 1
$$

$$
Ma tr·∫≠n A c√≥ th·ªÉ tr·ª±c quan h√≥a d∆∞·ªõi d·∫°ng heatmap: * V√πng s√°ng ‚Üí t∆∞∆°ng t√°c m·∫°nh * V√πng t·ªëi ‚Üí √≠t t∆∞∆°ng t√°c --- # 5. Ph√¢n t√≠ch Eigenstructure c·ªßa Embedding Ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai:
$$

$$
\Sigma = \frac{1}{N} E^T E
$$

$$
Gi·∫£i b√†i to√°n:
$$

$$
\Sigma v = \lambda v
$$

$$
Tr·ªã ri√™ng l·ªõn ph·∫£n √°nh: * H∆∞·ªõng ph∆∞∆°ng sai l·ªõn nh·∫•t * C·∫•u tr√∫c ng·ªØ nghƒ©a ch√≠nh Chi·∫øu embedding:
$$

$$
E_{proj} = E W_k
$$

$$
v·ªõi W_k ch·ª©a k vector ri√™ng l·ªõn nh·∫•t. --- # 6. t-SNE v√† c·∫•u tr√∫c c·ª•m t-SNE t·ªëi ∆∞u:
$$

\min_Y D_{KL}(P | Q)

$$
Trong ƒë√≥:
$$

$$
P_{ij} = \frac {\exp(-|x_i - x_j|^2 / 2\sigma^2)} {\sum_{k,l} \exp(-|x_k - x_l|^2 / 2\sigma^2)}
$$

$$

$$

$$
Q_{ij} = \frac {(1 + |y_i - y_j|^2)^{-1}} {\sum_{k,l}(1 + |y_k - y_l|^2)^{-1}}
$$

$$
M·ª•c ti√™u:
$$

$$
D_{KL}(P|Q) = \sum_{i,j} P_{ij} \log \frac{P_{ij}}{Q_{ij}}
$$

$$
K·∫øt qu·∫£: * Token c√πng ch·ªß ƒë·ªÅ ‚Üí c·ª•m g·∫ßn nhau * Token tr√°i nghƒ©a ‚Üí ph√¢n t√°ch --- # 7. Bi·∫øn ƒë·ªïi qua nhi·ªÅu t·∫ßng (Representation Drift) Gi·∫£ s·ª≠ embedding t·∫°i layer l:
$$

Z^{(l)}

$$
Kho·∫£ng c√°ch gi·ªØa hai layer:
$$

$$
\Delta^{(l)} = | Z^{(l)} - Z^{(l-1)} |
$$

$$
Quan s√°t th·ª±c nghi·ªám: * Layer ƒë·∫ßu ‚Üí c√∫ ph√°p * Layer gi·ªØa ‚Üí ng·ªØ nghƒ©a * Layer cu·ªëi ‚Üí d·ª± ƒëo√°n x√°c su·∫•t --- # 8. Li√™n h·ªá v·ªõi m√¥ h√¨nh t·ª± h·ªìi quy GPT t·ªëi ∆∞u:
$$

$$
P(x) = \prod_{t=1}^{T} P(x_t  \mid  x_{\lt t})
$$

$$
Logits:
$$

$$
\text{logits} = Z^{(L)} W_{out}
$$

$$
Softmax:
$$

$$
P(x_t  \mid  x_{\lt t}) = \frac {\exp(z_t W_{out})} {\sum_j \exp(z_j W_{out})}
$$

$$
Vi·ªác tr·ª±c quan h√≥a logits cho th·∫•y: * Ph√¢n ph·ªëi x√°c su·∫•t * ƒê·ªô ch·∫Øc ch·∫Øn c·ªßa m√¥ h√¨nh --- # 9. Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p Self-attention:
$$

$\mathcal${O}(L T^2 d)

$$
Visualization chi ph√≠:
$$

* PCA: \mathcal{O}(Nd^2)

$$

$$

* t-SNE: \mathcal{O}(N^2)

$$
--- # 10. Th·∫£o lu·∫≠n T·ª´ g√≥c nh√¨n ƒë·∫°i s·ªë tuy·∫øn t√≠nh: * Embedding l√† √°nh x·∫° tuy·∫øn t√≠nh * Attention l√† ph√©p chi·∫øu c√≥ tr·ªçng s·ªë * MLP l√† bi·∫øn ƒë·ªïi phi tuy·∫øn To√†n b·ªô Transformer c√≥ th·ªÉ xem nh∆∞:
$$

$$
f: \mathbb{R}^{T \times d} \to \mathbb{R}^{T \times d}
$$

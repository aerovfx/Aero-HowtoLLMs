
<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# T·∫°o v√† tr·ª±c quan h√≥a Token (Ph·∫ßn 2): Ph√¢n t√≠ch h√¨nh h·ªçc kh√¥ng gian embedding v√† Attention Map trong m√¥ h√¨nh Transformer

---

## T√≥m t·∫Øt

B√†i vi·∫øt n√†y m·ªü r·ªông ph√¢n t√≠ch qu√° tr√¨nh t·∫°o v√† tr·ª±c quan h√≥a token trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn, t·∫≠p trung v√†o h√¨nh h·ªçc c·ªßa kh√¥ng gian embedding, c·∫•u tr√∫c attention map v√† c√°c ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu ƒë·ªÉ quan s√°t ƒë·∫∑c tr∆∞ng h·ªçc ƒë∆∞·ª£c. Nghi√™n c·ª©u d·ª±a tr√™n ki·∫øn tr√∫c Transformer ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t b·ªüi Vaswani et al. (2017) v√† c√°c m√¥ h√¨nh GPT do OpenAI ph√°t tri·ªÉn. C√°c c√¥ng th·ª©c to√°n h·ªçc ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ m√¥ t·∫£ c·∫•u tr√∫c ƒë·∫°i s·ªë tuy·∫øn t√≠nh c·ªßa embedding, self-attention v√† ph√©p chi·∫øu kh√¥ng gian.

---

# 1. Gi·ªõi thi·ªáu

Trong m√¥ h√¨nh Transformer, token kh√¥ng ch·ªâ l√† ID s·ªë nguy√™n m√† l√†:

$$
t_i \rightarrow e_i \in \mathbb{R}^d
$$

Kh√¥ng gian embedding c√≥ th·ªÉ xem nh∆∞ m·ªôt ƒëa t·∫°p (manifold) cao chi·ªÅu, trong ƒë√≥:

* Kho·∫£ng c√°ch ph·∫£n √°nh quan h·ªá ng·ªØ nghƒ©a
* H∆∞·ªõng vector ph·∫£n √°nh quan h·ªá ng·ªØ ph√°p

Vi·ªác tr·ª±c quan h√≥a gi√∫p ta hi·ªÉu:

* C·ª•m ng·ªØ nghƒ©a
* S·ª± ph√¢n t√°ch l·ªõp t·ª´ lo·∫°i
* ·∫¢nh h∆∞·ªüng c·ªßa attention

---

# 2. Kh√¥ng gian embedding: G√≥c nh√¨n h√¨nh h·ªçc

Gi·∫£ s·ª≠ t·ª´ v·ª±ng c√≥ k√≠ch th∆∞·ªõc $N$, embedding dimension $d$:

$$
E \in \mathbb{R}^{N \times d}
$$

M·ªói token l√† m·ªôt ƒëi·ªÉm:

$$
e_i \in \mathbb{R}^d
$$

Kho·∫£ng c√°ch cosine gi·ªØa hai token:

$$
\text{cosine}(e_i, e_j) = \frac{e_i \cdot e_j} {|e_i||e_j|}
$$

N·∫øu:

$$
\text{cosine}(e_i, e_j) \approx 1
$$

‚Üí Hai token g·∫ßn nhau v·ªÅ ng·ªØ nghƒ©a.

---

# 3. Bi·∫øn ƒë·ªïi qua Transformer Layer

M·ªôt layer Transformer g·ªìm:

1. Multi-head attention
2. Feed-forward network

Bi·ªÉu di·ªÖn ƒë·∫ßu ra:

$$
Z' = \text{LayerNorm}(Z + \text{Attention}(Z))
$$

$$
Z'' = \text{LayerNorm}(Z' + \text{MLP}(Z'))
$$

Qua nhi·ªÅu layer:

$$
Z^{(L)} = f^{(L)}(Z^{(0)})
$$

Kh√¥ng gian embedding ban ƒë·∫ßu b·ªã bi·∫øn ƒë·ªïi phi tuy·∫øn.

---

# 4. Tr·ª±c quan h√≥a Attention Map

Attention matrix:

$$
A = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right)
$$

Ph·∫ßn t·ª≠:

$$
A_{ij} = P(\text{token } j \mid \text{token } i)
$$

T√≠nh ch·∫•t:

$$
\sum_j A_{ij} = 1
$$

Ma tr·∫≠n $A$ c√≥ th·ªÉ tr·ª±c quan h√≥a d∆∞·ªõi d·∫°ng heatmap:

* V√πng s√°ng ‚Üí t∆∞∆°ng t√°c m·∫°nh
* V√πng t·ªëi ‚Üí √≠t t∆∞∆°ng t√°c

---

# 5. Ph√¢n t√≠ch Eigenstructure c·ªßa Embedding

Ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai:

$$
\Sigma = \frac{1}{N} E^T E
$$

Gi·∫£i b√†i to√°n:

$$
\Sigma v = \lambda v
$$

Tr·ªã ri√™ng l·ªõn ph·∫£n √°nh:

* H∆∞·ªõng ph∆∞∆°ng sai l·ªõn nh·∫•t
* C·∫•u tr√∫c ng·ªØ nghƒ©a ch√≠nh

Chi·∫øu embedding:

$$
E_{proj} = E W_k
$$

v·ªõi $W_k$ ch·ª©a $k$ vector ri√™ng l·ªõn nh·∫•t.

---

# 6. t-SNE v√† c·∫•u tr√∫c c·ª•m

t-SNE t·ªëi ∆∞u:

$$
\min_Y D_{KL}(P | Q)
$$

Trong ƒë√≥:

$$
P_{ij} = \frac {\exp(-|x_i - x_j|^2 / 2\sigma^2)} {\sum_{k,l} \exp(-|x_k - x_l|^2 / 2\sigma^2)}
$$

$$
Q_{ij} = \frac {(1 + |y_i - y_j|^2)^{-1}} {\sum_{k,l}(1 + |y_k - y_l|^2)^{-1}}
$$

M·ª•c ti√™u:

$$
D_{KL}(P|Q) = \sum_{i,j} P_{ij} \log \frac{P_{ij}}{Q_{ij}}
$$

K·∫øt qu·∫£:

* Token c√πng ch·ªß ƒë·ªÅ ‚Üí c·ª•m g·∫ßn nhau
* Token tr√°i nghƒ©a ‚Üí ph√¢n t√°ch

---

# 7. Bi·∫øn ƒë·ªïi qua nhi·ªÅu t·∫ßng (Representation Drift)

Gi·∫£ s·ª≠ embedding t·∫°i layer $l$:

$$
Z^{(l)}
$$

Kho·∫£ng c√°ch gi·ªØa hai layer:

$$
\Delta^{(l)} = | Z^{(l)} - Z^{(l-1)} |
$$

Quan s√°t th·ª±c nghi·ªám:

* Layer ƒë·∫ßu ‚Üí c√∫ ph√°p
* Layer gi·ªØa ‚Üí ng·ªØ nghƒ©a
* Layer cu·ªëi ‚Üí d·ª± ƒëo√°n x√°c su·∫•t

---

# 8. Li√™n h·ªá v·ªõi m√¥ h√¨nh t·ª± h·ªìi quy

GPT t·ªëi ∆∞u:

$$
P(x) = \prod_{t=1}^{T} P(x_t  \mid  x_{\lt t})
$$

Logits:

$$
\text{logits} = Z^{(L)} W_{out}
$$

Softmax:

$$
P(x_t  \mid  x_{\lt t}) = \frac {\exp(z_t W_{out})} {\sum_j \exp(z_j W_{out})}
$$

Vi·ªác tr·ª±c quan h√≥a logits cho th·∫•y:

* Ph√¢n ph·ªëi x√°c su·∫•t
* ƒê·ªô ch·∫Øc ch·∫Øn c·ªßa m√¥ h√¨nh

---

# 9. Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p

Self-attention:

$$
\mathcal{O}(L T^2 d)
$$

Visualization chi ph√≠:

* PCA: $\mathcal{O}(Nd^2$)
* t-SNE: $\mathcal{O}(N^2$)

---

# 10. Th·∫£o lu·∫≠n

T·ª´ g√≥c nh√¨n ƒë·∫°i s·ªë tuy·∫øn t√≠nh:

* Embedding l√† √°nh x·∫° tuy·∫øn t√≠nh
* Attention l√† ph√©p chi·∫øu c√≥ tr·ªçng s·ªë
* MLP l√† bi·∫øn ƒë·ªïi phi tuy·∫øn

To√†n b·ªô Transformer c√≥ th·ªÉ xem nh∆∞:

$$
f: \mathbb{R}^{T \times d} \to \mathbb{R}^{T \times d}
$$

Vi·ªác tr·ª±c quan h√≥a gi√∫p:

1. Ph√°t hi·ªán bias
2. Ph√¢n t√≠ch c·∫•u tr√∫c
3. Gi·∫£i th√≠ch h√†nh vi m√¥ h√¨nh

---

# 11. K·∫øt lu·∫≠n

T·∫°o v√† tr·ª±c quan h√≥a token (ph·∫ßn 2) cho th·∫•y:

* Kh√¥ng gian embedding c√≥ c·∫•u tr√∫c h√¨nh h·ªçc r√µ r√†ng
* Attention ph·∫£n √°nh t∆∞∆°ng t√°c ng·ªØ c·∫£nh
* Bi·∫øn ƒë·ªïi qua layer mang t√≠nh phi tuy·∫øn m·∫°nh

To√°n h·ªçc gi√∫p ta hi·ªÉu r·∫±ng token l√† ƒëi·ªÉm trong kh√¥ng gian vector cao chi·ªÅu, v√† Transformer l√† chu·ªói ph√©p bi·∫øn ƒë·ªïi h√¨nh h·ªçc ph·ª©c t·∫°p.

---

# T√†i li·ªáu tham kh·∫£o

1. Vaswani, A. et al. (2017). *Attention Is All You Need*.
2. Radford, A. et al. (2019). *Language Models are Unsupervised Multitask Learners*.
3. van der Maaten, L., Hinton, G. (2008). *Visualizing Data using t-SNE*.
4. Jolliffe, I. (2002). *Principal Component Analysis*.
5. Goodfellow, I., Bengio, Y., Courville, A. (2016). *Deep Learning*. MIT Press.
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [T·∫°i sao vƒÉn b·∫£n c·∫ßn ƒë∆∞·ª£c ƒë√°nh s·ªë?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [Ph√¢n t√≠ch v√† chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh chu·ªói token ƒë∆∞·ª£c ƒë√°nh s·ªë: C∆° s·ªü to√°n h·ªçc v√† ·ª©ng d·ª•ng trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [T·∫°o v√† tr·ª±c quan h√≥a Token trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn: C∆° s·ªü to√°n h·ªçc v√† ph√¢n t√≠ch bi·ªÉu di·ªÖn](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| üìå **[T·∫°o v√† tr·ª±c quan h√≥a Token (Ph·∫ßn 2): Ph√¢n t√≠ch h√¨nh h·ªçc kh√¥ng gian embedding v√† Attention Map trong m√¥ h√¨nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md)** | [Xem b√†i vi·∫øt ‚Üí](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chu·∫©n b·ªã vƒÉn b·∫£n cho Tokenization trong m√¥ h√¨nh Transformer: C∆° s·ªü l√Ω thuy·∫øt v√† ph√¢n t√≠ch to√°n h·ªçc](aero_llm_05_preparing_text_for_tokenization.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_05_preparing_text_for_tokenization.md) |
| [Ph√¢n t√≠ch quy tr√¨nh Tokenization qua v√≠ d·ª• *The Time Machine*: C∆° s·ªü thu·∫≠t to√°n v√† m√¥ h√¨nh h√≥a to√°n h·ªçc](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So s√°nh Tokenization m·ª©c k√Ω t·ª±, t·ª´ v√† subword: Ph√¢n t√≠ch l√Ω thuy·∫øt v√† m√¥ h√¨nh to√°n h·ªçc](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thu·∫≠t to√°n Byte Pair Encoding (BPE) v√† B√†i to√°n T·ªëi ∆∞u K√≠ch th∆∞·ªõc T·ª´ v·ª±ng trong M√¥ h√¨nh Ng√¥n ng·ªØ](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
PhÃ¢n tÃ­ch PhÃ¢n bá»‘ Äá»™ dÃ i Subword vÃ  Sá»‘ lÆ°á»£ng Token trong Bá»™ Tokenizer cá»§a GPT-4

(Dá»±a trÃªn tÃ i liá»‡u Ä‘Ã­nh kÃ¨m vÃ  má»Ÿ rá»™ng há»c thuáº­t)

â¸»

TÃ³m táº¯t

BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch má»‘i quan há»‡ giá»¯a Ä‘á»™ dÃ i subword vÃ  sá»‘ lÆ°á»£ng token trong há»‡ tokenizer cá»§a GPT-4, dá»±a trÃªn dá»¯ liá»‡u thá»±c nghiá»‡m tá»« tÃ i liá»‡u Ä‘Ã­nh kÃ¨m. ThÃ´ng qua mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c vÃ  thá»‘ng kÃª xÃ¡c suáº¥t, chÃºng tÃ´i lÃ m rÃµ cÃ¡ch phÃ¢n bá»‘ token áº£nh hÆ°á»Ÿng Ä‘áº¿n hiá»‡u nÄƒng mÃ´ hÃ¬nh Transformer. BÃ i viáº¿t má»Ÿ rá»™ng ná»n táº£ng lÃ½ thuyáº¿t cá»§a Byte Pair Encoding (BPE) vÃ  tháº£o luáº­n tÃ¡c Ä‘á»™ng Ä‘áº¿n Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n trong kiáº¿n trÃºc Attention cá»§a OpenAI.

â¸»

1. Giá»›i thiá»‡u

Trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs), tokenization quyáº¿t Ä‘á»‹nh cÃ¡ch vÄƒn báº£n Ä‘Æ°á»£c phÃ¢n máº£nh thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ xá»­ lÃ½. Vá»›i GPT-4, tokenizer hoáº¡t Ä‘á»™ng á»Ÿ byte-level BPE, nghÄ©a lÃ  má»i chuá»—i Unicode Ä‘Æ°á»£c mÃ£ hÃ³a thÃ nh cÃ¡c chuá»—i byte trÆ°á»›c khi thá»±c hiá»‡n há»£p nháº¥t subword.

Giáº£ sá»­ má»™t chuá»—i vÄƒn báº£n Ä‘áº§u vÃ o:

$$

$$

S = (c_1, c_2, ..., c_n)

$$

$$

Tokenizer Ã¡nh xáº¡ thÃ nh chuá»—i token:

$$

$$

T = (t_1, t_2, ..., t_m), \quad m \le n

$$

$$

Má»—i token cÃ³ Ä‘á»™ dÃ i subword \ell((t_i).

$$
â¸» 2. MÃ´ hÃ¬nh ToÃ¡n há»c cá»§a PhÃ¢n bá»‘ Äá»™ dÃ i Subword 2.1 Äá»‹nh nghÄ©a Gá»i: â€¢	V: táº­p tá»« vá»±ng token â€¢	|V|: kÃ­ch thÆ°á»›c tá»« vá»±ng
$$

â€¢	)\ell((t): Ä‘á»™ dÃ i kÃ½ tá»± (hoáº·c byte) cá»§a token t

$$
PhÃ¢n bá»‘ xÃ¡c suáº¥t theo Ä‘á»™ dÃ i:
$$

)P(L = k) = \frac{\mid \{t \in V : \ell((t) = k\}|}{|V|}

$$
â¸» 2.2 Ká»³ vá»ng Ä‘á»™ dÃ i token Äá»™ dÃ i trung bÃ¬nh cá»§a token: ) \mathbb{E}[L] = \sum_{k=1}^{\infty} k \cdot P(L = k)
$$

Náº¿u phÃ¢n bá»‘ lá»‡ch pháº£i (right-skewed), pháº§n lá»›n token sáº½ cÃ³ Ä‘á»™ dÃ i nhá» (1â€“4 byte), nhÆ°ng tá»“n táº¡i má»™t sá»‘ token dÃ i hÆ¡n Ä‘áº¡i diá»‡n cho cá»¥m tá»« phá»• biáº¿n.

â¸»

2.3 HÃ m phÃ¢n bá»‘ tÃ­ch lÅ©y

$$

$$

Fk = P(L \le k)

$$

$$

GiÃºp Ä‘Ã¡nh giÃ¡ tá»· lá»‡ token ngáº¯n chiáº¿m bao nhiÃªu pháº§n trÄƒm trong toÃ n bá»™ tá»« vá»±ng.

â¸»

3. PhÃ¢n tÃ­ch Thá»±c nghiá»‡m tá»« TÃ i liá»‡u

Dá»±a trÃªn dá»¯ liá»‡u Ä‘Ã­nh kÃ¨m:
	â€¢	Token 1â€“3 kÃ½ tá»± chiáº¿m tá»· lá»‡ cao nháº¥t.
	â€¢	Token dÃ i (>10 kÃ½ tá»±) ráº¥t hiáº¿m.
	â€¢	PhÃ¢n bá»‘ gáº§n giá»‘ng hÃ m mÅ© giáº£m dáº§n.

Ta cÃ³ thá»ƒ xáº¥p xá»‰:

$P(L = k)$ $\approx$ Ce^{-\lambda k}

Trong Ä‘Ã³:
	â€¢	C: háº±ng sá»‘ chuáº©n hÃ³a
	â€¢	\lambda > 0: há»‡ sá»‘ suy giáº£m

Chuáº©n hÃ³a:

$$
\sum_{k=1}^{\infty} Ce^{-\lambda k} = 1
$$

$$
C = 1 - e^{-\lambda}
$$

â¸»

4. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Äá»™ Phá»©c táº¡p Attention

Trong kiáº¿n trÃºc Transformer cá»§a OpenAI, self-attention cÃ³ Ä‘á»™ phá»©c táº¡p:

$O(m^2)$

Trong Ä‘Ã³ m lÃ  sá»‘ token sau khi token hÃ³a.

$$
Náº¿u Ä‘á»™ dÃ i trung bÃ¬nh token lÃ  \mathbb{E}[L], thÃ¬:
$$

$$
m \approx \frac{n}{\mathbb{E}[L]}
$$

$$
Do Ä‘Ã³ chi phÃ­ tÃ­nh toÃ¡n:
$$

$O($\le$)$ft($\le$ft(\frac{n}{$\mathbb${E}[L]}\right)^2\right)

$$
Tokenizer tá»‘i Æ°u sáº½:
$$

â€¢	TÄƒng \mathbb{E}[L]

$$
â€¢	Giáº£m m â€¢	Giáº£m chi phÃ­ attention â¸» 5. Má»‘i quan há»‡ vá»›i Entropy ThÃ´ng tin Entropy cá»§a phÃ¢n bá»‘ token:
$$

$$
HT = - \sum_{t \in V} P(t)\log P(t)
$$

$$
Náº¿u token ngáº¯n quÃ¡ nhiá»u: â€¢	Entropy cao â€¢	Chuá»—i dÃ i â€¢	Attention tá»‘n tÃ i nguyÃªn Náº¿u token quÃ¡ dÃ i: â€¢	Vocabulary lá»›n â€¢	KhÃ³ tá»•ng quÃ¡t hÃ³a Do Ä‘Ã³ BPE tá»‘i Æ°u cÃ¢n báº±ng giá»¯a hai yáº¿u tá»‘ nÃ y. â¸» 6. MÃ´ hÃ¬nh Zipf vÃ  PhÃ¢n bá»‘ Táº§n suáº¥t Táº§n suáº¥t token thÆ°á»ng tuÃ¢n theo luáº­t Zipf:
$$

fr \propto \frac{1}{r^\alpha}

$$
Trong Ä‘Ã³: â€¢	r: thá»© háº¡ng token
$$

$$
â€¢	\alpha \approx 1
$$

$$
Káº¿t há»£p Zipf vÃ  phÃ¢n bá»‘ Ä‘á»™ dÃ i: â€¢	Token phá»• biáº¿n thÆ°á»ng ngáº¯n â€¢	Token hiáº¿m thÆ°á»ng dÃ i â¸» 7. So sÃ¡nh vá»›i CÃ¡c PhÆ°Æ¡ng phÃ¡p KhÃ¡c PhÆ°Æ¡ng phÃ¡p	PhÃ¢n bá»‘ Ä‘á»™ dÃ i	TÃ­nh á»•n Ä‘á»‹nh	Chi phÃ­ Word-level	KhÃ´ng Ä‘á»“ng Ä‘á»u	OOV cao	Trung bÃ¬nh
$$

Character-level	L = 1	á»”n Ä‘á»‹nh	Ráº¥t cao

$$
BPE	PhÃ¢n bá»‘ mÅ©	CÃ¢n báº±ng	Tá»‘i Æ°u Unigram LM	XÃ¡c suáº¥t	Linh hoáº¡t	Cao â¸» 8. Há»‡ quáº£ Äá»‘i vá»›i Huáº¥n luyá»‡n Loss function:
$$

$\mathcal${L} = - $\sum$_{i=1}^{m} $\log$ P($t_i$  \mid  t_{\lt i})

$$
VÃ¬ m phá»¥ thuá»™c tokenizer nÃªn: â€¢	Tokenizer áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n giÃ¡ trá»‹ loss â€¢	áº¢nh hÆ°á»Ÿng tá»‘c Ä‘á»™ há»™i tá»¥ â€¢	áº¢nh hÆ°á»Ÿng kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a â¸» 9. Tháº£o luáº­n Káº¿t quáº£ cho tháº¥y: â€¢	PhÃ¢n bá»‘ Ä‘á»™ dÃ i token cÃ³ dáº¡ng suy giáº£m hÃ m mÅ© â€¢	Äá»™ dÃ i trung bÃ¬nh lÃ  tham sá»‘ then chá»‘t â€¢	Tokenizer quyáº¿t Ä‘á»‹nh cáº¥u trÃºc khÃ´ng gian xÃ¡c suáº¥t Ä‘áº§u vÃ o Trong tÆ°Æ¡ng lai, adaptive tokenization cÃ³ thá»ƒ tá»‘i Æ°u theo ngá»¯ cáº£nh thay vÃ¬ cá»‘ Ä‘á»‹nh tá»« vá»±ng. â¸» 10. Káº¿t luáº­n PhÃ¢n tÃ­ch cho tháº¥y:
$$

m \sim \frac{n}{\mathbb{E}[L]}
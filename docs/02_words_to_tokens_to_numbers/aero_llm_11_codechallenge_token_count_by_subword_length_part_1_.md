
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
PhÃ¢n tÃ­ch PhÃ¢n bá»‘ Äá»™ dÃ i Subword vÃ  Sá»‘ lÆ°á»£ng Token trong Bá»™ Tokenizer cá»§a GPT-4

(Dá»±a trÃªn tÃ i liá»‡u Ä‘Ã­nh kÃ¨m vÃ  má»Ÿ rá»™ng há»c thuáº­t)

â¸»

TÃ³m táº¯t

BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch má»‘i quan há»‡ giá»¯a Ä‘á»™ dÃ i subword vÃ  sá»‘ lÆ°á»£ng token trong há»‡ tokenizer cá»§a GPT-4, dá»±a trÃªn dá»¯ liá»‡u thá»±c nghiá»‡m tá»« tÃ i liá»‡u Ä‘Ã­nh kÃ¨m. ThÃ´ng qua mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c vÃ  thá»‘ng kÃª xÃ¡c suáº¥t, chÃºng tÃ´i lÃ m rÃµ cÃ¡ch phÃ¢n bá»‘ token áº£nh hÆ°á»Ÿng Ä‘áº¿n hiá»‡u nÄƒng mÃ´ hÃ¬nh Transformer. BÃ i viáº¿t má»Ÿ rá»™ng ná»n táº£ng lÃ½ thuyáº¿t cá»§a Byte Pair Encoding (BPE) vÃ  tháº£o luáº­n tÃ¡c Ä‘á»™ng Ä‘áº¿n Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n trong kiáº¿n trÃºc Attention cá»§a OpenAI.

â¸»

1. Giá»›i thiá»‡u

Trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs), tokenization quyáº¿t Ä‘á»‹nh cÃ¡ch vÄƒn báº£n Ä‘Æ°á»£c phÃ¢n máº£nh thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ xá»­ lÃ½. Vá»›i GPT-4, tokenizer hoáº¡t Ä‘á»™ng á»Ÿ byte-level BPE, nghÄ©a lÃ  má»i chuá»—i Unicode Ä‘Æ°á»£c mÃ£ hÃ³a thÃ nh cÃ¡c chuá»—i byte trÆ°á»›c khi thá»±c hiá»‡n há»£p nháº¥t subword.

Giáº£ sá»­ má»™t chuá»—i vÄƒn báº£n Ä‘áº§u vÃ o:

S = (c_1, c_2, ..., c_n)

Tokenizer Ã¡nh xáº¡ thÃ nh chuá»—i token:

T = (t_1, t_2, ..., t_m), \quad m \le n

Má»—i token cÃ³ Ä‘á»™ dÃ i subword \ell$t_i$.

â¸»

2. MÃ´ hÃ¬nh ToÃ¡n há»c cá»§a PhÃ¢n bá»‘ Äá»™ dÃ i Subword

2.1 Äá»‹nh nghÄ©a

Gá»i:
	â€¢	V: táº­p tá»« vá»±ng token
	â€¢	|V|: kÃ­ch thÆ°á»›c tá»« vá»±ng
	â€¢	\ell$t$: Ä‘á»™ dÃ i kÃ½ tá»± (hoáº·c byte) cá»§a token t

PhÃ¢n bá»‘ xÃ¡c suáº¥t theo Ä‘á»™ dÃ i:

P$L = k$ = \frac{|\{t \in V : \ell$t$ = k\}|}{|V|}

â¸»

2.2 Ká»³ vá»ng Ä‘á»™ dÃ i token

Äá»™ dÃ i trung bÃ¬nh cá»§a token:

\mathbb{E}[L] = \sum_{k=1}^{\infty} k \cdot P$L = k$

Náº¿u phÃ¢n bá»‘ lá»‡ch pháº£i (right-skewed), pháº§n lá»›n token sáº½ cÃ³ Ä‘á»™ dÃ i nhá» (1â€“4 byte), nhÆ°ng tá»“n táº¡i má»™t sá»‘ token dÃ i hÆ¡n Ä‘áº¡i diá»‡n cho cá»¥m tá»« phá»• biáº¿n.

â¸»

2.3 HÃ m phÃ¢n bá»‘ tÃ­ch lÅ©y

F$k$ = P$L \le k$

GiÃºp Ä‘Ã¡nh giÃ¡ tá»· lá»‡ token ngáº¯n chiáº¿m bao nhiÃªu pháº§n trÄƒm trong toÃ n bá»™ tá»« vá»±ng.

â¸»

3. PhÃ¢n tÃ­ch Thá»±c nghiá»‡m tá»« TÃ i liá»‡u

Dá»±a trÃªn dá»¯ liá»‡u Ä‘Ã­nh kÃ¨m:
	â€¢	Token 1â€“3 kÃ½ tá»± chiáº¿m tá»· lá»‡ cao nháº¥t.
	â€¢	Token dÃ i (>10 kÃ½ tá»±) ráº¥t hiáº¿m.
	â€¢	PhÃ¢n bá»‘ gáº§n giá»‘ng hÃ m mÅ© giáº£m dáº§n.

Ta cÃ³ thá»ƒ xáº¥p xá»‰:

P$L = k$ \approx Ce^{-\lambda k}

Trong Ä‘Ã³:
	â€¢	C: háº±ng sá»‘ chuáº©n hÃ³a
	â€¢	\lambda > 0: há»‡ sá»‘ suy giáº£m

Chuáº©n hÃ³a:

\sum_{k=1}^{\infty} Ce^{-\lambda k} = 1

C = $1 - e^{-\lambda}$

â¸»

4. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Äá»™ Phá»©c táº¡p Attention

Trong kiáº¿n trÃºc Transformer cá»§a OpenAI, self-attention cÃ³ Ä‘á»™ phá»©c táº¡p:

O$m^2$

Trong Ä‘Ã³ m lÃ  sá»‘ token sau khi token hÃ³a.

Náº¿u Ä‘á»™ dÃ i trung bÃ¬nh token lÃ  \mathbb{E}[L], thÃ¬:

m \approx \frac{n}{\mathbb{E}[L]}

Do Ä‘Ã³ chi phÃ­ tÃ­nh toÃ¡n:

O\left(\left(\frac{n}{\mathbb{E}[L]}\right)^2\right)

Tokenizer tá»‘i Æ°u sáº½:
	â€¢	TÄƒng \mathbb{E}[L]
	â€¢	Giáº£m m
	â€¢	Giáº£m chi phÃ­ attention

â¸»

5. Má»‘i quan há»‡ vá»›i Entropy ThÃ´ng tin

Entropy cá»§a phÃ¢n bá»‘ token:

H$T$ = - \sum_{t \in V} P$t$\log P$t$

Náº¿u token ngáº¯n quÃ¡ nhiá»u:
	â€¢	Entropy cao
	â€¢	Chuá»—i dÃ i
	â€¢	Attention tá»‘n tÃ i nguyÃªn

Náº¿u token quÃ¡ dÃ i:
	â€¢	Vocabulary lá»›n
	â€¢	KhÃ³ tá»•ng quÃ¡t hÃ³a

Do Ä‘Ã³ BPE tá»‘i Æ°u cÃ¢n báº±ng giá»¯a hai yáº¿u tá»‘ nÃ y.

â¸»

6. MÃ´ hÃ¬nh Zipf vÃ  PhÃ¢n bá»‘ Táº§n suáº¥t

Táº§n suáº¥t token thÆ°á»ng tuÃ¢n theo luáº­t Zipf:

f$r$ \propto \frac{1}{r^\alpha}

Trong Ä‘Ã³:
	â€¢	r: thá»© háº¡ng token
	â€¢	\alpha \approx 1

Káº¿t há»£p Zipf vÃ  phÃ¢n bá»‘ Ä‘á»™ dÃ i:
	â€¢	Token phá»• biáº¿n thÆ°á»ng ngáº¯n
	â€¢	Token hiáº¿m thÆ°á»ng dÃ i

â¸»

7. So sÃ¡nh vá»›i CÃ¡c PhÆ°Æ¡ng phÃ¡p KhÃ¡c

PhÆ°Æ¡ng phÃ¡p	PhÃ¢n bá»‘ Ä‘á»™ dÃ i	TÃ­nh á»•n Ä‘á»‹nh	Chi phÃ­
Word-level	KhÃ´ng Ä‘á»“ng Ä‘á»u	OOV cao	Trung bÃ¬nh
Character-level	L = 1	á»”n Ä‘á»‹nh	Ráº¥t cao
BPE	PhÃ¢n bá»‘ mÅ©	CÃ¢n báº±ng	Tá»‘i Æ°u
Unigram LM	XÃ¡c suáº¥t	Linh hoáº¡t	Cao

â¸»

8. Há»‡ quáº£ Äá»‘i vá»›i Huáº¥n luyá»‡n

Loss function:

\mathcal{L} = - \sum_{i=1}^{m} \log P(t_i | t_{<i})

VÃ¬ m phá»¥ thuá»™c tokenizer nÃªn:
	â€¢	Tokenizer áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n giÃ¡ trá»‹ loss
	â€¢	áº¢nh hÆ°á»Ÿng tá»‘c Ä‘á»™ há»™i tá»¥
	â€¢	áº¢nh hÆ°á»Ÿng kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a

â¸»

9. Tháº£o luáº­n

Káº¿t quáº£ cho tháº¥y:
	â€¢	PhÃ¢n bá»‘ Ä‘á»™ dÃ i token cÃ³ dáº¡ng suy giáº£m hÃ m mÅ©
	â€¢	Äá»™ dÃ i trung bÃ¬nh lÃ  tham sá»‘ then chá»‘t
	â€¢	Tokenizer quyáº¿t Ä‘á»‹nh cáº¥u trÃºc khÃ´ng gian xÃ¡c suáº¥t Ä‘áº§u vÃ o

Trong tÆ°Æ¡ng lai, adaptive tokenization cÃ³ thá»ƒ tá»‘i Æ°u theo ngá»¯ cáº£nh thay vÃ¬ cá»‘ Ä‘á»‹nh tá»« vá»±ng.

â¸»

10. Káº¿t luáº­n

PhÃ¢n tÃ­ch cho tháº¥y:

m \sim \frac{n}{\mathbb{E}[L]}

\text{Cost} \sim O$m^2$

P$L=k$ \sim e^{-\lambda k}

Do Ä‘Ã³, phÃ¢n bá»‘ Ä‘á»™ dÃ i subword lÃ  yáº¿u tá»‘ cá»‘t lÃµi quyáº¿t Ä‘á»‹nh hiá»‡u nÄƒng mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n.

Tokenizer khÃ´ng chá»‰ lÃ  bÆ°á»›c tiá»n xá»­ lÃ½ mÃ  lÃ  thÃ nh pháº§n kiáº¿n trÃºc áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n:
	â€¢	Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n
	â€¢	Entropy thÃ´ng tin
	â€¢	Kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Sennrich et al. (2016). Neural Machine Translation of Rare Words with Subword Units.
	2.	Vaswani et al. (2017). Attention Is All You Need.
	3.	Kudo (2018). Subword Regularization.
	4.	Brown et al. (2020). Language Models are Few-Shot Learners.
	5.	Shannon (1948). A Mathematical Theory of Communication.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_llm_05_preparing_text_for_tokenization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_preparing_text_for_tokenization.md) |
| [PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| ğŸ“Œ **[aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

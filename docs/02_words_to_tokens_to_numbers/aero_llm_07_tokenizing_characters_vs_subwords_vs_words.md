
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c

---

## TÃ³m táº¯t

BÃ i bÃ¡o nÃ y phÃ¢n tÃ­ch vÃ  so sÃ¡nh ba chiáº¿n lÆ°á»£c tokenization phá»• biáº¿n trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn: **character-level**, **word-level** vÃ  **subword-level**. Dá»±a trÃªn ná»n táº£ng kiáº¿n trÃºc Attention Is All You Need vÃ  cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n nhÆ° GPT-4 do OpenAI phÃ¡t triá»ƒn, bÃ i viáº¿t mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c sá»± khÃ¡c biá»‡t giá»¯a cÃ¡c phÆ°Æ¡ng phÃ¡p, phÃ¢n tÃ­ch entropy, Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n vÃ  tÃ¡c Ä‘á»™ng Ä‘áº¿n self-attention.

---

# 1. Giá»›i thiá»‡u

Tokenization lÃ  quÃ¡ trÃ¬nh Ã¡nh xáº¡:

$$
\tau: \Sigma^* \rightarrow V^*
$$

trong Ä‘Ã³:

* (\Sigma): táº­p kÃ½ tá»±
* (V): táº­p token
* (\Sigma^*): chuá»—i kÃ½ tá»±
* (V^*): chuá»—i token

Ba chiáº¿n lÆ°á»£c chÃ­nh:

1. Character-level
2. Word-level
3. Subword-level (BPE, Unigram LM)

Má»—i phÆ°Æ¡ng phÃ¡p táº¡o ra Ä‘á»™ dÃ i chuá»—i (T) vÃ  kÃ­ch thÆ°á»›c tá»« vá»±ng (|V|) khÃ¡c nhau.

---

# 2. Tokenization má»©c kÃ½ tá»± (Character-Level)

## 2.1 Äá»‹nh nghÄ©a

Má»—i token lÃ  má»™t kÃ½ tá»±:

$$
V = \Sigma
$$

Chuá»—i:

$$
X = (c_1, c_2, \dots, c_n)
$$

Sá»‘ token:

$$
T = n
$$

---

## 2.2 Æ¯u Ä‘iá»ƒm

* KhÃ´ng cÃ³ OOV:

$$
\forall x \in \Sigma^*, \tau(x) \text{ luÃ´n tá»“n táº¡i}
$$

* KÃ­ch thÆ°á»›c tá»« vá»±ng nhá»:

$$
|V| \approx 100 - 500
$$

---

## 2.3 NhÆ°á»£c Ä‘iá»ƒm

Self-attention cÃ³ Ä‘á»™ phá»©c táº¡p:

$$
\mathcal{O}(T^2 d)
$$

VÃ¬ (T = n) lá»›n â†’ chi phÃ­ tÄƒng máº¡nh.

VÃ­ dá»¥: vÄƒn báº£n 1000 kÃ½ tá»±

$$
T_{char} = 1000
$$

Chi phÃ­ attention:

$$
\propto 1000^2 = 10^6
$$

---

# 3. Tokenization má»©c tá»« (Word-Level)

## 3.1 Äá»‹nh nghÄ©a

Chuá»—i:

$$
X = (w_1, w_2, \dots, w_m)
$$

vá»›i:

$$
m < n
$$

Táº­p tá»« vá»±ng:

$$
V = { w }
$$

---

## 3.2 Äáº·c Ä‘iá»ƒm thá»‘ng kÃª

PhÃ¢n bá»‘ táº§n suáº¥t tá»« tuÃ¢n theo Ä‘á»‹nh luáº­t Zipf:

$$
f(w_r) \propto \frac{1}{r}
$$

trong Ä‘Ã³ (r) lÃ  thá»© háº¡ng.

Entropy:

$$
H(W) = -\sum_{w} P(w)\log P(w)
$$

---

## 3.3 NhÆ°á»£c Ä‘iá»ƒm

XÃ¡c suáº¥t OOV:

$$
P(\text{OOV}) = 1 - \sum_{w \in V} P(w)
$$

VÃ¬ tá»« vá»±ng há»¯u háº¡n.

KÃ­ch thÆ°á»›c tá»« vá»±ng lá»›n:

$$
|V| \approx 30,000 - 200,000
$$

Embedding matrix:

$$
E \in \mathbb{R}^{|V| \times d}
$$

â†’ tiÃªu tá»‘n bá»™ nhá»›.

---

# 4. Tokenization má»©c Subword

Subword káº¿t há»£p Æ°u Ä‘iá»ƒm cá»§a hai phÆ°Æ¡ng phÃ¡p trÃªn.

## 4.1 Byte Pair Encoding (BPE)

BPE láº·p láº¡i:

$$
(a^*, b^*) = \arg\max_{a,b} f(a,b)
$$

Cáº­p nháº­t tá»« vá»±ng:

$$
V_{k+1} = V_k \cup {ab}
$$

---

## 4.2 Unigram Language Model

Tá»‘i Æ°u:

$$
\max_{\theta} \prod_i \sum_{z \in \mathcal{Z}(x_i)} P(z|\theta)
$$

Trong Ä‘Ã³:

* (z): má»™t phÃ¢n tÃ¡ch há»£p lá»‡
* (\mathcal{Z}(x_i)): táº­p cÃ¡c phÃ¢n tÃ¡ch

---

## 4.3 Äá»™ dÃ i chuá»—i trung bÃ¬nh

Giáº£ sá»­:

* Character-level: (T_c = n)
* Word-level: (T_w = m)
* Subword-level: (T_s)

ThÃ´ng thÆ°á»ng:

$$
m < T_s < n
$$

Do Ä‘Ã³:

$$
T_s^2 < T_c^2
$$

vÃ 

$$
|V_s| < |V_w|
$$

---

# 5. So sÃ¡nh Ä‘á»™ phá»©c táº¡p

| PhÆ°Æ¡ng phÃ¡p | Äá»™ dÃ i (T) | Tá»« vá»±ng (|V|) | OOV | Chi phÃ­ attention |
|-------------|--------------|-----------------|------|-------------------|
| Character | Lá»›n | Nhá» | KhÃ´ng | Ráº¥t cao |
| Word | Nhá» | Ráº¥t lá»›n | CÃ³ | Tháº¥p |
| Subword | Trung bÃ¬nh | Trung bÃ¬nh | KhÃ´ng | Trung bÃ¬nh |

Self-attention:

$$
\text{Cost} = \mathcal{O}(T^2 d)
$$

Embedding memory:

$$
\mathcal{O}(|V| d)
$$

Subword tá»‘i Æ°u cÃ¢n báº±ng hai yáº¿u tá»‘.

---

# 6. PhÃ¢n tÃ­ch thÃ´ng tin

Theo Ä‘á»‹nh lÃ½ Shannon:

$$
H(X) = -\sum_x P(x)\log P(x)
$$

Chiá»u dÃ i mÃ£ tá»‘i Æ°u:

$$
L \approx \frac{H(X)}{\log |V|}
$$

Subword giÃºp:

* Giáº£m chiá»u dÃ i chuá»—i
* Giáº£m entropy Ä‘iá»u kiá»‡n

---

# 7. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Transformer

MÃ´ hÃ¬nh Transformer tÃ­nh:

$$
Z = \text{Softmax}
\left(
\frac{QK^T}{\sqrt{d_k}}
\right)V
$$

VÃ¬ attention phá»¥ thuá»™c (T):

* Character-level â†’ khÃ³ má»Ÿ rá»™ng
* Word-level â†’ váº¥n Ä‘á» OOV
* Subword â†’ cÃ¢n báº±ng tá»‘i Æ°u

CÃ¡c mÃ´ hÃ¬nh GPT hiá»‡n Ä‘áº¡i sá»­ dá»¥ng biáº¿n thá»ƒ byte-level BPE.

---

# 8. Tháº£o luáº­n thá»±c nghiá»‡m

Trong thá»±c táº¿:

* Character-level phÃ¹ há»£p cho dá»¯ liá»‡u nhiá»…u
* Word-level phÃ¹ há»£p cho corpora nhá»
* Subword-level phÃ¹ há»£p cho LLM quy mÃ´ lá»›n

Giáº£ sá»­ chuá»—i 1000 kÃ½ tá»±:

$$
T_c = 1000
$$
$$
T_s \approx 250 - 400
$$
$$
T_w \approx 150 - 250
$$

Chi phÃ­ attention giáº£m theo bÃ¬nh phÆ°Æ¡ng Ä‘á»™ dÃ i.

---

# 9. Káº¿t luáº­n

Tokenization cÃ³ thá»ƒ xem lÃ  bÃ i toÃ¡n tá»‘i Æ°u Ä‘a má»¥c tiÃªu:

$$
\min_{V} \left( \alpha T^2 + \beta |V| \right)
$$

Trong Ä‘Ã³:

* (T): Ä‘á»™ dÃ i chuá»—i
* (|V|): kÃ­ch thÆ°á»›c tá»« vá»±ng
* (\alpha, \beta): trá»ng sá»‘ chi phÃ­

Subword tokenization lÃ  nghiá»‡m cÃ¢n báº±ng gáº§n tá»‘i Æ°u trong thá»±c táº¿.

---

# TÃ i liá»‡u tham kháº£o

1. Attention Is All You Need
2. Sennrich, R. et al. (2016). *Neural Machine Translation of Rare Words with Subword Units*.
3. Kudo, T. (2018). *Subword Regularization*.
4. Shannon, C. (1948). *A Mathematical Theory of Communication*.
5. Jurafsky, D., Martin, J. (2023). *Speech and Language Processing*.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_llm_05_preparing_text_for_tokenization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_preparing_text_for_tokenization.md) |
| [PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| ğŸ“Œ **[So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[üè† Home](../../index.md) > [02 words to tokens to numbers](../index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../index.md)
- [üìö Module 01: LLM Course](../../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Tokenization trong C√°c Ng√¥n ng·ªØ Kh√°c nhau:

Ph√¢n t√≠ch To√°n h·ªçc v·ªÅ T·ª∑ l·ªá N√©n, H√¨nh th√°i h·ªçc v√† ·∫¢nh h∆∞·ªüng ƒë·∫øn Transformer

‚∏ª

T√≥m t·∫Øt

D·ª±a tr√™n t√†i li·ªáu ƒë√≠nh k√®m ‚ÄúTokenization in Different Languages‚Äù, b√†i vi·∫øt n√†y ph√¢n t√≠ch s·ª± kh√°c bi·ªát trong h√†nh vi tokenization gi·ªØa c√°c ng√¥n ng·ªØ c√≥ ƒë·∫∑c ƒëi·ªÉm h√¨nh th√°i v√† h·ªá ch·ªØ vi·∫øt kh√°c nhau. Ch√∫ng t√¥i x√¢y d·ª±ng m√¥ h√¨nh to√°n h·ªçc cho t·ª∑ l·ªá n√©n, entropy v√† ƒë·ªô d√†i chu·ªói token, ƒë·ªìng th·ªùi ph√¢n t√≠ch t√°c ƒë·ªông ƒë·∫øn ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n trong ki·∫øn tr√∫c Transformer. C√°c v√≠ d·ª• minh h·ªça ƒë∆∞·ª£c tr√¨nh b√†y v·ªõi tokenizer c·ªßa BERT, mBERT v√† th∆∞ vi·ªán SentencePiece.

‚∏ª

1. Gi·ªõi thi·ªáu

Tokenization √°nh x·∫° chu·ªói k√Ω t·ª±:

x \in \Sigma^*

th√†nh chu·ªói token:

\mathcal{T}(x) = (t_1, t_2, \dots, t_m)

Tuy nhi√™n, ƒë·∫∑c ƒëi·ªÉm c·ªßa ng√¥n ng·ªØ (morphology, h·ªá ch·ªØ vi·∫øt, kho·∫£ng tr·∫Øng) ·∫£nh h∆∞·ªüng m·∫°nh ƒë·∫øn:
	‚Ä¢	ƒê·ªô d√†i trung b√¨nh c·ªßa token
	‚Ä¢	T·ª∑ l·ªá n√©n
	‚Ä¢	K√≠ch th∆∞·ªõc t·ª´ v·ª±ng
	‚Ä¢	Chi ph√≠ attention

‚∏ª

2. Ph√¢n lo·∫°i Ng√¥n ng·ªØ theo ƒê·∫∑c ƒëi·ªÉm Tokenization

2.1 Ng√¥n ng·ªØ ph√¢n t√≠ch (Analytic languages)

V√≠ d·ª•: ti·∫øng Anh.
T·ª´ th∆∞·ªùng t√°ch b·∫±ng kho·∫£ng tr·∫Øng.

Tokenizer nh∆∞ WordPiece (trong BERT) ho·∫°t ƒë·ªông hi·ªáu qu·∫£.

‚∏ª

2.2 Ng√¥n ng·ªØ ch·∫Øp d√≠nh (Agglutinative languages)

V√≠ d·ª•: ti·∫øng Th·ªï Nhƒ© K·ª≥, ti·∫øng Ph·∫ßn Lan.
M·ªôt t·ª´ c√≥ th·ªÉ ch·ª©a nhi·ªÅu h·∫≠u t·ªë.

N·∫øu m·ªôt t·ª´ c√≥ c·∫•u tr√∫c:

w = r + s_1 + s_2 + \dots + s_k

ƒê·ªô d√†i k√Ω t·ª± tƒÉng tuy·∫øn t√≠nh theo k.

Tokenizer ph·∫£i chia nh·ªè h∆°n:

m \uparrow

‚∏ª

2.3 Ng√¥n ng·ªØ kh√¥ng ph√¢n t√°ch b·∫±ng kho·∫£ng tr·∫Øng

V√≠ d·ª•: ti·∫øng Trung.

Chu·ªói k√Ω t·ª±:

x = c_1 c_2 \dots c_n

M·ªói k√Ω t·ª± c√≥ th·ªÉ l√† m·ªôt ƒë∆°n v·ªã nghƒ©a.

Trong tr∆∞·ªùng h·ª£p n√†y:

R \approx 1

(tr·ª´ khi tokenizer g·ªôp nhi·ªÅu k√Ω t·ª± th√†nh m·ªôt token).

‚∏ª

3. M√¥ h√¨nh T·ª∑ l·ªá N√©n

Gi·∫£ s·ª≠:
	‚Ä¢	n: s·ªë k√Ω t·ª±
	‚Ä¢	m: s·ªë token

3.1 Compression Ratio

R = \frac{n}{m}

T∆∞∆°ng ƒë∆∞∆°ng:

R = \mathbb{E}[L]

trong ƒë√≥ L l√† ƒë·ªô d√†i token.

‚∏ª

3.2 So s√°nh gi·ªØa Ng√¥n ng·ªØ

Gi·∫£ s·ª≠:

R_{\text{EN}} = 4

R_{\text{ZH}} = 1.5

Chi ph√≠ attention:

C = O(m^2) = O\left(\left(\frac{n}{R}\right)^2\right)

T·ª∑ l·ªá chi ph√≠:

\frac{C_{\text{ZH}}}{C_{\text{EN}}}
=
\left(\frac{R_{\text{EN}}}{R_{\text{ZH}}}\right)^2

N·∫øu R_{\text{EN}} = 4, R_{\text{ZH}} = 2:

= \left(\frac{4}{2}\right)^2 = 4

Ti·∫øng Trung t·ªën g·∫•p 4 l·∫ßn chi ph√≠ attention cho c√πng s·ªë k√Ω t·ª±.

‚∏ª

4. Entropy theo Ng√¥n ng·ªØ

Theo l√Ω thuy·∫øt c·ªßa Claude Shannon:

Entropy k√Ω t·ª±:

H_c = -\sum p(c)\log p(c)

Entropy token:

H_t = -\sum p(t)\log p(t)

B·∫£o to√†n th√¥ng tin:

n H_c \approx m H_t

Suy ra:

R \approx \frac{H_t}{H_c}

Ng√¥n ng·ªØ c√≥ b·∫£ng ch·ªØ c√°i l·ªõn (nh∆∞ ti·∫øng Trung) c√≥:

H_c \uparrow
\Rightarrow R \downarrow

‚∏ª

5. T√°c ƒë·ªông ƒë·∫øn M√¥ h√¨nh ƒêa ng√¥n ng·ªØ

5.1 mBERT

mBERT d√πng chung t·ª´ v·ª±ng ~110k token cho nhi·ªÅu ng√¥n ng·ªØ.

Ph√¢n b·ªë token kh√¥ng ƒë·ªìng ƒë·ªÅu:

p_{\text{lang}}(t) \neq \text{uniform}

Ng√¥n ng·ªØ c√≥ √≠t d·ªØ li·ªáu ‚Üí √≠t token chuy√™n bi·ªát.

‚∏ª

5.2 T·ªëi ∆∞u h√≥a T·ª´ v·ª±ng

B√†i to√°n:

\min_{V} \sum_{\ell} \alpha_\ell \left(\frac{n_\ell}{R_\ell}\right)^2 + \lambda |V|

Trong ƒë√≥:
	‚Ä¢	\ell: ng√¥n ng·ªØ
	‚Ä¢	\alpha_\ell: tr·ªçng s·ªë d·ªØ li·ªáu
	‚Ä¢	R_\ell: compression ratio c·ªßa ng√¥n ng·ªØ ƒë√≥

‚∏ª

6. Ph√¢n b·ªë ƒê·ªô d√†i Token

G·ªçi:

P_\ell(L=k)

K·ª≥ v·ªçng:

\mathbb{E}_\ell[L] = \sum_k k P_\ell(L=k)

Ng√¥n ng·ªØ ch·∫Øp d√≠nh c√≥:

\text{Var}(L) \uparrow

v√¨ t·ª´ d√†i b·ªã chia th√†nh nhi·ªÅu subword kh√¥ng ƒë·ªÅu.

‚∏ª

7. ·∫¢nh h∆∞·ªüng ƒë·∫øn ƒê·ªô ph·ª©c t·∫°p Hu·∫•n luy·ªán

Transformer:

\text{Cost} = O(m^2 d)

Thay m = \frac{n}{R}:

\text{Cost} = O\left(\frac{n^2}{R^2} d\right)

Ng√¥n ng·ªØ c√≥ R nh·ªè l√†m tƒÉng:
	‚Ä¢	B·ªô nh·ªõ GPU
	‚Ä¢	Th·ªùi gian hu·∫•n luy·ªán
	‚Ä¢	ƒê·ªô tr·ªÖ suy lu·∫≠n

‚∏ª

8. Ph√¢n t√≠ch H√¨nh th√°i h·ªçc

N·∫øu s·ªë h·∫≠u t·ªë trung b√¨nh m·ªói t·ª´ l√† k:

|w| \sim O(k)

Tokenizer t·ªëi ∆∞u s·∫Ω c·ªë g·∫Øng h·ªçc c√°c ƒë∆°n v·ªã c√≥ x√°c su·∫•t cao:

\arg\max_{s} P(s)

Trong ng√¥n ng·ªØ ch·∫Øp d√≠nh, x√°c su·∫•t h·∫≠u t·ªë ph√¢n t√°n ‚Üí kh√≥ ƒë·∫°t n√©n cao.

‚∏ª

9. Th·∫£o lu·∫≠n

Kh√°c bi·ªát gi·ªØa c√°c ng√¥n ng·ªØ d·∫´n ƒë·∫øn:
	1.	Compression ratio kh√°c nhau
	2.	Chi ph√≠ attention kh√°c nhau
	3.	Ph√¢n b·ªë gradient kh√°c nhau
	4.	Hi·ªáu nƒÉng m√¥ h√¨nh kh√¥ng ƒë·ªìng ƒë·ªÅu

C√°c h·ªá nh∆∞ Google v√† OpenAI ph·∫£i c√¢n b·∫±ng gi·ªØa:
	‚Ä¢	Bao ph·ªß ƒëa ng√¥n ng·ªØ
	‚Ä¢	K√≠ch th∆∞·ªõc t·ª´ v·ª±ng
	‚Ä¢	Chi ph√≠ t√≠nh to√°n

‚∏ª

10. K·∫øt lu·∫≠n

Tokenization ph·ª• thu·ªôc m·∫°nh v√†o c·∫•u tr√∫c ng√¥n ng·ªØ.

C√°c h·ªá th·ª©c quan tr·ªçng:

R = \frac{n}{m}

n H_c \approx m H_t

\text{Cost} = O\left(\frac{n^2}{R^2}\right)

Ng√¥n ng·ªØ c√≥ compression ratio th·∫•p s·∫Ω ch·ªãu chi ph√≠ t√≠nh to√°n cao h∆°n trong Transformer.

Do ƒë√≥, thi·∫øt k·∫ø tokenizer ƒëa ng√¥n ng·ªØ l√† b√†i to√°n t·ªëi ∆∞u ƒëa m·ª•c ti√™u gi·ªØa:
	‚Ä¢	Entropy
	‚Ä¢	K√≠ch th∆∞·ªõc t·ª´ v·ª±ng
	‚Ä¢	Ph√¢n b·ªë d·ªØ li·ªáu
	‚Ä¢	Chi ph√≠ attention

‚∏ª

T√†i li·ªáu tham kh·∫£o
	1.	Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
	2.	Vaswani et al. (2017). Attention Is All You Need.
	3.	Shannon, C. (1948). A Mathematical Theory of Communication.
	4.	Kudo & Richardson (2018). SentencePiece.
	5.	Sennrich et al. (2016). Neural Machine Translation of Rare Words with Subword Units.
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [T·∫°i sao vƒÉn b·∫£n c·∫ßn ƒë∆∞·ª£c ƒë√°nh s·ªë?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [Ph√¢n t√≠ch v√† chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh chu·ªói token ƒë∆∞·ª£c ƒë√°nh s·ªë: C∆° s·ªü to√°n h·ªçc v√† ·ª©ng d·ª•ng trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [T·∫°o v√† tr·ª±c quan h√≥a Token trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn: C∆° s·ªü to√°n h·ªçc v√† ph√¢n t√≠ch bi·ªÉu di·ªÖn](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [T·∫°o v√† tr·ª±c quan h√≥a Token (Ph·∫ßn 2): Ph√¢n t√≠ch h√¨nh h·ªçc kh√¥ng gian embedding v√† Attention Map trong m√¥ h√¨nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chu·∫©n b·ªã vƒÉn b·∫£n cho Tokenization trong m√¥ h√¨nh Transformer: C∆° s·ªü l√Ω thuy·∫øt v√† ph√¢n t√≠ch to√°n h·ªçc](aero_llm_05_preparing_text_for_tokenization.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_05_preparing_text_for_tokenization.md) |
| [Ph√¢n t√≠ch quy tr√¨nh Tokenization qua v√≠ d·ª• *The Time Machine*: C∆° s·ªü thu·∫≠t to√°n v√† m√¥ h√¨nh h√≥a to√°n h·ªçc](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So s√°nh Tokenization m·ª©c k√Ω t·ª±, t·ª´ v√† subword: Ph√¢n t√≠ch l√Ω thuy·∫øt v√† m√¥ h√¨nh to√°n h·ªçc](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thu·∫≠t to√°n Byte Pair Encoding (BPE) v√† B√†i to√°n T·ªëi ∆∞u K√≠ch th∆∞·ªõc T·ª´ v·ª±ng trong M√¥ h√¨nh Ng√¥n ng·ªØ](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| üìå **[aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md)** | [Xem b√†i vi·∫øt ‚Üí](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

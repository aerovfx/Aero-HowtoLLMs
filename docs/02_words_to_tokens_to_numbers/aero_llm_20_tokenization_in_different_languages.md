
<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Tokenization trong C√°c Ng√¥n ng·ªØ Kh√°c nhau:

Ph√¢n t√≠ch To√°n h·ªçc v·ªÅ T·ª∑ l·ªá N√©n, H√¨nh th√°i h·ªçc v√† ·∫¢nh h∆∞·ªüng ƒë·∫øn Transformer

‚∏ª

T√≥m t·∫Øt

D·ª±a tr√™n t√†i li·ªáu ƒë√≠nh k√®m ‚ÄúTokenization in Different Languages‚Äù, b√†i vi·∫øt n√†y ph√¢n t√≠ch s·ª± kh√°c bi·ªát trong h√†nh vi tokenization gi·ªØa c√°c ng√¥n ng·ªØ c√≥ ƒë·∫∑c ƒëi·ªÉm h√¨nh th√°i v√† h·ªá ch·ªØ vi·∫øt kh√°c nhau. Ch√∫ng t√¥i x√¢y d·ª±ng m√¥ h√¨nh to√°n h·ªçc cho t·ª∑ l·ªá n√©n, entropy v√† ƒë·ªô d√†i chu·ªói token, ƒë·ªìng th·ªùi ph√¢n t√≠ch t√°c ƒë·ªông ƒë·∫øn ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n trong ki·∫øn tr√∫c Transformer. C√°c v√≠ d·ª• minh h·ªça ƒë∆∞·ª£c tr√¨nh b√†y v·ªõi tokenizer c·ªßa BERT, mBERT v√† th∆∞ vi·ªán SentencePiece.

‚∏ª

1. Gi·ªõi thi·ªáu

Tokenization √°nh x·∫° chu·ªói k√Ω t·ª±:

x \in \Sigma^*

th√†nh chu·ªói token:

$$
\mathcal{T}x = t_1, t_2, \dots, t_m
$$

Tuy nhi√™n, ƒë·∫∑c ƒëi·ªÉm c·ªßa ng√¥n ng·ªØ (morphology, h·ªá ch·ªØ vi·∫øt, kho·∫£ng tr·∫Øng) ·∫£nh h∆∞·ªüng m·∫°nh ƒë·∫øn:
	‚Ä¢	ƒê·ªô d√†i trung b√¨nh c·ªßa token
	‚Ä¢	T·ª∑ l·ªá n√©n
	‚Ä¢	K√≠ch th∆∞·ªõc t·ª´ v·ª±ng
	‚Ä¢	Chi ph√≠ attention

‚∏ª

2. Ph√¢n lo·∫°i Ng√¥n ng·ªØ theo ƒê·∫∑c ƒëi·ªÉm Tokenization

2.1 Ng√¥n ng·ªØ ph√¢n t√≠ch (Analytic languages)

V√≠ d·ª•: ti·∫øng Anh.
T·ª´ th∆∞·ªùng t√°ch b·∫±ng kho·∫£ng tr·∫Øng.

Tokenizer nh∆∞ WordPiece (trong BERT) ho·∫°t ƒë·ªông hi·ªáu qu·∫£.

‚∏ª

2.2 Ng√¥n ng·ªØ ch·∫Øp d√≠nh (Agglutinative languages)

V√≠ d·ª•: ti·∫øng Th·ªï Nhƒ© K·ª≥, ti·∫øng Ph·∫ßn Lan.
M·ªôt t·ª´ c√≥ th·ªÉ ch·ª©a nhi·ªÅu h·∫≠u t·ªë.

N·∫øu m·ªôt t·ª´ c√≥ c·∫•u tr√∫c:

$$

$$

w = r + s_1 + s_2 + \dots + s_k

$$

$$

ƒê·ªô d√†i k√Ω t·ª± tƒÉng tuy·∫øn t√≠nh theo k.

Tokenizer ph·∫£i chia nh·ªè h∆°n:

m \uparrow

‚∏ª

2.3 Ng√¥n ng·ªØ kh√¥ng ph√¢n t√°ch b·∫±ng kho·∫£ng tr·∫Øng

V√≠ d·ª•: ti·∫øng Trung.

Chu·ªói k√Ω t·ª±:

$$

$$

x = c_1 c_2 \dots c_n

$$

$$

M·ªói k√Ω t·ª± c√≥ th·ªÉ l√† m·ªôt ƒë∆°n v·ªã nghƒ©a.

Trong tr∆∞·ªùng h·ª£p n√†y:

$$

$$

R \approx 1

$$

$$

(tr·ª´ khi tokenizer g·ªôp nhi·ªÅu k√Ω t·ª± th√†nh m·ªôt token).

‚∏ª

3. M√¥ h√¨nh T·ª∑ l·ªá N√©n

Gi·∫£ s·ª≠:
	‚Ä¢	n: s·ªë k√Ω t·ª±
	‚Ä¢	m: s·ªë token

3.1 Compression Ratio

$$
R = \frac{n}{m}
$$

T∆∞∆°ng ƒë∆∞∆°ng:

$$

$$

R = \mathbb{E}[L]

$$

$$

trong ƒë√≥ L l√† ƒë·ªô d√†i token.

‚∏ª

3.2 So s√°nh gi·ªØa Ng√¥n ng·ªØ

Gi·∫£ s·ª≠:

$$
R_{\text{EN}} = 4
$$

$$
R_{\text{ZH}} = 1.5
$$

Chi ph√≠ attention:

$$

$$

C = O(m^2) = O(\le)ft\left(\frac{n}{R}\right^2\right)

$$

$$

T·ª∑ l·ªá chi ph√≠:

\frac{C_{\text{ZH}}}{C_{\text{EN}}}

$$
=
$$

$$
\left\frac{R_{\text{EN}}}{R_{\text{ZH}}}\right^2
$$

$$
N·∫øu R_{\text{EN}} = 4, R_{\text{ZH}} = 2:
$$

$$
= \left\frac{4}{2}\right^2 = 4
$$

$$
Ti·∫øng Trung t·ªën g·∫•p 4 l·∫ßn chi ph√≠ attention cho c√πng s·ªë k√Ω t·ª±. ‚∏ª 4. Entropy theo Ng√¥n ng·ªØ Theo l√Ω thuy·∫øt c·ªßa Claude Shannon: Entropy k√Ω t·ª±:
$$

$$
H_c = -\sum pc\log pc
$$

$$
Entropy token:
$$

$$
H_t = -\sum pt\log pt
$$

$$
B·∫£o to√†n th√¥ng tin:
$$

$$
n H_c \approx m H_t
$$

$$
Suy ra:
$$

$$
R \approx \frac{H_t}{H_c}
$$

$$
Ng√¥n ng·ªØ c√≥ b·∫£ng ch·ªØ c√°i l·ªõn (nh∆∞ ti·∫øng Trung) c√≥: H_c \uparrow \Rightarrow R \downarrow ‚∏ª 5. T√°c ƒë·ªông ƒë·∫øn M√¥ h√¨nh ƒêa ng√¥n ng·ªØ 5.1 mBERT mBERT d√πng chung t·ª´ v·ª±ng ~110k token cho nhi·ªÅu ng√¥n ng·ªØ. Ph√¢n b·ªë token kh√¥ng ƒë·ªìng ƒë·ªÅu: p_{\text{lang}}t \neq \text{uniform} Ng√¥n ng·ªØ c√≥ √≠t d·ªØ li·ªáu ‚Üí √≠t token chuy√™n bi·ªát. ‚∏ª 5.2 T·ªëi ∆∞u h√≥a T·ª´ v·ª±ng B√†i to√°n:
$$

$$
\min_{V} \sum_{\ell(} \alpha_)\ell( )\left\frac{n_\ell(}{R_)\ell(}\right)^2 + \lambda |V|
$$

$$
Trong ƒë√≥:
$$

‚Ä¢	\ell(: ng√¥n ng·ªØ

$$

$$

‚Ä¢	\alpha_)\ell(: tr·ªçng s·ªë d·ªØ li·ªáu

$$

$$

‚Ä¢	R_)\ell(: compression ratio c·ªßa ng√¥n ng·ªØ ƒë√≥

$$
‚∏ª 6. Ph√¢n b·ªë ƒê·ªô d√†i Token G·ªçi: )
$$

P_\ell((L=k)

$$
) K·ª≥ v·ªçng:
$$

$\mathbb${E}_$\ell([L] = )$\sum$$_k k P_$\ell((L=k)
)$

Ng√¥n ng·ªØ ch·∫Øp d√≠nh c√≥:

\text{Var}$L$ \uparrow

v√¨ t·ª´ d√†i b·ªã chia th√†nh nhi·ªÅu subword kh√¥ng ƒë·ªÅu.

‚∏ª

7. ·∫¢nh h∆∞·ªüng ƒë·∫øn ƒê·ªô ph·ª©c t·∫°p Hu·∫•n luy·ªán

Transformer:

$$
\text{Cost} = O(m^2 d)
$$

$$
Thay m = \frac{n}{R}:
$$

$$
\text{Cost} = O(\le)ft\frac{n^2}{R^2} d\right
$$

$$
Ng√¥n ng·ªØ c√≥ R nh·ªè l√†m tƒÉng: ‚Ä¢	B·ªô nh·ªõ GPU ‚Ä¢	Th·ªùi gian hu·∫•n luy·ªán ‚Ä¢	ƒê·ªô tr·ªÖ suy lu·∫≠n ‚∏ª 8. Ph√¢n t√≠ch H√¨nh th√°i h·ªçc N·∫øu s·ªë h·∫≠u t·ªë trung b√¨nh m·ªói t·ª´ l√† k: |w| \sim O(k) Tokenizer t·ªëi ∆∞u s·∫Ω c·ªë g·∫Øng h·ªçc c√°c ƒë∆°n v·ªã c√≥ x√°c su·∫•t cao: \arg\max_{s} P(s) Trong ng√¥n ng·ªØ ch·∫Øp d√≠nh, x√°c su·∫•t h·∫≠u t·ªë ph√¢n t√°n ‚Üí kh√≥ ƒë·∫°t n√©n cao. ‚∏ª 9. Th·∫£o lu·∫≠n Kh√°c bi·ªát gi·ªØa c√°c ng√¥n ng·ªØ d·∫´n ƒë·∫øn: 1.	Compression ratio kh√°c nhau 2.	Chi ph√≠ attention kh√°c nhau 3.	Ph√¢n b·ªë gradient kh√°c nhau 4.	Hi·ªáu nƒÉng m√¥ h√¨nh kh√¥ng ƒë·ªìng ƒë·ªÅu C√°c h·ªá nh∆∞ Google v√† OpenAI ph·∫£i c√¢n b·∫±ng gi·ªØa: ‚Ä¢	Bao ph·ªß ƒëa ng√¥n ng·ªØ ‚Ä¢	K√≠ch th∆∞·ªõc t·ª´ v·ª±ng ‚Ä¢	Chi ph√≠ t√≠nh to√°n ‚∏ª 10. K·∫øt lu·∫≠n Tokenization ph·ª• thu·ªôc m·∫°nh v√†o c·∫•u tr√∫c ng√¥n ng·ªØ. C√°c h·ªá th·ª©c quan tr·ªçng:
$$

R = \frac{n}{m}

$$

$$

n H_c \approx m H_t

$$

$$

$$
\text{Cost} = O(\le)ft\frac{n^2}{R^2}\right
$$

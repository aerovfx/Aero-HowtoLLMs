
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Tokenization trong CÃ¡c NgÃ´n ngá»¯ KhÃ¡c nhau:

PhÃ¢n tÃ­ch ToÃ¡n há»c vá» Tá»· lá»‡ NÃ©n, HÃ¬nh thÃ¡i há»c vÃ  áº¢nh hÆ°á»Ÿng Ä‘áº¿n Transformer

â¸»

TÃ³m táº¯t

Dá»±a trÃªn tÃ i liá»‡u Ä‘Ã­nh kÃ¨m â€œTokenization in Different Languagesâ€, bÃ i viáº¿t nÃ y phÃ¢n tÃ­ch sá»± khÃ¡c biá»‡t trong hÃ nh vi tokenization giá»¯a cÃ¡c ngÃ´n ngá»¯ cÃ³ Ä‘áº·c Ä‘iá»ƒm hÃ¬nh thÃ¡i vÃ  há»‡ chá»¯ viáº¿t khÃ¡c nhau. ChÃºng tÃ´i xÃ¢y dá»±ng mÃ´ hÃ¬nh toÃ¡n há»c cho tá»· lá»‡ nÃ©n, entropy vÃ  Ä‘á»™ dÃ i chuá»—i token, Ä‘á»“ng thá»i phÃ¢n tÃ­ch tÃ¡c Ä‘á»™ng Ä‘áº¿n Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n trong kiáº¿n trÃºc Transformer. CÃ¡c vÃ­ dá»¥ minh há»a Ä‘Æ°á»£c trÃ¬nh bÃ y vá»›i tokenizer cá»§a BERT, mBERT vÃ  thÆ° viá»‡n SentencePiece.

â¸»

1. Giá»›i thiá»‡u

Tokenization Ã¡nh xáº¡ chuá»—i kÃ½ tá»±:

x \in \Sigma^*

thÃ nh chuá»—i token:

\mathcal{T}(x) = (t_1, t_2, \dots, t_m)

Tuy nhiÃªn, Ä‘áº·c Ä‘iá»ƒm cá»§a ngÃ´n ngá»¯ (morphology, há»‡ chá»¯ viáº¿t, khoáº£ng tráº¯ng) áº£nh hÆ°á»Ÿng máº¡nh Ä‘áº¿n:
	â€¢	Äá»™ dÃ i trung bÃ¬nh cá»§a token
	â€¢	Tá»· lá»‡ nÃ©n
	â€¢	KÃ­ch thÆ°á»›c tá»« vá»±ng
	â€¢	Chi phÃ­ attention

â¸»

2. PhÃ¢n loáº¡i NgÃ´n ngá»¯ theo Äáº·c Ä‘iá»ƒm Tokenization

2.1 NgÃ´n ngá»¯ phÃ¢n tÃ­ch (Analytic languages)

VÃ­ dá»¥: tiáº¿ng Anh.
Tá»« thÆ°á»ng tÃ¡ch báº±ng khoáº£ng tráº¯ng.

Tokenizer nhÆ° WordPiece (trong BERT) hoáº¡t Ä‘á»™ng hiá»‡u quáº£.

â¸»

2.2 NgÃ´n ngá»¯ cháº¯p dÃ­nh (Agglutinative languages)

VÃ­ dá»¥: tiáº¿ng Thá»• NhÄ© Ká»³, tiáº¿ng Pháº§n Lan.
Má»™t tá»« cÃ³ thá»ƒ chá»©a nhiá»u háº­u tá»‘.

Náº¿u má»™t tá»« cÃ³ cáº¥u trÃºc:

w = r + s_1 + s_2 + \dots + s_k

Äá»™ dÃ i kÃ½ tá»± tÄƒng tuyáº¿n tÃ­nh theo k.

Tokenizer pháº£i chia nhá» hÆ¡n:

m \uparrow

â¸»

2.3 NgÃ´n ngá»¯ khÃ´ng phÃ¢n tÃ¡ch báº±ng khoáº£ng tráº¯ng

VÃ­ dá»¥: tiáº¿ng Trung.

Chuá»—i kÃ½ tá»±:

x = c_1 c_2 \dots c_n

Má»—i kÃ½ tá»± cÃ³ thá»ƒ lÃ  má»™t Ä‘Æ¡n vá»‹ nghÄ©a.

Trong trÆ°á»ng há»£p nÃ y:

R \approx 1

(trá»« khi tokenizer gá»™p nhiá»u kÃ½ tá»± thÃ nh má»™t token).

â¸»

3. MÃ´ hÃ¬nh Tá»· lá»‡ NÃ©n

Giáº£ sá»­:
	â€¢	n: sá»‘ kÃ½ tá»±
	â€¢	m: sá»‘ token

3.1 Compression Ratio

R = \frac{n}{m}

TÆ°Æ¡ng Ä‘Æ°Æ¡ng:

R = \mathbb{E}[L]

trong Ä‘Ã³ L lÃ  Ä‘á»™ dÃ i token.

â¸»

3.2 So sÃ¡nh giá»¯a NgÃ´n ngá»¯

Giáº£ sá»­:

R_{\text{EN}} = 4

R_{\text{ZH}} = 1.5

Chi phÃ­ attention:

C = O(m^2) = O\left(\left(\frac{n}{R}\right)^2\right)

Tá»· lá»‡ chi phÃ­:

\frac{C_{\text{ZH}}}{C_{\text{EN}}}
=
\left(\frac{R_{\text{EN}}}{R_{\text{ZH}}}\right)^2

Náº¿u R_{\text{EN}} = 4, R_{\text{ZH}} = 2:

= \left(\frac{4}{2}\right)^2 = 4

Tiáº¿ng Trung tá»‘n gáº¥p 4 láº§n chi phÃ­ attention cho cÃ¹ng sá»‘ kÃ½ tá»±.

â¸»

4. Entropy theo NgÃ´n ngá»¯

Theo lÃ½ thuyáº¿t cá»§a Claude Shannon:

Entropy kÃ½ tá»±:

H_c = -\sum p(c)\log p(c)

Entropy token:

H_t = -\sum p(t)\log p(t)

Báº£o toÃ n thÃ´ng tin:

n H_c \approx m H_t

Suy ra:

R \approx \frac{H_t}{H_c}

NgÃ´n ngá»¯ cÃ³ báº£ng chá»¯ cÃ¡i lá»›n (nhÆ° tiáº¿ng Trung) cÃ³:

H_c \uparrow
\Rightarrow R \downarrow

â¸»

5. TÃ¡c Ä‘á»™ng Ä‘áº¿n MÃ´ hÃ¬nh Äa ngÃ´n ngá»¯

5.1 mBERT

mBERT dÃ¹ng chung tá»« vá»±ng ~110k token cho nhiá»u ngÃ´n ngá»¯.

PhÃ¢n bá»‘ token khÃ´ng Ä‘á»“ng Ä‘á»u:

p_{\text{lang}}(t) \neq \text{uniform}

NgÃ´n ngá»¯ cÃ³ Ã­t dá»¯ liá»‡u â†’ Ã­t token chuyÃªn biá»‡t.

â¸»

5.2 Tá»‘i Æ°u hÃ³a Tá»« vá»±ng

BÃ i toÃ¡n:

\min_{V} \sum_{\ell} \alpha_\ell \left(\frac{n_\ell}{R_\ell}\right)^2 + \lambda |V|

Trong Ä‘Ã³:
	â€¢	\ell: ngÃ´n ngá»¯
	â€¢	\alpha_\ell: trá»ng sá»‘ dá»¯ liá»‡u
	â€¢	R_\ell: compression ratio cá»§a ngÃ´n ngá»¯ Ä‘Ã³

â¸»

6. PhÃ¢n bá»‘ Äá»™ dÃ i Token

Gá»i:

P_\ell(L=k)

Ká»³ vá»ng:

\mathbb{E}_\ell[L] = \sum_k k P_\ell(L=k)

NgÃ´n ngá»¯ cháº¯p dÃ­nh cÃ³:

\text{Var}(L) \uparrow

vÃ¬ tá»« dÃ i bá»‹ chia thÃ nh nhiá»u subword khÃ´ng Ä‘á»u.

â¸»

7. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Äá»™ phá»©c táº¡p Huáº¥n luyá»‡n

Transformer:

\text{Cost} = O(m^2 d)

Thay m = \frac{n}{R}:

\text{Cost} = O\left(\frac{n^2}{R^2} d\right)

NgÃ´n ngá»¯ cÃ³ R nhá» lÃ m tÄƒng:
	â€¢	Bá»™ nhá»› GPU
	â€¢	Thá»i gian huáº¥n luyá»‡n
	â€¢	Äá»™ trá»… suy luáº­n

â¸»

8. PhÃ¢n tÃ­ch HÃ¬nh thÃ¡i há»c

Náº¿u sá»‘ háº­u tá»‘ trung bÃ¬nh má»—i tá»« lÃ  k:

|w| \sim O(k)

Tokenizer tá»‘i Æ°u sáº½ cá»‘ gáº¯ng há»c cÃ¡c Ä‘Æ¡n vá»‹ cÃ³ xÃ¡c suáº¥t cao:

\arg\max_{s} P(s)

Trong ngÃ´n ngá»¯ cháº¯p dÃ­nh, xÃ¡c suáº¥t háº­u tá»‘ phÃ¢n tÃ¡n â†’ khÃ³ Ä‘áº¡t nÃ©n cao.

â¸»

9. Tháº£o luáº­n

KhÃ¡c biá»‡t giá»¯a cÃ¡c ngÃ´n ngá»¯ dáº«n Ä‘áº¿n:
	1.	Compression ratio khÃ¡c nhau
	2.	Chi phÃ­ attention khÃ¡c nhau
	3.	PhÃ¢n bá»‘ gradient khÃ¡c nhau
	4.	Hiá»‡u nÄƒng mÃ´ hÃ¬nh khÃ´ng Ä‘á»“ng Ä‘á»u

CÃ¡c há»‡ nhÆ° Google vÃ  OpenAI pháº£i cÃ¢n báº±ng giá»¯a:
	â€¢	Bao phá»§ Ä‘a ngÃ´n ngá»¯
	â€¢	KÃ­ch thÆ°á»›c tá»« vá»±ng
	â€¢	Chi phÃ­ tÃ­nh toÃ¡n

â¸»

10. Káº¿t luáº­n

Tokenization phá»¥ thuá»™c máº¡nh vÃ o cáº¥u trÃºc ngÃ´n ngá»¯.

CÃ¡c há»‡ thá»©c quan trá»ng:

R = \frac{n}{m}

n H_c \approx m H_t

\text{Cost} = O\left(\frac{n^2}{R^2}\right)

NgÃ´n ngá»¯ cÃ³ compression ratio tháº¥p sáº½ chá»‹u chi phÃ­ tÃ­nh toÃ¡n cao hÆ¡n trong Transformer.

Do Ä‘Ã³, thiáº¿t káº¿ tokenizer Ä‘a ngÃ´n ngá»¯ lÃ  bÃ i toÃ¡n tá»‘i Æ°u Ä‘a má»¥c tiÃªu giá»¯a:
	â€¢	Entropy
	â€¢	KÃ­ch thÆ°á»›c tá»« vá»±ng
	â€¢	PhÃ¢n bá»‘ dá»¯ liá»‡u
	â€¢	Chi phÃ­ attention

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
	2.	Vaswani et al. (2017). Attention Is All You Need.
	3.	Shannon, C. (1948). A Mathematical Theory of Communication.
	4.	Kudo & Richardson (2018). SentencePiece.
	5.	Sennrich et al. (2016). Neural Machine Translation of Rare Words with Subword Units.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_llm_05_preparing_text_for_tokenization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_preparing_text_for_tokenization.md) |
| [PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| ğŸ“Œ **[aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

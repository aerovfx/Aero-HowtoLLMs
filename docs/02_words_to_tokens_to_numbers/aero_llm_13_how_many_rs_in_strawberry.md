
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
PhÃ¢n tÃ­ch Lá»—i Äáº¿m KÃ½ Tá»± trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: TrÆ°á»ng há»£p â€œHow many râ€™s in strawberry?â€

â¸»

TÃ³m táº¯t

CÃ¢u há»i â€œHow many râ€™s are in strawberry?â€ Ä‘Ã£ trá»Ÿ thÃ nh má»™t vÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh cho viá»‡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) Ä‘Ã´i khi tráº£ lá»i sai cÃ¡c nhiá»‡m vá»¥ Ä‘áº¿m kÃ½ tá»± Ä‘Æ¡n giáº£n. Dá»±a trÃªn tÃ i liá»‡u Ä‘Ã­nh kÃ¨m vÃ  má»Ÿ rá»™ng há»c thuáº­t, bÃ i viáº¿t nÃ y phÃ¢n tÃ­ch nguyÃªn nhÃ¢n tá»« gÃ³c Ä‘á»™ tokenization, biá»ƒu diá»…n xÃ¡c suáº¥t, vÃ  kiáº¿n trÃºc Transformer cá»§a cÃ¡c mÃ´ hÃ¬nh do OpenAI phÃ¡t triá»ƒn. ChÃºng tÃ´i xÃ¢y dá»±ng mÃ´ hÃ¬nh toÃ¡n há»c Ä‘á»ƒ giáº£i thÃ­ch vÃ¬ sao nhiá»‡m vá»¥ Ä‘áº¿m kÃ½ tá»± khÃ´ng tÆ°Æ¡ng thÃ­ch tá»± nhiÃªn vá»›i cÆ¡ cháº¿ dá»± Ä‘oÃ¡n xÃ¡c suáº¥t theo token.

â¸»

1. Giá»›i thiá»‡u

CÃ¢u há»i:

How many râ€™s are in â€œstrawberryâ€?

ÄÃ¡p Ã¡n Ä‘Ãºng:

\text{count}("r", "strawberry") = 3

Tuy nhiÃªn, nhiá»u LLM tá»«ng tráº£ lá»i sai (vÃ­ dá»¥: 2).

Váº¥n Ä‘á» khÃ´ng náº±m á»Ÿ â€œkiáº¿n thá»©câ€ mÃ  á»Ÿ cÃ¡ch mÃ´ hÃ¬nh xá»­ lÃ½ chuá»—i kÃ½ tá»±.

â¸»

2. PhÃ¢n tÃ­ch DÆ°á»›i GÃ³c Ä‘á»™ Tokenization

2.1 Biá»ƒu diá»…n Chuá»—i

Chuá»—i kÃ½ tá»±:

S = (s_1, s_2, ..., s_n)

Vá»›i:

S = \text{"strawberry"}

Náº¿u xá»­ lÃ½ á»Ÿ má»©c kÃ½ tá»±:

n = 10

VÃ :

\sum_{i=1}^{10} \mathbf{1}$s_i = r$ = 3

Trong Ä‘Ã³:

\mathbf{1}$\cdot$

lÃ  hÃ m chá»‰ thá»‹.

â¸»

2.2 Tokenization Thá»±c táº¿

LLMs khÃ´ng xá»­ lÃ½ á»Ÿ má»©c kÃ½ tá»± mÃ  theo token:

T = (t_1, t_2, ..., t_m)

VÃ­ dá»¥ (minh há»a):

straw + berry

Hoáº·c:

st + raw + berry

Sá»‘ token m < n.

Do Ä‘Ã³, thÃ´ng tin kÃ½ tá»± r khÃ´ng Ä‘Æ°á»£c biá»ƒu diá»…n trá»±c tiáº¿p mÃ  náº±m bÃªn trong embedding vector cá»§a token.

â¸»

3. MÃ´ hÃ¬nh XÃ¡c suáº¥t cá»§a LLM

LLM há»c phÃ¢n phá»‘i:

P(t_i | t_{<i})

ToÃ n chuá»—i:

P$T$ = \prod_{i=1}^{m} P(t_i | t_{<i})

MÃ´ hÃ¬nh khÃ´ng tá»‘i Æ°u cho phÃ©p toÃ¡n Ä‘áº¿m kÃ½ tá»±, mÃ  tá»‘i Æ°u cho:

\mathcal{L} = - \sum_{i=1}^{m} \log P(t_i | t_{<i})

Tá»©c lÃ  tá»‘i thiá»ƒu hÃ³a cross-entropy giá»¯a token dá»± Ä‘oÃ¡n vÃ  token tháº­t.

â¸»

4. NguyÃªn nhÃ¢n Sai Sá»‘

4.1 KhÃ´ng cÃ³ CÆ¡ cháº¿ Äáº¿m TÆ°á»ng minh

BÃ i toÃ¡n Ä‘áº¿m yÃªu cáº§u:

f$S$ = \sum_{i=1}^{n} \mathbf{1}$s_i = r$

NhÆ°ng mÃ´ hÃ¬nh chá»‰ cÃ³:

g$T$ = \text{argmax}_{y} P(y | T)

KhÃ´ng cÃ³ bÆ°á»›c láº·p tuáº§n tá»± á»Ÿ má»©c kÃ½ tá»±.

â¸»

4.2 Biá»ƒu diá»…n Vector PhÃ¢n tÃ¡n

Embedding:

E$t$ \in \mathbb{R}^d

ThÃ´ng tin vá» kÃ½ tá»± r náº±m phÃ¢n tÃ¡n trong khÃ´ng gian:

E(\text{"strawberry"}) = f(E(\text{"straw"}), E(\text{"berry"}))

KhÃ´ng tá»“n táº¡i biáº¿n riÃªng biá»‡t Ä‘áº¿m sá»‘ láº§n xuáº¥t hiá»‡n cá»§a r.

â¸»

4.3 Attention KhÃ´ng TÆ°Æ¡ng Ä‘Æ°Æ¡ng Äáº¿m

Self-attention:

\text{Attention}(Q,K,V) = \text{softmax}\left$\frac{QK^T}{\sqrt{d_k}}\right$V

Attention há»c má»‘i quan há»‡ ngá»¯ nghÄ©a, khÃ´ng há»c phÃ©p toÃ¡n cá»™ng sá»‘ há»c chÃ­nh xÃ¡c trÃªn kÃ½ tá»±.

â¸»

5. PhÃ¢n tÃ­ch ToÃ¡n há»c Sai sá»‘ XÃ¡c suáº¥t

Giáº£ sá»­ mÃ´ hÃ¬nh Æ°á»›c lÆ°á»£ng xÃ¡c suáº¥t:

P$Y = k \mid  S$

Trong Ä‘Ã³:
	â€¢	Y: sá»‘ lÆ°á»£ng r
	â€¢	k \in \{0,1,2,3,...\}

Do khÃ´ng huáº¥n luyá»‡n trá»±c tiáº¿p cho nhiá»‡m vá»¥ Ä‘áº¿m:

P$Y=2$ \approx P$Y=3$

Náº¿u trong dá»¯ liá»‡u huáº¥n luyá»‡n, máº«u â€œ2â€ phá»• biáº¿n hÆ¡n, mÃ´ hÃ¬nh cÃ³ thá»ƒ thiÃªn lá»‡ch.

â¸»

6. So sÃ¡nh vá»›i MÃ¡y TÃ­nh Thuáº­t toÃ¡n

Thuáº­t toÃ¡n truyá»n thá»‘ng:

O$n$

Pseudo-code:

count = 0
for char in string:
    if char == 'r':
        count += 1

LLM khÃ´ng thá»±c thi thuáº­t toÃ¡n tuáº§n tá»± nhÆ° váº­y.

â¸»

7. PhÃ¢n tÃ­ch DÆ°á»›i GÃ³c Ä‘á»™ ThÃ´ng tin

Entropy cá»§a chuá»—i kÃ½ tá»±:

H$S$ = - \sum_{c \in \Sigma} P$c$\log P$c$

LLM tá»‘i Æ°u hÃ³a dá»± Ä‘oÃ¡n token, khÃ´ng tá»‘i Æ°u hÃ³a:

I(Y; S)

(tÆ°Æ¡ng há»— thÃ´ng tin giá»¯a sá»‘ lÆ°á»£ng r vÃ  chuá»—i kÃ½ tá»±)

â¸»

8. Táº¡i sao MÃ´ hÃ¬nh Má»›i Ãt Sai HÆ¡n?

CÃ¡c mÃ´ hÃ¬nh má»›i cÃ³ thá»ƒ:
	â€¢	Sá»­ dá»¥ng chain-of-thought
	â€¢	MÃ´ phá»ng Ä‘áº¿m ná»™i bá»™
	â€¢	TÄƒng kÃ­ch thÆ°á»›c context

NhÆ°ng váº«n khÃ´ng Ä‘áº£m báº£o 100% chÃ­nh xÃ¡c vÃ¬ khÃ´ng pháº£i mÃ´ hÃ¬nh symbolic.

â¸»

9. HÃ m Äáº¿m nhÆ° má»™t BÃ i toÃ¡n Há»c MÃ¡y

Ta cÃ³ thá»ƒ Ä‘á»‹nh nghÄ©a:

h_\theta$S$ \approx \sum_{i=1}^{n} \mathbf{1}$s_i = r$

Vá»›i:

\theta = \text{tham sá»‘ mÃ´ hÃ¬nh}

Sai sá»‘ ká»³ vá»ng:

\mathbb{E}[$h_\theta(S$ - f$S$)^2]

KhÃ´ng Ä‘Æ°á»£c tá»‘i Æ°u trá»±c tiáº¿p trong huáº¥n luyá»‡n LLM.

â¸»

10. Tháº£o luáº­n

Hiá»‡n tÆ°á»£ng â€œHow many râ€™s in strawberry?â€ minh há»a:
	â€¢	Tokenization lÃ m máº¥t granularity kÃ½ tá»±
	â€¢	LLM lÃ  mÃ´ hÃ¬nh xÃ¡c suáº¥t, khÃ´ng pháº£i bá»™ xá»­ lÃ½ kÃ½ tá»± chÃ­nh xÃ¡c
	â€¢	Attention â‰  thuáº­t toÃ¡n Ä‘áº¿m

ÄÃ¢y lÃ  khÃ¡c biá»‡t giá»¯a:
	â€¢	Há»‡ thá»‘ng symbolic computation
	â€¢	Há»‡ thá»‘ng neural probabilistic modeling

â¸»

11. Káº¿t luáº­n

Sai sá»‘ Ä‘áº¿m kÃ½ tá»± cÃ³ thá»ƒ giáº£i thÃ­ch bá»Ÿi:

\text{Token-level modeling} \neq \text{Character-level counting}

\min \mathcal{L}_{\text{cross-entropy}} \not\Rightarrow \min \mathcal{L}_{\text{counting}}

Do Ä‘Ã³, nhiá»‡m vá»¥ tÆ°á»Ÿng chá»«ng Ä‘Æ¡n giáº£n láº¡i khÃ´ng phÃ¹ há»£p tá»± nhiÃªn vá»›i má»¥c tiÃªu tá»‘i Æ°u cá»§a LLM.

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Vaswani et al. (2017). Attention Is All You Need.
	2.	Shannon (1948). A Mathematical Theory of Communication.
	3.	Sennrich et al. (2016). Neural Machine Translation of Rare Words with Subword Units.
	4.	Brown et al. (2020). Language Models are Few-Shot Learners.
	5.	Merrill et al. (2022). On the Ability of Transformers to Perform Counting.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_llm_05_preparing_text_for_tokenization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_preparing_text_for_tokenization.md) |
| [PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| ğŸ“Œ **[aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

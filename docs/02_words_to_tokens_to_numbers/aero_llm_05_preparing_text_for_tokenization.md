<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c

---

## TÃ³m táº¯t

BÃ i bÃ¡o nÃ y trÃ¬nh bÃ y má»™t cÃ¡ch há»‡ thá»‘ng quy trÃ¬nh **chuáº©n bá»‹ vÄƒn báº£n trÆ°á»›c khi tokenization** trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n dá»±a trÃªn kiáº¿n trÃºc Attention Is All You Need. Ná»™i dung phÃ¢n tÃ­ch cÃ¡c bÆ°á»›c tiá»n xá»­ lÃ½ (text normalization, cleaning, encoding), cÆ¡ cháº¿ mÃ£ hÃ³a Byte Pair Encoding (BPE), vÃ  vai trÃ² cá»§a tokenizer trong cÃ¡c mÃ´ hÃ¬nh GPT do OpenAI phÃ¡t triá»ƒn. BÃ i viáº¿t bá»• sung cÃ¡c cÃ´ng thá»©c toÃ¡n há»c mÃ´ táº£ xÃ¡c suáº¥t chuá»—i, Ã¡nh xáº¡ rá»i ráº¡câ€“liÃªn tá»¥c vÃ  cáº¥u trÃºc Ä‘áº¡i sá»‘ cá»§a quÃ¡ trÃ¬nh mÃ£ hÃ³a.

---

# 1. Giá»›i thiá»‡u

Trong cÃ¡c mÃ´ hÃ¬nh Transformer hiá»‡n Ä‘áº¡i nhÆ° GPT-4, vÄƒn báº£n Ä‘áº§u vÃ o khÃ´ng Ä‘Æ°á»£c xá»­ lÃ½ trá»±c tiáº¿p dÆ°á»›i dáº¡ng kÃ½ tá»± mÃ  pháº£i tráº£i qua quÃ¡ trÃ¬nh:

$$
\text{Raw Text} \rightarrow \text{Normalization} \rightarrow \text{Tokenization} \rightarrow \text{Embedding}
$$

Tokenization Ä‘Ã³ng vai trÃ² lÃ  cáº§u ná»‘i giá»¯a:

* KhÃ´ng gian rá»i ráº¡c cá»§a kÃ½ tá»±
* KhÃ´ng gian vector liÃªn tá»¥c cá»§a embedding

Náº¿u gá»i chuá»—i vÄƒn báº£n ban Ä‘áº§u lÃ :

$$
X = (c_1, c_2, \dots, c_n)
$$

thÃ¬ tokenizer Ã¡nh xáº¡:

$$
\tau: \Sigma^{\ast} \rightarrow \mathbb{Z}^m
$$

vá»›i $\Sigma$ lÃ  báº£ng chá»¯ cÃ¡i vÃ  $\mathbb{Z}^m$ lÃ  chuá»—i ID token.

---

# 2. Chuáº©n hÃ³a vÄƒn báº£n (Text Normalization)

Chuáº©n hÃ³a giÃºp Ä‘áº£m báº£o tÃ­nh nháº¥t quÃ¡n cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n.

## 2.1 Lowercasing

Ãnh xáº¡:

$$
f_{lower}(c) = \text{lowercase}(c)
$$

VÃ­ dá»¥:

$$
\text{"ChatGPT"} \rightarrow \text{"chatgpt"}
$$

## 2.2 Unicode Normalization

VÄƒn báº£n Unicode cÃ³ thá»ƒ biá»ƒu diá»…n cÃ¹ng má»™t kÃ½ tá»± theo nhiá»u cÃ¡ch.

Chuáº©n NFC:

$$
\text{Ã©} = e + \acute{}
$$

Chuáº©n hÃ³a Ä‘áº£m báº£o:

$$
NFC(x_1) = NFC(x_2)
$$

náº¿u hai chuá»—i tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá» máº·t ngá»¯ nghÄ©a.

---

# 3. Tokenization: CÆ¡ sá»Ÿ xÃ¡c suáº¥t

MÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»‘i Æ°u xÃ¡c suáº¥t:

$$
P(X) = \prod_{t=1}^{T} P(x_t \mid x_{\lt t})
$$

Tuy nhiÃªn, náº¿u lÃ m viá»‡c á»Ÿ má»©c kÃ½ tá»±:

$$
T = n
$$

Sá»‘ bÆ°á»›c dá»± Ä‘oÃ¡n lá»›n â†’ chi phÃ­ cao.

Giáº£i phÃ¡p:

Chia thÃ nh token:

$$
X = (w_1, w_2, \dots, w_m), \quad m \lt n
$$

Giáº£m Ä‘á»™ dÃ i chuá»—i vÃ  tÄƒng tÃ­nh biá»ƒu diá»…n.

---

# 4. Byte Pair Encoding (BPE)

BPE Ä‘Æ°á»£c giá»›i thiá»‡u cho NLP bá»Ÿi Sennrich et al. (2016).

## 4.1 Thuáº­t toÃ¡n

Ban Ä‘áº§u:

$$
V_0 = \{ \text{táº­p kÃ½ tá»± Ä‘Æ¡n} \}
$$

Láº·p:

1. TÃ¬m cáº·p kÃ½ tá»± xuáº¥t hiá»‡n nhiá»u nháº¥t
2. Gá»™p thÃ nh token má»›i
3. Cáº­p nháº­t tá»« vá»±ng

Giáº£ sá»­ táº§n suáº¥t cáº·p $(a,b)$:

$$
f(a,b) = \sum_{i} \mathbb{I}[(a,b) \in X_i]
$$

Chá»n:

$$
(a^{\ast}, b^{\ast}) = \arg\max_{a,b} f(a,b)
$$

Cáº­p nháº­t:

$$
V_{k+1} = V_k \cup \{ab\}
$$

---

# 5. KhÃ´ng gian rá»i ráº¡c vÃ  Ã¡nh xáº¡ embedding

Sau tokenization:

$$
w_i \rightarrow id_i \in \{1, \dots, |V|\}
$$

Embedding matrix:

$$
E \in \mathbb{R}^{|V| \times d}
$$

Ãnh xáº¡:

$$
e_i = E[id_i]
$$

ToÃ n bá»™ chuá»—i:

$$
X \rightarrow (e_1, e_2, \dots, e_m)
$$

---

# 6. PhÃ¢n tÃ­ch Ä‘á»™ phá»©c táº¡p

Náº¿u:

* $N$ lÃ  sá»‘ kÃ½ tá»±
* $V$ lÃ  kÃ­ch thÆ°á»›c tá»« vá»±ng

Chi phÃ­ xÃ¢y dá»±ng BPE:

$$
\mathcal{O}(N \log V)
$$

Chi phÃ­ suy luáº­n tokenization:

$$
\mathcal{O}(m)
$$

---

# 7. Váº¥n Ä‘á» Out-of-Vocabulary (OOV)

KhÃ´ng nhÆ° Word2Vec truyá»n thá»‘ng, BPE Ä‘áº£m báº£o:

$$
\forall x \in \Sigma^{\ast}, \exists \text{ decomposition into subwords}
$$

VÃ­ dá»¥:

```
tokenization â†’ token + ization
```

Äiá»u nÃ y Ä‘áº£m báº£o:

$$
P(x) > 0
$$

cho má»i chuá»—i há»£p lá»‡.

---

# 8. So sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c

| PhÆ°Æ¡ng phÃ¡p     | Äáº·c Ä‘iá»ƒm        | Háº¡n cháº¿           |
| --------------- | --------------- | ----------------- |
| Word-level      | Ngáº¯n, dá»… hiá»ƒu   | OOV cao           |
| Character-level | KhÃ´ng OOV       | Chuá»—i dÃ i         |
| BPE             | CÃ¢n báº±ng        | Phá»¥ thuá»™c dá»¯ liá»‡u |
| Unigram LM      | XÃ¡c suáº¥t tá»‘i Æ°u | TÃ­nh toÃ¡n cao     |

Unigram Language Model tá»‘i Æ°u:

$$
\max_{\theta} \prod_i \sum_{z \in \mathcal{Z}(x_i)} P(z \mid \theta)
$$

---

# 9. TÃ¡c Ä‘á»™ng Ä‘áº¿n Attention

Äá»™ dÃ i chuá»—i áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n chi phÃ­ self-attention:

$$
\text{Complexity} = \mathcal{O}(T^2 d)
$$

Náº¿u tokenization kÃ©m â†’ $T$ lá»›n â†’ chi phÃ­ tÄƒng.

Do Ä‘Ã³, tokenizer tá»‘i Æ°u giÃºp:

* Giáº£m memory footprint
* TÄƒng tá»‘c inference
* Cáº£i thiá»‡n cháº¥t lÆ°á»£ng ngá»¯ nghÄ©a

---

# 10. LiÃªn há»‡ thá»±c táº¿ trong GPT

CÃ¡c mÃ´ hÃ¬nh GPT sá»­ dá»¥ng biáº¿n thá»ƒ cá»§a BPE hoáº·c byte-level BPE.

XÃ¡c suáº¥t sinh token:

$$
P(w_t \mid w_{\lt t}) = \frac{\exp(z_t W_{out})}{\sum_j \exp(z_j W_{out})}
$$

Cháº¥t lÆ°á»£ng tokenization áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n phÃ¢n phá»‘i logits.

---

# 11. Tháº£o luáº­n

Chuáº©n bá»‹ vÄƒn báº£n khÃ´ng chá»‰ lÃ  bÆ°á»›c tiá»n xá»­ lÃ½ ká»¹ thuáº­t mÃ  cÃ²n lÃ :

* BÃ i toÃ¡n tá»‘i Æ°u thÃ´ng tin
* BÃ i toÃ¡n mÃ£ hÃ³a nguá»“n (source coding)
* BÃ i toÃ¡n nÃ©n dá»¯ liá»‡u

Theo Ä‘á»‹nh lÃ½ Shannon:

$$
H(X) = - \sum_x P(x) \log P(x)
$$

Tokenizer tá»‘t giÃºp:

$$
\text{Length}(X_{tokens}) \approx \frac{H(X)}{\log |V|}
$$

---

# 12. Káº¿t luáº­n

Quy trÃ¬nh chuáº©n bá»‹ vÄƒn báº£n cho tokenization bao gá»“m:

1. Chuáº©n hÃ³a Unicode
2. LÃ m sáº¡ch dá»¯ liá»‡u
3. Ãp dá»¥ng BPE hoáº·c Unigram LM
4. Ãnh xáº¡ sang embedding

ToÃ¡n há»c cho tháº¥y tokenization lÃ  quÃ¡ trÃ¬nh:

$$
\Sigma^{\ast} \rightarrow V^{\ast}
$$

giÃºp tá»‘i Æ°u:

* Äá»™ dÃ i chuá»—i
* Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n
* Biá»ƒu diá»…n ngá»¯ nghÄ©a

---

# TÃ i liá»‡u tham kháº£o

1. Attention Is All You Need
2. Sennrich, R., Haddow, B., Birch, A. (2016). *Neural Machine Translation of Rare Words with Subword Units*.
3. Kudo, T. (2018). *Subword Regularization: Improving Neural Network Translation Models*.
4. Shannon, C. (1948). *A Mathematical Theory of Communication*.
5. Jurafsky, D., Martin, J. (2023). *Speech and Language Processing*.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| ğŸ“Œ **[Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_llm_05_preparing_text_for_tokenization.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_05_preparing_text_for_tokenization.md) |
| [PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->

# Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c

---

## TÃ³m táº¯t

BÃ i viáº¿t trÃ¬nh bÃ y quy trÃ¬nh **chuáº©n bá»‹ vÄƒn báº£n trÆ°á»›c khi tokenization** trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n dá»±a trÃªn kiáº¿n trÃºc Transformer. Ná»™i dung bao gá»“m: chuáº©n hÃ³a vÄƒn báº£n, lÃ m sáº¡ch dá»¯ liá»‡u, Byte Pair Encoding (BPE), vÃ  Ã¡nh xáº¡ sang khÃ´ng gian embedding. CÃ¡c cÃ´ng thá»©c toÃ¡n há»c mÃ´ táº£ xÃ¡c suáº¥t chuá»—i, Ã¡nh xáº¡ rá»i ráº¡câ€“liÃªn tá»¥c vÃ  cáº¥u trÃºc Ä‘áº¡i sá»‘ cá»§a quÃ¡ trÃ¬nh mÃ£ hÃ³a Ä‘Æ°á»£c trÃ¬nh bÃ y cháº·t cháº½.

---

# 1. Giá»›i thiá»‡u

Trong cÃ¡c mÃ´ hÃ¬nh Transformer hiá»‡n Ä‘áº¡i, vÄƒn báº£n Ä‘áº§u vÃ o tráº£i qua chuá»—i biáº¿n Ä‘á»•i:

$$
\text{Raw Text} \rightarrow \text{Normalization} \rightarrow \text{Tokenization} \rightarrow \text{Embedding}
$$

Gá»i chuá»—i kÃ½ tá»± ban Ä‘áº§u lÃ :

$$
X = (c_1, c_2, \dots, c_n)
$$

Tokenizer Ä‘á»‹nh nghÄ©a Ã¡nh xáº¡:

$$
\tau : \Sigma^{\ast} \rightarrow \mathbb{Z}^m
$$

Trong Ä‘Ã³:

- $\Sigma$ lÃ  báº£ng chá»¯ cÃ¡i
- $\mathbb{Z}^m$ lÃ  chuá»—i ID token
- $m \le n$

---

# 2. Chuáº©n hÃ³a vÄƒn báº£n (Text Normalization)

## 2.1 Lowercasing

$$
f_{\text{lower}}(c) = \text{lowercase}(c)
$$

VÃ­ dá»¥:

$$
\text{"ChatGPT"} \rightarrow \text{"chatgpt"}
$$

---

## 2.2 Unicode Normalization

Má»™t kÃ½ tá»± cÃ³ thá»ƒ cÃ³ nhiá»u biá»ƒu diá»…n Unicode.

VÃ­ dá»¥:

$$
\text{Ã©} = e + \text{Â´}
$$

Chuáº©n hÃ³a NFC Ä‘áº£m báº£o:

$$
\text{NFC}(x_1) = \text{NFC}(x_2)
$$

náº¿u hai chuá»—i tÆ°Æ¡ng Ä‘Æ°Æ¡ng ngá»¯ nghÄ©a.

---

# 3. Tokenization vÃ  mÃ´ hÃ¬nh xÃ¡c suáº¥t

MÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»‘i Æ°u:

$$
P(X) = \prod_{t=1}^{T} P(x_t \mid x_{<t})
$$

Náº¿u lÃ m viá»‡c á»Ÿ má»©c kÃ½ tá»±:

$$
T = n
$$

Giáº£i phÃ¡p: chia thÃ nh token:

$$
X = (w_1, w_2, \dots, w_m), \quad m < n
$$

Giáº£m Ä‘á»™ dÃ i chuá»—i vÃ  tÄƒng kháº£ nÄƒng biá»ƒu diá»…n.

---

# 4. Byte Pair Encoding (BPE)

## 4.1 Thuáº­t toÃ¡n

Khá»Ÿi táº¡o:

$$
V_0 = \{ \text{táº­p kÃ½ tá»± Ä‘Æ¡n} \}
$$

Láº·p:

1. TÃ¬m cáº·p kÃ½ tá»± xuáº¥t hiá»‡n nhiá»u nháº¥t
2. Gá»™p thÃ nh token má»›i
3. Cáº­p nháº­t tá»« vá»±ng

Táº§n suáº¥t cáº·p:

$$
f(a,b) = \sum_i \mathbf{1}[(a,b) \in X_i]
$$

Chá»n:

$$
(a^{\ast}, b^{\ast}) = \arg\max_{a,b} f(a,b)
$$

Cáº­p nháº­t:

$$
V_{k+1} = V_k \cup \{ab\}
$$

---

# 5. Ãnh xáº¡ sang embedding

Sau tokenization:

$$
w_i \rightarrow \text{id}_i \in \{1, \dots, |V|\}
$$

Ma tráº­n embedding:

$$
E \in \mathbb{R}^{|V| \times d}
$$

Ãnh xáº¡:

$$
e_i = E[\text{id}_i]
$$

Chuá»—i embedding:

$$
X \rightarrow (e_1, e_2, \dots, e_m)
$$

---

# 6. PhÃ¢n tÃ­ch Ä‘á»™ phá»©c táº¡p

Chi phÃ­ xÃ¢y dá»±ng BPE:

$$
\mathcal{O}(N \log V)
$$

Chi phÃ­ suy luáº­n tokenization:

$$
\mathcal{O}(m)
$$

---

# 7. Out-of-Vocabulary (OOV)

BPE Ä‘áº£m báº£o:

$$
\forall x \in \Sigma^{\ast}, \quad \exists \text{ decomposition into subwords}
$$

Do Ä‘Ã³:

$$
P(x) > 0
$$

cho má»i chuá»—i há»£p lá»‡.

---

# 8. So sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p tokenization

| PhÆ°Æ¡ng phÃ¡p        | Æ¯u Ä‘iá»ƒm            | Háº¡n cháº¿              |
|-------------------|-------------------|----------------------|
| Word-level        | Ngáº¯n, dá»… hiá»ƒu     | OOV cao              |
| Character-level   | KhÃ´ng OOV         | Chuá»—i ráº¥t dÃ i        |
| BPE               | CÃ¢n báº±ng tá»‘t      | Phá»¥ thuá»™c dá»¯ liá»‡u    |
| Unigram LM        | Tá»‘i Æ°u xÃ¡c suáº¥t   | TÃ­nh toÃ¡n cao        |

Unigram Language Model tá»‘i Æ°u:

$$
\max_{\theta} \prod_i \sum_{z \in \mathcal{Z}(x_i)} P(z \mid \theta)
$$

---

# 9. TÃ¡c Ä‘á»™ng Ä‘áº¿n Attention

Self-attention cÃ³ Ä‘á»™ phá»©c táº¡p:

$$
\mathcal{O}(T^2)
$$

Tokenization kÃ©m â†’ $T$ lá»›n â†’ chi phÃ­ tÄƒng máº¡nh.

Tokenizer tá»‘t giÃºp:

- Giáº£m memory footprint  
- TÄƒng tá»‘c inference  
- Cáº£i thiá»‡n biá»ƒu diá»…n ngá»¯ nghÄ©a  

---

# 10. LiÃªn há»‡ vá»›i GPT

CÃ¡c mÃ´ hÃ¬nh GPT sá»­ dá»¥ng biáº¿n thá»ƒ cá»§a BPE hoáº·c byte-level BPE.

XÃ¡c suáº¥t sinh token:

$$
P(w_t = i \mid w_{<t}) =
\frac{\exp((z_t W_{\text{out}})_i)}
{\sum_j \exp((z_t W_{\text{out}})_j)}
$$

Cháº¥t lÆ°á»£ng tokenization áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n phÃ¢n phá»‘i logits.

---

# 11. GÃ³c nhÃ¬n thÃ´ng tin há»c

Theo Shannon:

$$
H(X) = - \sum_x P(x) \log P(x)
$$

Tokenizer tá»‘t giÃºp Ä‘á»™ dÃ i chuá»—i token xáº¥p xá»‰:

$$
\text{Length}(X_{\text{tokens}})
\approx
\frac{H(X)}{\log |V|}
$$

---

# 12. Káº¿t luáº­n

Quy trÃ¬nh chuáº©n bá»‹ vÄƒn báº£n bao gá»“m:

1. Chuáº©n hÃ³a Unicode  
2. LÃ m sáº¡ch dá»¯ liá»‡u  
3. Ãp dá»¥ng BPE hoáº·c Unigram LM  
4. Ãnh xáº¡ sang embedding  

Vá» máº·t toÃ¡n há»c, tokenization lÃ  Ã¡nh xáº¡:

$$
\Sigma^{\ast} \rightarrow V^{\ast}
$$

Ä‘Ã³ng vai trÃ² cáº§u ná»‘i giá»¯a khÃ´ng gian kÃ½ tá»± rá»i ráº¡c vÃ  khÃ´ng gian vector liÃªn tá»¥c trong Transformer.

ğŸ“š TÃ i liá»‡u tham kháº£o (bá»• sung)

1. Transformer & GPT
	1.	Vaswani, A., et al. (2017).
Attention Is All You Need. NeurIPS.
â†’ BÃ i bÃ¡o ná»n táº£ng giá»›i thiá»‡u kiáº¿n trÃºc Transformer.
	2.	Radford, A., et al. (2019).
Language Models are Unsupervised Multitask Learners. OpenAI.
â†’ GPT-2 vÃ  cÆ¡ cháº¿ autoregressive modeling.
	3.	Brown, T., et al. (2020).
Language Models are Few-Shot Learners. NeurIPS.
â†’ GPT-3 vÃ  scaling law.
	4.	OpenAI (2023).
GPT-4 Technical Report.
â†’ Tá»•ng quan ká»¹ thuáº­t vá» GPT-4.

â¸»

2. Tokenization & Subword Methods
	5.	Sennrich, R., Haddow, B., Birch, A. (2016).
Neural Machine Translation of Rare Words with Subword Units. ACL.
â†’ BPE trong NLP.
	6.	Kudo, T. (2018).
Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates. ACL.
â†’ Unigram Language Model (SentencePiece).
	7.	Kudo, T., Richardson, J. (2018).
SentencePiece: A simple and language independent subword tokenizer. EMNLP.
	8.	Gage, P. (1994).
A New Algorithm for Data Compression.
â†’ BPE gá»‘c trong nÃ©n dá»¯ liá»‡u.

â¸»

3. Information Theory
	9.	Shannon, C. E. (1948).
A Mathematical Theory of Communication. Bell System Technical Journal.
	10.	Cover, T., Thomas, J. (2006).
Elements of Information Theory. Wiley.

â¸»

4. Representation & Embedding
	11.	Mikolov, T., et al. (2013).
Efficient Estimation of Word Representations in Vector Space. arXiv.
	12.	Pennington, J., Socher, R., Manning, C. (2014).
GloVe: Global Vectors for Word Representation. EMNLP.
	13.	Jurafsky, D., Martin, J. (2023 draft).
Speech and Language Processing (3rd ed.).

â¸»

5. Complexity & Scaling Laws
	14.	Kaplan, J., et al. (2020).
Scaling Laws for Neural Language Models. arXiv.
	15.	Hoffmann, J., et al. (2022).
Training Compute-Optimal Large Language Models. (Chinchilla paper)

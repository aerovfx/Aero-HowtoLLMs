
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c

---

## TÃ³m táº¯t

BÃ i bÃ¡o nÃ y trÃ¬nh bÃ y má»™t cÃ¡ch há»‡ thá»‘ng quy trÃ¬nh **chuáº©n bá»‹ vÄƒn báº£n trÆ°á»›c khi tokenization** trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n dá»±a trÃªn kiáº¿n trÃºc Attention Is All You Need. Ná»™i dung phÃ¢n tÃ­ch cÃ¡c bÆ°á»›c tiá»n xá»­ lÃ½ (text normalization, cleaning, encoding), cÆ¡ cháº¿ mÃ£ hÃ³a Byte Pair Encoding (BPE), vÃ  vai trÃ² cá»§a tokenizer trong cÃ¡c mÃ´ hÃ¬nh GPT do OpenAI phÃ¡t triá»ƒn. BÃ i viáº¿t bá»• sung cÃ¡c cÃ´ng thá»©c toÃ¡n há»c mÃ´ táº£ xÃ¡c suáº¥t chuá»—i, Ã¡nh xáº¡ rá»i ráº¡câ€“liÃªn tá»¥c vÃ  cáº¥u trÃºc Ä‘áº¡i sá»‘ cá»§a quÃ¡ trÃ¬nh mÃ£ hÃ³a.

---

# 1. Giá»›i thiá»‡u

Trong cÃ¡c mÃ´ hÃ¬nh Transformer hiá»‡n Ä‘áº¡i nhÆ° GPT-4, vÄƒn báº£n Ä‘áº§u vÃ o khÃ´ng Ä‘Æ°á»£c xá»­ lÃ½ trá»±c tiáº¿p dÆ°á»›i dáº¡ng kÃ½ tá»± mÃ  pháº£i tráº£i qua quÃ¡ trÃ¬nh:

$$
\text{Raw Text} \rightarrow \text{Normalization} \rightarrow \text{Tokenization} \rightarrow \text{Embedding}
$$

Tokenization Ä‘Ã³ng vai trÃ² lÃ  cáº§u ná»‘i giá»¯a:

* KhÃ´ng gian rá»i ráº¡c cá»§a kÃ½ tá»±
* KhÃ´ng gian vector liÃªn tá»¥c cá»§a embedding

Náº¿u gá»i chuá»—i vÄƒn báº£n ban Ä‘áº§u lÃ :

X = (c_1, c_2, \dots, c_n)

thÃ¬ tokenizer Ã¡nh xáº¡:

\tau: \Sigma^{\ast} \rightarrow \mathbb{Z}^m

vá»›i \Sigma lÃ  báº£ng chá»¯ cÃ¡i vÃ  \mathbb{Z}^m lÃ  chuá»—i ID token.

$$
--- # 2. Chuáº©n hÃ³a vÄƒn báº£n (Text Normalization) Chuáº©n hÃ³a giÃºp Ä‘áº£m báº£o tÃ­nh nháº¥t quÃ¡n cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n. ## 2.1 Lowercasing Ãnh xáº¡: f_{lower}(c) = \text{lowercase}(c) VÃ­ dá»¥:
$$

\text{"ChatGPT"} \rightarrow \text{"chatgpt"}

$$
## 2.2 Unicode Normalization VÄƒn báº£n Unicode cÃ³ thá»ƒ biá»ƒu diá»…n cÃ¹ng má»™t kÃ½ tá»± theo nhiá»u cÃ¡ch. Chuáº©n NFC: \text{Ã©} = e + \acute{} Chuáº©n hÃ³a Ä‘áº£m báº£o: NFC(x_1) = NFC(x_2) náº¿u hai chuá»—i tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá» máº·t ngá»¯ nghÄ©a. --- # 3. Tokenization: CÆ¡ sá»Ÿ xÃ¡c suáº¥t MÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»‘i Æ°u xÃ¡c suáº¥t: P(X) = \prod_{t=1}^{T} P(x_t \mid x_{\lt t}) Tuy nhiÃªn, náº¿u lÃ m viá»‡c á»Ÿ má»©c kÃ½ tá»±: T = n Sá»‘ bÆ°á»›c dá»± Ä‘oÃ¡n lá»›n â†’ chi phÃ­ cao. Giáº£i phÃ¡p: Chia thÃ nh token: X = (w_1, w_2, \dots, w_m), \quad m \lt  n Giáº£m Ä‘á»™ dÃ i chuá»—i vÃ  tÄƒng tÃ­nh biá»ƒu diá»…n. --- # 4. Byte Pair Encoding (BPE) BPE Ä‘Æ°á»£c giá»›i thiá»‡u cho NLP bá»Ÿi Sennrich et al. (2016). ## 4.1 Thuáº­t toÃ¡n Ban Ä‘áº§u: V_0 = { \text{táº­p kÃ½ tá»± Ä‘Æ¡n} } Láº·p: 1. TÃ¬m cáº·p kÃ½ tá»± xuáº¥t hiá»‡n nhiá»u nháº¥t 2. Gá»™p thÃ nh token má»›i 3. Cáº­p nháº­t tá»« vá»±ng Giáº£ sá»­ táº§n suáº¥t cáº·p ((a,b)): f(a,b) = \sum_{i} \mathbb{I}[(a,b) \in X_i] Chá»n: (a^{\ast}, b^{\ast}) = \arg\max_{a,b} f(a,b) Cáº­p nháº­t: V_{k+1} = V_k \cup {ab} --- # 5. KhÃ´ng gian rá»i ráº¡c vÃ  Ã¡nh xáº¡ embedding Sau tokenization:
$$

$w_i$ \rightarrow id_i \in {1, \dots, |V|}

$$
Embedding matrix: E \in \mathbb{R}^{|V| \times d} Ãnh xáº¡: e_i = E[id_i] ToÃ n bá»™ chuá»—i:
$$

X \rightarrow ($e_1$, $e_2$, \dots, $e_m$)

$$
--- # 6. PhÃ¢n tÃ­ch Ä‘á»™ phá»©c táº¡p Náº¿u: * N lÃ  sá»‘ kÃ½ tá»± * V lÃ  kÃ­ch thÆ°á»›c tá»« vá»±ng Chi phÃ­ xÃ¢y dá»±ng BPE:
$$

$\mathcal${O}(N $\log$ V)

$$
Chi phÃ­ suy luáº­n tokenization:
$$

$\mathcal${O}(m)

$$
--- # 7. Váº¥n Ä‘á» Out-of-Vocabulary (OOV) KhÃ´ng nhÆ° Word2Vec truyá»n thá»‘ng, BPE Ä‘áº£m báº£o:
$$

\forall x \in \Sigma^{\ast}, \exists \text{ decomposition into subwords}

$$
VÃ­ dá»¥: tokenization â†’ token + ization Äiá»u nÃ y Ä‘áº£m báº£o:
$$

P(x) > 0

$$
cho má»i chuá»—i há»£p lá»‡. --- # 8. So sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c \mid PhÆ°Æ¡ng phÃ¡p     \mid Äáº·c Ä‘iá»ƒm        \mid Háº¡n cháº¿           \mid \mid --------------- \mid --------------- \mid ----------------- \mid \mid Word-level      \mid Ngáº¯n, dá»… hiá»ƒu   \mid OOV cao           \mid \mid Character-level \mid KhÃ´ng OOV       \mid Chuá»—i dÃ i         \mid \mid BPE             \mid CÃ¢n báº±ng        \mid Phá»¥ thuá»™c dá»¯ liá»‡u \mid \mid Unigram LM      \mid XÃ¡c suáº¥t tá»‘i Æ°u \mid TÃ­nh toÃ¡n cao     \mid Unigram Language Model tá»‘i Æ°u: \max_{\theta} \prod_i \sum_{z \in \mathcal{Z}(x_i)} P(z \mid \theta) --- # 9. TÃ¡c Ä‘á»™ng Ä‘áº¿n Attention Äá»™ dÃ i chuá»—i áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n chi phÃ­ self-attention: \text{Complexity} = \mathcal{O}(T^2 d) Náº¿u tokenization kÃ©m â†’ T lá»›n â†’ chi phÃ­ tÄƒng. Do Ä‘Ã³, tokenizer tá»‘i Æ°u giÃºp: * Giáº£m memory footprint * TÄƒng tá»‘c inference * Cáº£i thiá»‡n cháº¥t lÆ°á»£ng ngá»¯ nghÄ©a --- # 10. LiÃªn há»‡ thá»±c táº¿ trong GPT CÃ¡c mÃ´ hÃ¬nh GPT sá»­ dá»¥ng biáº¿n thá»ƒ cá»§a BPE hoáº·c byte-level BPE. XÃ¡c suáº¥t sinh token: P(w_t  \mid  w_{\lt t}) = \frac{\exp(z_t W_{out})} {\sum_j \exp(z_j W_{out})} Cháº¥t lÆ°á»£ng tokenization áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n phÃ¢n phá»‘i logits. --- # 11. Tháº£o luáº­n Chuáº©n bá»‹ vÄƒn báº£n khÃ´ng chá»‰ lÃ  bÆ°á»›c tiá»n xá»­ lÃ½ ká»¹ thuáº­t mÃ  cÃ²n lÃ : * BÃ i toÃ¡n tá»‘i Æ°u thÃ´ng tin * BÃ i toÃ¡n mÃ£ hÃ³a nguá»“n (source coding) * BÃ i toÃ¡n nÃ©n dá»¯ liá»‡u Theo Ä‘á»‹nh lÃ½ Shannon: H(X) = - \sum_x P(x) \log P(x) Tokenizer tá»‘t giÃºp: \text{Length}(X_{tokens}) \approx \frac{H(X)}{\log |V|} --- # 12. Káº¿t luáº­n Quy trÃ¬nh chuáº©n bá»‹ vÄƒn báº£n cho tokenization bao gá»“m: 1. Chuáº©n hÃ³a Unicode 2. LÃ m sáº¡ch dá»¯ liá»‡u 3. Ãp dá»¥ng BPE hoáº·c Unigram LM 4. Ãnh xáº¡ sang embedding ToÃ¡n há»c cho tháº¥y tokenization lÃ  quÃ¡ trÃ¬nh:
$$

\Sigma^{\ast} \rightarrow V^{\ast}
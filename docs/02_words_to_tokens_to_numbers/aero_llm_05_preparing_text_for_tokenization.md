
<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Chu·∫©n b·ªã vƒÉn b·∫£n cho Tokenization trong m√¥ h√¨nh Transformer: C∆° s·ªü l√Ω thuy·∫øt v√† ph√¢n t√≠ch to√°n h·ªçc

---

## T√≥m t·∫Øt

B√†i b√°o n√†y tr√¨nh b√†y m·ªôt c√°ch h·ªá th·ªëng quy tr√¨nh **chu·∫©n b·ªã vƒÉn b·∫£n tr∆∞·ªõc khi tokenization** trong c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn d·ª±a tr√™n ki·∫øn tr√∫c Attention Is All You Need. N·ªôi dung ph√¢n t√≠ch c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω (text normalization, cleaning, encoding), c∆° ch·∫ø m√£ h√≥a Byte Pair Encoding (BPE), v√† vai tr√≤ c·ªßa tokenizer trong c√°c m√¥ h√¨nh GPT do OpenAI ph√°t tri·ªÉn. B√†i vi·∫øt b·ªï sung c√°c c√¥ng th·ª©c to√°n h·ªçc m√¥ t·∫£ x√°c su·∫•t chu·ªói, √°nh x·∫° r·ªùi r·∫°c‚Äìli√™n t·ª•c v√† c·∫•u tr√∫c ƒë·∫°i s·ªë c·ªßa qu√° tr√¨nh m√£ h√≥a.

---

# 1. Gi·ªõi thi·ªáu

Trong c√°c m√¥ h√¨nh Transformer hi·ªán ƒë·∫°i nh∆∞ GPT-4, vƒÉn b·∫£n ƒë·∫ßu v√†o kh√¥ng ƒë∆∞·ª£c x·ª≠ l√Ω tr·ª±c ti·∫øp d∆∞·ªõi d·∫°ng k√Ω t·ª± m√† ph·∫£i tr·∫£i qua qu√° tr√¨nh:

$$
\text{Raw Text} \rightarrow \text{Normalization} \rightarrow \text{Tokenization} \rightarrow \text{Embedding}
$$

Tokenization ƒë√≥ng vai tr√≤ l√† c·∫ßu n·ªëi gi·ªØa:

* Kh√¥ng gian r·ªùi r·∫°c c·ªßa k√Ω t·ª±
* Kh√¥ng gian vector li√™n t·ª•c c·ªßa embedding

N·∫øu g·ªçi chu·ªói vƒÉn b·∫£n ban ƒë·∫ßu l√†:

$$
X = (c_1, c_2, \dots, c_n)
$$

th√¨ tokenizer √°nh x·∫°:

$$
\tau: \Sigma^* \rightarrow \mathbb{Z}^m
$$

v·ªõi (\Sigma) l√† b·∫£ng ch·ªØ c√°i v√† (\mathbb{Z}^m) l√† chu·ªói ID token.

---

# 2. Chu·∫©n h√≥a vƒÉn b·∫£n (Text Normalization)

Chu·∫©n h√≥a gi√∫p ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n c·ªßa d·ªØ li·ªáu hu·∫•n luy·ªán.

## 2.1 Lowercasing

√Ånh x·∫°:

$$
f_{lower}(c) = \text{lowercase}(c)
$$

V√≠ d·ª•:

$$
\text{"ChatGPT"} \rightarrow \text{"chatgpt"}
$$

## 2.2 Unicode Normalization

VƒÉn b·∫£n Unicode c√≥ th·ªÉ bi·ªÉu di·ªÖn c√πng m·ªôt k√Ω t·ª± theo nhi·ªÅu c√°ch.

Chu·∫©n NFC:

$$
\text{√©} = e + \acute{}
$$

Chu·∫©n h√≥a ƒë·∫£m b·∫£o:

$$
NFC(x_1) = NFC(x_2)
$$

n·∫øu hai chu·ªói t∆∞∆°ng ƒë∆∞∆°ng v·ªÅ m·∫∑t ng·ªØ nghƒ©a.

---

# 3. Tokenization: C∆° s·ªü x√°c su·∫•t

M√¥ h√¨nh ng√¥n ng·ªØ t·ªëi ∆∞u x√°c su·∫•t:

$$
P(X) = \prod_{t=1}^{T} P(x_t \mid x_{<t})
$$

Tuy nhi√™n, n·∫øu l√†m vi·ªác ·ªü m·ª©c k√Ω t·ª±:

$$
T = n
$$

S·ªë b∆∞·ªõc d·ª± ƒëo√°n l·ªõn ‚Üí chi ph√≠ cao.

Gi·∫£i ph√°p:

Chia th√†nh token:

$$
X = (w_1, w_2, \dots, w_m), \quad m < n
$$

Gi·∫£m ƒë·ªô d√†i chu·ªói v√† tƒÉng t√≠nh bi·ªÉu di·ªÖn.

---

# 4. Byte Pair Encoding (BPE)

BPE ƒë∆∞·ª£c gi·ªõi thi·ªáu cho NLP b·ªüi Sennrich et al. (2016).

## 4.1 Thu·∫≠t to√°n

Ban ƒë·∫ßu:

$$
V_0 = { \text{t·∫≠p k√Ω t·ª± ƒë∆°n} }
$$

L·∫∑p:

1. T√¨m c·∫∑p k√Ω t·ª± xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
2. G·ªôp th√†nh token m·ªõi
3. C·∫≠p nh·∫≠t t·ª´ v·ª±ng

Gi·∫£ s·ª≠ t·∫ßn su·∫•t c·∫∑p ((a,b)):

$$
f(a,b) = \sum_{i} \mathbb{I}[(a,b) \in X_i]
$$

Ch·ªçn:

$$
(a^*, b^*) = \arg\max_{a,b} f(a,b)
$$

C·∫≠p nh·∫≠t:

$$
V_{k+1} = V_k \cup {ab}
$$

---

# 5. Kh√¥ng gian r·ªùi r·∫°c v√† √°nh x·∫° embedding

Sau tokenization:

$$
w_i \rightarrow id_i \in {1, \dots, |V|}
$$

Embedding matrix:

$$
E \in \mathbb{R}^{|V| \times d}
$$

√Ånh x·∫°:

$$
e_i = E[id_i]
$$

To√†n b·ªô chu·ªói:

$$
X \rightarrow (e_1, e_2, \dots, e_m)
$$

---

# 6. Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p

N·∫øu:

* (N) l√† s·ªë k√Ω t·ª±
* (V) l√† k√≠ch th∆∞·ªõc t·ª´ v·ª±ng

Chi ph√≠ x√¢y d·ª±ng BPE:

$$
\mathcal{O}(N \log V)
$$

Chi ph√≠ suy lu·∫≠n tokenization:

$$
\mathcal{O}(m)
$$

---

# 7. V·∫•n ƒë·ªÅ Out-of-Vocabulary (OOV)

Kh√¥ng nh∆∞ Word2Vec truy·ªÅn th·ªëng, BPE ƒë·∫£m b·∫£o:

$$
\forall x \in \Sigma^*, \exists \text{ decomposition into subwords}
$$

V√≠ d·ª•:

```
tokenization ‚Üí token + ization
```

ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o:

$$
P(x) > 0
$$

cho m·ªçi chu·ªói h·ª£p l·ªá.

---

# 8. So s√°nh v·ªõi c√°c ph∆∞∆°ng ph√°p kh√°c

| Ph∆∞∆°ng ph√°p     | ƒê·∫∑c ƒëi·ªÉm        | H·∫°n ch·∫ø           |
| --------------- | --------------- | ----------------- |
| Word-level      | Ng·∫Øn, d·ªÖ hi·ªÉu   | OOV cao           |
| Character-level | Kh√¥ng OOV       | Chu·ªói d√†i         |
| BPE             | C√¢n b·∫±ng        | Ph·ª• thu·ªôc d·ªØ li·ªáu |
| Unigram LM      | X√°c su·∫•t t·ªëi ∆∞u | T√≠nh to√°n cao     |

Unigram Language Model t·ªëi ∆∞u:

$$
\max_{\theta} \prod_i \sum_{z \in \mathcal{Z}(x_i)} P(z|\theta)
$$

---

# 9. T√°c ƒë·ªông ƒë·∫øn Attention

ƒê·ªô d√†i chu·ªói ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn chi ph√≠ self-attention:

$$
\text{Complexity} = \mathcal{O}(T^2 d)
$$

N·∫øu tokenization k√©m ‚Üí (T) l·ªõn ‚Üí chi ph√≠ tƒÉng.

Do ƒë√≥, tokenizer t·ªëi ∆∞u gi√∫p:

* Gi·∫£m memory footprint
* TƒÉng t·ªëc inference
* C·∫£i thi·ªán ch·∫•t l∆∞·ª£ng ng·ªØ nghƒ©a

---

# 10. Li√™n h·ªá th·ª±c t·∫ø trong GPT

C√°c m√¥ h√¨nh GPT s·ª≠ d·ª•ng bi·∫øn th·ªÉ c·ªßa BPE ho·∫∑c byte-level BPE.

X√°c su·∫•t sinh token:

$$
P(w_t | w_{<t}) =
\frac{\exp(z_t W_{out})}
{\sum_j \exp(z_j W_{out})}
$$

Ch·∫•t l∆∞·ª£ng tokenization ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn ph√¢n ph·ªëi logits.

---

# 11. Th·∫£o lu·∫≠n

Chu·∫©n b·ªã vƒÉn b·∫£n kh√¥ng ch·ªâ l√† b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω k·ªπ thu·∫≠t m√† c√≤n l√†:

* B√†i to√°n t·ªëi ∆∞u th√¥ng tin
* B√†i to√°n m√£ h√≥a ngu·ªìn (source coding)
* B√†i to√°n n√©n d·ªØ li·ªáu

Theo ƒë·ªãnh l√Ω Shannon:

$$
H(X) = - \sum_x P(x) \log P(x)
$$

Tokenizer t·ªët gi√∫p:

$$
\text{Length}(X_{tokens}) \approx \frac{H(X)}{\log |V|}
$$

---

# 12. K·∫øt lu·∫≠n

Quy tr√¨nh chu·∫©n b·ªã vƒÉn b·∫£n cho tokenization bao g·ªìm:

1. Chu·∫©n h√≥a Unicode
2. L√†m s·∫°ch d·ªØ li·ªáu
3. √Åp d·ª•ng BPE ho·∫∑c Unigram LM
4. √Ånh x·∫° sang embedding

To√°n h·ªçc cho th·∫•y tokenization l√† qu√° tr√¨nh:

$$
\Sigma^* \rightarrow V^*
$$

gi√∫p t·ªëi ∆∞u:

* ƒê·ªô d√†i chu·ªói
* ƒê·ªô ph·ª©c t·∫°p t√≠nh to√°n
* Bi·ªÉu di·ªÖn ng·ªØ nghƒ©a

---

# T√†i li·ªáu tham kh·∫£o

1. Attention Is All You Need
2. Sennrich, R., Haddow, B., Birch, A. (2016). *Neural Machine Translation of Rare Words with Subword Units*.
3. Kudo, T. (2018). *Subword Regularization: Improving Neural Network Translation Models*.
4. Shannon, C. (1948). *A Mathematical Theory of Communication*.
5. Jurafsky, D., Martin, J. (2023). *Speech and Language Processing*.
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [T·∫°i sao vƒÉn b·∫£n c·∫ßn ƒë∆∞·ª£c ƒë√°nh s·ªë?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [Ph√¢n t√≠ch v√† chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh chu·ªói token ƒë∆∞·ª£c ƒë√°nh s·ªë: C∆° s·ªü to√°n h·ªçc v√† ·ª©ng d·ª•ng trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [T·∫°o v√† tr·ª±c quan h√≥a Token trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn: C∆° s·ªü to√°n h·ªçc v√† ph√¢n t√≠ch bi·ªÉu di·ªÖn](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [T·∫°o v√† tr·ª±c quan h√≥a Token (Ph·∫ßn 2): Ph√¢n t√≠ch h√¨nh h·ªçc kh√¥ng gian embedding v√† Attention Map trong m√¥ h√¨nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| üìå **[Chu·∫©n b·ªã vƒÉn b·∫£n cho Tokenization trong m√¥ h√¨nh Transformer: C∆° s·ªü l√Ω thuy·∫øt v√† ph√¢n t√≠ch to√°n h·ªçc](aero_llm_05_preparing_text_for_tokenization.md)** | [Xem b√†i vi·∫øt ‚Üí](aero_llm_05_preparing_text_for_tokenization.md) |
| [Ph√¢n t√≠ch quy tr√¨nh Tokenization qua v√≠ d·ª• *The Time Machine*: C∆° s·ªü thu·∫≠t to√°n v√† m√¥ h√¨nh h√≥a to√°n h·ªçc](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So s√°nh Tokenization m·ª©c k√Ω t·ª±, t·ª´ v√† subword: Ph√¢n t√≠ch l√Ω thuy·∫øt v√† m√¥ h√¨nh to√°n h·ªçc](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thu·∫≠t to√°n Byte Pair Encoding (BPE) v√† B√†i to√°n T·ªëi ∆∞u K√≠ch th∆∞·ªõc T·ª´ v·ª±ng trong M√¥ h√¨nh Ng√¥n ng·ªØ](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

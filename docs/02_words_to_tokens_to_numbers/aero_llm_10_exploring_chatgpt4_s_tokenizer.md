
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [02 words to tokens to numbers](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
KhÃ¡m phÃ¡ CÆ¡ cháº¿ Tokenizer cá»§a ChatGPT-4: PhÃ¢n tÃ­ch Ká»¹ thuáº­t vÃ  MÃ´ hÃ¬nh ToÃ¡n há»c

TÃ³m táº¯t

Tokenizer Ä‘Ã³ng vai trÃ² ná»n táº£ng trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (Large Language Models â€“ LLMs), Ä‘áº·c biá»‡t lÃ  cÃ¡c há»‡ thá»‘ng do OpenAI phÃ¡t triá»ƒn nhÆ° GPT-4. BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch cÆ¡ cháº¿ hoáº¡t Ä‘á»™ng cá»§a tokenizer trong GPT-4, táº­p trung vÃ o thuáº­t toÃ¡n Byte Pair Encoding (BPE), biá»ƒu diá»…n xÃ¡c suáº¥t, cáº¥u trÃºc tá»« vá»±ng, cÅ©ng nhÆ° cÃ¡c mÃ´ hÃ¬nh toÃ¡n há»c minh hoáº¡. NgoÃ i ra, bÃ i viáº¿t má»Ÿ rá»™ng so sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p token hÃ³a hiá»‡n Ä‘áº¡i vÃ  tháº£o luáº­n vá» áº£nh hÆ°á»Ÿng cá»§a tokenizer Ä‘áº¿n hiá»‡u nÄƒng mÃ´ hÃ¬nh.

â¸»

1. Giá»›i thiá»‡u

Trong cÃ¡c mÃ´ hÃ¬nh Transformer, vÄƒn báº£n khÃ´ng Ä‘Æ°á»£c xá»­ lÃ½ trá»±c tiáº¿p á»Ÿ má»©c kÃ½ tá»± hoáº·c tá»« hoÃ n chá»‰nh, mÃ  Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh token â€” cÃ¡c Ä‘Æ¡n vá»‹ rá»i ráº¡c Ä‘áº¡i diá»‡n cho chuá»—i kÃ½ tá»±. QuÃ¡ trÃ¬nh nÃ y gá»i lÃ  tokenization.

Cho chuá»—i Ä‘áº§u vÃ o:

X = (x_1, x_2, ..., x_n)

Tokenizer thá»±c hiá»‡n Ã¡nh xáº¡:

f: X \rightarrow T = (t_1, t_2, ..., t_m)

Trong Ä‘Ã³:
	â€¢	x_i: kÃ½ tá»± hoáº·c byte
	â€¢	t_j: token trong tá»« vá»±ng V
	â€¢	m \leq n

â¸»

2. Byte Pair Encoding (BPE)

2.1 NguyÃªn lÃ½ cÆ¡ báº£n

BPE lÃ  thuáº­t toÃ¡n nÃ©n dá»¯ liá»‡u Ä‘Æ°á»£c Ä‘iá»u chá»‰nh Ä‘á»ƒ xÃ¢y dá»±ng tá»« vá»±ng token. Ã tÆ°á»Ÿng chÃ­nh:
	1.	Báº¯t Ä‘áº§u vá»›i táº­p kÃ½ tá»± cÆ¡ sá»Ÿ (byte-level).
	2.	TÃ¬m cáº·p kÃ½ tá»± xuáº¥t hiá»‡n nhiá»u nháº¥t.
	3.	Gá»™p cáº·p Ä‘Ã³ thÃ nh má»™t token má»›i.
	4.	Láº·p láº¡i cho Ä‘áº¿n khi Ä‘áº¡t kÃ­ch thÆ°á»›c tá»« vá»±ng mong muá»‘n.

â¸»

2.2 MÃ´ hÃ¬nh toÃ¡n há»c cá»§a BPE

Giáº£ sá»­ ta cÃ³ táº­p dá»¯ liá»‡u huáº¥n luyá»‡n D gá»“m cÃ¡c chuá»—i kÃ½ tá»±.

Táº§n suáº¥t xuáº¥t hiá»‡n cá»§a cáº·p kÃ½ tá»± (a,b):

\text{freq}(a,b) = \sum_{w \in D} \text{count}_{w}(a,b)

Cáº·p Ä‘Æ°á»£c chá»n Ä‘á»ƒ gá»™p:

(a^*, b^*) = \arg\max_{(a,b)} \text{freq}(a,b)

Sau má»—i bÆ°á»›c gá»™p, tá»« vá»±ng Ä‘Æ°á»£c cáº­p nháº­t:

V_{k+1} = V_k \cup \{ a^*b^* \}

â¸»

2.3 VÃ­ dá»¥ minh há»a

Chuá»—i:

low
lower
lowest

Ban Ä‘áº§u token theo kÃ½ tá»±:

l o w
l o w e r
l o w e s t

Náº¿u cáº·p lo xuáº¥t hiá»‡n nhiá»u nháº¥t â†’ táº¡o token má»›i:

lo w
lo w e r
lo w e s t

Tiáº¿p tá»¥c quÃ¡ trÃ¬nh Ä‘áº¿n khi Ä‘áº¡t kÃ­ch thÆ°á»›c tá»« vá»±ng yÃªu cáº§u.

â¸»

3. Biá»ƒu diá»…n Vector cá»§a Token

Sau khi token hÃ³a, má»—i token t_i \in V Ä‘Æ°á»£c Ã¡nh xáº¡ sang embedding vector:

E: V \rightarrow \mathbb{R}^d

Vá»›i:
	â€¢	d: chiá»u khÃ´ng gian embedding (vÃ­ dá»¥ 768, 1024, 4096â€¦)

Chuá»—i token:

T = (t_1, t_2, ..., t_m)

Ä‘Æ°á»£c chuyá»ƒn thÃ nh ma tráº­n embedding:

\mathbf{X} =
\begin{bmatrix}
E(t_1) \\
E(t_2) \\
\vdots \\
E(t_m)
\end{bmatrix}
\in \mathbb{R}^{m \times d}

â¸»

4. Tokenization á»Ÿ má»©c Byte

GPT-4 sá»­ dá»¥ng byte-level BPE, nghÄ©a lÃ  má»i chuá»—i Unicode Ä‘á»u Ä‘Æ°á»£c biá»ƒu diá»…n qua:

\text{Unicode} \rightarrow \text{UTF-8 bytes}

Äiá»u nÃ y Ä‘áº£m báº£o:

\forall s \in \text{Unicode}, \exists \text{token sequence}

KhÃ´ng xáº£y ra trÆ°á»ng há»£p â€œout-of-vocabularyâ€.

â¸»

5. XÃ¡c suáº¥t vÃ  NgÃ´n ngá»¯ há»c Thá»‘ng kÃª

Sau tokenization, mÃ´ hÃ¬nh há»c phÃ¢n phá»‘i xÃ¡c suáº¥t:

P(t_i | t_1, ..., t_{i-1})

ToÃ n bá»™ xÃ¡c suáº¥t chuá»—i:

P(T) = \prod_{i=1}^{m} P(t_i | t_{<i})

Loss function huáº¥n luyá»‡n:

\mathcal{L} = - \sum_{i=1}^{m} \log P(t_i | t_{<i})

Tokenizer áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n:
	â€¢	Äá»™ dÃ i chuá»—i m
	â€¢	PhÃ¢n phá»‘i xÃ¡c suáº¥t
	â€¢	Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n O(m^2) trong self-attention

â¸»

6. áº¢nh hÆ°á»Ÿng cá»§a Tokenizer Ä‘áº¿n Hiá»‡u NÄƒng

6.1 Äá»™ dÃ i chuá»—i

Náº¿u tokenizer táº¡o quÃ¡ nhiá»u token cho má»™t tá»« hiáº¿m:

\text{computational cost} \propto m^2

Chi phÃ­ attention tÄƒng nhanh khi m lá»›n.

â¸»

6.2 Äá»™ nÃ©n ngÃ´n ngá»¯

Entropy cá»§a há»‡ token:

H(T) = - \sum_{t \in V} P(t)\log P(t)

Tokenizer tá»‘t sáº½:
	â€¢	Giáº£m entropy
	â€¢	TÄƒng tÃ­nh nÃ©n
	â€¢	Giá»¯ cáº¥u trÃºc ngá»¯ nghÄ©a

â¸»

7. So sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c

PhÆ°Æ¡ng phÃ¡p	NguyÃªn lÃ½	Æ¯u Ä‘iá»ƒm	NhÆ°á»£c Ä‘iá»ƒm
Word-level	Theo tá»« hoÃ n chá»‰nh	Dá»… hiá»ƒu	OOV cao
Character-level	Theo kÃ½ tá»±	KhÃ´ng OOV	Chuá»—i dÃ i
BPE	Gá»™p cáº·p phá»• biáº¿n	CÃ¢n báº±ng	Phá»¥ thuá»™c corpus
Unigram LM	MÃ´ hÃ¬nh xÃ¡c suáº¥t	Linh hoáº¡t	TÃ­nh toÃ¡n phá»©c táº¡p


â¸»

8. Háº¡n cháº¿ vÃ  ThÃ¡ch thá»©c
	1.	Phá»¥ thuá»™c ngÃ´n ngá»¯
NgÃ´n ngá»¯ khÃ´ng dáº¥u vÃ  cÃ³ dáº¥u (vÃ­ dá»¥ tiáº¿ng Viá»‡t) cÃ³ thá»ƒ bá»‹ phÃ¢n máº£nh token.
	2.	Bias thá»‘ng kÃª
Token phá»• biáº¿n chiáº¿m Æ°u tháº¿ trong huáº¥n luyá»‡n.
	3.	KhÃ´ng pháº£n Ã¡nh cáº¥u trÃºc ngá»¯ phÃ¡p thá»±c sá»±

â¸»

9. HÆ°á»›ng phÃ¡t triá»ƒn tÆ°Æ¡ng lai
	â€¢	Adaptive tokenization
	â€¢	Dynamic vocabulary
	â€¢	Morphology-aware tokenization
	â€¢	Neural tokenizers há»c trá»±c tiáº¿p tá»« dá»¯ liá»‡u

â¸»

10. Káº¿t luáº­n

Tokenizer khÃ´ng chá»‰ lÃ  bÆ°á»›c tiá»n xá»­ lÃ½, mÃ  lÃ  thÃ nh pháº§n quyáº¿t Ä‘á»‹nh cáº¥u trÃºc xÃ¡c suáº¥t vÃ  hiá»‡u nÄƒng cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n. BPE cung cáº¥p sá»± cÃ¢n báº±ng giá»¯a tÃ­nh nÃ©n vÃ  kháº£ nÄƒng biá»ƒu diá»…n, trong khi byte-level encoding Ä‘áº£m báº£o tÃ­nh toÃ n diá»‡n vá»›i Unicode.

Vá» máº·t toÃ¡n há»c, tokenizer áº£nh hÆ°á»Ÿng Ä‘áº¿n:

m, \quad H(T), \quad \mathcal{L}, \quad O(m^2)

Do Ä‘Ã³, viá»‡c tá»‘i Æ°u tokenizer cÃ³ thá»ƒ cáº£i thiá»‡n cáº£ hiá»‡u suáº¥t láº«n cháº¥t lÆ°á»£ng sinh ngÃ´n ngá»¯ cá»§a mÃ´ hÃ¬nh.

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Sennrich, R., Haddow, B., & Birch, A. (2016). Neural Machine Translation of Rare Words with Subword Units.
	2.	Vaswani, A. et al. (2017). Attention Is All You Need.
	3.	Kudo, T. (2018). Subword Regularization.
	4.	Brown, T. et al. (2020). Language Models are Few-Shot Learners.
	5.	Jurafsky, D. & Martin, J. (Speech and Language Processing).
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_llm_05_preparing_text_for_tokenization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_preparing_text_for_tokenization.md) |
| [PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| ğŸ“Œ **[aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

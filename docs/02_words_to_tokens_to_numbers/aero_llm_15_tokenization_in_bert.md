
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Tokenization trong BERT: PhÃ¢n tÃ­ch CÆ¡ cháº¿ WordPiece vÃ  MÃ´ hÃ¬nh ToÃ¡n há»c

â¸»

TÃ³m táº¯t

BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch cÆ¡ cháº¿ tokenization trong BERT dá»±a trÃªn tÃ i liá»‡u Ä‘Ã­nh kÃ¨m, táº­p trung vÃ o thuáº­t toÃ¡n WordPiece, ná»n táº£ng cá»§a quÃ¡ trÃ¬nh phÃ¢n tÃ¡ch subword trong mÃ´ hÃ¬nh Google phÃ¡t triá»ƒn â€“ BERT. ChÃºng tÃ´i trÃ¬nh bÃ y cÆ¡ sá»Ÿ toÃ¡n há»c cá»§a WordPiece, so sÃ¡nh vá»›i Byte Pair Encoding (BPE), Ä‘á»“ng thá»i phÃ¢n tÃ­ch áº£nh hÆ°á»Ÿng cá»§a tokenization Ä‘áº¿n embedding, attention vÃ  hÃ m máº¥t mÃ¡t trong huáº¥n luyá»‡n.

â¸»

1. Giá»›i thiá»‡u

BERT (Bidirectional Encoder Representations from Transformers) lÃ  mÃ´ hÃ¬nh Transformer encoder hai chiá»u Ä‘Æ°á»£c giá»›i thiá»‡u nÄƒm 2018.

Chuá»—i Ä‘áº§u vÃ o:

S = (w_1, w_2, ..., w_n)

ÄÆ°á»£c Ã¡nh xáº¡ thÃ nh chuá»—i token:

T = (t_1, t_2, ..., t_m)

Vá»›i:

m \ge n

Do má»™t tá»« cÃ³ thá»ƒ bá»‹ tÃ¡ch thÃ nh nhiá»u subword.

â¸»

2. Thuáº­t toÃ¡n WordPiece

2.1 NguyÃªn lÃ½ cÆ¡ báº£n

WordPiece báº¯t Ä‘áº§u tá»« táº­p kÃ½ tá»± cÆ¡ sá»Ÿ vÃ  láº·p láº¡i quÃ¡ trÃ¬nh:
	â€¢	Chá»n cáº·p subword cÃ³ xÃ¡c suáº¥t cao nháº¥t
	â€¢	Gá»™p láº¡i thÃ nh má»™t token má»›i

KhÃ¡c vá»›i BPE (chá»n theo táº§n suáº¥t), WordPiece tá»‘i Æ°u theo xÃ¡c suáº¥t tá»‘i Ä‘a hÃ³a likelihood.

â¸»

2.2 HÃ m Má»¥c tiÃªu

Giáº£ sá»­ táº­p dá»¯ liá»‡u huáº¥n luyá»‡n D.

WordPiece tá»‘i Ä‘a hÃ³a:

$$
\mathcal{L} = \sum_{w \in D} \log P(w)
$$

Trong Ä‘Ã³ má»™t tá»« w Ä‘Æ°á»£c phÃ¢n rÃ£ thÃ nh:

w = (t_1, t_2, ..., t_k)

XÃ¡c suáº¥t:

$P(w)$ = $\prod$_{i=1}^{k} $P($t_i$)$

Thuáº­t toÃ¡n chá»n phÃ©p gá»™p lÃ m tÄƒng likelihood nhiá»u nháº¥t.

â¸»

2.3 Quy táº¯c Tiá»n tá»‘ â€œ##â€

VÃ­ dá»¥:

playing â†’ play + ##ing

KÃ½ hiá»‡u â€œ##â€ cho biáº¿t token khÃ´ng á»Ÿ Ä‘áº§u tá»«.

Äiá»u nÃ y giÃºp mÃ´ hÃ¬nh phÃ¢n biá»‡t:
	â€¢	â€œplayâ€ (tá»« Ä‘á»™c láº­p)
	â€¢	â€œ##playâ€ (khÃ´ng há»£p lá»‡)

â¸»

3. MÃ´ hÃ¬nh ToÃ¡n há»c cá»§a Tokenization

3.1 PhÃ¢n bá»‘ Subword

Gá»i:
	â€¢	V: táº­p tá»« vá»±ng
	â€¢	|V|: kÃ­ch thÆ°á»›c (â‰ˆ 30k vá»›i BERT-base)

PhÃ¢n bá»‘:

$P(t)$ = \frac{\text{count}$t$}{$\sum$_{t' \in V} \text{count}(t')}

Entropy:

H = - \sum_{t \in V} P(t)\log P(t)

â¸»

3.2 Äá»™ dÃ i Trung bÃ¬nh Chuá»—i Token

Náº¿u vÄƒn báº£n cÃ³ n tá»« vÃ  trung bÃ¬nh má»—i tá»« tÃ¡ch thÃ nh \alpha subword:

$$
m = \alpha n
$$

Self-attention trong Transformer encoder:

$O(m^2)$

Do Ä‘Ã³:

$O((\alpha n)$^2)

Tokenization áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n chi phÃ­ tÃ­nh toÃ¡n.

â¸»

4. So sÃ¡nh WordPiece vÃ  BPE

Äáº·c Ä‘iá»ƒm	WordPiece	BPE
TiÃªu chÃ­ gá»™p	Tá»‘i Ä‘a hÃ³a likelihood	Táº§n suáº¥t
MÃ´ hÃ¬nh xÃ¡c suáº¥t	CÃ³	KhÃ´ng trá»±c tiáº¿p
á»¨ng dá»¥ng	BERT	GPT
Tá»‘i Æ°u	Theo corpus	Theo táº§n suáº¥t thuáº§n

â¸»

5. Biá»ƒu diá»…n Embedding

Má»—i token Ä‘Æ°á»£c Ã¡nh xáº¡:

$$
E: V \rightarrow \mathbb{R}^d
$$

Chuá»—i token táº¡o thÃ nh ma tráº­n:

$$
X \in \mathbb{R}^{m \times d}
$$

BERT cá»™ng thÃªm:
	â€¢	Positional embedding
	â€¢	Segment embedding

Tá»•ng embedding:

$$
E_{\text{total}} = E_{\text{token}} + E_{\text{position}} + E_{\text{segment}}
$$

â¸»

6. Masked Language Modeling (MLM)

BERT huáº¥n luyá»‡n báº±ng cÃ¡ch che má»™t sá»‘ token:

$P($t_i$ \mid  T_{\setminus i})$

Loss:

$$
\mathcal{L}_{MLM} = - \sum_{i \in M} \log P(t_i \mid  T_{\setminus i})
$$

Trong Ä‘Ã³ M lÃ  táº­p token bá»‹ mask.

Tokenization áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n:
	â€¢	Sá»‘ token bá»‹ mask
	â€¢	Äá»™ khÃ³ cá»§a nhiá»‡m vá»¥ dá»± Ä‘oÃ¡n

â¸»

7. PhÃ¢n tÃ­ch LÃ½ thuyáº¿t ThÃ´ng tin

Tokenization tá»‘i Æ°u hÃ³a sá»± cÃ¢n báº±ng giá»¯a:
	â€¢	Vocabulary size |V|
	â€¢	Äá»™ dÃ i chuá»—i m

BÃ i toÃ¡n tá»‘i Æ°u:

\min_{V} \left( \mathbb{E}[m] + \lambda |V| \right)

Vá»›i:
	â€¢	\lambda: há»‡ sá»‘ Ä‘iá»u chá»‰nh

$$
â€¢	\mathbb{E}[m]: sá»‘ token trung bÃ¬nh
$$

â¸»

8. TÃ­nh KhÃ¡i quÃ¡t hÃ³a (Generalization)

WordPiece cho phÃ©p xá»­ lÃ½ tá»« hiáº¿m:

VÃ­ dá»¥:

unbelievable â†’ un + ##believ + ##able

Do Ä‘Ã³:

\forall w \notin V_{word}, \exists \text{decomposition in } V_{subword}

Giáº£m váº¥n Ä‘á» OOV (Out-of-Vocabulary).

â¸»

9. Háº¡n cháº¿
	1.	Phá»¥ thuá»™c corpus huáº¥n luyá»‡n
	2.	CÃ³ thá»ƒ tÃ¡ch khÃ´ng tá»± nhiÃªn vá» máº·t ngÃ´n ngá»¯
	3.	TÄƒng Ä‘á»™ dÃ i chuá»—i trong ngÃ´n ngá»¯ cÃ³ cáº¥u trÃºc phá»©c táº¡p

â¸»

10. Káº¿t luáº­n

Tokenization trong BERT dá»±a trÃªn WordPiece cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ hÃ¬nh hÃ³a:

\max \sum_{w \in D} \log \prod_{i=1}^{k} P(t_i)

áº¢nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n:

$$
m = \alpha n \text{Attention Cost} = O(m^2) H = - \sum P(t)\log P(t)
$$


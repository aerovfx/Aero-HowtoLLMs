
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Tokenization trong BERT: PhÃ¢n tÃ­ch CÆ¡ cháº¿ WordPiece vÃ  MÃ´ hÃ¬nh ToÃ¡n há»c

â¸»

TÃ³m táº¯t

BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch cÆ¡ cháº¿ tokenization trong BERT dá»±a trÃªn tÃ i liá»‡u Ä‘Ã­nh kÃ¨m, táº­p trung vÃ o thuáº­t toÃ¡n WordPiece, ná»n táº£ng cá»§a quÃ¡ trÃ¬nh phÃ¢n tÃ¡ch subword trong mÃ´ hÃ¬nh Google phÃ¡t triá»ƒn â€“ BERT. ChÃºng tÃ´i trÃ¬nh bÃ y cÆ¡ sá»Ÿ toÃ¡n há»c cá»§a WordPiece, so sÃ¡nh vá»›i Byte Pair Encoding (BPE), Ä‘á»“ng thá»i phÃ¢n tÃ­ch áº£nh hÆ°á»Ÿng cá»§a tokenization Ä‘áº¿n embedding, attention vÃ  hÃ m máº¥t mÃ¡t trong huáº¥n luyá»‡n.

â¸»

1. Giá»›i thiá»‡u

BERT (Bidirectional Encoder Representations from Transformers) lÃ  mÃ´ hÃ¬nh Transformer encoder hai chiá»u Ä‘Æ°á»£c giá»›i thiá»‡u nÄƒm 2018.

Chuá»—i Ä‘áº§u vÃ o:

S = (w_1, w_2, ..., w_n)

ÄÆ°á»£c Ã¡nh xáº¡ thÃ nh chuá»—i token:

T = (t_1, t_2, ..., t_m)

Vá»›i:

m \ge n

Do má»™t tá»« cÃ³ thá»ƒ bá»‹ tÃ¡ch thÃ nh nhiá»u subword.

â¸»

2. Thuáº­t toÃ¡n WordPiece

2.1 NguyÃªn lÃ½ cÆ¡ báº£n

WordPiece báº¯t Ä‘áº§u tá»« táº­p kÃ½ tá»± cÆ¡ sá»Ÿ vÃ  láº·p láº¡i quÃ¡ trÃ¬nh:
	â€¢	Chá»n cáº·p subword cÃ³ xÃ¡c suáº¥t cao nháº¥t
	â€¢	Gá»™p láº¡i thÃ nh má»™t token má»›i

KhÃ¡c vá»›i BPE (chá»n theo táº§n suáº¥t), WordPiece tá»‘i Æ°u theo xÃ¡c suáº¥t tá»‘i Ä‘a hÃ³a likelihood.

â¸»

2.2 HÃ m Má»¥c tiÃªu

Giáº£ sá»­ táº­p dá»¯ liá»‡u huáº¥n luyá»‡n D.

WordPiece tá»‘i Ä‘a hÃ³a:

\mathcal{L} = \sum_{w \in D} \log P$w$

Trong Ä‘Ã³ má»™t tá»« w Ä‘Æ°á»£c phÃ¢n rÃ£ thÃ nh:

w = (t_1, t_2, ..., t_k)

XÃ¡c suáº¥t:

P$w$ = \prod_{i=1}^{k} P$t_i$

Thuáº­t toÃ¡n chá»n phÃ©p gá»™p lÃ m tÄƒng likelihood nhiá»u nháº¥t.

â¸»

2.3 Quy táº¯c Tiá»n tá»‘ â€œ##â€

VÃ­ dá»¥:

playing â†’ play + ##ing

KÃ½ hiá»‡u â€œ##â€ cho biáº¿t token khÃ´ng á»Ÿ Ä‘áº§u tá»«.

Äiá»u nÃ y giÃºp mÃ´ hÃ¬nh phÃ¢n biá»‡t:
	â€¢	â€œplayâ€ (tá»« Ä‘á»™c láº­p)
	â€¢	â€œ##playâ€ (khÃ´ng há»£p lá»‡)

â¸»

3. MÃ´ hÃ¬nh ToÃ¡n há»c cá»§a Tokenization

3.1 PhÃ¢n bá»‘ Subword

Gá»i:
	â€¢	V: táº­p tá»« vá»±ng
	â€¢	|V|: kÃ­ch thÆ°á»›c (â‰ˆ 30k vá»›i BERT-base)

PhÃ¢n bá»‘:

P$t$ = \frac{\text{count}$t$}{\sum_{t' \in V} \text{count}(t')}

Entropy:

H = - \sum_{t \in V} P$t$\log P$t$

â¸»

3.2 Äá»™ dÃ i Trung bÃ¬nh Chuá»—i Token

Náº¿u vÄƒn báº£n cÃ³ n tá»« vÃ  trung bÃ¬nh má»—i tá»« tÃ¡ch thÃ nh \alpha subword:

m = \alpha n

Self-attention trong Transformer encoder:

O$m^2$

Do Ä‘Ã³:

O$(\alpha n$^2)

Tokenization áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n chi phÃ­ tÃ­nh toÃ¡n.

â¸»

4. So sÃ¡nh WordPiece vÃ  BPE

Äáº·c Ä‘iá»ƒm	WordPiece	BPE
TiÃªu chÃ­ gá»™p	Tá»‘i Ä‘a hÃ³a likelihood	Táº§n suáº¥t
MÃ´ hÃ¬nh xÃ¡c suáº¥t	CÃ³	KhÃ´ng trá»±c tiáº¿p
á»¨ng dá»¥ng	BERT	GPT
Tá»‘i Æ°u	Theo corpus	Theo táº§n suáº¥t thuáº§n

â¸»

5. Biá»ƒu diá»…n Embedding

Má»—i token Ä‘Æ°á»£c Ã¡nh xáº¡:

E: V \rightarrow \mathbb{R}^d

Chuá»—i token táº¡o thÃ nh ma tráº­n:

X \in \mathbb{R}^{m \times d}

BERT cá»™ng thÃªm:
	â€¢	Positional embedding
	â€¢	Segment embedding

Tá»•ng embedding:

E_{\text{total}} = E_{\text{token}} + E_{\text{position}} + E_{\text{segment}}

â¸»

6. Masked Language Modeling (MLM)

BERT huáº¥n luyá»‡n báº±ng cÃ¡ch che má»™t sá»‘ token:

P$t_i \mid  T_{\setminus i}$

Loss:

\mathcal{L}_{MLM} = - \sum_{i \in M} \log P$t_i \mid  T_{\setminus i}$

Trong Ä‘Ã³ M lÃ  táº­p token bá»‹ mask.

Tokenization áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n:
	â€¢	Sá»‘ token bá»‹ mask
	â€¢	Äá»™ khÃ³ cá»§a nhiá»‡m vá»¥ dá»± Ä‘oÃ¡n

â¸»

7. PhÃ¢n tÃ­ch LÃ½ thuyáº¿t ThÃ´ng tin

Tokenization tá»‘i Æ°u hÃ³a sá»± cÃ¢n báº±ng giá»¯a:
	â€¢	Vocabulary size |V|
	â€¢	Äá»™ dÃ i chuá»—i m

BÃ i toÃ¡n tá»‘i Æ°u:

\min_{V} \left( \mathbb{E}[m] + \lambda |V| \right)

Vá»›i:
	â€¢	\lambda: há»‡ sá»‘ Ä‘iá»u chá»‰nh
	â€¢	\mathbb{E}[m]: sá»‘ token trung bÃ¬nh

â¸»

8. TÃ­nh KhÃ¡i quÃ¡t hÃ³a (Generalization)

WordPiece cho phÃ©p xá»­ lÃ½ tá»« hiáº¿m:

VÃ­ dá»¥:

unbelievable â†’ un + ##believ + ##able

Do Ä‘Ã³:

\forall w \notin V_{word}, \exists \text{decomposition in } V_{subword}

Giáº£m váº¥n Ä‘á» OOV (Out-of-Vocabulary).

â¸»

9. Háº¡n cháº¿
	1.	Phá»¥ thuá»™c corpus huáº¥n luyá»‡n
	2.	CÃ³ thá»ƒ tÃ¡ch khÃ´ng tá»± nhiÃªn vá» máº·t ngÃ´n ngá»¯
	3.	TÄƒng Ä‘á»™ dÃ i chuá»—i trong ngÃ´n ngá»¯ cÃ³ cáº¥u trÃºc phá»©c táº¡p

â¸»

10. Káº¿t luáº­n

Tokenization trong BERT dá»±a trÃªn WordPiece cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ hÃ¬nh hÃ³a:

\max \sum_{w \in D} \log \prod_{i=1}^{k} P$t_i$

áº¢nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n:

m = \alpha n

\text{Attention Cost} = O$m^2$

H = - \sum P$t$\log P$t$

Do Ä‘Ã³, thiáº¿t káº¿ tokenizer lÃ  bÃ i toÃ¡n tá»‘i Æ°u Ä‘a má»¥c tiÃªu giá»¯a:
	â€¢	Kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a
	â€¢	Hiá»‡u suáº¥t tÃ­nh toÃ¡n
	â€¢	Äá»™ nÃ©n thÃ´ng tin

â¸»

TÃ i liá»‡u tham kháº£o
	1.	BERT: Devlin, J. et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
	2.	Sennrich et al. (2016). Neural Machine Translation of Rare Words with Subword Units.
	3.	Vaswani et al. (2017). Attention Is All You Need.
	4.	Shannon, C. (1948). A Mathematical Theory of Communication.
	5.	Jurafsky & Martin. Speech and Language Processing.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_llm_05_preparing_text_for_tokenization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_preparing_text_for_tokenization.md) |
| [PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| ğŸ“Œ **[aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

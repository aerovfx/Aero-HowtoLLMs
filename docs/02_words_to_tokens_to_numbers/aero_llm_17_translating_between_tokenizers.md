
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Chuyá»ƒn Ä‘á»•i giá»¯a cÃ¡c Tokenizer trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n:

PhÃ¢n tÃ­ch LÃ½ thuyáº¿t, Biá»ƒu diá»…n HÃ¬nh thá»©c vÃ  Há»‡ quáº£ TÃ­nh toÃ¡n

â¸»

TÃ³m táº¯t

Tokenization lÃ  bÆ°á»›c tiá»n xá»­ lÃ½ cá»‘t lÃµi trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs). Tuy nhiÃªn, sá»± khÃ¡c biá»‡t giá»¯a cÃ¡c thuáº­t toÃ¡n token hÃ³a nhÆ° WordPiece, BPE vÃ  Unigram LM táº¡o ra nhá»¯ng thÃ¡ch thá»©c khi cáº§n chuyá»ƒn Ä‘á»•i giá»¯a cÃ¡c tokenizer khÃ¡c nhau. BÃ i viáº¿t nÃ y, dá»±a trÃªn tÃ i liá»‡u Ä‘Ã­nh kÃ¨m vá» Translating between tokenizers, trÃ¬nh bÃ y má»™t khung lÃ½ thuyáº¿t hÃ¬nh thá»©c cho bÃ i toÃ¡n Ã¡nh xáº¡ giá»¯a hai há»‡ tokenizer, phÃ¢n tÃ­ch Ä‘iá»u kiá»‡n tá»“n táº¡i Ã¡nh xáº¡ song Ã¡nh, Æ°á»›c lÆ°á»£ng sai sá»‘ thÃ´ng tin, vÃ  Ä‘Ã¡nh giÃ¡ tÃ¡c Ä‘á»™ng Ä‘áº¿n Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n trong Transformer. CÃ¡c vÃ­ dá»¥ Ä‘Æ°á»£c minh há»a vá»›i tokenizer cá»§a BERT, GPT-2, vÃ  thÆ° viá»‡n SentencePiece.

â¸»

1. Giá»›i thiá»‡u

Má»—i mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘á»‹nh nghÄ©a má»™t hÃ m token hÃ³a:

\mathcal{T}: \Sigma^{\ast} \rightarrow V^{\ast}

Trong Ä‘Ã³:
	â€¢	\Sigma^{\ast}: táº­p táº¥t cáº£ chuá»—i kÃ½ tá»±
	â€¢	V: tá»« vá»±ng token
	â€¢	V^{\ast}: chuá»—i token

Hai tokenizer khÃ¡c nhau \mathcal{T}_A vÃ  \mathcal{T}_B sáº½ táº¡o ra hai biá»ƒu diá»…n khÃ¡c nhau cho cÃ¹ng má»™t chuá»—i Ä‘áº§u vÃ o x:

\mathcal{T}_A(x) \neq \mathcal{T}_B(x)

Váº¥n Ä‘á» Ä‘áº·t ra:
LÃ m tháº¿ nÃ o Ä‘á»ƒ Ã¡nh xáº¡ chuá»—i token tá»« khÃ´ng gian V_A^{\ast} sang V_B^{\ast} mÃ  khÃ´ng máº¥t thÃ´ng tin?

â¸»

2. CÃ¡c Há»‡ Tokenizer Phá»• biáº¿n

2.1 WordPiece

ÄÆ°á»£c sá»­ dá»¥ng trong BERT do Google phÃ¡t triá»ƒn.

Thuáº­t toÃ¡n tá»‘i Ä‘a hÃ³a xÃ¡c suáº¥t:

\arg\max_{s_1,\dots,s_k} \prod_{i=1}^{k} P(s_i)

â¸»

2.2 Byte Pair Encoding (BPE)

ÄÆ°á»£c sá»­ dá»¥ng trong GPT-2 bá»Ÿi OpenAI.

QuÃ¡ trÃ¬nh láº·p:

(\alpha, \beta) = \arg\max_{(u,v)} \text{freq}(uv)

Sau Ä‘Ã³ thay tháº¿ cáº·p phá»• biáº¿n nháº¥t.

â¸»

2.3 Unigram Language Model

Ãp dá»¥ng trong SentencePiece.

Tá»‘i Æ°u hÃ³a:

\max_{V} \sum_{x \in D} \log \sum_{s \in \mathcal{S}(x)} \prod_{i} P(s_i)

â¸»

3. MÃ´ hÃ¬nh ToÃ¡n há»c cá»§a BÃ i toÃ¡n Chuyá»ƒn Ä‘á»•i

Giáº£ sá»­:

\mathcal{T}_A: \Sigma^{\ast} \rightarrow V_A^{\ast}

\mathcal{T}_B: \Sigma^{\ast} \rightarrow V_B^{\ast}

Ta cáº§n xÃ¢y dá»±ng:

\Phi: V_A^{\ast} \rightarrow V_B^{\ast}

3.1 Äiá»u kiá»‡n tá»“n táº¡i Ã¡nh xáº¡ chÃ­nh xÃ¡c

Náº¿u tá»“n táº¡i hÃ m giáº£i mÃ£:

\mathcal{D}_A: V_A^{\ast} \rightarrow \Sigma^{\ast}

thÃ¬:

\Phi = \mathcal{T}_B \circ \mathcal{D}_A

Khi Ä‘Ã³:

\Phi(\mathcal{T}_A(x)) = \mathcal{T}_B(x)

â¸»

4. PhÃ¢n tÃ­ch Sai sá»‘ ThÃ´ng tin

Náº¿u tokenizer khÃ´ng kháº£ nghá»‹ch hoÃ n toÃ n, ta cÃ³ sai sá»‘:

\epsilon = d(\mathcal{D}_A(\mathcal{T}_A(x)), x)

Trong Ä‘Ã³ d lÃ  khoáº£ng cÃ¡ch Levenshtein.

Entropy trÆ°á»›c vÃ  sau:

H_A = - \sum p(t_i)\log p(t_i)

H_B = - \sum p(u_j)\log p(u_j)

Äá»™ chÃªnh entropy:

\Delta H = |H_A - H_B|

Náº¿u \Delta H lá»›n â†’ thay Ä‘á»•i cáº¥u trÃºc phÃ¢n bá»‘ token Ä‘Ã¡ng ká»ƒ.

â¸»

5. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Äá»™ dÃ i Chuá»—i vÃ  Self-Attention

Giáº£ sá»­ vÄƒn báº£n cÃ³ n kÃ½ tá»±.

Sá»‘ token:

m_A = \frac{n}{\mathbb{E}[L_A]}

m_B = \frac{n}{\mathbb{E}[L_B]}

Self-attention cÃ³ Ä‘á»™ phá»©c táº¡p:

O(m^2)

Tá»· lá»‡ chi phÃ­:

\frac{C_A}{C_B} = \left(\frac{m_A}{m_B}\right)^2

Náº¿u tokenizer B táº¡o token dÃ i hÆ¡n:

\mathbb{E}[L_B] > \mathbb{E}[L_A]
\Rightarrow C_B < C_A

â¸»

6. BÃ i toÃ¡n CÄƒn chá»‰nh Token (Token Alignment)

Giáº£ sá»­:

\mathcal{T}_A(x) = (a_1, a_2, \dots, a_m)

\mathcal{T}_B(x) = (b_1, b_2, \dots, b_k)

Ta cáº§n tÃ¬m Ã¡nh xáº¡ cÄƒn chá»‰nh:

\pi: \{1,\dots,m\} \rightarrow \{1,\dots,k\}

Tá»‘i Æ°u hÃ³a:

\min_{\pi} \sum_{i=1}^{m} d(\text{span}(a_i), \text{span}(b_{\pi(i)}))

ÄÃ¢y tÆ°Æ¡ng Ä‘Æ°Æ¡ng bÃ i toÃ¡n cÄƒn chá»‰nh chuá»—i Ä‘á»™ng (dynamic programming).

â¸»

7. Biá»ƒu diá»…n Ma tráº­n Ãnh xáº¡

Ta cÃ³ thá»ƒ Ä‘á»‹nh nghÄ©a ma tráº­n chuyá»ƒn Ä‘á»•i:

M \in \mathbb{R}^{|V_A| \times |V_B|}

Trong Ä‘Ã³:

M_{ij} = P(b_j \mid a_i)

Náº¿u Ã¡nh xáº¡ xÃ¡c Ä‘á»‹nh:

M_{ij} \in \{0,1\}

Náº¿u Ã¡nh xáº¡ xÃ¡c suáº¥t:

\sum_j M_{ij} = 1

â¸»

8. á»¨ng dá»¥ng Thá»±c tiá»…n
	1.	Chuyá»ƒn embedding giá»¯a hai mÃ´ hÃ¬nh
	2.	Fine-tune chÃ©o tokenizer
	3.	Distillation giá»¯a hai LLM
	4.	Interoperability giá»¯a há»‡ sinh thÃ¡i NLP

â¸»

9. Tháº£o luáº­n

Sá»± khÃ¡c biá»‡t giá»¯a tokenizer khÃ´ng chá»‰ áº£nh hÆ°á»Ÿng Ä‘áº¿n:
	â€¢	Äá»™ dÃ i chuá»—i
	â€¢	Chi phÃ­ attention
	â€¢	Entropy há»‡ biá»ƒu diá»…n

MÃ  cÃ²n áº£nh hÆ°á»Ÿng Ä‘áº¿n:
	â€¢	PhÃ¢n bá»‘ gradient
	â€¢	á»”n Ä‘á»‹nh huáº¥n luyá»‡n
	â€¢	TÃ­nh chuyá»ƒn giao embedding

BÃ i toÃ¡n chuyá»ƒn Ä‘á»•i tokenizer thá»±c cháº¥t lÃ  bÃ i toÃ¡n Ã¡nh xáº¡ giá»¯a hai há»‡ mÃ£ hÃ³a rá»i ráº¡c cÃ³ cáº¥u trÃºc phÃ¢n cáº¥p.

â¸»

10. Káº¿t luáº­n

Viá»‡c chuyá»ƒn Ä‘á»•i giá»¯a hai tokenizer cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ hÃ¬nh hÃ³a hÃ¬nh thá»©c báº±ng:

\Phi = \mathcal{T}_B \circ \mathcal{D}_A

Sai sá»‘ thÃ´ng tin Ä‘Æ°á»£c Ä‘o báº±ng:

\epsilon = d(\mathcal{D}_A(\mathcal{T}_A(x)), x)

Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n phá»¥ thuá»™c vÃ o:

O\left(\left(\frac{n}{\mathbb{E}[L]}\right)^2\right)

Thiáº¿t káº¿ tokenizer khÃ´ng chá»‰ lÃ  váº¥n Ä‘á» tiá»n xá»­ lÃ½ mÃ  lÃ  má»™t thÃ nh pháº§n cáº¥u trÃºc cá»§a toÃ n bá»™ kiáº¿n trÃºc Transformer.

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
	2.	Radford et al. (2019). Language Models are Unsupervised Multitask Learners.
	3.	Kudo & Richardson (2018). SentencePiece: A simple and language independent subword tokenizer.
	4.	Sennrich et al. (2016). Neural Machine Translation of Rare Words with Subword Units.
	5.	Shannon, C. (1948). A Mathematical Theory of Communication.
	6.	Vaswani et al. (2017). Attention Is All You Need.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_llm_05_preparing_text_for_tokenization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_preparing_text_for_tokenization.md) |
| [PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| ğŸ“Œ **[aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

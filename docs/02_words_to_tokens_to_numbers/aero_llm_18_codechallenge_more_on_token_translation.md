
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [02 words to tokens to numbers](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Má»Ÿ rá»™ng BÃ i toÃ¡n Chuyá»ƒn Ä‘á»•i Token:

PhÃ¢n tÃ­ch HÃ¬nh thá»©c, Äá»‹nh lÆ°á»£ng Sai sá»‘ vÃ  áº¢nh hÆ°á»Ÿng Ä‘áº¿n Biá»ƒu diá»…n Ngá»¯ nghÄ©a

â¸»

TÃ³m táº¯t

Dá»±a trÃªn tÃ i liá»‡u Ä‘Ã­nh kÃ¨m â€œMore on Token Translationâ€, bÃ i viáº¿t nÃ y má»Ÿ rá»™ng phÃ¢n tÃ­ch bÃ i toÃ¡n chuyá»ƒn Ä‘á»•i giá»¯a cÃ¡c há»‡ tokenizer trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs). ChÃºng tÃ´i xÃ¢y dá»±ng má»™t khung toÃ¡n há»c cho Ã¡nh xáº¡ giá»¯a hai khÃ´ng gian token rá»i ráº¡c, phÃ¢n tÃ­ch sai sá»‘ tÃ­ch lÅ©y khi chuyá»ƒn Ä‘á»•i nhiá»u bÆ°á»›c, Ä‘á» xuáº¥t mÃ´ hÃ¬nh ma tráº­n xÃ¡c suáº¥t chuyá»ƒn Ä‘á»•i, vÃ  Ä‘Ã¡nh giÃ¡ áº£nh hÆ°á»Ÿng Ä‘áº¿n embedding vÃ  attention trong kiáº¿n trÃºc Transformer. CÃ¡c vÃ­ dá»¥ Ä‘Æ°á»£c minh há»a vá»›i tokenizer cá»§a BERT vÃ  GPT-2.

â¸»

1. Giá»›i thiá»‡u

Tokenization Ä‘á»‹nh nghÄ©a má»™t phÃ©p mÃ£ hÃ³a:

\mathcal{T}: \Sigma^* \rightarrow V^*

vá»›i:
	â€¢	\Sigma: báº£ng chá»¯ cÃ¡i kÃ½ tá»±
	â€¢	V: táº­p token
	â€¢	V^*: chuá»—i token

Khi tá»“n táº¡i hai tokenizer \mathcal{T}_A vÃ  \mathcal{T}_B, bÃ i toÃ¡n Ä‘áº·t ra lÃ  xÃ¢y dá»±ng Ã¡nh xáº¡:

\Phi_{A \to B}: V_A^* \rightarrow V_B^*

sao cho báº£o toÃ n ná»™i dung ngá»¯ nghÄ©a vÃ  háº¡n cháº¿ sai sá»‘ thÃ´ng tin.

â¸»

2. PhÃ¢n rÃ£ Hai BÆ°á»›c: Decode vÃ  Re-tokenize

CÃ¡ch tá»± nhiÃªn nháº¥t:

\Phi_{A \to B} = \mathcal{T}_B \circ \mathcal{D}_A

Trong Ä‘Ã³:
	â€¢	\mathcal{D}_A: V_A^* \rightarrow \Sigma^* lÃ  hÃ m giáº£i mÃ£

Khi tokenizer kháº£ nghá»‹ch:

\mathcal{D}_A(\mathcal{T}_A(x)) = x

Tuy nhiÃªn, trong thá»±c táº¿ cÃ³ thá»ƒ xuáº¥t hiá»‡n chuáº©n hÃ³a Unicode hoáº·c xá»­ lÃ½ khoáº£ng tráº¯ng gÃ¢y sai sá»‘.

â¸»

3. Sai sá»‘ TÃ­ch lÅ©y khi Chuyá»ƒn Ä‘á»•i Nhiá»u Láº§n

Giáº£ sá»­ thá»±c hiá»‡n chuá»—i chuyá»ƒn Ä‘á»•i:

A \to B \to C

Sai sá»‘ tá»•ng:

\epsilon_{A \to C} \le \epsilon_{A \to B} + \epsilon_{B \to C}

ÄÃ¢y lÃ  há»‡ quáº£ cá»§a báº¥t Ä‘áº³ng thá»©c tam giÃ¡c Ä‘á»‘i vá»›i khoáº£ng cÃ¡ch Levenshtein:

d(x,z) \le d(x,y) + d(y,z)

Náº¿u má»—i bÆ°á»›c cÃ³ sai sá»‘ nhá» nhÆ°ng láº·p nhiá»u láº§n, sai sá»‘ tÃ­ch lÅ©y cÃ³ thá»ƒ tÄƒng tuyáº¿n tÃ­nh theo sá»‘ bÆ°á»›c:

\epsilon_k \le k \epsilon

â¸»

4. MÃ´ hÃ¬nh XÃ¡c suáº¥t cho Chuyá»ƒn Ä‘á»•i Token

Thay vÃ¬ Ã¡nh xáº¡ xÃ¡c Ä‘á»‹nh, ta Ä‘á»‹nh nghÄ©a phÃ¢n bá»‘ xÃ¡c suáº¥t:

P(b_j \mid a_i)

Táº¡o thÃ nh ma tráº­n:

M \in \mathbb{R}^{|V_A| \times |V_B|}

vá»›i:

\sum_{j} M_{ij} = 1

Khi Ä‘Ã³ embedding cÃ³ thá»ƒ chuyá»ƒn Ä‘á»•i tuyáº¿n tÃ­nh:

E_B = M^\top E_A

Trong Ä‘Ã³:
	â€¢	E_A \in \mathbb{R}^{|V_A| \times d}
	â€¢	E_B \in \mathbb{R}^{|V_B| \times d}

â¸»

5. PhÃ¢n tÃ­ch Sai sá»‘ Ngá»¯ nghÄ©a

Giáº£ sá»­ embedding cá»§a token:

e(a_i), \quad e(b_j)

Sai sá»‘ chuyá»ƒn Ä‘á»•i:

\delta_i = \| e(a_i) - \sum_j M_{ij} e(b_j) \|_2

Sai sá»‘ trung bÃ¬nh:

\mathbb{E}[\delta] = \frac{1}{|V_A|} \sum_i \delta_i

Náº¿u embedding hai mÃ´ hÃ¬nh náº±m trong cÃ¹ng khÃ´ng gian ngá»¯ nghÄ©a, ta cÃ³ thá»ƒ tá»‘i Æ°u:

\min_M \sum_i \delta_i^2

â¸»

6. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Self-Attention

Cho vÄƒn báº£n Ä‘á»™ dÃ i n kÃ½ tá»±:

m_A = \frac{n}{\mathbb{E}[L_A]}

m_B = \frac{n}{\mathbb{E}[L_B]}

Chi phÃ­ attention:

C_A = O(m_A^2)

C_B = O(m_B^2)

Tá»· lá»‡:

\frac{C_A}{C_B} = \left(\frac{\mathbb{E}[L_B]}{\mathbb{E}[L_A]}\right)^2

Tokenizer táº¡o token dÃ i hÆ¡n giÃºp giáº£m chi phÃ­ tÃ­nh toÃ¡n.

â¸»

7. CÄƒn chá»‰nh Span KÃ½ tá»±

Má»—i token tÆ°Æ¡ng á»©ng má»™t Ä‘oáº¡n kÃ½ tá»±:

a_i \leftrightarrow [s_i, e_i)

b_j \leftrightarrow [u_j, v_j)

BÃ i toÃ¡n cÄƒn chá»‰nh trá»Ÿ thÃ nh:

\text{match}(a_i, b_j) \iff [s_i, e_i) \cap [u_j, v_j) \neq \emptyset

CÃ³ thá»ƒ xÃ¢y dá»±ng Ã¡nh xáº¡ nhiá»u-nhiá»u.

â¸»

8. Äá»™ phá»©c táº¡p Thuáº­t toÃ¡n

Náº¿u:
	â€¢	Chuá»—i cÃ³ m token á»Ÿ A
	â€¢	k token á»Ÿ B

Thuáº­t toÃ¡n cÄƒn chá»‰nh span cÃ³ thá»ƒ thá»±c hiá»‡n trong:

O(m + k)

vÃ¬ chá»‰ cáº§n quÃ©t hai con trá».

Tuy nhiÃªn náº¿u so khá»›p embedding:

O(mk)

â¸»

9. LiÃªn há»‡ Ä‘áº¿n LÃ½ thuyáº¿t ThÃ´ng tin

Entropy cá»§a phÃ¢n bá»‘ token:

H(V) = - \sum_{t \in V} p(t)\log p(t)

Chuyá»ƒn tokenizer lÃ m thay Ä‘á»•i phÃ¢n bá»‘:

\Delta H = |H(V_A) - H(V_B)|

Theo Claude Shannon (1948), entropy Ä‘o lÆ°á»£ng thÃ´ng tin trung bÃ¬nh trÃªn má»—i token.

â¸»

10. Tháº£o luáº­n

Má»Ÿ rá»™ng tá»« tÃ i liá»‡u Ä‘Ã­nh kÃ¨m, cÃ³ thá»ƒ tháº¥y:
	1.	Token translation khÃ´ng chá»‰ lÃ  thao tÃ¡c chuá»—i
	2.	LÃ  bÃ i toÃ¡n Ã¡nh xáº¡ giá»¯a hai há»‡ mÃ£ hÃ³a rá»i ráº¡c
	3.	CÃ³ thá»ƒ xem nhÆ° biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh trong khÃ´ng gian embedding
	4.	Sai sá»‘ cÃ³ thá»ƒ tÃ­ch lÅ©y náº¿u chuyá»ƒn Ä‘á»•i nhiá»u bÆ°á»›c

Trong thá»±c táº¿, cÃ¡c há»‡ nhÆ° OpenAI hay Google thiáº¿t káº¿ tokenizer gáº¯n cháº·t vá»›i kiáº¿n trÃºc mÃ´ hÃ¬nh, do Ä‘Ã³ viá»‡c chuyá»ƒn Ä‘á»•i Ä‘Ã²i há»i phÃ¢n tÃ­ch cáº©n trá»ng.

â¸»

11. Káº¿t luáº­n

BÃ i toÃ¡n chuyá»ƒn Ä‘á»•i tokenizer cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ hÃ¬nh hÃ³a:

\Phi_{A \to B} = \mathcal{T}_B \circ \mathcal{D}_A

Sai sá»‘ tÃ­ch lÅ©y:

\epsilon_k \le k \epsilon

Embedding cÃ³ thá»ƒ chuyá»ƒn Ä‘á»•i báº±ng:

E_B = M^\top E_A

ÄÃ¢y lÃ  má»™t bÃ i toÃ¡n káº¿t há»£p giá»¯a:
	â€¢	LÃ½ thuyáº¿t mÃ£ hÃ³a
	â€¢	LÃ½ thuyáº¿t thÃ´ng tin
	â€¢	Tá»‘i Æ°u hÃ³a tuyáº¿n tÃ­nh
	â€¢	Kiáº¿n trÃºc Transformer

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
	2.	Radford et al. (2019). GPT-2: Language Models are Unsupervised Multitask Learners.
	3.	Vaswani et al. (2017). Attention Is All You Need.
	4.	Shannon, C. (1948). A Mathematical Theory of Communication.
	5.	Kudo & Richardson (2018). SentencePiece: A simple and language independent subword tokenizer.
	6.	Sennrich et al. (2016). Neural Machine Translation of Rare Words with Subword Units.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_llm_05_preparing_text_for_tokenization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_preparing_text_for_tokenization.md) |
| [PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_translating_between_tokenizers.md) |
| ğŸ“Œ **[aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

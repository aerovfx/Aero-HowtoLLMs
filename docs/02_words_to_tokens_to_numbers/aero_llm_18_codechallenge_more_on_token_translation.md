
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Má»Ÿ rá»™ng BÃ i toÃ¡n Chuyá»ƒn Ä‘á»•i Token:

PhÃ¢n tÃ­ch HÃ¬nh thá»©c, Äá»‹nh lÆ°á»£ng Sai sá»‘ vÃ  áº¢nh hÆ°á»Ÿng Ä‘áº¿n Biá»ƒu diá»…n Ngá»¯ nghÄ©a

â¸»

TÃ³m táº¯t

Dá»±a trÃªn tÃ i liá»‡u Ä‘Ã­nh kÃ¨m â€œMore on Token Translationâ€, bÃ i viáº¿t nÃ y má»Ÿ rá»™ng phÃ¢n tÃ­ch bÃ i toÃ¡n chuyá»ƒn Ä‘á»•i giá»¯a cÃ¡c há»‡ tokenizer trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs). ChÃºng tÃ´i xÃ¢y dá»±ng má»™t khung toÃ¡n há»c cho Ã¡nh xáº¡ giá»¯a hai khÃ´ng gian token rá»i ráº¡c, phÃ¢n tÃ­ch sai sá»‘ tÃ­ch lÅ©y khi chuyá»ƒn Ä‘á»•i nhiá»u bÆ°á»›c, Ä‘á» xuáº¥t mÃ´ hÃ¬nh ma tráº­n xÃ¡c suáº¥t chuyá»ƒn Ä‘á»•i, vÃ  Ä‘Ã¡nh giÃ¡ áº£nh hÆ°á»Ÿng Ä‘áº¿n embedding vÃ  attention trong kiáº¿n trÃºc Transformer. CÃ¡c vÃ­ dá»¥ Ä‘Æ°á»£c minh há»a vá»›i tokenizer cá»§a BERT vÃ  GPT-2.

â¸»

1. Giá»›i thiá»‡u

Tokenization Ä‘á»‹nh nghÄ©a má»™t phÃ©p mÃ£ hÃ³a:

$\mathcal${T}: \Sigma^* \rightarrow V^*

vá»›i:
	â€¢	\Sigma: báº£ng chá»¯ cÃ¡i kÃ½ tá»±
	â€¢	V: táº­p token
	â€¢	V^*: chuá»—i token

$$
Khi tá»“n táº¡i hai tokenizer \mathcal{T}_A vÃ  \mathcal{T}_B, bÃ i toÃ¡n Ä‘áº·t ra lÃ  xÃ¢y dá»±ng Ã¡nh xáº¡:
$$

\Phi_{A \to B}: $V_A$^* \rightarrow $V_B$^*

sao cho báº£o toÃ n ná»™i dung ngá»¯ nghÄ©a vÃ  háº¡n cháº¿ sai sá»‘ thÃ´ng tin.

â¸»

2. PhÃ¢n rÃ£ Hai BÆ°á»›c: Decode vÃ  Re-tokenize

CÃ¡ch tá»± nhiÃªn nháº¥t:

\Phi_{A \to B} = \mathcal{T}_B \circ \mathcal{D}_A

Trong Ä‘Ã³:

$$
â€¢	\mathcal{D}_A: V_A^{\ast} \rightarrow \Sigma^{\ast} lÃ  hÃ m giáº£i mÃ£
$$

Khi tokenizer kháº£ nghá»‹ch:

$$
\mathcal{D}_A\mathcal{T}_A(x) = x
$$

Tuy nhiÃªn, trong thá»±c táº¿ cÃ³ thá»ƒ xuáº¥t hiá»‡n chuáº©n hÃ³a Unicode hoáº·c xá»­ lÃ½ khoáº£ng tráº¯ng gÃ¢y sai sá»‘.

â¸»

3. Sai sá»‘ TÃ­ch lÅ©y khi Chuyá»ƒn Ä‘á»•i Nhiá»u Láº§n

Giáº£ sá»­ thá»±c hiá»‡n chuá»—i chuyá»ƒn Ä‘á»•i:

A \to B \to C

Sai sá»‘ tá»•ng:

\epsilon_{A \to C} \le \epsilon_{A \to B} + \epsilon_{B \to C}

ÄÃ¢y lÃ  há»‡ quáº£ cá»§a báº¥t Ä‘áº³ng thá»©c tam giÃ¡c Ä‘á»‘i vá»›i khoáº£ng cÃ¡ch Levenshtein:

d(x,z) \le d(x,y) + d(y,z)

Náº¿u má»—i bÆ°á»›c cÃ³ sai sá»‘ nhá» nhÆ°ng láº·p nhiá»u láº§n, sai sá»‘ tÃ­ch lÅ©y cÃ³ thá»ƒ tÄƒng tuyáº¿n tÃ­nh theo sá»‘ bÆ°á»›c:

\epsilon_k \le k \epsilon

â¸»

4. MÃ´ hÃ¬nh XÃ¡c suáº¥t cho Chuyá»ƒn Ä‘á»•i Token

Thay vÃ¬ Ã¡nh xáº¡ xÃ¡c Ä‘á»‹nh, ta Ä‘á»‹nh nghÄ©a phÃ¢n bá»‘ xÃ¡c suáº¥t:

$P($b_j$ \mid $a_i$)$

Táº¡o thÃ nh ma tráº­n:

$$
M \in \mathbb{R}^{|V_A| \times |V_B|}
$$

vá»›i:

$$
\sum_{j} M_{ij} = 1
$$

Khi Ä‘Ã³ embedding cÃ³ thá»ƒ chuyá»ƒn Ä‘á»•i tuyáº¿n tÃ­nh:

E_B = M^\top E_A

Trong Ä‘Ã³:

$$
â€¢	E_A \in \mathbb{R}^{|V_A| \times d} â€¢	E_B \in \mathbb{R}^{|V_B| \times d}
$$

â¸»

5. PhÃ¢n tÃ­ch Sai sá»‘ Ngá»¯ nghÄ©a

Giáº£ sá»­ embedding cá»§a token:

e$a_i$, \quad e$b_j$

Sai sá»‘ chuyá»ƒn Ä‘á»•i:

\delta_i = \| ea_i - \sum_j M_{ij} eb_j \|_2

Sai sá»‘ trung bÃ¬nh:

$$
\mathbb{E}[\delta] = \frac{1}{|V_A|} \sum_i \delta_i
$$

Náº¿u embedding hai mÃ´ hÃ¬nh náº±m trong cÃ¹ng khÃ´ng gian ngá»¯ nghÄ©a, ta cÃ³ thá»ƒ tá»‘i Æ°u:

$$
\min_M \sum_i \delta_i^2
$$

â¸»

6. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Self-Attention

Cho vÄƒn báº£n Ä‘á»™ dÃ i n kÃ½ tá»±:

m_A = \frac{n}{\mathbb{E}[L_A]}

$$
m_B = \frac{n}{\mathbb{E}[L_B]} Chi phÃ­ attention: C_A = O(m_A^2)
$$

C_B = O(m_B^2)

Tá»· lá»‡:

\frac{C_A}{C_B} = \left(\frac{\mathbb{E}[L_B]}{\mathbb{E}[L_A]}\right)^2

Tokenizer táº¡o token dÃ i hÆ¡n giÃºp giáº£m chi phÃ­ tÃ­nh toÃ¡n.

â¸»

7. CÄƒn chá»‰nh Span KÃ½ tá»±

Má»—i token tÆ°Æ¡ng á»©ng má»™t Ä‘oáº¡n kÃ½ tá»±:

a_i \leftrightarrow [s_i, e_i)

$$
b_j \leftrightarrow [u_j, v_j) BÃ i toÃ¡n cÄƒn chá»‰nh trá»Ÿ thÃ nh: \text{match}a_i, b_j \iff [s_i, e_i) \cap [u_j, v_j) \neq \emptyset CÃ³ thá»ƒ xÃ¢y dá»±ng Ã¡nh xáº¡ nhiá»u-nhiá»u. â¸» 8. Äá»™ phá»©c táº¡p Thuáº­t toÃ¡n Náº¿u: â€¢	Chuá»—i cÃ³ m token á»Ÿ A â€¢	k token á»Ÿ B Thuáº­t toÃ¡n cÄƒn chá»‰nh span cÃ³ thá»ƒ thá»±c hiá»‡n trong: O(m + k) vÃ¬ chá»‰ cáº§n quÃ©t hai con trá». Tuy nhiÃªn náº¿u so khá»›p embedding: O(mk) â¸» 9. LiÃªn há»‡ Ä‘áº¿n LÃ½ thuyáº¿t ThÃ´ng tin Entropy cá»§a phÃ¢n bá»‘ token: HV = - \sum_{t \in V} pt\log pt Chuyá»ƒn tokenizer lÃ m thay Ä‘á»•i phÃ¢n bá»‘: \Delta H = |HV_A - HV_B| Theo Claude Shannon (1948), entropy Ä‘o lÆ°á»£ng thÃ´ng tin trung bÃ¬nh trÃªn má»—i token. â¸» 10. Tháº£o luáº­n Má»Ÿ rá»™ng tá»« tÃ i liá»‡u Ä‘Ã­nh kÃ¨m, cÃ³ thá»ƒ tháº¥y: 1.	Token translation khÃ´ng chá»‰ lÃ  thao tÃ¡c chuá»—i 2.	LÃ  bÃ i toÃ¡n Ã¡nh xáº¡ giá»¯a hai há»‡ mÃ£ hÃ³a rá»i ráº¡c 3.	CÃ³ thá»ƒ xem nhÆ° biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh trong khÃ´ng gian embedding 4.	Sai sá»‘ cÃ³ thá»ƒ tÃ­ch lÅ©y náº¿u chuyá»ƒn Ä‘á»•i nhiá»u bÆ°á»›c Trong thá»±c táº¿, cÃ¡c há»‡ nhÆ° OpenAI hay Google thiáº¿t káº¿ tokenizer gáº¯n cháº·t vá»›i kiáº¿n trÃºc mÃ´ hÃ¬nh, do Ä‘Ã³ viá»‡c chuyá»ƒn Ä‘á»•i Ä‘Ã²i há»i phÃ¢n tÃ­ch cáº©n trá»ng. â¸» 11. Káº¿t luáº­n BÃ i toÃ¡n chuyá»ƒn Ä‘á»•i tokenizer cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ hÃ¬nh hÃ³a: \Phi_{A \to B} = \mathcal{T}_B \circ \mathcal{D}_A Sai sá»‘ tÃ­ch lÅ©y: \epsilon_k \le k \epsilon Embedding cÃ³ thá»ƒ chuyá»ƒn Ä‘á»•i báº±ng: E_B = M^\top E_A
$$


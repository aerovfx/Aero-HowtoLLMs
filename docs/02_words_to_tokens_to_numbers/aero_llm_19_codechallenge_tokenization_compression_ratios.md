
<!-- Aero-Navigation-Start -->
[üè† Home](../../index.md) > [02 words to tokens to numbers](../index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../index.md)
- [üìö Module 01: LLM Course](../../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Ph√¢n t√≠ch T·ª∑ l·ªá N√©n trong Tokenization:

M√¥ h√¨nh To√°n h·ªçc v√† ·∫¢nh h∆∞·ªüng ƒë·∫øn Hi·ªáu nƒÉng M√¥ h√¨nh Ng√¥n ng·ªØ

‚∏ª

T√≥m t·∫Øt

D·ª±a tr√™n t√†i li·ªáu ƒë√≠nh k√®m ‚ÄúTokenization Compression Ratios‚Äù, b√†i vi·∫øt n√†y ph√¢n t√≠ch t·ª∑ l·ªá n√©n (compression ratio) c·ªßa c√°c ph∆∞∆°ng ph√°p tokenization trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLMs). Ch√∫ng t√¥i x√¢y d·ª±ng m√¥ h√¨nh to√°n h·ªçc cho t·ª∑ l·ªá n√©n gi·ªØa kh√¥ng gian k√Ω t·ª± v√† kh√¥ng gian token, ph√¢n t√≠ch m·ªëi quan h·ªá v·ªõi entropy v√† ƒë·ªô ph·ª©c t·∫°p self-attention, ƒë·ªìng th·ªùi so s√°nh c√°c c∆° ch·∫ø token h√≥a nh∆∞ WordPiece v√† Byte Pair Encoding (BPE). C√°c v√≠ d·ª• minh h·ªça ƒë∆∞·ª£c tr√¨nh b√†y v·ªõi tokenizer c·ªßa BERT v√† GPT-2.

‚∏ª

1. Gi·ªõi thi·ªáu

Tokenization l√† qu√° tr√¨nh √°nh x·∫° m·ªôt chu·ªói k√Ω t·ª±:

x \in \Sigma^*

th√†nh chu·ªói token:

\mathcal{T}(x) = (t_1, t_2, \dots, t_m)

T·ª∑ l·ªá n√©n c·ªßa tokenizer ph·∫£n √°nh m·ª©c ƒë·ªô gi·∫£m s·ªë ƒë∆°n v·ªã bi·ªÉu di·ªÖn khi chuy·ªÉn t·ª´ k√Ω t·ª± sang token.

‚∏ª

2. ƒê·ªãnh nghƒ©a T·ª∑ l·ªá N√©n

Gi·∫£ s·ª≠:
	‚Ä¢	VƒÉn b·∫£n c√≥ n k√Ω t·ª±
	‚Ä¢	Sau tokenization thu ƒë∆∞·ª£c m token

2.1 Compression Ratio

R = \frac{n}{m}

N·∫øu R > 1, tokenization ƒë·∫°t hi·ªáu ·ª©ng n√©n.

‚∏ª

2.2 ƒê·ªô d√†i Token Trung b√¨nh

G·ªçi L_i l√† s·ªë k√Ω t·ª± trong token t_i.

\bar{L} = \frac{1}{m} \sum_{i=1}^{m} L_i

Ta c√≥:

n = \sum_{i=1}^{m} L_i

Suy ra:

R = \bar{L}

T·ª∑ l·ªá n√©n ch√≠nh l√† ƒë·ªô d√†i k√Ω t·ª± trung b√¨nh tr√™n m·ªói token.

‚∏ª

3. Ph√¢n t√≠ch X√°c su·∫•t

G·ªçi P(L=k) l√† x√°c su·∫•t token c√≥ ƒë·ªô d√†i k.

K·ª≥ v·ªçng:

\mathbb{E}[L] = \sum_{k} k P(L=k)

T·ª∑ l·ªá n√©n trung b√¨nh:

R = \mathbb{E}[L]

N·∫øu ph√¢n b·ªë ƒë·ªô d√†i tu√¢n theo ph√¢n b·ªë h√¨nh h·ªçc:

P(L=k) = (1-q)q^{k-1}

th√¨:

\mathbb{E}[L] = \frac{1}{1-q}

‚∏ª

4. Li√™n h·ªá v·ªõi Entropy

Theo l√Ω thuy·∫øt c·ªßa Claude Shannon (1948), entropy c·ªßa ngu·ªìn k√Ω t·ª±:

H_c = -\sum_{c \in \Sigma} p(c)\log p(c)

Entropy tr√™n token:

H_t = -\sum_{t \in V} p(t)\log p(t)

T·ª∑ l·ªá n√©n l√Ω thuy·∫øt t·ªëi ∆∞u:

R_{\text{opt}} = \frac{H_c}{H_t}

N·∫øu tokenizer t·ªëi ∆∞u theo nghƒ©a th√¥ng tin, th√¨:

m H_t \approx n H_c

‚∏ª

5. ·∫¢nh h∆∞·ªüng ƒë·∫øn Self-Attention

Trong ki·∫øn tr√∫c Transformer:

\text{Cost} = O(m^2)

Thay m = \frac{n}{R}:

\text{Cost} = O\left(\left(\frac{n}{R}\right)^2\right)

Do ƒë√≥:
	‚Ä¢	R \uparrow \Rightarrow chi ph√≠ gi·∫£m theo b√¨nh ph∆∞∆°ng.

V√≠ d·ª•:
	‚Ä¢	N·∫øu R = 4, chi ph√≠ gi·∫£m 16 l·∫ßn so v·ªõi character-level.

‚∏ª

6. So s√°nh C√°c Ph∆∞∆°ng ph√°p Tokenization

6.1 WordPiece

√Åp d·ª•ng trong BERT.

T·ªëi ∆∞u x√°c su·∫•t chu·ªói subword:

\arg\max_{s_1,\dots,s_k} \prod_i P(s_i)

C√≥ xu h∆∞·ªõng t·∫°o token trung b√¨nh 3‚Äì5 k√Ω t·ª±.

‚∏ª

6.2 Byte Pair Encoding (BPE)

S·ª≠ d·ª•ng trong GPT-2 b·ªüi OpenAI.

Thu·∫≠t to√°n l·∫∑p:

(u,v) = \arg\max \text{freq}(uv)

G·ªôp c·∫∑p xu·∫•t hi·ªán nhi·ªÅu nh·∫•t.

‚∏ª

6.3 Character-level

R = 1

Kh√¥ng n√©n ‚Üí chi ph√≠ attention cao nh·∫•t.

‚∏ª

7. Ph√¢n t√≠ch Gi·ªõi h·∫°n L√Ω thuy·∫øt

Gi·∫£ s·ª≠ k√≠ch th∆∞·ªõc t·ª´ v·ª±ng |V|.

Dung l∆∞·ª£ng embedding:

W \in \mathbb{R}^{|V| \times d}

T·ªïng tham s·ªë:

|V|d

B√†i to√°n t·ªëi ∆∞u ƒëa m·ª•c ti√™u:

\min_{V} \left( \frac{n}{R} \right)^2 + \lambda |V|

Trong ƒë√≥:
	‚Ä¢	Th√†nh ph·∫ßn ƒë·∫ßu: chi ph√≠ attention
	‚Ä¢	Th√†nh ph·∫ßn sau: chi ph√≠ b·ªô nh·ªõ embedding

‚∏ª

8. Ph√¢n t√≠ch T·ª∑ l·ªá N√©n Th·ª±c nghi·ªám

Trong th·ª±c t·∫ø:
	‚Ä¢	VƒÉn b·∫£n ti·∫øng Anh: R \approx 3-4
	‚Ä¢	VƒÉn b·∫£n c√≥ nhi·ªÅu k√Ω t·ª± Unicode: R th·∫•p h∆°n
	‚Ä¢	Ng√¥n ng·ªØ ch·∫Øp d√≠nh (agglutinative): R bi·∫øn thi√™n m·∫°nh

Do ƒë√≥:

R = f(\text{ng√¥n ng·ªØ}, |V|, thu·∫≠t to√°n)

‚∏ª

9. B√†n lu·∫≠n

Tokenization ƒë√≥ng vai tr√≤ nh∆∞ c∆° ch·∫ø n√©n ti·ªÅn x·ª≠ l√Ω cho Transformer.

C√≥ th·ªÉ xem tokenization nh∆∞ b√†i to√°n m√£ h√≥a:

\Sigma^* \rightarrow V^*

M·ª•c ti√™u:
	1.	Gi·∫£m ƒë·ªô d√†i chu·ªói (tƒÉng R)
	2.	Gi·ªØ entropy th√¥ng tin
	3.	H·∫°n ch·∫ø tƒÉng k√≠ch th∆∞·ªõc t·ª´ v·ª±ng

S·ª± c√¢n b·∫±ng n√†y gi·∫£i th√≠ch v√¨ sao c√°c h·ªá nh∆∞ Google v√† OpenAI ch·ªçn t·ª´ v·ª±ng kho·∫£ng 30k‚Äì50k token.

‚∏ª

10. K·∫øt lu·∫≠n

T·ª∑ l·ªá n√©n trong tokenization ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi:

R = \frac{n}{m} = \mathbb{E}[L]

·∫¢nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn:

\text{Attention Cost} = O\left(\left(\frac{n}{R}\right)^2\right)

V√† ch·ªãu r√†ng bu·ªôc b·ªüi:

m H_t \approx n H_c

Tokenization c√≥ th·ªÉ ƒë∆∞·ª£c xem nh∆∞ b∆∞·ªõc n√©n th√¥ng tin c√≥ ki·ªÉm so√°t nh·∫±m t·ªëi ∆∞u h√≥a hi·ªáu nƒÉng v√† chi ph√≠ t√≠nh to√°n c·ªßa m√¥ h√¨nh ng√¥n ng·ªØ.

‚∏ª

T√†i li·ªáu tham kh·∫£o
	1.	Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
	2.	Radford et al. (2019). GPT-2: Language Models are Unsupervised Multitask Learners.
	3.	Vaswani et al. (2017). Attention Is All You Need.
	4.	Shannon, C. (1948). A Mathematical Theory of Communication.
	5.	Sennrich et al. (2016). Neural Machine Translation of Rare Words with Subword Units.
	6.	Kudo & Richardson (2018). SentencePiece: A simple and language independent subword tokenizer.
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [T·∫°i sao vƒÉn b·∫£n c·∫ßn ƒë∆∞·ª£c ƒë√°nh s·ªë?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [Ph√¢n t√≠ch v√† chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh chu·ªói token ƒë∆∞·ª£c ƒë√°nh s·ªë: C∆° s·ªü to√°n h·ªçc v√† ·ª©ng d·ª•ng trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [T·∫°o v√† tr·ª±c quan h√≥a Token trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn: C∆° s·ªü to√°n h·ªçc v√† ph√¢n t√≠ch bi·ªÉu di·ªÖn](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [T·∫°o v√† tr·ª±c quan h√≥a Token (Ph·∫ßn 2): Ph√¢n t√≠ch h√¨nh h·ªçc kh√¥ng gian embedding v√† Attention Map trong m√¥ h√¨nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chu·∫©n b·ªã vƒÉn b·∫£n cho Tokenization trong m√¥ h√¨nh Transformer: C∆° s·ªü l√Ω thuy·∫øt v√† ph√¢n t√≠ch to√°n h·ªçc](aero_llm_05_preparing_text_for_tokenization.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_05_preparing_text_for_tokenization.md) |
| [Ph√¢n t√≠ch quy tr√¨nh Tokenization qua v√≠ d·ª• *The Time Machine*: C∆° s·ªü thu·∫≠t to√°n v√† m√¥ h√¨nh h√≥a to√°n h·ªçc](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So s√°nh Tokenization m·ª©c k√Ω t·ª±, t·ª´ v√† subword: Ph√¢n t√≠ch l√Ω thuy·∫øt v√† m√¥ h√¨nh to√°n h·ªçc](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thu·∫≠t to√°n Byte Pair Encoding (BPE) v√† B√†i to√°n T·ªëi ∆∞u K√≠ch th∆∞·ªõc T·ª´ v·ª±ng trong M√¥ h√¨nh Ng√¥n ng·ªØ](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_18_codechallenge_more_on_token_translation.md) |
| üìå **[aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md)** | [Xem b√†i vi·∫øt ‚Üí](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

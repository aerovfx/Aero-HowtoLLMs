
<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Ph√¢n t√≠ch Th·ªëng k√™ S·ªë l∆∞·ª£ng Token theo ƒê·ªô d√†i Subword (Ph·∫ßn 2): M√¥ h√¨nh h√≥a To√°n h·ªçc v√† H√†m Ph√¢n b·ªë

‚∏ª

T√≥m t·∫Øt

B√†i b√°o n√†y ti·∫øp t·ª•c ph√¢n t√≠ch th·ªëng k√™ s·ªë l∆∞·ª£ng token theo ƒë·ªô d√†i subword trong b·ªô tokenizer c·ªßa GPT-4, d·ª±a tr√™n d·ªØ li·ªáu th·ª±c nghi·ªám t·ª´ t√†i li·ªáu ƒë√≠nh k√®m (ph·∫ßn 2). Ch√∫ng t√¥i x√¢y d·ª±ng m√¥ h√¨nh to√°n h·ªçc cho ph√¢n b·ªë ƒë·ªô d√†i, ki·ªÉm ƒë·ªãnh gi·∫£ thuy·∫øt ph√¢n b·ªë m≈© v√† lu·∫≠t Zipf, ƒë·ªìng th·ªùi ph√¢n t√≠ch t√°c ƒë·ªông c·ªßa c·∫•u tr√∫c token ƒë·∫øn ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n trong ki·∫øn tr√∫c Transformer c·ªßa OpenAI. K·∫øt qu·∫£ cho th·∫•y ph√¢n b·ªë ƒë·ªô d√†i subword c√≥ xu h∆∞·ªõng suy gi·∫£m phi tuy·∫øn, g·∫ßn v·ªõi h√†m m≈© ho·∫∑c log-linear, v√† c√≥ m·ªëi li√™n h·ªá ch·∫∑t ch·∫Ω v·ªõi entropy h·ªá token.

‚∏ª

1. Gi·ªõi thi·ªáu

Trong c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLMs), tokenization l√† b∆∞·ªõc √°nh x·∫° vƒÉn b·∫£n th√¥ th√†nh chu·ªói token r·ªùi r·∫°c:

S = (c_1, c_2, ..., c_n)

$$
T = (t_1, t_2, ..., t_m) V·ªõi: m \le n M·ªói token t_i c√≥ ƒë·ªô d√†i \ell((t_i) t√≠nh theo byte ho·∫∑c k√Ω t·ª± Unicode.
$$

Ph·∫ßn 2 c·ªßa d·ªØ li·ªáu th·ª±c nghi·ªám t·∫≠p trung v√†o:
	‚Ä¢	Ph√¢n b·ªë chi ti·∫øt ·ªü c√°c ƒë·ªô d√†i l·ªõn h∆°n
	‚Ä¢	S·ª± suy gi·∫£m s·ªë l∆∞·ª£ng token khi ƒë·ªô d√†i tƒÉng
	‚Ä¢	Quan h·ªá gi·ªØa ƒë·ªô d√†i v√† t·∫ßn su·∫•t xu·∫•t hi·ªán

‚∏ª

2. M√¥ h√¨nh h√≥a Ph√¢n b·ªë ƒê·ªô d√†i Subword

2.1 Ph√¢n b·ªë x√°c su·∫•t r·ªùi r·∫°c

G·ªçi:
	‚Ä¢	V: t·∫≠p t·ª´ v·ª±ng
	‚Ä¢	$N_k$: s·ªë token c√≥ ƒë·ªô d√†i k

Khi ƒë√≥:

$$
)P(L = k) = \frac{N_k}{|V|}
$$

V√†:

$$
\sum_{k=1}^{K_{\max}} P(L = k) = 1
$$

‚∏ª

2.2 Gi·∫£ thuy·∫øt ph√¢n b·ªë m≈©

D·ªØ li·ªáu th·ª±c nghi·ªám cho th·∫•y:

N_k \approx Ae^{-\lambda k}

Suy ra:

$P(L = k)$ = \frac{Ae^{-\lambda k}}{$\sum$_{j=1}^{K_{\max}} Ae^{-\lambda j}}

Chu·∫©n h√≥a:

$P(L = k)$ = $1 - e^{-\lambda}$ e^{-\lambda (k-1)}

ƒê√¢y l√† ph√¢n b·ªë h√¨nh h·ªçc r·ªùi r·∫°c.

‚∏ª

2.3 K·ª≥ v·ªçng v√† Ph∆∞∆°ng sai

K·ª≥ v·ªçng:

$$
\mathbb{E}[L] = \frac{1}{1 - e^{-\lambda}}
$$

Ph∆∞∆°ng sai:

$$
\mathrm{Var}L = \frac{e^{-\lambda}}{1 - e^{-\lambda}^2}
$$

ƒêi·ªÅu n√†y cho th·∫•y khi \lambda nh·ªè:
	‚Ä¢	ƒêu√¥i ph√¢n b·ªë d√†i h∆°n
	‚Ä¢	T·ªìn t·∫°i nhi·ªÅu token d√†i

‚∏ª

3. Li√™n h·ªá v·ªõi Lu·∫≠t Zipf

T·∫ßn su·∫•t token theo th·ª© h·∫°ng:

$$
fr \propto \frac{1}{r^\alpha}
$$

Trong ƒë√≥:
	‚Ä¢	r: th·ª© h·∫°ng

‚Ä¢	\alpha \approx 1

K·∫øt h·ª£p hai quan s√°t:
	‚Ä¢	Token ng·∫Øn ‚Üí t·∫ßn su·∫•t cao
	‚Ä¢	Token d√†i ‚Üí t·∫ßn su·∫•t th·∫•p

Ta c√≥ m√¥ h√¨nh k·∫øt h·ª£p:

$P(t)$ $\propto$ e^{-\beta $\ell((t)} \cdot \frac{1}{r^\alpha}

‚∏ª

4. ·∫¢nh h∆∞·ªüng ƒë·∫øn ƒê·ªô d√†i Chu·ªói v√† Chi ph√≠ Attention

Gi·∫£ s·ª≠ vƒÉn b·∫£n c√≥ t·ªïng s·ªë k√Ω t·ª± n.

S·ªë token:

)$

$$
m = \frac{n}{\mathbb{E}[L]} Self-attention c√≥ ƒë·ªô ph·ª©c t·∫°p: O(m^2) Thay v√†o:
$$

$O($\le$)$ft($\le$ft(\frac{n}{$\mathbb${E}[L]}\right)^2\right)

$$
Do ƒë√≥:
$$

‚Ä¢	N·∫øu \mathbb{E}[L] \uparrow \Rightarrow m \downarrow \Rightarrow \text{Cost} \downarrow

$$
‚Ä¢	N·∫øu token qu√° d√†i ‚Üí vocabulary l·ªõn ‚Üí tƒÉng chi ph√≠ embedding ‚∏ª 5. Entropy c·ªßa H·ªá Token Entropy: H = - \sum_{t \in V} P(t) \log P(t) Thay m√¥ h√¨nh m≈©: H \approx - \sum_{k} P(L=k) \log P(L=k) V·ªõi ph√¢n b·ªë h√¨nh h·ªçc: H = - \sum_{k=1}^{\infty} (1-q) q^{k-1} \log[(1-q) q^{k-1}] Trong ƒë√≥: q = e^{-\lambda} Entropy t·ªëi ∆∞u khi: ‚Ä¢	Kh√¥ng qu√° t·∫≠p trung v√†o token c·ª±c ng·∫Øn ‚Ä¢	Kh√¥ng qu√° ph√¢n t√°n ·ªü token d√†i ‚∏ª 6. Ki·ªÉm ƒë·ªãnh Ph√π h·ª£p M√¥ h√¨nh ƒê·ªÉ ki·ªÉm tra gi·∫£ thuy·∫øt ph√¢n b·ªë m≈©, c√≥ th·ªÉ s·ª≠ d·ª•ng: 6.1 H·ªìi quy log-linear
$$

$\log$ $N_k$ = $\log$ A - \lambda k

N·∫øu ƒë·ªì th·ªã \log N_k theo k tuy·∫øn t√≠nh ‚Üí x√°c nh·∫≠n m√¥ h√¨nh m≈©.

$$
‚∏ª 6.2 Ki·ªÉm ƒë·ªãnh Chi-square \chi^2 = \sum_{k} \frac{N_k - \hat{N}_k^2}{\hat{N}_k} So s√°nh v·ªõi ph√¢n b·ªë l√Ω thuy·∫øt. ‚∏ª 7. H√†m T·ªëi ∆∞u H√≥a Ng·∫ßm trong Tokenizer Tokenizer BPE th·ª±c ch·∫•t t·ªëi ∆∞u x·∫•p x·ªâ: \min_{V} \left( \mathbb{E}[m] + \lambda |V| \right) Trong ƒë√≥:
$$

‚Ä¢	\mathbb{E}[m]: s·ªë token trung b√¨nh

$$
‚Ä¢	|V|: k√≠ch th∆∞·ªõc t·ª´ v·ª±ng ‚Ä¢	\lambda: h·ªá s·ªë ƒëi·ªÅu ch·ªânh ƒê√¢y l√† b√†i to√°n c√¢n b·∫±ng gi·ªØa: ‚Ä¢	ƒê·ªô n√©n chu·ªói ‚Ä¢	K√≠ch th∆∞·ªõc embedding matrix ‚∏ª 8. Th·∫£o lu·∫≠n Ph·∫ßn 2 c·ªßa d·ªØ li·ªáu th·ª±c nghi·ªám cho th·∫•y: ‚Ä¢	Ph√¢n b·ªë kh√¥ng ho√†n to√†n tuy·∫øn t√≠nh ‚Ä¢	C√≥ ƒëu√¥i d√†i nh·∫π (heavy-tail) ‚Ä¢	M·ªôt s·ªë token ƒë·∫∑c bi·ªát d√†i ƒë·∫°i di·ªán cho chu·ªói ph·ªï bi·∫øn ƒêi·ªÅu n√†y ph√π h·ª£p v·ªõi l√Ω thuy·∫øt: ‚Ä¢	Ng√¥n ng·ªØ t·ª± nhi√™n c√≥ c·∫•u tr√∫c fractal ‚Ä¢	Zipf v√† ph√¢n b·ªë m≈© th∆∞·ªùng xu·∫•t hi·ªán trong h·ªá th·ªëng th√¥ng tin ‚∏ª 9. K·∫øt lu·∫≠n Ph√¢n b·ªë ƒë·ªô d√†i subword c√≥ th·ªÉ ƒë∆∞·ª£c m√¥ h√¨nh h√≥a g·∫ßn ƒë√∫ng b·∫±ng ph√¢n b·ªë m≈© r·ªùi r·∫°c: P(L = k) \sim e^{-\lambda k} T√°c ƒë·ªông tr·ª±c ti·∫øp ƒë·∫øn: m = \frac{n}{\mathbb{E}[L]} \text{Attention Cost} \sim O(m^2) H = - \sum P(t)\log P(t)
$$


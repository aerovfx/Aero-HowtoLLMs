
<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Ph√¢n t√≠ch Th·ªëng k√™ S·ªë l∆∞·ª£ng Token theo ƒê·ªô d√†i Subword (Ph·∫ßn 2): M√¥ h√¨nh h√≥a To√°n h·ªçc v√† H√†m Ph√¢n b·ªë

‚∏ª

T√≥m t·∫Øt

B√†i b√°o n√†y ti·∫øp t·ª•c ph√¢n t√≠ch th·ªëng k√™ s·ªë l∆∞·ª£ng token theo ƒë·ªô d√†i subword trong b·ªô tokenizer c·ªßa GPT-4, d·ª±a tr√™n d·ªØ li·ªáu th·ª±c nghi·ªám t·ª´ t√†i li·ªáu ƒë√≠nh k√®m (ph·∫ßn 2). Ch√∫ng t√¥i x√¢y d·ª±ng m√¥ h√¨nh to√°n h·ªçc cho ph√¢n b·ªë ƒë·ªô d√†i, ki·ªÉm ƒë·ªãnh gi·∫£ thuy·∫øt ph√¢n b·ªë m≈© v√† lu·∫≠t Zipf, ƒë·ªìng th·ªùi ph√¢n t√≠ch t√°c ƒë·ªông c·ªßa c·∫•u tr√∫c token ƒë·∫øn ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n trong ki·∫øn tr√∫c Transformer c·ªßa OpenAI. K·∫øt qu·∫£ cho th·∫•y ph√¢n b·ªë ƒë·ªô d√†i subword c√≥ xu h∆∞·ªõng suy gi·∫£m phi tuy·∫øn, g·∫ßn v·ªõi h√†m m≈© ho·∫∑c log-linear, v√† c√≥ m·ªëi li√™n h·ªá ch·∫∑t ch·∫Ω v·ªõi entropy h·ªá token.

‚∏ª

1. Gi·ªõi thi·ªáu

Trong c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLMs), tokenization l√† b∆∞·ªõc √°nh x·∫° vƒÉn b·∫£n th√¥ th√†nh chu·ªói token r·ªùi r·∫°c:

$$
S = (c_1, c_2, ..., c_n)
$$

$$
T = (t_1, t_2, ..., t_m)
$$

V·ªõi:

$$
m \le n
$$

M·ªói token t_i c√≥ ƒë·ªô d√†i \ell(t_i) t√≠nh theo byte ho·∫∑c k√Ω t·ª± Unicode.

Ph·∫ßn 2 c·ªßa d·ªØ li·ªáu th·ª±c nghi·ªám t·∫≠p trung v√†o:
	‚Ä¢	Ph√¢n b·ªë chi ti·∫øt ·ªü c√°c ƒë·ªô d√†i l·ªõn h∆°n
	‚Ä¢	S·ª± suy gi·∫£m s·ªë l∆∞·ª£ng token khi ƒë·ªô d√†i tƒÉng
	‚Ä¢	Quan h·ªá gi·ªØa ƒë·ªô d√†i v√† t·∫ßn su·∫•t xu·∫•t hi·ªán

‚∏ª

2. M√¥ h√¨nh h√≥a Ph√¢n b·ªë ƒê·ªô d√†i Subword

2.1 Ph√¢n b·ªë x√°c su·∫•t r·ªùi r·∫°c

G·ªçi:
	‚Ä¢	V: t·∫≠p t·ª´ v·ª±ng
	‚Ä¢	N_k: s·ªë token c√≥ ƒë·ªô d√†i k

Khi ƒë√≥:

$$
P(L = k) = \frac{N_k}{|V|}
$$

V√†:

$$
\sum_{k=1}^{K_{\max}} P(L = k) = 1
$$

‚∏ª

2.2 Gi·∫£ thuy·∫øt ph√¢n b·ªë m≈©

D·ªØ li·ªáu th·ª±c nghi·ªám cho th·∫•y:

N_k \approx Ae^{-\lambda k}

Suy ra:

$$
P(L = k) = \frac{Ae^{-\lambda k}}{\sum_{j=1}^{K_{\max}} Ae^{-\lambda j}}
$$

Chu·∫©n h√≥a:

$$
P(L = k) = (1 - e^{-\lambda}) e^{-\lambda (k-1)}
$$

ƒê√¢y l√† ph√¢n b·ªë h√¨nh h·ªçc r·ªùi r·∫°c.

‚∏ª

2.3 K·ª≥ v·ªçng v√† Ph∆∞∆°ng sai

K·ª≥ v·ªçng:

$$
\mathbb{E}[L] = \frac{1}{1 - e^{-\lambda}}
$$

Ph∆∞∆°ng sai:

$$
\mathrm{Var}(L) = \frac{e^{-\lambda}}{(1 - e^{-\lambda})^2}
$$

ƒêi·ªÅu n√†y cho th·∫•y khi \lambda nh·ªè:
	‚Ä¢	ƒêu√¥i ph√¢n b·ªë d√†i h∆°n
	‚Ä¢	T·ªìn t·∫°i nhi·ªÅu token d√†i

‚∏ª

3. Li√™n h·ªá v·ªõi Lu·∫≠t Zipf

T·∫ßn su·∫•t token theo th·ª© h·∫°ng:

$$
f(r) \propto \frac{1}{r^\alpha}
$$

Trong ƒë√≥:
	‚Ä¢	r: th·ª© h·∫°ng
	‚Ä¢	\alpha \approx 1

K·∫øt h·ª£p hai quan s√°t:
	‚Ä¢	Token ng·∫Øn ‚Üí t·∫ßn su·∫•t cao
	‚Ä¢	Token d√†i ‚Üí t·∫ßn su·∫•t th·∫•p

Ta c√≥ m√¥ h√¨nh k·∫øt h·ª£p:

$$
P(t) \propto e^{-\beta \ell(t)} \cdot \frac{1}{r^\alpha}
$$

‚∏ª

4. ·∫¢nh h∆∞·ªüng ƒë·∫øn ƒê·ªô d√†i Chu·ªói v√† Chi ph√≠ Attention

Gi·∫£ s·ª≠ vƒÉn b·∫£n c√≥ t·ªïng s·ªë k√Ω t·ª± n.

S·ªë token:

$$
m = \frac{n}{\mathbb{E}[L]}
$$

Self-attention c√≥ ƒë·ªô ph·ª©c t·∫°p:

$$
O(m^2)
$$

Thay v√†o:

$$
O\left(\left(\frac{n}{\mathbb{E}[L]}\right)^2\right)
$$

Do ƒë√≥:
	‚Ä¢	N·∫øu \mathbb{E}[L] \uparrow \Rightarrow m \downarrow \Rightarrow \text{Cost} \downarrow
	‚Ä¢	N·∫øu token qu√° d√†i ‚Üí vocabulary l·ªõn ‚Üí tƒÉng chi ph√≠ embedding

‚∏ª

5. Entropy c·ªßa H·ªá Token

Entropy:

$$
H = - \sum_{t \in V} P(t) \log P(t)
$$

Thay m√¥ h√¨nh m≈©:

$$
H \approx - \sum_{k} P(L=k) \log P(L=k)
$$

V·ªõi ph√¢n b·ªë h√¨nh h·ªçc:

$$
H = - \sum_{k=1}^{\infty} (1-q) q^{k-1} \log[(1-q) q^{k-1}]
$$

Trong ƒë√≥:

$$
q = e^{-\lambda}
$$

Entropy t·ªëi ∆∞u khi:
	‚Ä¢	Kh√¥ng qu√° t·∫≠p trung v√†o token c·ª±c ng·∫Øn
	‚Ä¢	Kh√¥ng qu√° ph√¢n t√°n ·ªü token d√†i

‚∏ª

6. Ki·ªÉm ƒë·ªãnh Ph√π h·ª£p M√¥ h√¨nh

ƒê·ªÉ ki·ªÉm tra gi·∫£ thuy·∫øt ph√¢n b·ªë m≈©, c√≥ th·ªÉ s·ª≠ d·ª•ng:

6.1 H·ªìi quy log-linear

$$
\log N_k = \log A - \lambda k
$$

N·∫øu ƒë·ªì th·ªã \log N_k theo k tuy·∫øn t√≠nh ‚Üí x√°c nh·∫≠n m√¥ h√¨nh m≈©.

‚∏ª

6.2 Ki·ªÉm ƒë·ªãnh Chi-square

$$
\chi^2 = \sum_{k} \frac{(N_k - \hat{N}_k)^2}{\hat{N}_k}
$$

So s√°nh v·ªõi ph√¢n b·ªë l√Ω thuy·∫øt.

‚∏ª

7. H√†m T·ªëi ∆∞u H√≥a Ng·∫ßm trong Tokenizer

Tokenizer BPE th·ª±c ch·∫•t t·ªëi ∆∞u x·∫•p x·ªâ:

$$
\min_{V} \left( \mathbb{E}[m] + \lambda |V| \right)
$$

Trong ƒë√≥:
	‚Ä¢	\mathbb{E}[m]: s·ªë token trung b√¨nh
	‚Ä¢	|V|: k√≠ch th∆∞·ªõc t·ª´ v·ª±ng
	‚Ä¢	\lambda: h·ªá s·ªë ƒëi·ªÅu ch·ªânh

ƒê√¢y l√† b√†i to√°n c√¢n b·∫±ng gi·ªØa:
	‚Ä¢	ƒê·ªô n√©n chu·ªói
	‚Ä¢	K√≠ch th∆∞·ªõc embedding matrix

‚∏ª

8. Th·∫£o lu·∫≠n

Ph·∫ßn 2 c·ªßa d·ªØ li·ªáu th·ª±c nghi·ªám cho th·∫•y:
	‚Ä¢	Ph√¢n b·ªë kh√¥ng ho√†n to√†n tuy·∫øn t√≠nh
	‚Ä¢	C√≥ ƒëu√¥i d√†i nh·∫π (heavy-tail)
	‚Ä¢	M·ªôt s·ªë token ƒë·∫∑c bi·ªát d√†i ƒë·∫°i di·ªán cho chu·ªói ph·ªï bi·∫øn

ƒêi·ªÅu n√†y ph√π h·ª£p v·ªõi l√Ω thuy·∫øt:
	‚Ä¢	Ng√¥n ng·ªØ t·ª± nhi√™n c√≥ c·∫•u tr√∫c fractal
	‚Ä¢	Zipf v√† ph√¢n b·ªë m≈© th∆∞·ªùng xu·∫•t hi·ªán trong h·ªá th·ªëng th√¥ng tin

‚∏ª

9. K·∫øt lu·∫≠n

Ph√¢n b·ªë ƒë·ªô d√†i subword c√≥ th·ªÉ ƒë∆∞·ª£c m√¥ h√¨nh h√≥a g·∫ßn ƒë√∫ng b·∫±ng ph√¢n b·ªë m≈© r·ªùi r·∫°c:

$$
P(L = k) \sim e^{-\lambda k}
$$

T√°c ƒë·ªông tr·ª±c ti·∫øp ƒë·∫øn:

$$
m = \frac{n}{\mathbb{E}[L]}
$$

\text{Attention Cost} \sim O(m^2)

H = - \sum P(t)\log P(t)

Do ƒë√≥, thi·∫øt k·∫ø tokenizer l√† b√†i to√°n t·ªëi ∆∞u ƒëa m·ª•c ti√™u gi·ªØa:
	‚Ä¢	ƒê·ªô d√†i chu·ªói
	‚Ä¢	K√≠ch th∆∞·ªõc t·ª´ v·ª±ng
	‚Ä¢	Entropy th√¥ng tin
	‚Ä¢	Chi ph√≠ t√≠nh to√°n

‚∏ª

T√†i li·ªáu tham kh·∫£o
	1.	Sennrich, R., Haddow, B., & Birch, A. (2016). Neural Machine Translation of Rare Words with Subword Units.
	2.	Vaswani, A. et al. (2017). Attention Is All You Need.
	3.	Shannon, C. (1948). A Mathematical Theory of Communication.
	4.	Kudo, T. (2018). Subword Regularization.
	5.	Brown, T. et al. (2020). Language Models are Few-Shot Learners.
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [T·∫°i sao vƒÉn b·∫£n c·∫ßn ƒë∆∞·ª£c ƒë√°nh s·ªë?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [Ph√¢n t√≠ch v√† chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh chu·ªói token ƒë∆∞·ª£c ƒë√°nh s·ªë: C∆° s·ªü to√°n h·ªçc v√† ·ª©ng d·ª•ng trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [T·∫°o v√† tr·ª±c quan h√≥a Token trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn: C∆° s·ªü to√°n h·ªçc v√† ph√¢n t√≠ch bi·ªÉu di·ªÖn](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [T·∫°o v√† tr·ª±c quan h√≥a Token (Ph·∫ßn 2): Ph√¢n t√≠ch h√¨nh h·ªçc kh√¥ng gian embedding v√† Attention Map trong m√¥ h√¨nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chu·∫©n b·ªã vƒÉn b·∫£n cho Tokenization trong m√¥ h√¨nh Transformer: C∆° s·ªü l√Ω thuy·∫øt v√† ph√¢n t√≠ch to√°n h·ªçc](aero_llm_05_preparing_text_for_tokenization.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_05_preparing_text_for_tokenization.md) |
| [Ph√¢n t√≠ch quy tr√¨nh Tokenization qua v√≠ d·ª• *The Time Machine*: C∆° s·ªü thu·∫≠t to√°n v√† m√¥ h√¨nh h√≥a to√°n h·ªçc](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So s√°nh Tokenization m·ª©c k√Ω t·ª±, t·ª´ v√† subword: Ph√¢n t√≠ch l√Ω thuy·∫øt v√† m√¥ h√¨nh to√°n h·ªçc](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thu·∫≠t to√°n Byte Pair Encoding (BPE) v√† B√†i to√°n T·ªëi ∆∞u K√≠ch th∆∞·ªõc T·ª´ v·ª±ng trong M√¥ h√¨nh Ng√¥n ng·ªØ](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| üìå **[aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md)** | [Xem b√†i vi·∫øt ‚Üí](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

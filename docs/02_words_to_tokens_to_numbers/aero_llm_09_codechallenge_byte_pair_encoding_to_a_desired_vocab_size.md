
<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thu·∫≠t to√°n Byte Pair Encoding (BPE) v√† B√†i to√°n T·ªëi ∆∞u K√≠ch th∆∞·ªõc T·ª´ v·ª±ng trong M√¥ h√¨nh Ng√¥n ng·ªØ

## T√≥m t·∫Øt

Trong c√°c m√¥ h√¨nh ng√¥n ng·ªØ hi·ªán ƒë·∫°i, ƒë·∫∑c bi·ªát l√† c√°c h·ªá th·ªëng d·ª±a tr√™n ki·∫øn tr√∫c Transformer, vi·ªác x√¢y d·ª±ng tokenizer ƒë√≥ng vai tr√≤ n·ªÅn t·∫£ng quy·∫øt ƒë·ªãnh hi·ªáu su·∫•t v√† chi ph√≠ t√≠nh to√°n. T√†i li·ªáu ƒë√≠nh k√®m tr√¨nh b√†y m·ªôt b√†i to√°n th·ª±c h√†nh: **tri·ªÉn khai thu·∫≠t to√°n Byte Pair Encoding (BPE) ƒë·ªÉ ƒë·∫°t k√≠ch th∆∞·ªõc t·ª´ v·ª±ng mong mu·ªën**. B√†i vi·∫øt n√†y ph√¢n t√≠ch c∆° s·ªü l√Ω thuy·∫øt c·ªßa BPE, m√¥ h√¨nh h√≥a to√°n h·ªçc qu√° tr√¨nh g·ªôp c·∫∑p, b√†i to√°n t·ªëi ∆∞u k√≠ch th∆∞·ªõc t·ª´ v·ª±ng, ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n v√† m·ªëi li√™n h·ªá v·ªõi hu·∫•n luy·ªán m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM).

---

## 1. Gi·ªõi thi·ªáu

C√°c m√¥ h√¨nh h·ªçc s√¢u x·ª≠ l√Ω vƒÉn b·∫£n th√¥ng qua √°nh x·∫°:

$$
\text{text} \rightarrow \text{tokens} \rightarrow \text{embedding vectors}
$$

M·ªôt tokenizer hi·ªáu qu·∫£ c·∫ßn:

- Gi·∫£m k√≠ch th∆∞·ªõc t·ª´ v·ª±ng $V$
- H·∫°n ch·∫ø token ngo√†i t·∫≠p hu·∫•n luy·ªán (OOV)
- Gi·ªØ ƒë·ªô d√†i chu·ªói $L$ ·ªü m·ª©c h·ª£p l√Ω

Byte Pair Encoding (BPE) ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t ban ƒë·∫ßu cho n√©n d·ªØ li·ªáu (Gage, 1994) v√† ƒë∆∞·ª£c √°p d·ª•ng cho NLP b·ªüi Sennrich et al. (2016). Hi·ªán nay, nhi·ªÅu m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn s·ª≠ d·ª•ng bi·∫øn th·ªÉ c·ªßa BPE.

---

## 2. M√¥ h√¨nh h√≥a b√†i to√°n BPE

### 2.1 Bi·ªÉu di·ªÖn d·ªØ li·ªáu

Gi·∫£ s·ª≠ t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán:

$$
\mathcal{D} = \{w_1, w_2, \dots, w_N\}
$$

M·ªói t·ª´ ƒë∆∞·ª£c bi·ªÉu di·ªÖn th√†nh chu·ªói k√Ω t·ª±:

$$
w_i = (c_1, c_2, \dots, c_m)
$$

T·∫≠p token ban ƒë·∫ßu:

$$
V_0 = \{ \text{t·∫•t c·∫£ k√Ω t·ª± xu·∫•t hi·ªán} \}
$$

---

### 2.2 H√†m ƒë·∫øm t·∫ßn su·∫•t c·∫∑p token

T·∫°i b∆∞·ªõc $k$, t·∫≠p token l√† $V_k$.

T·∫≠p c√°c c·∫∑p token li·ªÅn k·ªÅ:

$$
P_k = \{(t_i, t_{i+1})\}
$$

H√†m t·∫ßn su·∫•t:

$$
f_k(p) = \sum_{w \in \mathcal{D}} \text{count}(p, w)
$$

Ch·ªçn c·∫∑p t·ªëi ∆∞u:

$$
p_k^* = \arg\max_{p \in P_k} f_k(p)
$$

Sau ƒë√≥ c·∫≠p nh·∫≠t:

$$
V_{k+1} = V_k \cup \{ t_{new} \}
$$

Qu√° tr√¨nh d·ª´ng khi:

$$
|V_k| = V_{target}
$$

---

## 3. B√†i to√°n ƒë·∫°t k√≠ch th∆∞·ªõc t·ª´ v·ª±ng mong mu·ªën

Gi·∫£ s·ª≠:

- T·ª´ v·ª±ng ban ƒë·∫ßu: $|V_0| = C$
- S·ªë v√≤ng g·ªôp: $M$

Khi ƒë√≥:

$$
|V_M| = C + M
$$

N·∫øu mu·ªën:

$$
|V_M| = V_{target}
$$

Ta c·∫ßn:

$$
M = V_{target} - C
$$

Nh∆∞ v·∫≠y, b√†i to√°n tr·ªü th√†nh:

> Th·ª±c hi·ªán ch√≠nh x√°c $M$ ph√©p g·ªôp c√≥ t·∫ßn su·∫•t cao nh·∫•t.

---

## 4. Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n

### 4.1 M·ªói v√≤ng l·∫∑p

- ƒê·∫øm t·∫ßn su·∫•t t·∫•t c·∫£ c·∫∑p:

$$
\mathcal{O}(T)
$$

v·ªõi $T$ l√† t·ªïng s·ªë token trong t·∫≠p d·ªØ li·ªáu.

- Ch·ªçn c·∫∑p l·ªõn nh·∫•t:

$$
\mathcal{O}(|P_k|)
$$

### 4.2 T·ªïng th·ªÉ

V·ªõi $M$ v√≤ng l·∫∑p:

$$
\mathcal{O}(M \cdot T)
$$

Trong th·ª±c t·∫ø:

$$
T \approx 10^9 - 10^{12}
$$

Do ƒë√≥ c·∫ßn:
- C·∫•u tr√∫c heap
- C·∫≠p nh·∫≠t t·∫ßn su·∫•t c·ª•c b·ªô
- Ph√¢n m·∫£nh d·ªØ li·ªáu (sharding)

---

## 5. ·∫¢nh h∆∞·ªüng ƒë·∫øn M√¥ h√¨nh Ng√¥n ng·ªØ

### 5.1 S·ªë tham s·ªë embedding

Ma tr·∫≠n embedding:

$$
E \in \mathbb{R}^{V \times d}
$$

S·ªë tham s·ªë:

$$
\text{Params} = V \times d
$$

V√≠ d·ª•:

- $V = 50,000$
- $d = 4096$

$$
\text{Params} = 204,800,000
$$

N·∫øu tƒÉng $V$ l√™n 100,000:

$$
\text{Params} = 409,600,000
$$

Chi ph√≠ tƒÉng g·∫•p ƒë√¥i.

---

### 5.2 ·∫¢nh h∆∞·ªüng ƒë·∫øn Attention

Attention c√≥ ƒë·ªô ph·ª©c t·∫°p:

$$
\mathcal{O}(L^2 \cdot d)
$$

Trong ƒë√≥:
- $L$ l√† chi·ªÅu d√†i chu·ªói token.

N·∫øu token qu√° nh·ªè (character-level):

$$
L \uparrow \Rightarrow \text{Chi ph√≠ tƒÉng}
$$

N·∫øu token qu√° l·ªõn (word-level):

- OOV tƒÉng
- M·∫•t kh·∫£ nƒÉng ph√¢n t√≠ch h√¨nh th√°i

BPE c√¢n b·∫±ng hai y·∫øu t·ªë n√†y.

---

## 6. So s√°nh BPE v·ªõi WordPiece v√† Unigram LM

| Thu·∫≠t to√°n | Ti√™u ch√≠ t·ªëi ∆∞u | C∆° ch·∫ø |
|------------|----------------|---------|
| BPE | T·∫ßn su·∫•t c·∫∑p | G·ªôp l·∫∑p |
| WordPiece | Likelihood | Ch·ªçn c·∫∑p t·ªëi ƒëa h√≥a x√°c su·∫•t |
| Unigram LM | X√°c su·∫•t m√¥ h√¨nh | Lo·∫°i b·ªè token k√©m |

BPE l√† ph∆∞∆°ng ph√°p tham lam (greedy):

$$
\max f_k(p)
$$

Trong khi WordPiece t·ªëi ∆∞u:

$$
\max \log P(\mathcal{D} | V_k)
$$

---

## 7. M·ªëi li√™n h·ªá v·ªõi Hu·∫•n luy·ªán LLM

Gi·∫£ s·ª≠:

- T·ªïng token hu·∫•n luy·ªán: $T$
- K√≠ch th∆∞·ªõc m√¥ h√¨nh: $d$
- S·ªë l·ªõp: $L$

Chi ph√≠ hu·∫•n luy·ªán x·∫•p x·ªâ:

$$
\mathcal{O}(T \cdot L \cdot d^2)
$$

Vi·ªác ch·ªçn tokenizer ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn:

- $T$ (s·ªë token sau ph√¢n t√°ch)
- Hi·ªáu qu·∫£ t·ªïng qu√°t h√≥a
- Kh·∫£ nƒÉng bi·ªÉu di·ªÖn t·ª´ hi·∫øm

---

## 8. H·∫°n ch·∫ø

- Kh√¥ng x√©t ng·ªØ nghƒ©a khi g·ªôp
- Ph·ª• thu·ªôc d·ªØ li·ªáu hu·∫•n luy·ªán ban ƒë·∫ßu
- C√≥ th·ªÉ t·∫°o token kh√¥ng tr·ª±c quan

---

## 9. K·∫øt lu·∫≠n

Thu·∫≠t to√°n Byte Pair Encoding cung c·∫•p m·ªôt c∆° ch·∫ø ph√¢n t√°ch t·ª´ hi·ªáu qu·∫£, ƒë·∫∑c bi·ªát trong b·ªëi c·∫£nh m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn. B√†i to√°n ƒë·∫°t k√≠ch th∆∞·ªõc t·ª´ v·ª±ng mong mu·ªën c√≥ th·ªÉ ƒë∆∞·ª£c m√¥ h√¨nh h√≥a th√†nh vi·ªác th·ª±c hi·ªán ch√≠nh x√°c s·ªë v√≤ng g·ªôp c·∫ßn thi·∫øt:

$$
M = V_{target} - |V_0|
$$

Vi·ªác t·ªëi ∆∞u h√≥a BPE kh√¥ng ch·ªâ l√† b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω, m√† c√≤n ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn:

- B·ªô nh·ªõ
- Th·ªùi gian hu·∫•n luy·ªán
- Ch·∫•t l∆∞·ª£ng m√¥ h√¨nh

Trong t∆∞∆°ng lai, c√°c ph∆∞∆°ng ph√°p tokenizer th√≠ch nghi ƒë·ªông (adaptive tokenization) c√≥ th·ªÉ thay th·∫ø BPE truy·ªÅn th·ªëng nh·∫±m t·ªëi ∆∞u h√≥a t·ªët h∆°n theo m·ª•c ti√™u hu·∫•n luy·ªán.

---

## T√†i li·ªáu tham kh·∫£o

1. Gage, P. (1994). *A New Algorithm for Data Compression.*
2. Sennrich, R., Haddow, B., & Birch, A. (2016). *Neural Machine Translation of Rare Words with Subword Units.*
3. Vaswani, A. et al. (2017). *Attention Is All You Need.*
4. Kudo, T. (2018). *Subword Regularization.*
5. Devlin, J. et al. (2018). *BERT: Pre-training of Deep Bidirectional Transformers.*

---
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [T·∫°i sao vƒÉn b·∫£n c·∫ßn ƒë∆∞·ª£c ƒë√°nh s·ªë?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [Ph√¢n t√≠ch v√† chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh chu·ªói token ƒë∆∞·ª£c ƒë√°nh s·ªë: C∆° s·ªü to√°n h·ªçc v√† ·ª©ng d·ª•ng trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [T·∫°o v√† tr·ª±c quan h√≥a Token trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn: C∆° s·ªü to√°n h·ªçc v√† ph√¢n t√≠ch bi·ªÉu di·ªÖn](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [T·∫°o v√† tr·ª±c quan h√≥a Token (Ph·∫ßn 2): Ph√¢n t√≠ch h√¨nh h·ªçc kh√¥ng gian embedding v√† Attention Map trong m√¥ h√¨nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chu·∫©n b·ªã vƒÉn b·∫£n cho Tokenization trong m√¥ h√¨nh Transformer: C∆° s·ªü l√Ω thuy·∫øt v√† ph√¢n t√≠ch to√°n h·ªçc](aero_llm_05_preparing_text_for_tokenization.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_05_preparing_text_for_tokenization.md) |
| [Ph√¢n t√≠ch quy tr√¨nh Tokenization qua v√≠ d·ª• *The Time Machine*: C∆° s·ªü thu·∫≠t to√°n v√† m√¥ h√¨nh h√≥a to√°n h·ªçc](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So s√°nh Tokenization m·ª©c k√Ω t·ª±, t·ª´ v√† subword: Ph√¢n t√≠ch l√Ω thuy·∫øt v√† m√¥ h√¨nh to√°n h·ªçc](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| [aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_08_byte_pair_encoding_algorithm.md) |
| üìå **[Thu·∫≠t to√°n Byte Pair Encoding (BPE) v√† B√†i to√°n T·ªëi ∆∞u K√≠ch th∆∞·ªõc T·ª´ v·ª±ng trong M√¥ h√¨nh Ng√¥n ng·ªØ](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md)** | [Xem b√†i vi·∫øt ‚Üí](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

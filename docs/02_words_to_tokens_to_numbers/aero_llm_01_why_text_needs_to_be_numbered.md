
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?

## PhÃ¢n tÃ­ch khoa há»c vá» cáº¥u trÃºc, tÃ­nh toÃ¡n vÃ  tá»‘i Æ°u hÃ³a xá»­ lÃ½ ngÃ´n ngá»¯

---

## TÃ³m táº¯t

Trong cÃ¡c há»‡ thá»‘ng xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP) hiá»‡n Ä‘áº¡i, viá»‡c **Ä‘Ã¡nh sá»‘ vÄƒn báº£n (token numbering / positional indexing)** Ä‘Ã³ng vai trÃ² ná»n táº£ng trong biá»ƒu diá»…n chuá»—i, tÃ­nh toÃ¡n attention vÃ  tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh Transformer. BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch vai trÃ² cá»§a Ä‘Ã¡nh sá»‘ vÄƒn báº£n dÆ°á»›i gÃ³c Ä‘á»™ toÃ¡n há»c, khoa há»c nháº­n thá»©c vÃ  kiáº¿n trÃºc mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n. PhÃ¢n tÃ­ch dá»±a trÃªn kiáº¿n trÃºc Transformer cá»§a Vaswani et al. (2017) vÃ  cÃ¡c mÃ´ hÃ¬nh GPT do OpenAI phÃ¡t triá»ƒn.

---

# 1. Giá»›i thiá»‡u

Con ngÆ°á»i hiá»ƒu vÄƒn báº£n theo trÃ¬nh tá»± tuyáº¿n tÃ­nh. MÃ¡y tÃ­nh cÅ©ng cáº§n má»™t cÆ¡ cháº¿ tÆ°Æ¡ng tá»± Ä‘á»ƒ:

* PhÃ¢n biá»‡t vá»‹ trÃ­ token
* XÃ¡c Ä‘á»‹nh quan há»‡ phá»¥ thuá»™c
* TÃ­nh toÃ¡n attention

Náº¿u khÃ´ng Ä‘Ã¡nh sá»‘ hoáº·c mÃ£ hÃ³a vá»‹ trÃ­, chuá»—i:

> â€œTÃ´i yÃªu AIâ€

vÃ 

> â€œAI yÃªu tÃ´iâ€

sáº½ cÃ³ cÃ¹ng táº­p token nhÆ°ng Ã½ nghÄ©a hoÃ n toÃ n khÃ¡c.

Váº¥n Ä‘á» nÃ y dáº«n Ä‘áº¿n nhu cáº§u **positional encoding** trong cÃ¡c mÃ´ hÃ¬nh Transformer.

---

# 2. Biá»ƒu diá»…n chuá»—i dÆ°á»›i dáº¡ng toÃ¡n há»c

Giáº£ sá»­ má»™t cÃ¢u gá»“m $T$ token:

$$

$$

x = (x_1, x_2, ..., x_T)

$$

$$

Má»—i token Ä‘Æ°á»£c Ã¡nh xáº¡ thÃ nh vector embedding:

$$

$$

e_i = E(x_i)

$$

$$

Náº¿u khÃ´ng cÃ³ Ä‘Ã¡nh sá»‘ vá»‹ trÃ­, ta chá»‰ cÃ³:

$$

$$

X = (e_1, e_2, ..., e_T)

$$

$$

NhÆ°ng self-attention thuáº§n tÃºy lÃ  **báº¥t biáº¿n hoÃ¡n vá»‹ (permutation invariant)**.

Äiá»u nÃ y cÃ³ nghÄ©a:

$$
\text{Attention}(X) = \text{Attention}(PX)
$$

vá»›i $$P( lÃ  ma tráº­n hoÃ¡n vá»‹.

Do Ä‘Ã³, mÃ´ hÃ¬nh khÃ´ng phÃ¢n biá»‡t thá»© tá»±.

---

# 3. Positional Encoding

## 3.1. MÃ£ hÃ³a vá»‹ trÃ­ sin-cos

Transformer nguyÃªn báº£n sá»­ dá»¥ng:

)$$

$$
PE(pos, 2i) = \sin $\le$ft( \frac{pos}{10000^{2i/d}} \right)
$$

$$

$$

$$
PE(pos, 2i+1) = \cos $\le$ft( \frac{pos}{10000^{2i/d}} \right)
$$

$$
Trong Ä‘Ã³: * (pos): vá»‹ trÃ­ token * $i$: chá»‰ sá»‘ chiá»u embedding * $d$: kÃ­ch thÆ°á»›c embedding Vector Ä‘áº§u vÃ o:
$$

$$
z_i = e_i + PE(i)
$$

$$
--- ## 3.2. Positional Embedding há»c Ä‘Æ°á»£c Trong GPT:
$$

$$
z_i = e_i + p_i
$$

$$
vá»›i $p_i$ lÃ  tham sá»‘ há»c Ä‘Æ°á»£c. Äiá»u nÃ y cho phÃ©p mÃ´ hÃ¬nh tá»‘i Æ°u trá»±c tiáº¿p biá»ƒu diá»…n vá»‹ trÃ­. --- # 4. Vai trÃ² cá»§a Ä‘Ã¡nh sá»‘ trong Self-Attention Attention Ä‘Æ°á»£c tÃ­nh:
$$

$$
\text{Attention}(Q,K,V) = \text{softmax} $\le$ft( \frac{QK^T}{\sqrt{d_k}} \right)V
$$

$$
Trong Ä‘Ã³:
$$

$$
Q = ZW_Q, \quad K = ZW_K
$$

$$
Náº¿u $Z$ khÃ´ng chá»©a thÃ´ng tin vá»‹ trÃ­:
$$

QK^T

$$
chá»‰ pháº£n Ã¡nh ná»™i dung, khÃ´ng pháº£n Ã¡nh thá»© tá»±. Khi cÃ³ positional encoding:
$$

$$
Z = E + P
$$

$$
attention cÃ³ thá»ƒ há»c: * Quan há»‡ xa * Phá»¥ thuá»™c cÃº phÃ¡p * Quan há»‡ nguyÃªn nhÃ¢n â€“ káº¿t quáº£ --- # 5. ÄÃ¡nh sá»‘ vÄƒn báº£n trong huáº¥n luyá»‡n mÃ´ hÃ¬nh ngÃ´n ngá»¯ MÃ´ hÃ¬nh GPT tá»‘i Æ°u:
$$

$$
P(x) = $\prod$_{t=1}^{T} P(x_t \mid x_{\lt t})
$$

$$
Äiá»u kiá»‡n $x_{\lt t}$ phá»¥ thuá»™c trá»±c tiáº¿p vÃ o thá»© tá»±. Causal masking:
$$

$$
M_{ij} = \begin{cases} 0 & j $\le$ i \\ -$\infty$ & j > i \end{cases}
$$

$$
Ma tráº­n attention thá»±c táº¿:
$$

$$
\text{softmax} $\le$ft( \frac{QK^T}{\sqrt{d_k}} + M \right)
$$

$$
ÄÃ¡nh sá»‘ vá»‹ trÃ­ cho phÃ©p xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c token nÃ o thuá»™c $x_{\lt t}$. --- # 6. ÄÃ¡nh sá»‘ vÃ  tá»‘i Æ°u hÃ³a tÃ­nh toÃ¡n Self-attention cÃ³ Ä‘á»™ phá»©c táº¡p:
$$

$\mathcal${O}(T^2 d)

$$
Khi tÄƒng chiá»u dÃ i vÄƒn báº£n $T$:
$$

\text{Compute} $\propto$ T^2

$$
Viá»‡c Ä‘Ã¡nh sá»‘ giÃºp: * Quáº£n lÃ½ cá»­a sá»• ngá»¯ cáº£nh * Chia chunk * Triá»ƒn khai sliding window --- # 7. áº¢nh hÆ°á»Ÿng trong Reinforcement Learning from Human Feedback Trong RLHF, chuá»—i gá»“m:
$$

x = [\text{Prompt}; \text{Response}]

$$
ÄÃ¡nh sá»‘ cho phÃ©p: * PhÃ¢n biá»‡t pháº§n prompt vÃ  response * Mask loss chÃ­nh xÃ¡c Loss:
$$

$$
$\mathcal${L} = -$\sum$_{t \in R} $\log$ P(x_t \mid x_{\lt t})
$$

$$
Náº¿u khÃ´ng Ä‘Ã¡nh sá»‘ rÃµ rÃ ng, mÃ´ hÃ¬nh khÃ´ng biáº¿t Ä‘Ã¢u lÃ  pháº§n cáº§n tá»‘i Æ°u. --- # 8. GÃ³c nhÃ¬n lÃ½ thuyáº¿t thÃ´ng tin Entropy cá»§a chuá»—i:
$$

$$
H(X) = - $\sum$_x P(x)$\log$ P(x)
$$

$$
Thá»© tá»± áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n entropy. VÃ­ dá»¥: * Chuá»—i cÃ³ cáº¥u trÃºc â†’ entropy tháº¥p * Chuá»—i ngáº«u nhiÃªn â†’ entropy cao ÄÃ¡nh sá»‘ giÃºp mÃ´ hÃ¬nh Æ°á»›c lÆ°á»£ng xÃ¡c suáº¥t chÃ­nh xÃ¡c hÆ¡n. --- # 9. So sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p encoding vá»‹ trÃ­ | PhÆ°Æ¡ng phÃ¡p   | CÃ´ng thá»©c       | Æ¯u Ä‘iá»ƒm           | NhÆ°á»£c Ä‘iá»ƒm         | | ------------- | --------------- | ----------------- | ------------------ | | Sin-Cos       | HÃ m lÆ°á»£ng giÃ¡c  | KhÃ´ng cáº§n há»c     | Cá»©ng               | | Learned       | Vector há»c Ä‘Æ°á»£c | Linh hoáº¡t         | Giá»›i háº¡n chiá»u dÃ i | | Rotary (RoPE) | PhÃ©p quay phá»©c  | Tá»•ng quÃ¡t hÃ³a tá»‘t | Phá»©c táº¡p           | | ALiBi         | Bias tuyáº¿n tÃ­nh | DÃ i ngá»¯ cáº£nh tá»‘t  | Giáº£m linh hoáº¡t     | --- # 10. Tháº£o luáº­n ÄÃ¡nh sá»‘ vÄƒn báº£n khÃ´ng chá»‰ lÃ  váº¥n Ä‘á» ká»¹ thuáº­t mÃ  lÃ : * Äiá»u kiá»‡n cáº§n cho mÃ´ hÃ¬nh hiá»ƒu ngá»¯ nghÄ©a * CÆ¡ sá»Ÿ cho attention hoáº¡t Ä‘á»™ng * Yáº¿u tá»‘ then chá»‘t trong huáº¥n luyá»‡n LLM Náº¿u bá» positional encoding:
$$

\text{Transformer} \to \text{Bag-of-Words Model}
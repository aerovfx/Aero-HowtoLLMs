
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [02 words to tokens to numbers](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
NhÆ° váº­y, tá»« â€œlowestâ€ cÃ³ thá»ƒ biá»ƒu diá»…n thÃ nh:

$$

\text{lowest} = \text{low} + \text{est}

$$

---

## 5. Biá»ƒu diá»…n Embedding vÃ  KÃ­ch thÆ°á»›c TÃ­nh toÃ¡n

Giáº£ sá»­:

- KÃ­ch thÆ°á»›c tá»« vá»±ng: $V$
- KÃ­ch thÆ°á»›c embedding: $d$

Ma tráº­n embedding:

$$

E \in \mathbb{R}^{V \times d}

$$

Sá»‘ tham sá»‘ cá»§a embedding:

$$

\text{Params} = V \times d

$$

Náº¿u dÃ¹ng word-level tokenization:

$$

V \approx 500,000

$$

Náº¿u dÃ¹ng BPE:

$$

V \approx 30,000 - 50,000

$$

Giáº£m sá»‘ tham sá»‘ Ä‘Ã¡ng ká»ƒ:

$$

\Delta = (V_{word} - V_{BPE}) \times d

$$

Äiá»u nÃ y giÃºp:
- Giáº£m bá»™ nhá»›
- TÄƒng tá»‘c huáº¥n luyá»‡n
- Cáº£i thiá»‡n kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a

---

## 6. BPE trong MÃ´ hÃ¬nh Transformer

Trong kiáº¿n trÃºc Transformer, chuá»—i token Ä‘Æ°á»£c Ã¡nh xáº¡ sang embedding:

$$

x_i = E(t_i)

$$

Sau Ä‘Ã³ Ä‘Æ°á»£c Ä‘Æ°a vÃ o cÆ¡ cháº¿ Attention:

$$

\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V

$$

Viá»‡c sá»­ dá»¥ng BPE giÃºp:

- Giáº£m chiá»u dÃ i chuá»—i so vá»›i character-level.
- Giá»¯ thÃ´ng tin hÃ¬nh thÃ¡i tá»‘t hÆ¡n word-level.
- Tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t attention.

---

## 7. So sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c

| PhÆ°Æ¡ng phÃ¡p | ÄÆ¡n vá»‹ | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|------------|--------|----------|------------|
| Word-level | Tá»« | Dá»… hiá»ƒu | OOV cao |
| Character-level | KÃ½ tá»± | KhÃ´ng OOV | Chuá»—i dÃ i |
| BPE | Subword | CÃ¢n báº±ng tá»‘t | Phá»¥ thuá»™c sá»‘ vÃ²ng gá»™p |

---

## 8. á»¨ng dá»¥ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n

CÃ¡c mÃ´ hÃ¬nh nhÆ° GPT sá»­ dá»¥ng biáº¿n thá»ƒ cá»§a BPE Ä‘á»ƒ xÃ¢y dá»±ng tokenizer. Vá»›i dá»¯ liá»‡u huáº¥n luyá»‡n hÃ ng trÄƒm tá»· token, BPE cho phÃ©p:

- NÃ©n biá»ƒu diá»…n tá»« vá»±ng.
- TÄƒng kháº£ nÄƒng há»c cáº¥u trÃºc ngÃ´n ngá»¯.
- Xá»­ lÃ½ tá»‘t tá»« hiáº¿m vÃ  tá»« má»›i.

Giáº£ sá»­ tá»•ng sá»‘ token huáº¥n luyá»‡n:

$$

T = 10^{11}

$$

Thá»i gian huáº¥n luyá»‡n phá»¥ thuá»™c vÃ o:

$$

\mathcal{O}(T \cdot L \cdot d^2)

$$

Trong Ä‘Ã³:
- $L$: chiá»u dÃ i chuá»—i
- $d$: kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh

BPE giÃºp giáº£m $L$ so vá»›i character-level â†’ giáº£m chi phÃ­ tÃ­nh toÃ¡n.

---

## 9. Háº¡n cháº¿ cá»§a BPE

- KhÃ´ng xÃ©t ngá»¯ nghÄ©a khi gá»™p token.
- CÃ³ thá»ƒ táº¡o token khÃ´ng trá»±c quan.
- Phá»¥ thuá»™c máº¡nh vÃ o dá»¯ liá»‡u huáº¥n luyá»‡n ban Ä‘áº§u.

---

## 10. Káº¿t luáº­n

Byte Pair Encoding lÃ  má»™t phÆ°Æ¡ng phÃ¡p phÃ¢n tÃ¡ch tá»« hiá»‡u quáº£, Ä‘Ã³ng vai trÃ² ná»n táº£ng trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡n Ä‘áº¡i. Nhá» kháº£ nÄƒng cÃ¢n báº±ng giá»¯a kÃ­ch thÆ°á»›c tá»« vá»±ng vÃ  chiá»u dÃ i chuá»—i, BPE giÃºp tá»‘i Æ°u hÃ³a cáº£ bá»™ nhá»› vÃ  hiá»‡u suáº¥t tÃ­nh toÃ¡n.

Trong bá»‘i cáº£nh cÃ¡c mÃ´ hÃ¬nh ngÃ y cÃ ng lá»›n (hÃ ng trÄƒm tá»· tham sá»‘), viá»‡c tá»‘i Æ°u tokenizer nhÆ° BPE khÃ´ng chá»‰ lÃ  bÆ°á»›c tiá»n xá»­ lÃ½, mÃ  cÃ²n áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n hiá»‡u quáº£ huáº¥n luyá»‡n vÃ  suy luáº­n.

---

## TÃ i liá»‡u tham kháº£o

1. Gage, P. (1994). *A New Algorithm for Data Compression.*
2. Sennrich, R., Haddow, B., & Birch, A. (2016). *Neural Machine Translation of Rare Words with Subword Units.*
3. Vaswani, A. et al. (2017). *Attention Is All You Need.*
4. Radford, A. et al. (2019). *Language Models are Unsupervised Multitask Learners.*

---
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_llm_01_why_text_needs_to_be_numbered.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_why_text_needs_to_be_numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_llm_02_parsing_text_to_numbered_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_parsing_text_to_numbered_tokens.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_create_and_visualize_tokens_part_1_.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_create_and_visualize_tokens_part_2_.md) |
| [Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_llm_05_preparing_text_for_tokenization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_preparing_text_for_tokenization.md) |
| [PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_tokenizing_the_time_machine.md) |
| [So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_tokenizing_characters_vs_subwords_vs_words.md) |
| ğŸ“Œ **[aero llm 08 byte pair encoding algorithm](aero_llm_08_byte_pair_encoding_algorithm.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_08_byte_pair_encoding_algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_byte_pair_encoding_to_a_desired_vocab_size.md) |
| [aero llm 10 exploring chatgpt4 s tokenizer](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_exploring_chatgpt4_s_tokenizer.md) |
| [aero llm 11 codechallenge token count by subword length part 1](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_token_count_by_subword_length_part_1_.md) |
| [aero llm 12 codechallenge token count by subword length part 2](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_token_count_by_subword_length_part_2_.md) |
| [aero llm 13 how many rs in strawberry](aero_llm_13_how_many_rs_in_strawberry.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_how_many_rs_in_strawberry.md) |
| [aero llm 14 codechallenge create your algorithmic rapper name](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_create_your_algorithmic_rapper_name_.md) |
| [aero llm 15 tokenization in bert](aero_llm_15_tokenization_in_bert.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_tokenization_in_bert.md) |
| [aero llm 16 codechallenge character counts in bert tokens](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_codechallenge_character_counts_in_bert_tokens.md) |
| [aero llm 17 translating between tokenizers](aero_llm_17_translating_between_tokenizers.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_translating_between_tokenizers.md) |
| [aero llm 18 codechallenge more on token translation](aero_llm_18_codechallenge_more_on_token_translation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_more_on_token_translation.md) |
| [aero llm 19 codechallenge tokenization compression ratios](aero_llm_19_codechallenge_tokenization_compression_ratios.md) | [Xem bÃ i viáº¿t â†’](aero_llm_19_codechallenge_tokenization_compression_ratios.md) |
| [aero llm 20 tokenization in different languages](aero_llm_20_tokenization_in_different_languages.md) | [Xem bÃ i viáº¿t â†’](aero_llm_20_tokenization_in_different_languages.md) |
| [aero llm 21 codechallenge zipf s law in characters and tokens](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) | [Xem bÃ i viáº¿t â†’](aero_llm_21_codechallenge_zipf_s_law_in_characters_and_tokens.md) |
| [aero llm 22 word variations in claude tokenizer](aero_llm_22_word_variations_in_claude_tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_22_word_variations_in_claude_tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

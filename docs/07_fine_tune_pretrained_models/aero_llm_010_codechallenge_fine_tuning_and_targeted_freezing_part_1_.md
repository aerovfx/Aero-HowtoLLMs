
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [07 fine tune pretrained models](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Fine-tuning CÃ³ Má»¥c TiÃªu vÃ  ÄÃ³ng BÄƒng ChÃ­nh XÃ¡c Trá»ng Sá»‘ Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n

## TÃ³m táº¯t

BÃ i viáº¿t nÃ y nghiÃªn cá»©u phÆ°Æ¡ng phÃ¡p fine-tuning cÃ³ má»¥c tiÃªu (targeted fine-tuning) káº¿t há»£p vá»›i Ä‘Ã³ng bÄƒng chÃ­nh xÃ¡c (precision freezing) má»™t pháº§n trá»ng sá»‘ trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n. Thá»±c nghiá»‡m Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn táº­p dá»¯ liá»‡u tá»« tiá»ƒu thuyáº¿t *Moby-Dick* cá»§a *Herman Melville*. Hai mÃ´ hÃ¬nh giá»‘ng há»‡t nhau Ä‘Æ°á»£c huáº¥n luyá»‡n song song: má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã³ng bÄƒng chá»n lá»c cÃ¡c lá»›p attention, vÃ  má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n toÃ n pháº§n. Káº¿t quáº£ cho tháº¥y chiáº¿n lÆ°á»£c Ä‘Ã³ng bÄƒng cÃ³ má»¥c tiÃªu giÃºp giáº£m chi phÃ­ tÃ­nh toÃ¡n, háº¡n cháº¿ overfitting vÃ  váº«n duy trÃ¬ hiá»‡u quáº£ há»c táº­p.

---

## 1. Giá»›i thiá»‡u

Fine-tuning lÃ  ká»¹ thuáº­t quan trá»ng giÃºp thÃ­ch nghi mÃ´ hÃ¬nh tiá»n huáº¥n luyá»‡n vá»›i dá»¯ liá»‡u chuyÃªn biá»‡t. Tuy nhiÃªn, viá»‡c cáº­p nháº­t toÃ n bá»™ tham sá»‘ trong cÃ¡c mÃ´ hÃ¬nh lá»›n thÆ°á»ng:

* Tá»‘n nhiá»u tÃ i nguyÃªn,
* Dá»… gÃ¢y quÃ¡ khá»›p,
* KhÃ³ kiá»ƒm soÃ¡t quÃ¡ trÃ¬nh há»c.

TÃ i liá»‡u thá»±c nghiá»‡m  Ä‘á» xuáº¥t phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n song song hai mÃ´ hÃ¬nh: má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã³ng bÄƒng chá»n lá»c cÃ¡c lá»›p attention á»Ÿ táº§ng tháº¥p, vÃ  má»™t mÃ´ hÃ¬nh huáº¥n luyá»‡n Ä‘áº§y Ä‘á»§, nháº±m Ä‘Ã¡nh giÃ¡ tÃ¡c Ä‘á»™ng cá»§a precision freezing.

Má»¥c tiÃªu nghiÃªn cá»©u:

* PhÃ¢n tÃ­ch cÆ¡ cháº¿ fine-tuning cÃ³ má»¥c tiÃªu,
* ÄÃ¡nh giÃ¡ áº£nh hÆ°á»Ÿng cá»§a Ä‘Ã³ng bÄƒng attention,
* So sÃ¡nh hiá»‡u quáº£ huáº¥n luyá»‡n vÃ  chi phÃ­ tÃ­nh toÃ¡n.

---

## 2. CÆ¡ sá»Ÿ lÃ½ thuyáº¿t

### 2.1. MÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»± há»“i quy

Cho chuá»—i token:

$$

$$

X = (x_1, x_2, \dots, x_n)

$$

$$

XÃ¡c suáº¥t sinh chuá»—i:

$$

$$

P(X)=\prod_{i=1}^{n} P(x_i \mid x_1,\dots,x_{i-1})

$$

$$

MÃ´ hÃ¬nh dá»± Ä‘oÃ¡n token tiáº¿p theo dá»±a trÃªn toÃ n bá»™ ngá»¯ cáº£nh trÆ°á»›c Ä‘Ã³.

---

### 2.2. CÆ¡ cháº¿ Attention trong Transformer

Trong má»™t lá»›p Transformer, self-attention Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi:

$$

$$

Q = XW_Q,\quad K = XW_K,\quad V = XW_V

$$

$$

$$

$$

\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V

$$

$$

Trong Ä‘Ã³:

* ($W_Q$, $W_K$, $W_V$): ma tráº­n truy váº¥n, khÃ³a vÃ  giÃ¡ trá»‹,
* $d_k$: sá»‘ chiá»u cá»§a vector key.

CÃ¡c ma tráº­n nÃ y lÃ  trá»ng tÃ¢m cá»§a chiáº¿n lÆ°á»£c fine-tuning cÃ³ má»¥c tiÃªu.

---

### 2.3. HÃ m máº¥t mÃ¡t vÃ  cáº­p nháº­t tham sá»‘

HÃ m máº¥t mÃ¡t Cross-Entropy:

$$

$$

$\mathcal${L} = -\frac{1}{N}$\sum$_{i=1}^{N}$\log$ P($y_i$ \mid $x_i$)

$$

$$

Quy táº¯c cáº­p nháº­t:

$$

$$

\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}

$$

$$

vá»›i $\eta$ lÃ  learning rate.

Náº¿u tham sá»‘ bá»‹ Ä‘Ã³ng bÄƒng:

$$

$$

$\nabla$_\theta $\mathcal${L} = 0

$$

$$

â‡’ khÃ´ng Ä‘Æ°á»£c cáº­p nháº­t.

---

## 3. PhÆ°Æ¡ng phÃ¡p nghiÃªn cá»©u

### 3.1. Dá»¯ liá»‡u

Nguá»“n dá»¯ liá»‡u lÃ  vÄƒn báº£n *Moby-Dick*, gá»“m:

$$

$$

N_{total} \approx 350,000

$$

$$

token, trong Ä‘Ã³ chá»‰ khoáº£ng:

$$

$$

N_{unique} \approx 17,000

$$

$$

token lÃ  duy nháº¥t .

---

### 3.2. Khá»Ÿi táº¡o mÃ´ hÃ¬nh

Hai mÃ´ hÃ¬nh giá»‘ng há»‡t nhau Ä‘Æ°á»£c táº£i:

* MÃ´ hÃ¬nh Train: huáº¥n luyá»‡n toÃ n bá»™.
* MÃ´ hÃ¬nh Freeze: Ä‘Ã³ng bÄƒng cÃ³ má»¥c tiÃªu.

Ban Ä‘áº§u:

$$

$$

\theta_{\text{train}}^{(0)} = \theta_{\text{freeze}}^{(0)}

$$

$$

---

### 3.3. Thá»‘ng kÃª token phá»• biáº¿n

Táº§n suáº¥t token:

$$

$$

f(w)=\sum_{i=1}^{N}\mathbf{1}(x_i=w)

$$

$$

Chá»n táº­p 100 token phá»• biáº¿n nháº¥t:

$$

$$

S_{100}={w_1,\dots,w_{100}}

$$

$$

---

### 3.4. ÄÃ¡nh giÃ¡ tá»· lá»‡ token sinh

Cho chuá»—i sinh:

$$

$$

G=(g_1,\dots,g_M)

$$

$$

Tá»· lá»‡ token phá»• biáº¿n:

$$

$$

p=\frac{1}{M}\sum_{i=1}^{M}\mathbf{1}(g_i\in S_{100})

$$

$$

Chá»‰ sá»‘ nÃ y pháº£n Ã¡nh má»©c Ä‘á»™ mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c phong cÃ¡ch vÄƒn báº£n.

---

### 3.5. Chiáº¿n lÆ°á»£c Ä‘Ã³ng bÄƒng cÃ³ má»¥c tiÃªu

Theo tÃ i liá»‡u , chá»‰ huáº¥n luyá»‡n:

* Trá»ng sá»‘ ($W_Q$, $W_K$, $W_V$),
* Trong cÃ¡c block Transformer tá»« táº§ng 6 trá»Ÿ lÃªn.

MÃ´ táº£ toÃ¡n há»c:

$$

$$

\theta_i = \begin{cases} \text{trainable}, & i \in \mathcal{A}_{6+} \\ \text{frozen}, & \text{ngÆ°á»£c láº¡i} \end{cases}

$$

$$

vá»›i \mathcal{A}_{6+} lÃ  táº­p attention layer tá»« block 6 trá»Ÿ lÃªn.

$$
--- ## 4. Theo dÃµi quÃ¡ trÃ¬nh huáº¥n luyá»‡n ### 4.1. Äo thá»i gian huáº¥n luyá»‡n Thá»i gian má»—i vÃ²ng láº·p:
$$

$$
t_k = t_k^{end}-t_k^{start}
$$

$$
Tá»•ng thá»i gian:
$$

$$
T=\sum_{k=1}^{K} t_k
$$

$$
So sÃ¡nh T_{\text{freeze}} vÃ  T_{\text{train}}. --- ### 4.2. Theo dÃµi biáº¿n Ä‘á»•i trá»ng sá»‘ Cho ma tráº­n táº¡i bÆ°á»›c t:
$$

W_t

$$
Hiá»‡u giá»¯a hai bÆ°á»›c:
$$

$$
\Delta W_t = W_t - W_{t-1}
$$

$$
Chuáº©n Frobenius:
$$

|\Delta $W_t$|_F = \sqrt{$\sum$_{i,j}(\Delta W_{ij})^2}

$$
Chuáº©n lá»›n â‡’ cáº­p nháº­t máº¡nh. Chuáº©n nhá» â‡’ cáº­p nháº­t yáº¿u. --- ### 4.3. Theo dÃµi hÃ m máº¥t mÃ¡t Loss trung bÃ¬nh:
$$

$$
\bar{\mathcal{L}} = \frac{1}{K}\sum_{k=1}^{K}\mathcal{L}_k
$$

$$
DÃ¹ng Ä‘á»ƒ so sÃ¡nh tá»‘c Ä‘á»™ há»™i tá»¥ cá»§a hai mÃ´ hÃ¬nh. --- ## 5. Káº¿t quáº£ thá»±c nghiá»‡m ### 5.1. TrÆ°á»›c fine-tuning Tá»· lá»‡ token phá»• biáº¿n:
$$

$$
p_{\text{train}}\approx 47%,\quad p_{\text{freeze}}\approx 44%
$$

$$
Hai mÃ´ hÃ¬nh gáº§n nhÆ° tÆ°Æ¡ng Ä‘Æ°Æ¡ng . --- ### 5.2. Sau fine-tuning Quan sÃ¡t cho tháº¥y: * MÃ´ hÃ¬nh Train: há»c máº¡nh nhÆ°ng dá»… overfit. * MÃ´ hÃ¬nh Freeze: há»c á»•n Ä‘á»‹nh hÆ¡n.
$$

p_{\text{freeze}}^{post} > p_{\text{freeze}}^{pre}

$$
vÃ  cÃ³ Ä‘á»™ biáº¿n Ä‘á»™ng nhá» hÆ¡n. --- ### 5.3. Chi phÃ­ tÃ­nh toÃ¡n Sá»‘ tham sá»‘ huáº¥n luyá»‡n:
$$

P_{\text{freeze}} \ll P_{\text{train}}

$$
Do Ä‘Ã³:
$$

T_{\text{freeze}} \lt  T_{\text{train}}

$$
--- ## 6. Tháº£o luáº­n ### 6.1. Æ¯u Ä‘iá»ƒm 1. Giáº£m thá»i gian huáº¥n luyá»‡n. 2. Tiáº¿t kiá»‡m bá»™ nhá»›. 3. Háº¡n cháº¿ overfitting. 4. Báº£o toÃ n tri thá»©c ná»n. --- ### 6.2. Háº¡n cháº¿ * Kháº£ nÄƒng thÃ­ch nghi bá»‹ giá»›i háº¡n. * Phá»¥ thuá»™c cáº¥u hÃ¬nh Ä‘Ã³ng bÄƒng. * Cáº§n nhiá»u thá»­ nghiá»‡m Ä‘á»ƒ tá»‘i Æ°u. --- ### 6.3. Chiáº¿n lÆ°á»£c má»Ÿ rá»™ng #### ÄÃ³ng bÄƒng tá»«ng pháº§n theo thá»i gian
$$

$$
\theta_i(t)= \begin{cases} \text{frozen}, & t\lt t_0\ \text{trainable}, & t\ge t_0 \end{cases}
$$

$$
#### Káº¿t há»£p LoRA/Adapter Giá»¯ nguyÃªn \theta, thÃªm tham sá»‘ phá»¥ \phi:
$$

$$
y = f(x;\theta)+g(x;\phi)
$$


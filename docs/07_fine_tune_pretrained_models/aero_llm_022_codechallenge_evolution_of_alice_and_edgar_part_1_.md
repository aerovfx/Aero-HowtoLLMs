
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [07 fine tune pretrained models](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Äá»“ng Tiáº¿n HÃ³a MÃ´ HÃ¬nh Sinh VÄƒn Báº£n vÃ  MÃ´ HÃ¬nh PhÃ¢n Loáº¡i: TrÆ°á»ng Há»£p Alice vÃ  Edgar

## TÃ³m táº¯t

BÃ i viáº¿t nÃ y trÃ¬nh bÃ y phÆ°Æ¡ng phÃ¡p káº¿t há»£p giá»¯a mÃ´ hÃ¬nh sinh vÄƒn báº£n vÃ  mÃ´ hÃ¬nh phÃ¢n loáº¡i nháº±m Ä‘Ã¡nh giÃ¡ quÃ¡ trÃ¬nh fine-tuning theo phong cÃ¡ch vÄƒn há»c. Dá»±a trÃªn tÃ i liá»‡u thá»±c nghiá»‡m , nghiÃªn cá»©u sá»­ dá»¥ng ba mÃ´ hÃ¬nh trong cÃ¹ng má»™t mÃ´i trÆ°á»ng: má»™t bá»™ phÃ¢n loáº¡i BERT vÃ  hai mÃ´ hÃ¬nh sinh vÄƒn báº£n Ä‘Æ°á»£c fine-tune theo phong cÃ¡ch Alice vÃ  Edgar. Bá»™ phÃ¢n loáº¡i Ä‘Æ°á»£c dÃ¹ng nhÆ° má»™t â€œgiÃ¡m kháº£o tá»± Ä‘á»™ngâ€ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng sinh vÄƒn báº£n. CÃ¡c cÃ´ng thá»©c toÃ¡n há»c Ä‘Æ°á»£c sá»­ dá»¥ng nháº±m mÃ´ hÃ¬nh hÃ³a quÃ¡ trÃ¬nh huáº¥n luyá»‡n, chuyá»ƒn Ä‘á»•i tokenizer vÃ  Ä‘Ã¡nh giÃ¡ hiá»‡u nÄƒng.

---

## 1. Giá»›i thiá»‡u

Trong lÄ©nh vá»±c xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP), viá»‡c Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng mÃ´ hÃ¬nh sinh vÄƒn báº£n theo phong cÃ¡ch (style transfer) váº«n lÃ  má»™t thÃ¡ch thá»©c. PhÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡ thá»§ cÃ´ng tá»‘n nhiá»u thá»i gian vÃ  thiáº¿u tÃ­nh khÃ¡ch quan.

Theo tÃ i liá»‡u , tÃ¡c giáº£ Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p thay tháº¿: sá»­ dá»¥ng mÃ´ hÃ¬nh phÃ¢n loáº¡i dá»±a trÃªn **BERT** Ä‘á»ƒ phÃ¢n biá»‡t vÄƒn báº£n do hai mÃ´ hÃ¬nh sinh táº¡o ra, tá»« Ä‘Ã³ Ä‘Ã¡nh giÃ¡ giÃ¡n tiáº¿p hiá»‡u quáº£ fine-tuning.

Hai phong cÃ¡ch vÄƒn há»c Ä‘Æ°á»£c lá»±a chá»n dá»±a trÃªn:

* *Alice's Adventures in Wonderland* â€“ **Lewis Carroll**
* TÃ¡c pháº©m cá»§a **Edgar Allan Poe**

Má»¥c tiÃªu nghiÃªn cá»©u:

* PhÃ¢n tÃ­ch mÃ´ hÃ¬nh káº¿t há»£p sinh â€“ phÃ¢n loáº¡i,
* TrÃ¬nh bÃ y cÆ¡ cháº¿ Ä‘á»“ng tiáº¿n hÃ³a (co-evolution),
* MÃ´ hÃ¬nh hÃ³a toÃ¡n há»c quÃ¡ trÃ¬nh huáº¥n luyá»‡n,
* ÄÃ¡nh giÃ¡ vai trÃ² cá»§a tokenizer vÃ  bá»™ nhá»› GPU.

---

## 2. CÆ¡ sá»Ÿ lÃ½ thuyáº¿t

### 2.1. MÃ´ hÃ¬nh ngÃ´n ngá»¯ sinh tá»± há»“i quy

Cho chuá»—i token:
$$
X=(x_1,x_2,\dots,x_n)
$$


XÃ¡c suáº¥t sinh:
$$
P(X)=\prod_{i=1}^{n}P(x_i\mid x_{<i};\theta_g)
$$


Trong Ä‘Ã³ $\theta_g$ lÃ  tham sá»‘ mÃ´ hÃ¬nh sinh.

---

### 2.2. MÃ´ hÃ¬nh phÃ¢n loáº¡i vÄƒn báº£n

Vá»›i Ä‘áº§u ra [CLS]:
$$
h_{CLS}\in\mathbb{R}^d
$$


Bá»™ phÃ¢n loáº¡i:
$$
z = Wh_{CLS}+b
$$

$$
\hat{y}=\text{softmax}(z)
$$


Trong Ä‘Ã³ $\hat{y}$ lÃ  xÃ¡c suáº¥t Alice/Edgar.

---

### 2.3. HÃ m máº¥t mÃ¡t

#### $a$ MÃ´ hÃ¬nh sinh
$$
\mathcal{L}_{gen}
=================

-\frac{1}{N}\sum_{i=1}^{N}\log P(x_i\mid x_{<i})
$$


#### $b$ MÃ´ hÃ¬nh phÃ¢n loáº¡i
$$
\mathcal{L}_{cls}
=================

-\frac{1}{N}\sum_{i=1}^{N}\sum_{c}y_{ic}\log\hat{y}_{ic}
$$


---

## 3. PhÆ°Æ¡ng phÃ¡p nghiÃªn cá»©u

### 3.1. Kiáº¿n trÃºc há»‡ thá»‘ng ba mÃ´ hÃ¬nh

Theo , há»‡ thá»‘ng gá»“m:

1. BERT phÃ¢n loáº¡i (Ä‘Ã£ fine-tune),
2. MÃ´ hÃ¬nh sinh Alice,
3. MÃ´ hÃ¬nh sinh Edgar.

Hai mÃ´ hÃ¬nh sinh dá»±a trÃªn **EleutherAI** GPT-Neo:

> **GPT-Neo 125M**

SÆ¡ Ä‘á»“ tá»•ng quÃ¡t:
$$
\text{Alice/Edgar} \rightarrow \text{Text} \rightarrow \text{BERT} \rightarrow \text{Label}
$$


---

### 3.2. Quáº£n lÃ½ bá»™ nhá»› vÃ  Half Precision

BERT Ä‘Æ°á»£c chuyá»ƒn sang half precision:
$$
\text{float32} \rightarrow \text{float16}
$$


Giáº£m dung lÆ°á»£ng:
$$
M_{fp16}\approx \frac{1}{2}M_{fp32}
$$


GiÃºp tiáº¿t kiá»‡m GPU.

---

### 3.3. Dá»‹ch chuyá»ƒn tokenizer

Hai tokenizer khÃ¡c nhau:

* Tokenizer GPT-Neo,
* Tokenizer BERT.

Ãnh xáº¡ giÃ¡n tiáº¿p:
$$
T_{bert}(T^{-1}_{neo}(x))
$$


Trong Ä‘Ã³:

* $T_{neo}$: encode GPT-Neo,
* $T_{bert}$: encode BERT.

Quy trÃ¬nh:
$$
\text{Token}*{neo}
\rightarrow \text{Text}
\rightarrow \text{Token}*{bert}
$$


---

### 3.4. Táº¡o batch huáº¥n luyá»‡n

Theo tÃ i liá»‡u :

* Batch size: 64,
* Sequence length: 128,
* 32 Alice + 32 Edgar.

Ma tráº­n batch:
$$
B\in\mathbb{R}^{64\times128}
$$


Vector nhÃ£n:
$$
y=(\underbrace{0,\dots,0}*{32},
\underbrace{1,\dots,1}*{32})
$$


---

## 4. Chiáº¿n lÆ°á»£c sinh dá»¯ liá»‡u

### 4.1. Sinh dÆ° token

Äá»ƒ Ä‘áº£m báº£o Ä‘á»§ token BERT:
$$
L_{neo}=kL_{bert},\quad k>1
$$


Trong thá»±c nghiá»‡m:
$$
k\approx4
$$


Sau Ä‘Ã³ cáº¯t:
$$
X_{bert}=X_{neo}[1:L]
$$


---

### 4.2. Loáº¡i bá» token khÃ´ng mong muá»‘n

Danh sÃ¡ch token xáº¥u:
$$
\mathcal{B}={\text{space},\text{tab},\text{newline},\dots}
$$


RÃ ng buá»™c sinh:
$$
x_t\notin\mathcal{B}
$$


---

### 4.3. Repetition Penalty

Háº¡n cháº¿ láº·p:
$$
p_i'=\frac{p_i}{r^{c_i}}
$$


Trong Ä‘Ã³:

* $c_i$: sá»‘ láº§n láº·p token,
* (r>1): há»‡ sá»‘ pháº¡t.

---

## 5. PhÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡

### 5.1. Äá»™ chÃ­nh xÃ¡c phÃ¢n loáº¡i
$$
\text{Acc}
==========

\frac{1}{N}\sum_{i=1}^{N}\mathbf{1}(\hat{y}_i=y_i)
$$


TrÆ°á»›c fine-tuning:
$$
\text{Acc}\approx 0.5
$$


.

---

### 5.2. HÃ m máº¥t mÃ¡t BERT
$$
\mathcal{L}*{cls}^{(t+1)}
<
\mathcal{L}*{cls}^{(t)}
$$


â‡’ mÃ´ hÃ¬nh sinh tiáº¿n gáº§n phong cÃ¡ch má»¥c tiÃªu.

---

### 5.3. ÄÃ¡nh giÃ¡ Ä‘á»“ng tiáº¿n hÃ³a

Gá»i:
$$
S(t)=P_{BERT}(\text{Alice}\mid X_t)
$$


Náº¿u:
$$
S(t)\uparrow
$$


â‡’ mÃ´ hÃ¬nh Alice cáº£i thiá»‡n.

---

## 6. Káº¿t quáº£ thá»±c nghiá»‡m

Theo :

* Khi chÆ°a fine-tune: Acc â‰ˆ 45â€“50%,
* Sau fine-tune: Acc tÄƒng dáº§n,
* Loss giáº£m á»•n Ä‘á»‹nh.

Quan há»‡ tá»•ng quÃ¡t:
$$
\frac{d}{dt}\mathcal{L}_{cls}<0
$$


Cho tháº¥y quÃ¡ trÃ¬nh há»™i tá»¥.

---

## 7. Tháº£o luáº­n

### 7.1. Äá»“ng tiáº¿n hÃ³a sinh â€“ phÃ¢n loáº¡i

Há»‡ thá»‘ng táº¡o vÃ²ng láº·p:
$$
\text{Generate}\rightarrow\text{Classify}\rightarrow\text{Optimize}
$$


Giá»‘ng mÃ´ hÃ¬nh há»c Ä‘á»‘i khÃ¡ng nháº¹ (weak adversarial learning).

---

### 7.2. Vai trÃ² cá»§a tokenizer

Sai lá»‡ch tokenizer:
$$
|T_{neo}(x)|\ne|T_{bert}(x)|
$$


LÃ  nguá»“n gÃ¢y nhiá»…u chÃ­nh trong huáº¥n luyá»‡n.

---

### 7.3. Háº¡n cháº¿

* Phá»¥ thuá»™c máº¡nh vÃ o BERT,
* Chi phÃ­ GPU lá»›n,
* Token translation phá»©c táº¡p,
* Dá»… nhiá»…u vá»›i dá»¯ liá»‡u nhá».

---

## 8. á»¨ng dá»¥ng thá»±c tiá»…n

PhÆ°Æ¡ng phÃ¡p nÃ y cÃ³ thá»ƒ á»©ng dá»¥ng trong:

* ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh sÃ¡ng táº¡o vÄƒn há»c,
* Huáº¥n luyá»‡n chatbot theo phong cÃ¡ch,
* PhÃ¡t hiá»‡n Ä‘áº¡o vÄƒn,
* NghiÃªn cá»©u AI sÃ¡ng táº¡o.

MÃ´ hÃ¬nh â€œgiÃ¡m kháº£o tá»± Ä‘á»™ngâ€ giÃºp giáº£m phá»¥ thuá»™c vÃ o con ngÆ°á»i.

---

## 9. Káº¿t luáº­n

BÃ i viáº¿t Ä‘Ã£ trÃ¬nh bÃ y há»‡ thá»‘ng káº¿t há»£p mÃ´ hÃ¬nh sinh vÃ  mÃ´ hÃ¬nh phÃ¢n loáº¡i trong nghiÃªn cá»©u Aliceâ€“Edgar. CÃ¡c káº¿t luáº­n chÃ­nh:

1. BERT cÃ³ thá»ƒ dÃ¹ng lÃ m bá»™ Ä‘Ã¡nh giÃ¡ phong cÃ¡ch,
2. Äá»“ng tiáº¿n hÃ³a giÃºp Ä‘o lÆ°á»ng hiá»‡u quáº£ fine-tuning,
3. Tokenizer lÃ  yáº¿u tá»‘ then chá»‘t,
4. Half precision giÃºp tá»‘i Æ°u tÃ i nguyÃªn.

Trong tÆ°Æ¡ng lai, cÃ³ thá»ƒ má»Ÿ rá»™ng sang há»c tÄƒng cÆ°á»ng (RLHF) vÃ  Ä‘a phong cÃ¡ch.

---

## TÃ i liá»‡u tham kháº£o

1. Evolution of Alice and Edgar (Part 1) â€“ Code Challenge 
2. Devlin et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers.
3. Nijkamp et al. (2022). CodeGen: An Open Large Language Model for Code.
4. Vaswani et al. (2017). Attention Is All You Need.
5. Goodfellow et al. (2016). Deep Learning.

---
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [ğŸ“‚ Module: 07_fine_tune_pretrained_models](README.md) | [Xem bÃ i viáº¿t â†’](README.md) |
| [Fine-tuning CÃ³ Má»¥c TiÃªu vÃ  ÄÃ³ng BÄƒng ChÃ­nh XÃ¡c Trá»ng Sá»‘ Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_llm_010_codechallenge_fine_tuning_and_targeted_freezing_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_010_codechallenge_fine_tuning_and_targeted_freezing_part_1_.md) |
| [PhÃ¢n TÃ­ch Hiá»‡u Quáº£ Fine-tuning vÃ  Targeted Freezing (Pháº§n 2): ÄÃ¡nh GiÃ¡ Báº±ng Trá»±c Quan HÃ³a vÃ  Chuáº©n Ma Tráº­n](aero_llm_011_codechallenge_fine_tuning_and_targeted_freezing_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_011_codechallenge_fine_tuning_and_targeted_freezing_part_2_.md) |
| [Fine-tuning Hiá»‡u Quáº£ Tham Sá»‘ (Parameter-Efficient Fine-Tuning â€“ PEFT) Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_llm_012_parameter_efficient_fine_tuning_peft_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_012_parameter_efficient_fine_tuning_peft_.md) |
| [MÃ´ HÃ¬nh CodeGen Cho BÃ i ToÃ¡n HoÃ n ThÃ nh MÃ£ Nguá»“n: Kiáº¿n TrÃºc, Huáº¥n Luyá»‡n vÃ  á»¨ng Dá»¥ng](aero_llm_013_codegen_for_code_completion.md) | [Xem bÃ i viáº¿t â†’](aero_llm_013_codegen_for_code_completion.md) |
| [Fine-tuning MÃ´ HÃ¬nh CodeGen Cho BÃ i ToÃ¡n Giáº£i TÃ­ch: PhÆ°Æ¡ng PhÃ¡p, ÄÃ¡nh GiÃ¡ vÃ  á»¨ng Dá»¥ng](aero_llm_014_codechallenge_fine_tune_codegen_for_calculus.md) | [Xem bÃ i viáº¿t â†’](aero_llm_014_codechallenge_fine_tune_codegen_for_calculus.md) |
| [Tinh Chá»‰nh MÃ´ HÃ¬nh BERT Cho BÃ i ToÃ¡n PhÃ¢n Loáº¡i Cáº£m XÃºc VÄƒn Báº£n IMDb](aero_llm_015_fine_tuning_bert_for_classification.md) | [Xem bÃ i viáº¿t â†’](aero_llm_015_fine_tuning_bert_for_classification.md) |
| [ğŸ“˜ á»¨ng Dá»¥ng MÃ´ HÃ¬nh BERT Trong PhÃ¢n TÃ­ch Cáº£m XÃºc ÄÃ¡nh GiÃ¡ Phim IMDB](aero_llm_016_codechallenge_imdb_sentiment_analysis_using_bert_en_us.md) | [Xem bÃ i viáº¿t â†’](aero_llm_016_codechallenge_imdb_sentiment_analysis_using_bert_en_us.md) |
| [ğŸ“˜ á»¨ng Dá»¥ng Gradient Clipping vÃ  Learning Rate Scheduler Trong Huáº¥n Luyá»‡n MÃ´ HÃ¬nh Há»c SÃ¢u](aero_llm_017_gradient_clipping_and_learning_rate_scheduler_part_1_en_us.md) | [Xem bÃ i viáº¿t â†’](aero_llm_017_gradient_clipping_and_learning_rate_scheduler_part_1_en_us.md) |
| [ğŸ“˜ PhÃ¢n TÃ­ch Learning Rate Scheduler Trong Huáº¥n Luyá»‡n MÃ´ HÃ¬nh Há»c SÃ¢u Quy MÃ´ Lá»›n](aero_llm_018_gradient_clipping_and_learning_rate_scheduler_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_018_gradient_clipping_and_learning_rate_scheduler_part_2_.md) |
| [ğŸ“˜ Káº¿t Há»£p Gradient Clipping, Freezing vÃ  Learning Rate Scheduler Trong Fine-Tuning MÃ´ HÃ¬nh BERT](aero_llm_019_codechallenge_clip_freeze_and_schedule_bert.md) | [Xem bÃ i viáº¿t â†’](aero_llm_019_codechallenge_clip_freeze_and_schedule_bert.md) |
| [Tá»‘i Æ¯u HÃ³a QuÃ¡ TrÃ¬nh Tiá»n Huáº¥n Luyá»‡n MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: PhÃ¢n TÃ­ch CÃ¡c Chiáº¿n LÆ°á»£c TÃ­nh ToÃ¡n vÃ  Há»c Táº­p](aero_llm_01_what_does_fine_tuning_mean.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_what_does_fine_tuning_mean.md) |
| [LÆ°u Trá»¯ vÃ  Táº£i Láº¡i MÃ´ HÃ¬nh Há»c SÃ¢u Trong PyTorch vÃ  Hugging Face: PhÆ°Æ¡ng PhÃ¡p, Cáº¥u TrÃºc vÃ  ÄÃ¡nh GiÃ¡](aero_llm_020_saving_and_loading_trained_models.md) | [Xem bÃ i viáº¿t â†’](aero_llm_020_saving_and_loading_trained_models.md) |
| [á»¨ng Dá»¥ng MÃ´ HÃ¬nh BERT Trong PhÃ¢n Loáº¡i VÄƒn Báº£n VÄƒn Há»c: TrÆ°á»ng Há»£p Alice vÃ  Edgar](aero_llm_021_bert_decides_alice_or_edgar.md) | [Xem bÃ i viáº¿t â†’](aero_llm_021_bert_decides_alice_or_edgar.md) |
| ğŸ“Œ **[Äá»“ng Tiáº¿n HÃ³a MÃ´ HÃ¬nh Sinh VÄƒn Báº£n vÃ  MÃ´ HÃ¬nh PhÃ¢n Loáº¡i: TrÆ°á»ng Há»£p Alice vÃ  Edgar](aero_llm_022_codechallenge_evolution_of_alice_and_edgar_part_1_.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_022_codechallenge_evolution_of_alice_and_edgar_part_1_.md) |
| [ğŸ“˜ ÄÃ¡nh GiÃ¡ MÃ´ HÃ¬nh Sinh VÄƒn Báº£n ThÃ´ng Qua PhÃ¢n Loáº¡i BERT: NghiÃªn Cá»©u TrÆ°á»ng Há»£p Alice vÃ  Edgar](aero_llm_023_codechallenge_evolution_of_alice_and_edgar_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_023_codechallenge_evolution_of_alice_and_edgar_part_2_.md) |
| [Fine-tuning MÃ´ hÃ¬nh GPT-2 trÃªn TÃ¡c pháº©m *Gulliverâ€™s Travels*: PhÃ¢n tÃ­ch Thá»±c nghiá»‡m vÃ  ÄÃ¡nh giÃ¡ Hiá»‡u quáº£](aero_llm_02_fine_tune_a_pretrained_gpt2.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_fine_tune_a_pretrained_gpt2.md) |
| [ÄÃ¡nh giÃ¡ áº¢nh hÆ°á»Ÿng cá»§a Learning Rate trong Fine-tuning GPT-2 trÃªn *Gulliverâ€™s Travels*](aero_llm_03codechallenge_gulliver_s_learning_rates.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03codechallenge_gulliver_s_learning_rates.md) |
| [NghiÃªn cá»©u Quy trÃ¬nh Sinh VÄƒn báº£n tá»« MÃ´ hÃ¬nh NgÃ´n ngá»¯ Tiá»n Huáº¥n luyá»‡n GPT-2](aero_llm_04_on_generating_text_from_pretrained_models.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_on_generating_text_from_pretrained_models.md) |
| [Tinh Chá»‰nh MÃ´ HÃ¬nh GPT-2 Báº±ng HÃ m Máº¥t MÃ¡t KL Divergence Äá»ƒ Tá»‘i Æ¯u HÃ³a Viá»‡c Sinh Token Chá»©a KÃ½ Tá»± â€œXâ€](aero_llm_05_codechallenge_maximize_the_x_factor_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_codechallenge_maximize_the_x_factor_.md) |
| [Tinh Chá»‰nh MÃ´ HÃ¬nh GPT-Neo Äá»ƒ MÃ´ Phá»ng Phong CÃ¡ch VÄƒn Há»c Alice in Wonderland vÃ  Edgar Allan Poe](aero_llm_06_alice_in_wonderland_and_edgar_allen_poe_with_gpt_neo_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_alice_in_wonderland_and_edgar_allen_poe_with_gpt_neo_.md) |
| [ÄÃ¡nh GiÃ¡ Äá»‹nh LÆ°á»£ng vÃ  Äá»‹nh TÃ­nh MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Sau Fine-tuning: TrÆ°á»ng Há»£p VÄƒn Phong *Alice* vÃ  *Edgar Allan Poe*](aero_llm_07_codechallenge_quantify_the_aliceedgar_fine_tunin.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_codechallenge_quantify_the_aliceedgar_fine_tunin.md) |
| [Äá»‹nh LÆ°á»£ng Hiá»‡u Quáº£ Tinh Chá»‰nh Phong CÃ¡ch VÄƒn Há»c: Thá»­ ThÃ¡ch Alice vÃ  Edgar](aero_llm_07_codechallenge_quantify_the_aliceedgar_fine_tuning.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_codechallenge_quantify_the_aliceedgar_fine_tuning.md) |
| [MÃ´ Phá»ng Há»™i Thoáº¡i Giá»¯a Hai MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Sau Fine-tuning: TrÆ°á»ng Há»£p *Alice* vÃ  *Edgar*](aero_llm_08_codechallenge_a_chat_between_alice_and_edgar.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_codechallenge_a_chat_between_alice_and_edgar.md) |
| [Tinh Chá»‰nh Tá»«ng Pháº§n Báº±ng CÃ¡ch ÄÃ³ng BÄƒng Trá»ng Sá»‘ Attention: Chiáº¿n LÆ°á»£c Tá»‘i Æ¯u HÃ³a Tham Sá»‘ Cho LLM](aero_llm_09_partial_fine_tuning_by_freezing_attention_weights.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_partial_fine_tuning_by_freezing_attention_weights.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

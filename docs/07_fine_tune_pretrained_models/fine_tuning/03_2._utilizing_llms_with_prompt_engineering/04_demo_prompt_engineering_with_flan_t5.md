
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../../index.md) > [07 fine tune pretrained models](../../index.md) > [fine tuning](../index.md) > [03 2. utilizing llms with prompt engineering](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../../index.md)
- [ğŸ“š Module 01: LLM Course](../../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Demo Prompt Engineering Vá»›i FLAN-T5

## Giá»›i Thiá»‡u

ChÃ o má»«ng má»i ngÆ°á»i Ä‘áº¿n vá»›i demo Ä‘áº§u tiÃªn cá»§a khÃ³a há»c nÃ y. Táº¥t cáº£ cÃ¡c demo trong khÃ³a há»c nÃ y sáº½ sá»­ dá»¥ng Google Colaboratory.

Google Colab lÃ  má»™t ná»n táº£ng cho phÃ©p chÃºng ta lÆ°u trá»¯ cÃ¡c file notebook vÃ  káº¿t ná»‘i miá»…n phÃ­ Ä‘áº¿n má»™t instance trÃªn Google Cloud Platform nÆ¡i chÃºng ta cÅ©ng cÃ³ thá»ƒ káº¿t ná»‘i GPU. Äiá»u nÃ y ráº¥t há»¯u Ã­ch, Ä‘áº·c biá»‡t cho viá»‡c prototype cÃ¡c Ã½ tÆ°á»Ÿng.

Truy cáº­p: colab.research.google.com

## Thiáº¿t Láº­p MÃ´i TrÆ°á»ng

### Káº¿t Ná»‘i Google Colab

1. Truy cáº­p trang web Colab
2. Upload notebook tá»« Exercise Files
3. Click "Connect" Ä‘á»ƒ káº¿t ná»‘i vá»›i GPU miá»…n phÃ­

**LÆ°u Ã½:** Loáº¡i GPU phá»¥ thuá»™c vÃ o:
- Kháº£ nÄƒng sáºµn cÃ³ theo mÃºi giá»
- Táº§n suáº¥t sá»­ dá»¥ng GPU gáº§n Ä‘Ã¢y
- VÃ¬ lÃ  miá»…n phÃ­ nÃªn khÃ´ng Ä‘áº£m báº£o Ä‘Æ°á»£c loáº¡i GPU cá»¥ thá»ƒ

## CÃ i Äáº·t ThÆ° Viá»‡n

```python
# CÃ i Ä‘áº·t Transformers vÃ  TensorFlow
!pip install transformers tensorflow

## Táº£i MÃ´ HÃ¬nh FLAN-T5

```python
from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM

# Táº£i tokenizer vÃ  model

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-large")

$$
model = TFAutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-large") **LÆ°u Ã½ vá» warnings:** - Warning vá» xÃ¡c thá»±c HuggingFace lÃ  bÃ¬nh thÆ°á»ng - Warning vá» viá»‡c model Ä‘Æ°á»£c train báº±ng PyTorch rá»“i convert sang TensorFlow - Ä‘á»™ chÃ­nh xÃ¡c 99.9% tÆ°Æ¡ng Ä‘Æ°Æ¡ng ## Quy TrÃ¬nh Prompt Vá»›i FLAN-T5 Viá»‡c prompt má»™t LLM luÃ´n gá»“m 4 bÆ°á»›c: 1. Äá»‹nh nghÄ©a prompt 2. Tokenize (chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh tokens) 3. Model.generate() (táº¡o output) 4. Tokenizer.decode() (chuyá»ƒn Ä‘á»•i IDs vá» vÄƒn báº£n) ### 1. TÃ³m Táº¯t VÄƒn Báº£n (Summarization) ```python # Äá»‹nh nghÄ©a prompt
$$

prompt = "summarize: Studies show that eating carrots help improve vision..."

$$
# Tokenize inputs = tokenizer(prompt, return_tensors="tf", max_length=512,
$$

truncation=True, padding=True)

# Generate

outputs = model.generate(inputs.input_ids, max_length=50)

# Decode

summary = tokenizer.decode(outputs[0])

print(summary)

**Káº¿t quáº£:** "eat carrots" - má»™t báº£n tÃ³m táº¯t ngáº¯n gá»n

### 2. Dá»‹ch Thuáº­t (Translation)

```python
# Prompt dá»‹ch tiáº¿ng Anh sang tiáº¿ng TÃ¢y Ban Nha

$$
prompt = "translate English to Spanish: cheese is delicious"
$$

# Tokenize

inputs = tokenizer(prompt, return_tensors="tf", max_length=512,

$$
truncation=True, padding=True) # Generate outputs = model.generate(inputs.input_ids, max_length=40) # Decode translation = tokenizer.decode(outputs[0]) print(translation) ### 3. Tráº£ Lá»i CÃ¢u Há»i (Question Answering) ```python # Context vÃ  cÃ¢u há»i
$$

context = "The Great Wall of China is over 13,000 miles long."

question = "question: How long is the Great Wall of China?"

prompt = context + " " + question

# Tokenize

inputs = tokenizer(prompt, return_tensors="tf", max_length=512,

$$
truncation=True, padding=True) # Generate outputs = model.generate(inputs.input_ids, max_length=50) # Decode answer = tokenizer.decode(outputs[0])
$$


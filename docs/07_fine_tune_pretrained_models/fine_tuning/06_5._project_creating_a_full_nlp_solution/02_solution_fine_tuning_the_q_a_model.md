
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../../index.md) > [07 fine tune pretrained models](../../index.md) > [fine tuning](../index.md) > [06 5. project creating a full nlp solution](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../../index.md)
- [ğŸ“š Module 01: LLM Course](../../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Giáº£i PhÃ¡p Fine-tuning MÃ´ HÃ¬nh Question Answering

## Tá»•ng Quan

Trong bÃ i há»c nÃ y, chÃºng ta sáº½ xem xÃ©t giáº£i phÃ¡p cho bÃ i táº­p fine-tuning mÃ´ hÃ¬nh Question Answering (QA) sá»­ dá»¥ng FLAN-T5 trÃªn táº­p dá»¯ liá»‡u SQuAD v2. ÄÃ¢y lÃ  má»™t tÃ¡c vá»¥ quan trá»ng trong cÃ¡c á»©ng dá»¥ng chatbot vÃ  há»‡ thá»‘ng há»— trá»£.

## 1. Giá»›i Thiá»‡u Question Answering

### 1.1 KhÃ¡i Niá»‡m

Question Answering (QA) lÃ  tÃ¡c vá»¥ trong Ä‘Ã³ mÃ´ hÃ¬nh cáº§n tráº£ lá»i cÃ¢u há»i dá»±a trÃªn má»™t Ä‘oáº¡n vÄƒn báº£n ngá»¯ cáº£nh. CÃ²n Ä‘Æ°á»£c gá»i lÃ  **Question Answering with Context**.

### 1.2 Äá»‹nh Dáº¡ng Dá»¯ Liá»‡u

Má»—i vÃ­ dá»¥ trong SQuAD bao gá»“m:
- **Context**: Äoáº¡n vÄƒn báº£n chá»©a thÃ´ng tin
- **Question**: CÃ¢u há»i vá» ná»™i dung trong context
- **Answer**: CÃ¢u tráº£ lá»i (trÃ­ch xuáº¥t tá»« context)

## 2. Triá»ƒn Khai Giáº£i PhÃ¡p

### 2.1 CÃ i Äáº·t vÃ  Import

```python
!pip install transformers tensorflow datasets rouge-score

import tensorflow as tf
from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer
from datasets import load_dataset
from rouge_score import rouge_scorer
```

### 2.2 Táº£i Dá»¯ Liá»‡u SQuAD v2

```python
# Táº£i táº­p dá»¯ liá»‡u SQuAD v2
dataset = load_dataset("squad_v2", split="train")

# Xem vÃ­ dá»¥
print(dataset[0])
```

### 2.3 Tiá»n Xá»­ LÃ½ Dá»¯ Liá»‡u

```python
# Táº£i tokenizer
model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def preprocess_qa(examples):
    # Táº¡o prompt vá»›i format: context + question
    inputs = []
    for context, question in zip(examples['context'], examples['questions']):
        prompt = f"{context} Question: {question} Answer:"
        inputs.append(prompt)
    
    # Xá»­ lÃ½ cÃ¢u tráº£ lá»i
    answers = []
    for ans_text in examples['answers']:
        if len(ans_text['text']) > 0:
            answers.append(ans_text['text'][0])
        else:
            answers.append("")  # CÃ¢u tráº£ lá»i trá»‘ng
    
    # Tokenize
    model_inputs = tokenizer(inputs, max_length=384, truncation=True, padding="max_length")
    labels = tokenizer(answers, max_length=128, truncation=True, padding="max_length")
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Giá»›i háº¡n dá»¯ liá»‡u (SQuAD ráº¥t lá»›n)
train_data = dataset.select(range(25000))
test_data = dataset.select(range(25000, 27000))

# Ãp dá»¥ng tiá»n xá»­ lÃ½
train_data = train_data.map(preprocess_qa, batched=True)
test_data = test_data.map(preprocess_qa, batched=True)
```

### 2.4 Chuyá»ƒn Äá»•i Sang TensorFlow

```python
tf_train = train_data.to_tf_dataset(
    columns=["input_ids", "attention_mask"],
    label_cols=["labels"],
    batch_size=16,
    shuffle=True
)

tf_test = test_data.to_tf_dataset(
    columns=["input_ids", "attention_mask"],
    label_cols=["labels"],
    batch_size=16
)
```

### 2.5 Táº£i vÃ  Cáº¥u HÃ¬nh MÃ´ HÃ¬nh

```python
# Táº£i mÃ´ hÃ¬nh FLAN-T5-base
model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)

# Freeze cÃ¡c lá»›p Ä‘áº§u (transfer learning)
for layer in model.layers[:3]:
    layer.trainable = False

print(f"Tá»•ng tham sá»‘: {model.count_params() / 1e6:.1f}M")
```

### 2.6 Huáº¥n Luyá»‡n

```python
# Compile
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
)

# Huáº¥n luyá»‡n
print("Báº¯t Ä‘áº§u huáº¥n luyá»‡n...")
history = model.fit(
    tf_train,
    validation_data=tf_test,
    epochs=3
)
```

### 2.7 ÄÃ¡nh GiÃ¡ vá»›i ROUGE

```python
# Khá»Ÿi táº¡o ROUGE scorer
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

# Láº¥y má»™t vÃ­ dá»¥ tá»« test set
batch = next(iter(tf_test))

# Láº¥y má»™t vÃ­ dá»¥ cá»¥ thá»ƒ
input_ids = batch['input_ids'][0:1]
label_ids = batch['labels'][0:1]

# Generate answer
outputs = model.generate(input_ids)
predicted_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Reference
reference = tokenizer.decode(label_ids[0], skip_special_tokens=True)

# TÃ­nh ROUGE
scores = scorer.score(reference, predicted_answer)

print(f"Question: What is the capital of France?")
print(f"Reference: {reference}")
print(f"Predicted: {predicted_answer}")
print(f"ROUGE-1: {scores['rouge1'].precision:.4f}")
print(f"ROUGE-2: {scores['rouge2'].precision:.4f}")
print(f"ROUGE-L: {scores['rougeL'].precision:.4f}")
```

## 3. Káº¿t Quáº£

### 3.1 Thá»‘ng KÃª Huáº¥n Luyá»‡n

| Epoch | Training Loss | Validation Loss | Thá»i gian |
|-------|---------------|-----------------|-----------|
| 1     | 1.8           | 1.5             | ~30 phÃºt  |
| 2     | 1.2           | 1.0             | ~30 phÃºt  |
| 3     | 0.8           | 0.7             | ~30 phÃºt  |

**Tá»•ng thá»i gian:** ~90 phÃºt

### 3.2 ÄÃ¡nh GiÃ¡ ROUGE

| Chá»‰ sá»‘ | GiÃ¡ trá»‹ |
|--------|---------|
| ROUGE-1 | 1.0 |
| ROUGE-2 | 0.0 |
| ROUGE-L | 1.0 |

**VÃ­ dá»¥:**
- **CÃ¢u há»i:** "What is the capital of France?"
- **Reference:** "France"
- **Predicted:** "France"

## 4. PhÃ¢n TÃ­ch Chi Tiáº¿t

### 4.1 Táº¡i Sao Sá»­ Dá»¥ng Transfer Learning?

$$\text{Performance}_{TL} > \text{Performance}_{from\_scratch}$$

LÃ½ do:
- MÃ´ hÃ¬nh FLAN-T5 Ä‘Ã£ Ä‘Æ°á»£c pre-train trÃªn nhiá»u tÃ¡c vá»¥
- Transfer learning giáº£m nhu cáº§u dá»¯ liá»‡u lá»›n
- Huáº¥n luyá»‡n nhanh hÆ¡n

### 4.2 Xá»­ LÃ½ CÃ¢u Tráº£ Lá»i Trá»‘ng

Trong SQuAD v2, cÃ³ nhá»¯ng cÃ¢u há»i khÃ´ng cÃ³ cÃ¢u tráº£ lá»i. ChÃºng ta xá»­ lÃ½:

```python
# Kiá»ƒm tra vÃ  xá»­ lÃ½ cÃ¢u tráº£ lá»i trá»‘ng
if len(answer['text']) == 0:
    answer_text = ""  # Model sáº½ há»c "I don't know"
else:
    answer_text = answer['text'][0]
```

Äiá»u nÃ y quan trá»ng Ä‘á»ƒ trÃ¡nh **hallucination** - hiá»‡n tÆ°á»£ng mÃ´ hÃ¬nh táº¡o ra cÃ¢u tráº£ lá»i khÃ´ng cÃ³ tháº­t.

## 5. BÃ i Há»c RÃºt Ra

### 5.1 Äiá»ƒm Quan Trá»ng

1. **Format prompt**: Context + Question + Answer
2. **Xá»­ lÃ½ cÃ¢u tráº£ lá»i trá»‘ng**: TrÃ¡nh hallucination
3. **ÄÃ¡nh giÃ¡**: ROUGE cho QA

### 5.2 Khuyáº¿n Nghá»‹

- Sá»­ dá»¥ng FLAN-T5-large Ä‘á»ƒ cáº£i thiá»‡n káº¿t quáº£
- TÄƒng sá»‘ lÆ°á»£ng dá»¯ liá»‡u huáº¥n luyá»‡n
- Fine-tune nhiá»u epoch hÆ¡n

## 6. Káº¿t Luáº­n

BÃ i táº­p nÃ y Ä‘Ã£ hÆ°á»›ng dáº«n chÃºng ta cÃ¡ch:
1. Fine-tune FLAN-T5 cho tÃ¡c vá»¥ Question Answering
2. Xá»­ lÃ½ dá»¯ liá»‡u SQuAD v2
3. Ãp dá»¥ng transfer learning
4. ÄÃ¡nh giÃ¡ vá»›i ROUGE

Question Answering lÃ  má»™t tÃ¡c vá»¥ quan trá»ng trong cÃ¡c á»©ng dá»¥ng chatbot vÃ  há»‡ thá»‘ng há»— trá»£ thÃ´ng minh.

## TÃ i Liá»‡u Tham Kháº£o

1. Rajpurkar, P., et al. (2018). "Know What You Don't Know: Unanswerable Questions for SQuAD." *ACL 2018*.

2. Alberti, C., et al. (2019). "Synthetic Data for Natural Language Inference." *EMNLP 2019*.

3. Devlin, J., et al. (2019). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding." *NAACL 2019*.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Giáº£i PhÃ¡p: Fine-tuning MÃ´ HÃ¬nh PhÃ¢n TÃ­ch Cáº£m XÃºc](01_solution_fine_tuning_the_sentiment_analysis_model.md) | [Xem bÃ i viáº¿t â†’](01_solution_fine_tuning_the_sentiment_analysis_model.md) |
| ğŸ“Œ **[Giáº£i PhÃ¡p Fine-tuning MÃ´ HÃ¬nh Question Answering](02_solution_fine_tuning_the_q_a_model.md)** | [Xem bÃ i viáº¿t â†’](02_solution_fine_tuning_the_q_a_model.md) |
| [Giáº£i PhÃ¡p Fine-tuning MÃ´ HÃ¬nh TÃ³m Táº¯t vá»›i LoRA](03_solution_fine_tuning_the_summarization_model.md) | [Xem bÃ i viáº¿t â†’](03_solution_fine_tuning_the_summarization_model.md) |
| [Demo TÃ­ch Há»£p Má»i Thá»© vÃ o Giáº£i PhÃ¡p](04_demo_integrating_everything_into_our_solution.md) | [Xem bÃ i viáº¿t â†’](04_demo_integrating_everything_into_our_solution.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

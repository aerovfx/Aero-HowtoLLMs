
<!-- Aero-Navigation-Start -->
[üè† Home](../../../../index.md) > [07 fine tune pretrained models](../../../index.md) > [fine tuning](../../index.md) > [06 5. project creating a full nlp solution](../index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../../../index.md)
- [üìö Module 01: LLM Course](../../../../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../../../../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../../../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../../../../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../../../../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../../../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Gi·∫£i Ph√°p Fine-tuning M√¥ H√¨nh T√≥m T·∫Øt v·ªõi LoRA

## T·ªïng Quan

Trong b√†i h·ªçc n√†y, ch√∫ng ta s·∫Ω xem x√©t gi·∫£i ph√°p cho b√†i t·∫≠p fine-tuning m√¥ h√¨nh t√≥m t·∫Øt (summarization) s·ª≠ d·ª•ng k·ªπ thu·∫≠t LoRA tr√™n t·∫≠p d·ªØ li·ªáu CNN DailyMail. ƒê√¢y l√† t√°c v·ª• quan tr·ªçng trong c√°c ·ª©ng d·ª•ng chatbot ƒë·ªÉ t·∫°o t√≥m t·∫Øt cu·ªôc tr√≤ chuy·ªán.

## 1. Gi·ªõi Thi·ªáu T√≥m T·∫Øt vƒÉn b·∫£n

### 1.1 T·∫°i Sao C·∫ßn T√≥m T·∫Øt?

Trong c√°c ·ª©ng d·ª•ng chatbot:
- T√≥m t·∫Øt cu·ªôc tr√≤ chuy·ªán d√†i
- Gi·∫£m thi·ªÉu token trong inference
- L∆∞u tr·ªØ l·ªãch s·ª≠ h·ªôi tho·∫°i hi·ªáu qu·∫£

### 1.2 T·∫≠p D·ªØ Li·ªáu CNN DailyMail

- **Context**: B√†i b√°o d√†i
- **Summary**: C√°c ƒëi·ªÉm n·ªïi b·∫≠t (highlights)
- **K√≠ch th∆∞·ªõc**: ~300,000 b√†i b√°o

## 2. Tri·ªÉn Khai Gi·∫£i Ph√°p

### 2.1 C√†i ƒê·∫∑t v√† Import

```python
!pip install transformers tensorflow datasets rouge-score

import tensorflow as tf
from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer
from datasets import load_dataset
import numpy as np
```

### 2.2 T·∫£i D·ªØ Li·ªáu

```python
# T·∫£i t·∫≠pu CNN DailyMail d·ªØ li·ªá
dataset = load_dataset("cnn_dailymail", "3.0.0", split="train[:0.5%]")

print(f"S·ªë l∆∞·ª£ng v√≠ d·ª•: {len(dataset)}")
print(dataset[0])
```

### 2.3 Ti·ªÅn X·ª≠ L√Ω

```python
# T·∫£i tokenizer
model_name = "google/flan-t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def preprocess_summarization(examples):
    # T·∫°o prompt v·ªõi format: summarize: article
    inputs = ["summarize: " + article for article in examples['article']]
    
    # Highlights l√† t√≥m t·∫Øt
    targets = [highlight for highlight in examples['highlights']]
    
    # Tokenize
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length")
    labels = tokenizer(targets, max_length=128, truncation=True, padding="max_length")
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Gi·ªõi h·∫°n d·ªØ li·ªáu (CNN r·∫•t l·ªõn)
train_data = dataset.select(range(10000))
test_data = dataset.select(range(10000, 10500))

# √Åp d·ª•ng ti·ªÅn x·ª≠ l√Ω
train_data = train_data.map(preprocess_summarization, batched=True)
test_data = test_data.map(preprocess_summarization, batched=True)
```

### 2.4 Chuy·ªÉn ƒê·ªïi Sang TensorFlow

```python
# Chuy·ªÉn sang TensorFlow Dataset
tf_train = train_data.to_tf_dataset(
    columns=["input_ids", "attention_mask"],
    label_cols=["labels"],
    batch_size=8,
    shuffle=True
)

tf_test = test_data.to_tf_dataset(
    columns=["input_ids", "attention_mask"],
    label_cols=["labels"],
    batch_size=8
)
```

### 2.5 Tri·ªÉn Khai LoRA

```python
# T·∫£i m√¥ h√¨nh
model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)

# ƒê·ªãnh nghƒ©a l·ªõp LoRA
class LoRALayer(tf.keras.layers.Layer):
    def __init__(self, original_layer, rank=4, **kwargs):
        super(LoRALayer, self).__init__(**kwargs)
        self.original_layer = original_layer
        self.rank = rank
        
    def build(self, input_shape):
        # Ma tr·∫≠n A (input_dim x rank)
        self.A = self.add_weight(
            name="lora_A",
            shape=(input_shape[-1], self.rank),
            initializer="glorot_uniform",
            trainable=True
        )
        # Ma tr·∫≠n B (rank x input_dim)
        self.B = self.add_weight(
            name="lora_B",
            shape=(self.rank, input_shape[-1]),
            initializer="zeros",
            trainable=True
        )
        super(LoRALayer, self).build(input_shape)
        
    def call(self, inputs):
        original_output = self.original_layer(inputs)
        # LoRA: A √ó B √ó input
        lora_output = tf.matmul(tf.matmul(inputs, self.A), self.B)
        return original_output + lora_output
```

### 2.6 √Åp D·ª•ng LoRA

```python
# Freeze c√°c l·ªõp ƒë·∫ßu
for layer in model.layers[:3]:
    layer.trainable = False

# √Åp d·ª•ng LoRA cho decoder v√† l·ªõp dense cu·ªëi
# (L∆∞u √Ω: Trong th·ª±c t·∫ø, c·∫ßn duy·ªát qua c√°c l·ªõp v√† thay th·∫ø)
```

### 2.7 Th·ªëng K√™ Tham S·ªë

| Lo·∫°i | S·ªë l∆∞·ª£ng tham s·ªë |
|------|------------------|
| T·ªïng | ~76M |
| Trainable (v·ªõi LoRA) | ~16M |
| T·ª∑ l·ªá | ~21% |

### 2.8 Hu·∫•n Luy·ªán

```python
# Compile
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
)

# Hu·∫•n luy·ªán
print("B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán...")
history = model.fit(
    tf_train,
    validation_data=tf_test,
    epochs=3
)

print(f"Ho√†n th√†nh trong ~6 ph√∫t")
```

### 2.9 ƒê√°nh Gi√° v·ªõi BLEU

```python
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

smoothing = SmoothingFunction().method1

# L·∫•y m·ªôt batch t·ª´ test set
batch = next(iter(tf_test))

# T√≠nh BLEU score trung b√¨nh
bleu_scores = []
for i in range(8):
    # Reference
    ref = tokenizer.decode(batch['labels'][i], skip_special_tokens=True)
    
    # Generate
    input_ids = batch['input_ids'][i:i+1]
    outputs = model.generate(input_ids)
    hyp = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # T√≠nh BLEU
    score = sentence_bleu([ref.split()], hyp.split(), smoothing_function=smoothing)
    bleu_scores.append(score)

avg_bleu = np.mean(bleu_scores)
print(f"Average BLEU Score: {avg_bleu:.4f}")
```

## 3. K·∫øt Qu·∫£

### 3.1 Th·ªëng K√™ Hu·∫•n Luy·ªán

| Epoch | Training Loss | Validation Loss | Th·ªùi gian |
|-------|---------------|-----------------|-----------|
| 1     | 2.5           | 2.2             | ~2 ph√∫t   |
| 2     | 2.0           | 1.8             | ~2 ph√∫t   |
| 3     | 1.6           | 1.5             | ~2 ph√∫t   |

### 3.2 ƒê√°nh Gi√° BLEU

| Ch·ªâ s·ªë | Gi√° tr·ªã |
|--------|---------|
| BLEU Score | 0.03 |

**Nh·∫≠n x√©t:**
- BLEU = 0.03 l√† gi√° tr·ªã ch·∫•p nh·∫≠n ƒë∆∞·ª£c
- C√≥ th·ªÉ c·∫£i thi·ªán v·ªõi:
  - M√¥ h√¨nh l·ªõn h∆°n (base, large)
  - Nhi·ªÅu d·ªØ li·ªáu h∆°n
  - TƒÉng max_length

## 4. Ph√¢n T√≠ch Chi Ti·∫øt

### 4.1 T·∫°i Sao BLEU Th·∫•p?

BLEU ƒëo l∆∞·ªùng s·ª± kh·ªõp t·ª´ng t·ª´ gi·ªØa reference v√† hypothesis. Trong t√≥m t·∫Øt:
- C√≥ nhi·ªÅu c√°ch t√≥m t·∫Øt c√πng m·ªôt vƒÉn b·∫£n
- BLEU kh√¥ng ƒë√°nh gi√° meaning

### 4.2 C√°c Ch·ªâ S·ªë Thay Th·∫ø

| Ch·ªâ s·ªë | ∆Øu ƒëi·ªÉm |
|--------|---------|
| ROUGE | ƒê√°nh gi√° recall |
| BERTScore | ƒê√°nh gi√° semantic |
| METEOR | Linh ho·∫°t h∆°n |

## 5. B√†i H·ªçc R√∫t Ra

### 5.1 ƒêi·ªÉm Quan Tr·ªçng

1. **ƒê·ªãnh d·∫°ng prompt**: "summarize: " + article
2. **Gi·ªõi h·∫°n d·ªØ li·ªáu**: CNN r·∫•t l·ªõn, c·∫ßn gi·ªõi h·∫°n
3. **LoRA**: Gi·∫£m tham s·ªë ƒë√°ng k·ªÉ

### 5.2 Khuy·∫øn Ngh·ªã Cho S·∫£n Ph·∫©m

- S·ª≠ d·ª•ng FLAN-T5-large
- TƒÉng max_length l√™n 512 ho·∫∑c 1024
- Hu·∫•n luy·ªán nhi·ªÅu epoch h∆°n

## 6. K·∫øt Lu·∫≠n

B√†i t·∫≠p n√†y ƒë√£ h∆∞·ªõng d·∫´n ch√∫ng ta:
1. Fine-tune FLAN-T5 v·ªõi LoRA cho t√°c v·ª• t√≥m t·∫Øt
2. X·ª≠ l√Ω d·ªØ li·ªáu CNN DailyMail
3. ƒê√°nh gi√° v·ªõi BLEU score
4. ƒê·∫°t ƒë∆∞·ª£c k·∫øt qu·∫£ BLEU = 0.03

T√≥m t·∫Øt l√† m·ªôt t√°c v·ª• quan tr·ªçng trong c√°c ·ª©ng d·ª•ng chatbot ƒë·ªÉ l∆∞u tr·ªØ v√† x·ª≠ l√Ω cu·ªôc tr√≤ chuy·ªán d√†i.

## T√†i Li·ªáu Tham Kh·∫£o

1. See, A., et al. (2017). "Get To The Point: Summarization with Pointer-Generator Networks." *ACL 2017*.

2. Hermann, K.M., et al. (2015). "Teaching Machines to Read and Comprehend." *NIPS 2015*.

3. Hu, E.J., et al. (2021). "LoRA: Low-Rank Adaptation of Large Language Models." *ICLR 2022*.
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [Gi·∫£i Ph√°p: Fine-tuning M√¥ H√¨nh Ph√¢n T√≠ch C·∫£m X√∫c](01_solution_fine_tuning_the_sentiment_analysis_model.md) | [Xem b√†i vi·∫øt ‚Üí](01_solution_fine_tuning_the_sentiment_analysis_model.md) |
| [Gi·∫£i Ph√°p Fine-tuning M√¥ H√¨nh Question Answering](02_solution_fine_tuning_the_q_a_model.md) | [Xem b√†i vi·∫øt ‚Üí](02_solution_fine_tuning_the_q_a_model.md) |
| üìå **[Gi·∫£i Ph√°p Fine-tuning M√¥ H√¨nh T√≥m T·∫Øt v·ªõi LoRA](03_solution_fine_tuning_the_summarization_model.md)** | [Xem b√†i vi·∫øt ‚Üí](03_solution_fine_tuning_the_summarization_model.md) |
| [Demo T√≠ch H·ª£p M·ªçi Th·ª© v√†o Gi·∫£i Ph√°p](04_demo_integrating_everything_into_our_solution.md) | [Xem b√†i vi·∫øt ‚Üí](04_demo_integrating_everything_into_our_solution.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[üè† Home](../../../index.md) > [07 fine tune pretrained models](../../index.md) > [fine tuning](../index.md) > [04 3. transfer learning for nlp tasks](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../../index.md)
- [üìö Module 01: LLM Course](../../../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../../../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../../../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../../../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# ƒê√°nh Gi√° K·∫øt Qu·∫£ Transfer Learning

## Gi·ªõi Thi·ªáu

B√¢y gi·ªù ch√∫ng ta ƒë√£ th·∫•y transfer learning ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o, h√£y t·∫≠p trung v√†o vi·ªác ƒë√°nh gi√° k·∫øt qu·∫£ c·ªßa transfer learning, c·ª• th·ªÉ l√† s·ª≠ d·ª•ng c√°c metrics nh∆∞ ROUGE v√† BLEU. C√°c metrics n√†y r·∫•t c·∫ßn thi·∫øt ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng c·ªßa c√°c t√°c v·ª• nh∆∞ t√≥m t·∫Øt vƒÉn b·∫£n v√† d·ªãch m√°y, t∆∞∆°ng t·ª± nh∆∞ c√°ch ƒë·ªô ch√≠nh x√°c ƒë∆∞·ª£c s·ª≠ d·ª•ng trong c√°c t√°c v·ª• ph√¢n lo·∫°i.

## ROUGE L√† G√¨?

ROUGE vi·∫øt t·∫Øt c·ªßa Recall-Oriented Understudy for Gisting Evaluation. N√≥ ch·ªß y·∫øu ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng c·ªßa c√°c b·∫£n t√≥m t·∫Øt b·∫±ng c√°ch ƒëo l∆∞·ªùng s·ª± ch·ªìng ch√©o gi·ªØa b·∫£n t√≥m t·∫Øt ƒë∆∞·ª£c t·∫°o v√† m·ªôt t·∫≠p h·ª£p c√°c b·∫£n t√≥m t·∫Øt tham chi·∫øu.

H√£y nghƒ© v·ªÅ ROUGE nh∆∞ m·ªôt th∆∞·ªõc ƒëo v·ªÅ s·ª± ch·ªìng ch√©o gi·ªØa vƒÉn b·∫£n ƒë∆∞·ª£c t·∫°o v√† vƒÉn b·∫£n tham chi·∫øu, gi·ªëng nh∆∞ so s√°nh hai c√¥ng th·ª©c c√≥ bao nhi√™u nguy√™n li·ªáu gi·ªëng nhau.

### T√≠nh ROUGE-1

**V√≠ d·ª•:**
- B·∫£n t√≥m t·∫Øt ƒë∆∞·ª£c t·∫°o: "The cat sat on the mat."
- B·∫£n t√≥m t·∫Øt tham chi·∫øu: "The cat is sitting on the mat."

ƒê·ªÉ t√≠nh ROUGE-1 (unigram overlap), ch√∫ng ta ƒë·∫øm s·ªë t·ª´ ch·ªìng ch√©o v√† chia cho t·ªïng s·ªë t·ª´ trong b·∫£n t√≥m t·∫Øt tham chi·∫øu.

C√°c t·ª´ ch·ªìng ch√©o: "the", "cat", "on", "the", "mat" (5 t·ª´)
T·ªïng s·ªë t·ª´ trong tham chi·∫øu: 6 t·ª´
**ROUGE-1 = 5/6 = 83%**

## BLEU L√† G√¨?

BLEU vi·∫øt t·∫Øt c·ªßa Bilingual Evaluation Understudy. N√≥ ƒëo l∆∞·ªùng m·ª©c ƒë·ªô vƒÉn b·∫£n ƒë∆∞·ª£c t·∫°o kh·ªõp v·ªõi vƒÉn b·∫£n tham chi·∫øu b·∫±ng c√°ch so s√°nh c√°c n-grams, c√≥ nghƒ©a l√† c√°c chu·ªói c·ªßa n t·ª´.

BLEU gi·ªëng nh∆∞ m·ªôt nh√† ph√™ b√¨nh th·ª±c ph·∫©m t·ªâ m·ªâ, ng∆∞·ªùi kh√¥ng ch·ªâ ki·ªÉm tra c√°c nguy√™n li·ªáu c√≥ ƒë√∫ng kh√¥ng, m√† c√≤n xem ch√∫ng c√≥ ƒë∆∞·ª£c k·∫øt h·ª£p ƒë√∫ng th·ª© t·ª± v√† t·ª∑ l·ªá hay kh√¥ng.

### T√≠nh BLEU-1

S·ª≠ d·ª•ng c√πng v√≠ d·ª•:
- B·∫£n t·∫°o: "The cat sat on the mat"
- Tham chi·∫øu: "The cat is sitting on the mat"

BLEU-1 = 5/5 = 100%

## So S√°nh ROUGE V√† BLEU

- **ROUGE:** T·∫≠p trung v√†o recall - n·∫Øm b·∫Øt bao nhi√™u c·ªßa b·∫£n t√≥m t·∫Øt tham chi·∫øu c√≥ m·∫∑t trong b·∫£n t√≥m t·∫Øt ƒë∆∞·ª£c t·∫°o
- **BLEU:** T·∫≠p trung v√†o precision - ƒë√°nh gi√° m·ª©c ƒë·ªô vƒÉn b·∫£n ƒë∆∞·ª£c t·∫°o kh·ªõp v·ªõi tham chi·∫øu v·ªÅ c√°c c·ª•m t·ª´ ch√≠nh x√°c v√† th·ª© t·ª± c·ªßa ch√∫ng

B·∫°n c√≥ th·ªÉ nghƒ© v·ªÅ BLEU nh∆∞ m·ªôt ph√©p ƒëo precision v√† ROUGE nh∆∞ m·ªôt ph√©p ƒëo recall.

·ª•ng Tr## S·ª≠ Dong Code

```python
# ƒê√°nh gi√° v·ªõi ROUGE
from datasets import load_metric
rouge = load_metric("rouge")
results = rouge.compute(predictions=predictions, references=references)

# ƒê√°nh gi√° v·ªõi BLEU
bleu = load_metric("bleu")
results = bleu.compute(predictions=predictions, references=references)

## K·∫øt Lu·∫≠n

Hi·ªÉu v√† s·ª≠ d·ª•ng c√°c metrics ROUGE v√† BLEU l√† ƒëi·ªÅu c·∫ßn thi·∫øt ƒë·ªÉ ƒë√°nh gi√° hi·ªáu qu·∫£ c√°c t√°c v·ª• t·∫°o vƒÉn b·∫£n. B·∫±ng c√°ch so s√°nh c√°c metrics n√†y v·ªõi ƒë·ªô ch√≠nh x√°c v√† ph√¢n lo·∫°i, ch√∫ng ta c√≥ th·ªÉ ƒë√°nh gi√° cao h∆°n vai tr√≤ c·ªßa ch√∫ng trong vi·ªác ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng vƒÉn b·∫£n ƒë∆∞·ª£c t·∫°o b·ªüi AI.

## T√†i li·ªáu tham kh·∫£o

1. **Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., & Polosukhin, I. (2017).** *Attention Is All You Need.* Advances in Neural Information Processing Systems, 30, 5998-6008. https://arxiv.org/abs/1706.03762

2. **Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018).** *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.* arXiv preprint arXiv:1810.04805. https://arxiv.org/abs/1810.04805

3. **Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2019).** *Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.* arXiv preprint arXiv:1910.10683. https://arxiv.org/abs/1910.10683

4. **Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021).** *LoRA: Low-Rank Adaptation of Large Language Models.* arXiv preprint arXiv:2106.09685. https://arxiv.org/abs/2106.09685

5. **Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., ... & Wei, J. (2022).** *Scaling Instruction-Finetuned Language Models.* arXiv preprint arXiv:2210.11416. https://arxiv.org/abs/2210.11416

6. **Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J., & Roberts, A. (2023).** *The Flan Collection: Designing Data and Methods for Effective Instruction Tuning.* arXiv preprint arXiv:2301.13688. https://arxiv.org/abs/2301.13688

7. **Han, Z., Gao, C., Liu, J., Zhang, J., & Zhang, S. Q. (2024).** *Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey.* arXiv preprint arXiv:2403.14608. https://arxiv.org/abs/2403.14608

8. **Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P. S., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., Kenton, Z., Brown, S., Hawkins, W., Stepleton, T., Biles, C., Birhane, A., Haas, J., Rimell, L., Hendricks, L. A., ... & Gabriel, I. (2021).** *Ethical and Social Risks of Harm from Language Models.* DeepMind. https://storage.googleapis.com/deepmind-media/research/language-research/Ethical%20and%20social%20risks.pdf

9. **Bengio, Y., Mindermann, S., Privitera, D., Besiroglu, T., Bommasani, R., Casper, S., Choi, Y., Goldfarb, D., Heidari, H., Khalatbari, L., Longpre, S., Mavroudis, V., Mazeika, M., Ng, K. Y., Okolo, C. T., Raji, D., Skeadas, T., Tram√®r, F., Adekanmbi, B., ... & Zhou, D. (2024).** *International Scientific Report on the Safety of Advanced AI (Interim Report).* arXiv preprint arXiv:2412.05282. https://arxiv.org/abs/2412.05282

10. **Amodei, D., Ananthanarayanan, S., Bapna, R., Chen, Z., Du, E., Goodfellow, I., ... & Sutskever, I. (2016).** *Concrete Problems in AI Safety.* arXiv preprint arXiv:1606.06565. https://arxiv.org/abs/1606.06565

11. **Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2024).** *QLoRA: Efficient Finetuning of Quantized LLMs.* arXiv preprint arXiv:2305.14314. https://arxiv.org/abs/2305.14314

12. **Zhang, Y., Yang, X., Cai, Y., & Giannakis, G. B. (2025).** *ScaLoRA: Optimally Scaled Low-Rank Adaptation for Efficient High-Rank Fine-Tuning.* arXiv preprint arXiv:2510.23818. https://arxiv.org/abs/2510.23818

13. **Wang, R., Dvijotham, K. D., & Manchester, I. R. (2025).** *Norm-Bounded Low-Rank Adaptation.* arXiv preprint arXiv:2501.19050. https://arxiv.org/abs/2501.19050

14. **Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T. A., Bernhard, M., ... & Houlsby, N. (2022).** *LoRA+: Efficient Low Rank Adaptation of Large Models.* arXiv preprint arXiv:2402.12354. https://arxiv.org/abs/2402.12354

15. **Wang, L., Lyu, C., Ji, T., Chen, M., Yu, Z., Shi, A., ... & Yu, P. S. (2023).** *A Survey on Parameter-Efficient Fine-Tuning for Foundation Models.* arXiv preprint arXiv:2504.21099. https://arxiv.org/abs/2504.21099

16. **Laakso, A., Kemell, K. K., & Nurminen, J. K. (2024).** *Ethical Issues in Large Language Models: A Systematic Literature Review.* CEUR Workshop Proceedings, 3901. https://ceur-ws.org/Vol-3901/paper_4.pdf

17. **Bosma, M., & Wei, J. (2021).** *Introducing FLAN: More Generalizable Language Models with Instruction Fine-Tuning.* Google AI Blog. https://research.google/blog/introducing-flan-more-generalizable-language-models-with-instruction-fine-tuning/

18. **Roberts, A., & Raffel, C. (2020).** *Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer.* Google AI Blog. https://research.google/blog/exploring-transfer-learning-with-t5-the-text-to-text-transfer-transformer/

19. **Lester, B., Al-Rfou, R., & Wang, L. (2021).** *The Power of Scale for Parameter-Efficient Prompt Tuning.* Proceedings of EMNLP 2021. https://arxiv.org/abs/2104.08691

20. **Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020).** *Language Models are Few-Shot Learners.* Advances in Neural Information Processing Systems, 33, 1877-1901. https://arxiv.org/abs/2005.14165
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [Transfer Learning Trong LLMs](01_transfer_learning_in_llms.md) | [Xem b√†i vi·∫øt ‚Üí](01_transfer_learning_in_llms.md) |
| [Ch·ªçn M√¥ H√¨nh Cho Transfer Learning](02_choosing_models_for_transfer_learning.md) | [Xem b√†i vi·∫øt ‚Üí](02_choosing_models_for_transfer_learning.md) |
| [Demo Transfer Learning v·ªõi FLAN-T5](03_demo_transfer_learning_with_flan_t5.md) | [Xem b√†i vi·∫øt ‚Üí](03_demo_transfer_learning_with_flan_t5.md) |
| üìå **[ƒê√°nh Gi√° K·∫øt Qu·∫£ Transfer Learning](04_evaluating_transfer_learning_outcomes.md)** | [Xem b√†i vi·∫øt ‚Üí](04_evaluating_transfer_learning_outcomes.md) |
| [Demo ƒê√°nh Gi√° B·∫£n D·ªãch](05_demo_evaluating_translations.md) | [Xem b√†i vi·∫øt ‚Üí](05_demo_evaluating_translations.md) |
| [Gi·∫£i Ph√°p N√¢ng Cao D·ªãch Thu·∫≠t v·ªõi Transfer Learning](06_solution_enhancing_translation_with_transfer_learning.md) | [Xem b√†i vi·∫øt ‚Üí](06_solution_enhancing_translation_with_transfer_learning.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[üè† Home](../../../index.md) > [07 fine tune pretrained models](../../index.md) > [fine tuning](../index.md) > [04 3. transfer learning for nlp tasks](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../../index.md)
- [üìö Module 01: LLM Course](../../../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../../../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../../../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../../../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Gi·∫£i Ph√°p N√¢ng Cao D·ªãch Thu·∫≠t v·ªõi Transfer Learning

## T·ªïng Quan

Trong b√†i h·ªçc n√†y, ch√∫ng ta s·∫Ω xem x√©t gi·∫£i ph√°p cho b√†i t·∫≠p n√¢ng cao v·ªÅ transfer learning trong d·ªãch thu·∫≠t. B√†i t·∫≠p y√™u c·∫ßu s·ª≠ d·ª•ng t·∫≠p d·ªØ li·ªáu WMT16 ƒë·ªÉ d·ªãch t·ª´ ti·∫øng ƒê·ª©c sang ti·∫øng Anh, m·ªôt t√°c v·ª• kh√≥ h∆°n so v·ªõi c√°c b√†i t·∫≠p tr∆∞·ªõc.

## 1. Gi·ªõi Thi·ªáu B√†i To√°n

### 1.1 M·ª•c Ti√™u

- S·ª≠ d·ª•ng t·∫≠p d·ªØ li·ªáu WMT16
- D·ªãch t·ª´ ti·∫øng ƒê·ª©c sang ti·∫øng Anh
- √Åp d·ª•ng transfer learning v·ªõi FLAN-T5

### 1.2 Th√°ch Th·ª©c

- T·∫≠p d·ªØ li·ªáu m·ªõi (ch∆∞a quen thu·ªôc)
- Ng√¥n ng·ªØ ngu·ªìn kh√°c v·ªõi c√°c b√†i t·∫≠p tr∆∞·ªõc
- C·∫ßn x·ª≠ l√Ω ƒë·ªãnh d·∫°ng d·ªØ li·ªáu ƒë·∫∑c bi·ªát

## 2. Tri·ªÉn Khai Gi·∫£i Ph√°p

### 2.1 B∆∞·ªõc 1: C√†i ƒê·∫∑t v√† T·∫£i D·ªØ Li·ªáu

```python
# C√†i ƒë·∫∑t th∆∞ vi·ªán
!pip install transformers tensorflow datasets

# T·∫£i t·∫≠p d·ªØ li·ªáu WMT16 (ƒê·ª©c - Anh)
from datasets import load_dataset

$$
dataset = load_dataset("wmt16", "de-en", split="train[:1%]")
$$

### 2.2 B∆∞·ªõc 2: Ti·ªÅn X·ª≠ L√Ω

```python
from transformers import AutoTokenizer

# T·∫£i tokenizer

$$
model_name = "google/flan-t5-base"
$$

$$
tokenizer = AutoTokenizer.from_pretrained(model_name)
$$

def preprocess_function(examples):
    # L·∫•y c√¢u ti·∫øng ƒê·ª©c v√† ti·∫øng Anh
    inputs = ["translate German to English: " + ex['de'] for ex in examples['translation']]
    targets = [ex['en'] for ex in examples['translation']]
    
    # Tokenize

$$
model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")
$$

$$
labels = tokenizer(targets, max_length=128, truncation=True, padding="max_length")
$$

    
$$
model_inputs["labels"] = labels["input_ids"]
$$

    
    return model_inputs

# √Åp d·ª•ng ti·ªÅn x·ª≠ l√Ω

$$
processed_dataset = dataset.map(preprocess_function, batched=True)
$$

### 2.3 B∆∞·ªõc 3: T·∫°o TensorFlow Dataset

```python
# Chuy·ªÉn ƒë·ªïi sang TensorFlow

$$
tf_train = processed_dataset["train"].to_tf_dataset(
$$

$$
columns=["input_ids", "decoder_input_ids", "attention_mask"],
$$

$$
label_cols=["labels"],
$$

$$
batch_size=32,
$$

$$
shuffle=True
$$

)

### 2.4 B∆∞·ªõc 4: T·∫£i v√† C·∫•u H√¨nh M√¥ H√¨nh

```python
from transformers import TFAutoModelForSeq2SeqLM

# T·∫£i m√¥ h√¨nh

$$
model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)
$$

# Freeze c√°c l·ªõp ƒë·∫ßu
for layer in model.layers[:3]:

$$
layer.trainable = False
$$

### 2.5 B∆∞·ªõc 5: Hu·∫•n Luy·ªán

```python
# Compile
model.compile(

$$
optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),
$$

$$
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
$$

)

# Hu·∫•n luy·ªán
model.fit(
    tf_train,

$$
validation_data=tf_test,
$$

    epochs=3
)

## 3. ƒê√°nh Gi√° K·∫øt Qu·∫£

### 3.1 T√≠nh BLEU Score

```python
from nltk.translate.bleu_score import sentence_bleu

# L·∫•y m·ªôt batch t·ª´ test set

$$
batch = next(iter(test_dataset))
$$

# D·ªãch v√† ƒë√°nh gi√°

$$
bleu_scores = []
$$

for i in range(batch_size):
    # Decode reference

$$
reference = tokenizer.decode(batch['labels'][i], skip_special_tokens=True)
$$

    
    # Generate translation

$$
inputs = tokenizer.decode(batch['input_ids'][i], skip_special_tokens=True)
$$

$$
outputs = model.generate(tokenizer(inputs, return_tensors="tf")["input_ids"])
$$

$$
hypothesis = tokenizer.decode(outputs[0], skip_special_tokens=True)
$$

    
    # T√≠nh BLEU

$$
score = sentence_bleu([reference.split()], hypothesis.split())
$$

    bleu_scores.append(score)

# Trung b√¨nh BLEU

$$
avg_bleu = sum(bleu_scores) / len(bleu_scores)
$$

print(f"Average BLEU Score: {avg_bleu:.4f}")

### 3.2 K·∫øt Qu·∫£

| Ch·ªâ s·ªë | Gi√° tr·ªã |
|--------|---------|
| BLEU Score | 0.12 |

**Nh·∫≠n x√©t:**
- BLEU = 0.12 cho th·∫•y m√¥ h√¨nh ƒë√£ h·ªçc ƒë∆∞·ª£c c∆° b·∫£n
- Gi√° tr·ªã n√†y c√≥ th·ªÉ ƒë∆∞·ª£c c·∫£i thi·ªán v·ªõi:
  - Nhi·ªÅu d·ªØ li·ªáu h∆°n
  - Nhi·ªÅu epoch h∆°n
  - M√¥ h√¨nh l·ªõn h∆°n

## 4. Ph√¢n T√≠ch Chi Ti·∫øt

### 4.1 Transfer Learning cho Ng√¥n Ng·ªØ M·ªõi

M·ªôt ƒëi·ªÉm quan tr·ªçng trong b√†i t·∫≠p n√†y l√† ch√∫ng ta ƒëang d·ªãch t·ª´ ti·∫øng ƒê·ª©c sang ti·∫øng Anh - m·ªôt ng√¥n ng·ªØ m√† FLAN-T5 kh√¥ng ƒë∆∞·ª£c hu·∫•n luy·ªán tr·ª±c ti·∫øp. ƒêi·ªÅu n√†y th·ªÉ hi·ªán:

1. **Kh·∫£ nƒÉng t·ªïng qu√°t h√≥a** c·ªßa m√¥ h√¨nh pre-trained
2. **Transfer learning** ho·∫°t ƒë·ªông ngay c·∫£ v·ªõi ng√¥n ng·ªØ ch∆∞a t·ª´ng th·∫•y
3. **H·∫°n ch·∫ø** c·ªßa zero-shot cho t√°c v·ª• ph·ª©c t·∫°p

### 4.2 M√¥ H√¨nh To√°n H·ªçc

$$

$$

\text{BLEU}_{\text{avg}} = \frac{1}{N} $\sum$_{i=1}^{N} \text{BLEU}(ref_i, hyp_i)

$$

$$

Trong ƒë√≥:
- $N$ l√† s·ªë l∆∞·ª£ng v√≠ d·ª•
- $ref_i$ l√† b·∫£n d·ªãch tham chi·∫øu
- $hyp_i$ l√† b·∫£n d·ªãch c·ªßa m√¥ h√¨nh

## 5. B√†i H·ªçc R√∫t Ra

### 5.1 ƒêi·ªÉm Quan Tr·ªçng

1. **X·ª≠ l√Ω d·ªØ li·ªáu m·ªõi**: C·∫ßn hi·ªÉu ƒë·ªãnh d·∫°ng d·ªØ li·ªáu tr∆∞·ªõc khi x·ª≠ l√Ω
2. **Transfer learning**: C√≥ th·ªÉ √°p d·ª•ng cho c√°c ng√¥n ng·ªØ kh√°c nhau
3. **ƒê√°nh gi√°**: BLEU score cung c·∫•p ƒë√°nh gi√° ƒë·ªãnh l∆∞·ª£ng

### 5.2 Khuy·∫øn Ngh·ªã

- S·ª≠ d·ª•ng nhi·ªÅu d·ªØ li·ªáu h∆°n ƒë·ªÉ c·∫£i thi·ªán
- Th·ª≠ nghi·ªám v·ªõi c√°c m√¥ h√¨nh l·ªõn h∆°n
- ƒêi·ªÅu ch·ªânh hyperparameters

## 6. K·∫øt Lu·∫≠n

B√†i t·∫≠p n√†y ƒë√£ ch·ª©ng minh kh·∫£ nƒÉng c·ªßa transfer learning trong vi·ªác:
- M·ªü r·ªông kh·∫£ nƒÉng c·ªßa m√¥ h√¨nh sang ng√¥n ng·ªØ m·ªõi
- X·ª≠ l√Ω c√°c t·∫≠p d·ªØ li·ªáu m·ªõi
- ƒê√°nh gi√° hi·ªáu su·∫•t b·∫±ng c√°c ch·ªâ s·ªë ti√™u chu·∫©n

V·ªõi k·∫øt qu·∫£ BLEU = 0.12, m√¥ h√¨nh ƒë√£ th·ªÉ hi·ªán kh·∫£ nƒÉng h·ªçc d·ªãch thu·∫≠t c∆° b·∫£n v√† c√≥ th·ªÉ ƒë∆∞·ª£c c·∫£i thi·ªán th√™m v·ªõi nhi·ªÅu t√†i nguy√™n h∆°n.

## T√†i Li·ªáu Tham Kh·∫£o

1. Bojar, O., et al. (2016). "Findings of the 2016 Conference on Machine Translation." *WMT 2016*.

2. Ott, M., et al. (2018). "Scaling Neural Machine Translation." *ACL 2018*.

3. Vaswani, A., et al. (2017). "Attention Is All You Need." *NIPS 2017*.
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [Transfer Learning Trong LLMs](01_transfer_learning_in_llms.md) | [Xem b√†i vi·∫øt ‚Üí](01_transfer_learning_in_llms.md) |
| [Ch·ªçn M√¥ H√¨nh Cho Transfer Learning](02_choosing_models_for_transfer_learning.md) | [Xem b√†i vi·∫øt ‚Üí](02_choosing_models_for_transfer_learning.md) |
| [Demo Transfer Learning v·ªõi FLAN-T5](03_demo_transfer_learning_with_flan_t5.md) | [Xem b√†i vi·∫øt ‚Üí](03_demo_transfer_learning_with_flan_t5.md) |
| [ƒê√°nh Gi√° K·∫øt Qu·∫£ Transfer Learning](04_evaluating_transfer_learning_outcomes.md) | [Xem b√†i vi·∫øt ‚Üí](04_evaluating_transfer_learning_outcomes.md) |
| [Demo ƒê√°nh Gi√° B·∫£n D·ªãch](05_demo_evaluating_translations.md) | [Xem b√†i vi·∫øt ‚Üí](05_demo_evaluating_translations.md) |
| üìå **[Gi·∫£i Ph√°p N√¢ng Cao D·ªãch Thu·∫≠t v·ªõi Transfer Learning](06_solution_enhancing_translation_with_transfer_learning.md)** | [Xem b√†i vi·∫øt ‚Üí](06_solution_enhancing_translation_with_transfer_learning.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../../index.md) > [07 fine tune pretrained models](../../index.md) > [fine tuning](../index.md) > [04 3. transfer learning for nlp tasks](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../../index.md)
- [ğŸ“š Module 01: LLM Course](../../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Giáº£i PhÃ¡p NÃ¢ng Cao Dá»‹ch Thuáº­t vá»›i Transfer Learning

## Tá»•ng Quan

Trong bÃ i há»c nÃ y, chÃºng ta sáº½ xem xÃ©t giáº£i phÃ¡p cho bÃ i táº­p nÃ¢ng cao vá» transfer learning trong dá»‹ch thuáº­t. BÃ i táº­p yÃªu cáº§u sá»­ dá»¥ng táº­p dá»¯ liá»‡u WMT16 Ä‘á»ƒ dá»‹ch tá»« tiáº¿ng Äá»©c sang tiáº¿ng Anh, má»™t tÃ¡c vá»¥ khÃ³ hÆ¡n so vá»›i cÃ¡c bÃ i táº­p trÆ°á»›c.

## 1. Giá»›i Thiá»‡u BÃ i ToÃ¡n

### 1.1 Má»¥c TiÃªu

- Sá»­ dá»¥ng táº­p dá»¯ liá»‡u WMT16
- Dá»‹ch tá»« tiáº¿ng Äá»©c sang tiáº¿ng Anh
- Ãp dá»¥ng transfer learning vá»›i FLAN-T5

### 1.2 ThÃ¡ch Thá»©c

- Táº­p dá»¯ liá»‡u má»›i (chÆ°a quen thuá»™c)
- NgÃ´n ngá»¯ nguá»“n khÃ¡c vá»›i cÃ¡c bÃ i táº­p trÆ°á»›c
- Cáº§n xá»­ lÃ½ Ä‘á»‹nh dáº¡ng dá»¯ liá»‡u Ä‘áº·c biá»‡t

## 2. Triá»ƒn Khai Giáº£i PhÃ¡p

### 2.1 BÆ°á»›c 1: CÃ i Äáº·t vÃ  Táº£i Dá»¯ Liá»‡u

```python
# CÃ i Ä‘áº·t thÆ° viá»‡n
!pip install transformers tensorflow datasets

# Táº£i táº­p dá»¯ liá»‡u WMT16 (Äá»©c - Anh)
from datasets import load_dataset

dataset = load_dataset("wmt16", "de-en", split="train[:1%]")
```

### 2.2 BÆ°á»›c 2: Tiá»n Xá»­ LÃ½

```python
from transformers import AutoTokenizer

# Táº£i tokenizer
model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def preprocess_function(examples):
    # Láº¥y cÃ¢u tiáº¿ng Äá»©c vÃ  tiáº¿ng Anh
    inputs = ["translate German to English: " + ex['de'] for ex in examples['translation']]
    targets = [ex['en'] for ex in examples['translation']]
    
    # Tokenize
    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")
    labels = tokenizer(targets, max_length=128, truncation=True, padding="max_length")
    
    model_inputs["labels"] = labels["input_ids"]
    
    return model_inputs

# Ãp dá»¥ng tiá»n xá»­ lÃ½
processed_dataset = dataset.map(preprocess_function, batched=True)
```

### 2.3 BÆ°á»›c 3: Táº¡o TensorFlow Dataset

```python
# Chuyá»ƒn Ä‘á»•i sang TensorFlow
tf_train = processed_dataset["train"].to_tf_dataset(
    columns=["input_ids", "decoder_input_ids", "attention_mask"],
    label_cols=["labels"],
    batch_size=32,
    shuffle=True
)
```

### 2.4 BÆ°á»›c 4: Táº£i vÃ  Cáº¥u HÃ¬nh MÃ´ HÃ¬nh

```python
from transformers import TFAutoModelForSeq2SeqLM

# Táº£i mÃ´ hÃ¬nh
model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)

# Freeze cÃ¡c lá»›p Ä‘áº§u
for layer in model.layers[:3]:
    layer.trainable = False
```

### 2.5 BÆ°á»›c 5: Huáº¥n Luyá»‡n

```python
# Compile
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
)

# Huáº¥n luyá»‡n
model.fit(
    tf_train,
    validation_data=tf_test,
    epochs=3
)
```

## 3. ÄÃ¡nh GiÃ¡ Káº¿t Quáº£

### 3.1 TÃ­nh BLEU Score

```python
from nltk.translate.bleu_score import sentence_bleu

# Láº¥y má»™t batch tá»« test set
batch = next(iter(test_dataset))

# Dá»‹ch vÃ  Ä‘Ã¡nh giÃ¡
bleu_scores = []
for i in range(batch_size):
    # Decode reference
    reference = tokenizer.decode(batch['labels'][i], skip_special_tokens=True)
    
    # Generate translation
    inputs = tokenizer.decode(batch['input_ids'][i], skip_special_tokens=True)
    outputs = model.generate(tokenizer(inputs, return_tensors="tf")["input_ids"])
    hypothesis = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # TÃ­nh BLEU
    score = sentence_bleu([reference.split()], hypothesis.split())
    bleu_scores.append(score)

# Trung bÃ¬nh BLEU
avg_bleu = sum(bleu_scores) / len(bleu_scores)
print(f"Average BLEU Score: {avg_bleu:.4f}")
```

### 3.2 Káº¿t Quáº£

| Chá»‰ sá»‘ | GiÃ¡ trá»‹ |
|--------|---------|
| BLEU Score | 0.12 |

**Nháº­n xÃ©t:**
- BLEU = 0.12 cho tháº¥y mÃ´ hÃ¬nh Ä‘Ã£ há»c Ä‘Æ°á»£c cÆ¡ báº£n
- GiÃ¡ trá»‹ nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c cáº£i thiá»‡n vá»›i:
  - Nhiá»u dá»¯ liá»‡u hÆ¡n
  - Nhiá»u epoch hÆ¡n
  - MÃ´ hÃ¬nh lá»›n hÆ¡n

## 4. PhÃ¢n TÃ­ch Chi Tiáº¿t

### 4.1 Transfer Learning cho NgÃ´n Ngá»¯ Má»›i

Má»™t Ä‘iá»ƒm quan trá»ng trong bÃ i táº­p nÃ y lÃ  chÃºng ta Ä‘ang dá»‹ch tá»« tiáº¿ng Äá»©c sang tiáº¿ng Anh - má»™t ngÃ´n ngá»¯ mÃ  FLAN-T5 khÃ´ng Ä‘Æ°á»£c huáº¥n luyá»‡n trá»±c tiáº¿p. Äiá»u nÃ y thá»ƒ hiá»‡n:

1. **Kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a** cá»§a mÃ´ hÃ¬nh pre-trained
2. **Transfer learning** hoáº¡t Ä‘á»™ng ngay cáº£ vá»›i ngÃ´n ngá»¯ chÆ°a tá»«ng tháº¥y
3. **Háº¡n cháº¿** cá»§a zero-shot cho tÃ¡c vá»¥ phá»©c táº¡p

### 4.2 MÃ´ HÃ¬nh ToÃ¡n Há»c

$$
\text{BLEU}_{\text{avg}} = \frac{1}{N} \sum_{i=1}^{N} \text{BLEU}(ref_i, hyp_i)
$$

Trong Ä‘Ã³:
- $N$ lÃ  sá»‘ lÆ°á»£ng vÃ­ dá»¥
- $ref_i$ lÃ  báº£n dá»‹ch tham chiáº¿u
- $hyp_i$ lÃ  báº£n dá»‹ch cá»§a mÃ´ hÃ¬nh

## 5. BÃ i Há»c RÃºt Ra

### 5.1 Äiá»ƒm Quan Trá»ng

1. **Xá»­ lÃ½ dá»¯ liá»‡u má»›i**: Cáº§n hiá»ƒu Ä‘á»‹nh dáº¡ng dá»¯ liá»‡u trÆ°á»›c khi xá»­ lÃ½
2. **Transfer learning**: CÃ³ thá»ƒ Ã¡p dá»¥ng cho cÃ¡c ngÃ´n ngá»¯ khÃ¡c nhau
3. **ÄÃ¡nh giÃ¡**: BLEU score cung cáº¥p Ä‘Ã¡nh giÃ¡ Ä‘á»‹nh lÆ°á»£ng

### 5.2 Khuyáº¿n Nghá»‹

- Sá»­ dá»¥ng nhiá»u dá»¯ liá»‡u hÆ¡n Ä‘á»ƒ cáº£i thiá»‡n
- Thá»­ nghiá»‡m vá»›i cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n
- Äiá»u chá»‰nh hyperparameters

## 6. Káº¿t Luáº­n

BÃ i táº­p nÃ y Ä‘Ã£ chá»©ng minh kháº£ nÄƒng cá»§a transfer learning trong viá»‡c:
- Má»Ÿ rá»™ng kháº£ nÄƒng cá»§a mÃ´ hÃ¬nh sang ngÃ´n ngá»¯ má»›i
- Xá»­ lÃ½ cÃ¡c táº­p dá»¯ liá»‡u má»›i
- ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t báº±ng cÃ¡c chá»‰ sá»‘ tiÃªu chuáº©n

Vá»›i káº¿t quáº£ BLEU = 0.12, mÃ´ hÃ¬nh Ä‘Ã£ thá»ƒ hiá»‡n kháº£ nÄƒng há»c dá»‹ch thuáº­t cÆ¡ báº£n vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c cáº£i thiá»‡n thÃªm vá»›i nhiá»u tÃ i nguyÃªn hÆ¡n.

## TÃ i Liá»‡u Tham Kháº£o

1. Bojar, O., et al. (2016). "Findings of the 2016 Conference on Machine Translation." *WMT 2016*.

2. Ott, M., et al. (2018). "Scaling Neural Machine Translation." *ACL 2018*.

3. Vaswani, A., et al. (2017). "Attention Is All You Need." *NIPS 2017*.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Transfer Learning Trong LLMs](01_transfer_learning_in_llms.md) | [Xem bÃ i viáº¿t â†’](01_transfer_learning_in_llms.md) |
| [Chá»n MÃ´ HÃ¬nh Cho Transfer Learning](02_choosing_models_for_transfer_learning.md) | [Xem bÃ i viáº¿t â†’](02_choosing_models_for_transfer_learning.md) |
| [Demo Transfer Learning vá»›i FLAN-T5](03_demo_transfer_learning_with_flan_t5.md) | [Xem bÃ i viáº¿t â†’](03_demo_transfer_learning_with_flan_t5.md) |
| [ÄÃ¡nh GiÃ¡ Káº¿t Quáº£ Transfer Learning](04_evaluating_transfer_learning_outcomes.md) | [Xem bÃ i viáº¿t â†’](04_evaluating_transfer_learning_outcomes.md) |
| [Demo ÄÃ¡nh GiÃ¡ Báº£n Dá»‹ch](05_demo_evaluating_translations.md) | [Xem bÃ i viáº¿t â†’](05_demo_evaluating_translations.md) |
| ğŸ“Œ **[Giáº£i PhÃ¡p NÃ¢ng Cao Dá»‹ch Thuáº­t vá»›i Transfer Learning](06_solution_enhancing_translation_with_transfer_learning.md)** | [Xem bÃ i viáº¿t â†’](06_solution_enhancing_translation_with_transfer_learning.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

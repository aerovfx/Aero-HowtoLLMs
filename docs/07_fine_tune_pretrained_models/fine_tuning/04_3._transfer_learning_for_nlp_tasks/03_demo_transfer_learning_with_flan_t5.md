
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../../index.md) > [07 fine tune pretrained models](../../index.md) > [fine tuning](../index.md) > [04 3. transfer learning for nlp tasks](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../../index.md)
- [ğŸ“š Module 01: LLM Course](../../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Demo Transfer Learning vá»›i FLAN-T5

## Tá»•ng Quan

Trong bÃ i hÆ°á»›ng dáº«n thá»±c hÃ nh nÃ y, chÃºng ta sáº½ thá»±c hiá»‡n transfer learning vá»›i mÃ´ hÃ¬nh FLAN-T5 Ä‘á»ƒ dá»‹ch thuáº­t tá»« tiáº¿ng Anh sang tiáº¿ng TÃ¢y Ban Nha trÃªn táº­p dá»¯ liá»‡u OPUS-100. ÄÃ¢y lÃ  má»™t trong nhá»¯ng tÃ¡c vá»¥ phá»• biáº¿n nháº¥t trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn.

## 1. Giá»›i Thiá»‡u Transfer Learning

### 1.1 KhÃ¡i Niá»‡m

Transfer learning lÃ  ká»¹ thuáº­t cho phÃ©p sá»­ dá»¥ng kiáº¿n thá»©c tá»« má»™t tÃ¡c vá»¥ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ giáº£i quyáº¿t má»™t tÃ¡c vá»¥ má»›i cÃ³ liÃªn quan. Trong ngá»¯ cáº£nh cá»§a LLMs:

- MÃ´ hÃ¬nh Ä‘Æ°á»£c pre-train trÃªn má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u vÄƒn báº£n
- Sau Ä‘Ã³ fine-tune cho má»™t tÃ¡c vá»¥ cá»¥ thá»ƒ
- Kiáº¿n thá»©c tá»« pre-training Ä‘Æ°á»£c "chuyá»ƒn giao" sang tÃ¡c vá»¥ má»›i

### 1.2 Lá»£i Ãch

$$\text{Efficiency} \propto \frac{\text{Pre-trained Knowledge}}{\text{New Task Data}}$$

- Giáº£m thá»i gian huáº¥n luyá»‡n
- Giáº£m nhu cáº§u dá»¯ liá»‡u cÃ³ nhÃ£n
- Cáº£i thiá»‡n hiá»‡u suáº¥t trÃªn cÃ¡c tÃ¡c vá»¥ Ã­t dá»¯ liá»‡u

## 2. Triá»ƒn Khai Chi Tiáº¿t

### 2.1 CÃ i Äáº·t vÃ  Táº£i MÃ´ HÃ¬nh

```python
# CÃ i Ä‘áº·t thÆ° viá»‡n
!pip install transformers tensorflow datasets

# Táº£i tokenizer vÃ  mÃ´ hÃ¬nh
from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM

model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)
```

### 2.2 Táº£i Dá»¯ Liá»‡u

```python
from datasets import load_dataset

# Táº£i táº­p dá»¯ liá»‡u OPUS-100 (Anh - TÃ¢y Ban Nha)
dataset = load_dataset("Helsinki-NLP/opus-100", "en-es")
```

### 2.3 Tiá»n Xá»­ LÃ½ Dá»¯ Liá»‡u

```python
# Táº¡o prompt cho tÃ¡c vá»¥ dá»‹ch thuáº­t
def preprocess_function(examples):
    # ThÃªm prefix "translate English to Spanish"
    inputs = ["translate English to Spanish: " + ex for ex in examples["en"]]
    targets = [ex for ex in examples["es"]]
    
    # Tokenize inputs
    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")
    
    # Tokenize targets
    labels = tokenizer(targets, max_length=128, truncation=True, padding="max_length")
    
    model_inputs["labels"] = labels["input_ids"]
    
    return model_inputs
```

### 2.4 Chuyá»ƒn Äá»•i Sang TensorFlow Dataset

```python
# Chuyá»ƒn Ä‘á»•i sang TensorFlow
tf_train = train_dataset.to_tf_dataset(
    columns=["input_ids", "decoder_input_ids", "attention_mask"],
    label_cols=["labels"],
    batch_size=64,
    shuffle=True
)
```

## 3. Transfer Learning trong Thá»±c Táº¿

### 3.1 Freeze CÃ¡c Lá»›p

```python
# Freeze embedding, encoder, decoder
for layer in model.layers[:3]:
    layer.trainable = False

# Chá»‰ train lá»›p dense cuá»‘i
model.summary()
```

**Káº¿t quáº£:**
- Tá»•ng tham sá»‘: ~247 triá»‡u
- Tham sá»‘ khÃ´ng train: ~222 triá»‡u
- Tham sá»‘ trainable: ~24 triá»‡u

### 3.2 Huáº¥n Luyá»‡n

```python
# Compile model
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
)

# Huáº¥n luyá»‡n
model.fit(
    tf_train,
    validation_data=tf_test,
    epochs=3
)
```

### 3.3 ÄÃ¡nh GiÃ¡

| Epoch | Training Loss | Validation Loss |
|-------|--------------|-----------------|
| 1     | 2.5          | 1.8             |
| 2     | 1.5          | 1.2             |
| 3     | 0.77         | 0.9             |

**Nháº­n xÃ©t:**
- Loss giáº£m Ä‘á»u qua cÃ¡c epoch
- Validation loss khÃ´ng tÄƒng (khÃ´ng overfitting)
- MÃ´ hÃ¬nh há»c tá»‘t tÃ¡c vá»¥ dá»‹ch thuáº­t

## 4. Dá»‹ch Thuáº­t Vá»›i MÃ´ HÃ¬nh ÄÃ£ Fine-tune

### 4.1 Inference

```python
def translate(text):
    # Táº¡o prompt
    prompt = f"translate English to Spanish: {text}"
    
    # Tokenize
    inputs = tokenizer(prompt, return_tensors="tf")
    
    # Generate
    outputs = model.generate(**inputs, max_length=128)
    
    # Decode
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# VÃ­ dá»¥
input_text = "Bird, you don't have to be super brave all the time."
translation = translate(input_text)
print(f"Input: {input_text}")
print(f"Translation: {translation}")
```

### 4.2 Káº¿t Quáº£

**Input:** "Bird, you don't have to be super brave all the time."

**Output:** "PÃ¡jaro, no tienes que ser tan valiente todo el tiempo."

## 5. PhÃ¢n TÃ­ch ToÃ¡n Há»c

### 5.1 Kiáº¿n TrÃºc Seq2Seq

MÃ´ hÃ¬nh FLAN-T5 sá»­ dá»¥ng kiáº¿n trÃºc Encoder-Decoder:

$$\text{Output} = \text{Decoder}(\text{Encoder}(X), Y_{<t})$$

Trong Ä‘Ã³:
- $X$ lÃ  chuá»—i Ä‘áº§u vÃ o
- $Y_{<t}$ lÃ  cÃ¡c token Ä‘Ã£ Ä‘Æ°á»£c sinh ra trÆ°á»›c Ä‘Ã³

### 5.2 Transfer Learning Efficiency

Hiá»‡u quáº£ cá»§a transfer learning cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n:

$$\eta_{TL} = \frac{||\theta^*_{new} - \theta_{pre}||}{||\theta_{new}||} \times 100\%$$

Vá»›i LoRA, $\eta_{TL}$ thÆ°á»ng < 5%, cho tháº¥y chá»‰ má»™t pháº§n nhá» tham sá»‘ cáº§n Ä‘Æ°á»£c Ä‘iá»u chá»‰nh.

## 6. Káº¿t Luáº­n

Trong bÃ i hÆ°á»›ng dáº«n nÃ y, chÃºng ta Ä‘Ã£:

1. Sá»­ dá»¥ng FLAN-T5-base cho transfer learning
2. Fine-tune trÃªn táº­p dá»¯ liá»‡u OPUS-100
3. Freeze cÃ¡c lá»›p Ä‘áº§u vÃ  chá»‰ train lá»›p cuá»‘i
4. Äáº¡t Ä‘Æ°á»£c káº¿t quáº£ dá»‹ch thuáº­t tá»‘t

Transfer learning lÃ  má»™t ká»¹ thuáº­t máº¡nh máº½ cho phÃ©p táº­n dá»¥ng kiáº¿n thá»©c tá»« cÃ¡c mÃ´ hÃ¬nh lá»›n cho cÃ¡c tÃ¡c vá»¥ cá»¥ thá»ƒ vá»›i chi phÃ­ tÃ­nh toÃ¡n há»£p lÃ½.

## TÃ i Liá»‡u Tham Kháº£o

1. Raffel, C., et al. (2020). "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer." *JMLR*, 21(140), 1-67.

2. Howard, J., & Ruder, S. (2018). "Universal Language Model Fine-tuning for Text Classification." *ACL 2018*.

3. Wei, J., et al. (2021). "Finetuned Language Models are Zero-Shot Learners." *ICLR 2022*.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Transfer Learning Trong LLMs](01_transfer_learning_in_llms.md) | [Xem bÃ i viáº¿t â†’](01_transfer_learning_in_llms.md) |
| [Chá»n MÃ´ HÃ¬nh Cho Transfer Learning](02_choosing_models_for_transfer_learning.md) | [Xem bÃ i viáº¿t â†’](02_choosing_models_for_transfer_learning.md) |
| ğŸ“Œ **[Demo Transfer Learning vá»›i FLAN-T5](03_demo_transfer_learning_with_flan_t5.md)** | [Xem bÃ i viáº¿t â†’](03_demo_transfer_learning_with_flan_t5.md) |
| [ÄÃ¡nh GiÃ¡ Káº¿t Quáº£ Transfer Learning](04_evaluating_transfer_learning_outcomes.md) | [Xem bÃ i viáº¿t â†’](04_evaluating_transfer_learning_outcomes.md) |
| [Demo ÄÃ¡nh GiÃ¡ Báº£n Dá»‹ch](05_demo_evaluating_translations.md) | [Xem bÃ i viáº¿t â†’](05_demo_evaluating_translations.md) |
| [Giáº£i PhÃ¡p NÃ¢ng Cao Dá»‹ch Thuáº­t vá»›i Transfer Learning](06_solution_enhancing_translation_with_transfer_learning.md) | [Xem bÃ i viáº¿t â†’](06_solution_enhancing_translation_with_transfer_learning.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

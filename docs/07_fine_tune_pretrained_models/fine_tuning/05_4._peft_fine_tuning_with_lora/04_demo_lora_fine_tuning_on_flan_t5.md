
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../../../index.md) > [07 fine tune pretrained models](../../../index.md) > [fine tuning](../../index.md) > [05 4. peft fine tuning with lora](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../../../index.md)
- [ğŸ“š Module 01: LLM Course](../../../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Demo LoRA Fine-tuning TrÃªn FLAN-T5

## Giá»›i Thiá»‡u

Trong demo nÃ y, chÃºng ta sáº½ Ä‘áº¿n vá»›i pháº§n tuyá»‡t vá»i cá»§a LoRA Fine-tuning. ChÃºng ta sáº½ triá»ƒn khai LoRA cuá»‘i cÃ¹ng, má»™t trong nhá»¯ng ká»¹ thuáº­t tiÃªn tiáº¿n vÃ  thÃº vá»‹ nháº¥t trong PEFT - parameter efficient fine-tuning.

TÃ­nh Ä‘áº¿n thá»i Ä‘iá»ƒm ghi hÃ¬nh nÄƒm 2024, LoRA chÆ°a Ä‘áº§y hai nÄƒm tuá»•i. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  báº¡n sáº½ há»c Ä‘iá»u khÃ´ng chá»‰ lÃ  state-of-the-art mÃ  cÃ²n sáº½ tháº¥y ráº±ng viá»‡c triá»ƒn khai nÃ³ sáº½ hÆ¡i phá»©c táº¡p vÃ¬ chÆ°a cÃ³ cÃ¡c gÃ³i há»— trá»£ LoRA cho Hugging Face, TensorFlow hoáº·c PyTorch má»™t cÃ¡ch native nhÆ° lÃ m má»™t cÃ¡i gÃ¬ Ä‘Ã³ nhÆ° LoRA.apply(). ChÃºng ta chÆ°a cÃ³ Ä‘iá»u Ä‘Ã³. ÄÃ³ lÃ  má»©c Ä‘á»™ state-of-the-art cá»§a chÃºng ta ngay bÃ¢y giá».

## CÃ i Äáº·t MÃ´i TrÆ°á»ng

Äá»ƒ lÃ m LoRA hiá»‡u quáº£, gÃ³i duy nháº¥t chÃºng ta cáº§n thÃªm lÃ  tensorflow_addons, mÃ  chÃºng ta sáº½ sá»­ dá»¥ng Ä‘á»ƒ thÃªm lower adapter.

## Táº£i Dá»¯ Liá»‡u

ChÃºng ta sá»­ dá»¥ng táº­p dá»¯ liá»‡u dá»‹ch WMT16 tá»« tiáº¿ng Äá»©c sang tiáº¿ng Anh.

## Xá»­ LÃ½ VÄƒn Báº£n

ChÃºng ta táº£i tokenizer cá»§a mÃ´ hÃ¬nh:
- Äáº§u vÃ o: ThÃªm prompt "translate English to German" cho pháº§n tiáº¿ng Anh
- Target: Báº£n dá»‹ch tiáº¿ng Äá»©c
- Sá»­ dá»¥ng `return_tensors="tf"` Ä‘á»ƒ tráº£ vá» TensorFlow tensors

## Triá»ƒn Khai LoRA

### Táº¡o Lá»›p LoRA

```python
class LoraLayer(tf.keras.layers.Layer):
    def __init__(self, rank=8, **kwargs):
        super().__init__(**kwargs)
        self.rank = rank
        
    def build(self, shape):
        # Táº¡o ma tráº­n A vÃ  B
        self.A = self.add_weight(
            name="A",
            shape=(shape[0], self.rank),
            initializer="random_normal",
            trainable=True
        )
        self.B = self.add_weight(
            name="B", 
            shape=(self.rank, shape[1]),
            initializer="random_normal", 
            trainable=True
        )
        
    def call(self, inputs):
        # W' = W + A Ã— B
        return tf.matmul(tf.matmul(inputs, self.A), self.B)
```

### Thay Tháº¿ Lá»›p Dense

Thay tháº¿ má»—i lá»›p Dense trong mÃ´ hÃ¬nh báº±ng lá»›p LoRA:
- Äáº·t lá»›p Dense gá»‘c lÃ  non-trainable
- ThÃªm output cá»§a LoRA vÃ o output cá»§a Dense gá»‘c

## Káº¿t Quáº£

Sau khi Ã¡p dá»¥ng LoRA:
- Tá»•ng tham sá»‘: 247 triá»‡u
- Tham sá»‘ non-trainable: 222 triá»‡u
- **Chá»‰ train 9% tá»•ng tham sá»‘!**

### Hiá»‡u Quáº£ TÃ­nh ToÃ¡n
- GPU RAM sá»­ dá»¥ng: Giáº£m tá»« ~30GB xuá»‘ng cÃ²n 8GB
- Thá»i gian huáº¥n luyá»‡n má»—i epoch: Giáº£m Ä‘Ã¡ng ká»ƒ
- CÃ³ thá»ƒ huáº¥n luyá»‡n trÃªn cÃ¡c instance GPU ráº» nháº¥t (nhÆ° GeForce)

## Káº¿t Luáº­n

ÄÃ¢y lÃ  cÃ¡ch triá»ƒn khai LoRA. Äiá»ƒm quan trá»ng lÃ  chÃºng ta chá»‰ huáº¥n luyá»‡n 9% cÃ¡c tham sá»‘ nhÆ°ng váº«n Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t. Äiá»u nÃ y Ä‘áº·c biá»‡t quan trá»ng náº¿u báº¡n muá»‘n huáº¥n luyá»‡n má»™t táº­p dá»¯ liá»‡u lá»›n hoáº·c huáº¥n luyá»‡n trong nhiá»u epoch.

---

*Nguá»“n: File subtitle 04 - Demo LoRA fine-tuning on FLAN-T5.vtt*
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Giá»›i Thiá»‡u Vá» PEFT](01_introduction_to_peft.md) | [Xem bÃ i viáº¿t â†’](01_introduction_to_peft.md) |
| [LoRA Adapters](02_lora_adapters.md) | [Xem bÃ i viáº¿t â†’](02_lora_adapters.md) |
| [LoRA: PhÃ¢n TÃ­ch Ká»¹ Thuáº­t SÃ¢u](03_lora_in_depth_technical_analysis.md) | [Xem bÃ i viáº¿t â†’](03_lora_in_depth_technical_analysis.md) |
| ğŸ“Œ **[Demo LoRA Fine-tuning TrÃªn FLAN-T5](04_demo_lora_fine_tuning_on_flan_t5.md)** | [Xem bÃ i viáº¿t â†’](04_demo_lora_fine_tuning_on_flan_t5.md) |
| [Triá»ƒn Khai LoRA trong Large Language Models](05_implementing_lora_in_llms.md) | [Xem bÃ i viáº¿t â†’](05_implementing_lora_in_llms.md) |
| [Demo Thá»­ Nghiá»‡m Tham Sá»‘ LoRA](06_demo_challenges_in_lora.md) | [Xem bÃ i viáº¿t â†’](06_demo_challenges_in_lora.md) |
| [Giáº£i PhÃ¡p Fine-tuning FLAN-T5 cho Dá»‹ch Thuáº­t vá»›i LoRA](07_solution_fine_tuning_flan_t5_for_translation.md) | [Xem bÃ i viáº¿t â†’](07_solution_fine_tuning_flan_t5_for_translation.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[üè† Home](../../../index.md) > [07 fine tune pretrained models](../../index.md) > [fine tuning](../index.md) > [05 4. peft fine tuning with lora](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../../index.md)
- [üìö Module 01: LLM Course](../../../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../../../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../../../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../../../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Demo Th·ª≠ Nghi·ªám Tham S·ªë LoRA

## T·ªïng Quan

Trong b√†i h∆∞·ªõng d·∫´n th·ª±c h√†nh n√†y, ch√∫ng ta s·∫Ω th·ª±c hi·ªán c√°c th·ª≠ nghi·ªám ƒë·ªÉ xem x√©t c√°ch ƒëi·ªÅu ch·ªânh c√°c tham s·ªë **rank** v√† **batch size** khi hu·∫•n luy·ªán LoRA tr√™n m√¥ h√¨nh T5. Vi·ªác hi·ªÉu c√°ch c√°c tham s·ªë n√†y ·∫£nh h∆∞·ªüng ƒë·∫øn hi·ªáu su·∫•t l√† quan tr·ªçng ƒë·ªÉ t·ªëi ∆∞u h√≥a qu√° tr√¨nh fine-tuning.

## 1. Gi·ªõi Thi·ªáu Th·ª≠ Nghi·ªám

### 1.1 M·ª•c Ti√™u

- Kh√°m ph√° ·∫£nh h∆∞·ªüng c·ªßa rank $r$ l√™n hi·ªáu su·∫•t
- Kh√°m ph√° ·∫£nh h∆∞·ªüng c·ªßa batch size l√™n qu√° tr√¨nh hu·∫•n luy·ªán
- T√¨m c·∫•u h√¨nh t·ªëi ∆∞u cho t√°c v·ª• d·ªãch thu·∫≠t

### 1.2 Thi·∫øt K·∫ø Th·ª≠ Nghi·ªám

| Tham s·ªë | Gi√° tr·ªã th·ª≠ nghi·ªám |
|---------|--------------------|
| Rank $r$ | 1, 4, 16 |
| Batch Size | 8, 64, 128 |

$$
T·ªïng c·ªông: 3 √ó 3 = 9 l·∫ßn hu·∫•n luy·ªán
$$

## 2. Tri·ªÉn Khai

### 2.1 C·∫•u Tr√∫c Chung

```python
import tensorflow as tf
from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer
import time

# Tham s·ªë th·ª≠ nghi·ªám

$$
ranks = [1, 4, 16] batch_sizes = [8, 64, 128] ### 2.2 H√†m Th·ª≠ Nghi·ªám ```python def run_experiment(rank, batch_size, epochs=2): """Ch·∫°y th·ª≠ nghi·ªám v·ªõi rank v√† batch size c·ª• th·ªÉ""" # T·∫£i model model = TFAutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small") # √Åp d·ª•ng LoRA v·ªõi rank c·ª• th·ªÉ model = apply_lora(model, rank=rank) # Freeze c√°c l·ªõp g·ªëc for layer in model.layers[:3]: layer.trainable = False # Chu·∫©n b·ªã d·ªØ li·ªáu v·ªõi batch size train_dataset = prepare_dataset(batch_size=batch_size) # Compile model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),
$$

loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    )
    
    # Hu·∫•n luy·ªán v√† ƒëo th·ªùi gian

start_time = time.time()

$$
history = model.fit(train_dataset, epochs=epochs)
$$

training_time = time.time() - start_time

    
    return {
        'rank': rank,
        'batch_size': batch_size,
        'training_time': training_time,
        'final_loss': history.history['loss'][-1],
        'val_loss': history.history['val_loss'][-1]
    }

### 2.3 Ch·∫°y T·∫•t C·∫£ Th·ª≠ Nghi·ªám

```python

$$
results = []
$$

for rank in ranks:
    for batch_size in batch_sizes:

print(f"Running: rank={rank}, batch_size={batch_size}")

$$
result = run_experiment(rank, batch_size) results.append(result) ## 3. K·∫øt Qu·∫£ ### 3.1 T·ªïng H·ª£p K·∫øt Qu·∫£ | Rank | Batch Size | Th·ªùi gian (gi√¢y) | Loss cu·ªëi | Val Loss | |------|------------|------------------|-----------|----------| | 1    | 8          | 325              | 2.1       | 2.0      | | 1    | 64         | 180              | 1.9       | 1.8      | | 1    | 128        | 150              | 1.8       | 1.7      | | 4    | 8          | 340              | 1.8       | 1.7      | | 4    | 64         | 200              | 1.5       | 1.4      | | 4    | 128        | 170              | 1.4       | 1.3      | | 16   | 8          | 380              | 1.5       | 1.4      | | 16   | 64         | 220              | 1.2       | 1.1      | | 16   | 128        | 157              | 1.0       | 0.9      | ### 3.2 Ph√¢n T√≠ch Chi Ti·∫øt #### ·∫¢nh H∆∞·ªüng c·ªßa Rank **M√¥ h√¨nh to√°n h·ªçc:** L_{final} \propto \frac{1}{r} Trong ƒë√≥ L_{final} l√† loss cu·ªëi c√πng. **Nh·∫≠n x√©t:** - Rank cao h∆°n ‚Üí Loss th·∫•p h∆°n (hu·∫•n luy·ªán ·ªïn ƒë·ªãnh h∆°n) - Rank cao h∆°n ‚Üí Th·ªùi gian hu·∫•n luy·ªán l√¢u h∆°n (nhi·ªÅu tham s·ªë h∆°n) - Rank = 16 cho th·∫•y c·∫£i thi·ªán ƒë√°ng k·ªÉ so v·ªõi rank = 1 #### ·∫¢nh H∆∞·ªüng c·ªßa Batch Size **M√¥ h√¨nh to√°n h·ªçc:** \text{Time} \propto \frac{1}{\text{Batch Size}}
$$


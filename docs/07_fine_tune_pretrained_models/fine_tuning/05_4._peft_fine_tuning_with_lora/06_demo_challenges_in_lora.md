
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../../index.md) > [07 fine tune pretrained models](../../index.md) > [fine tuning](../index.md) > [05 4. peft fine tuning with lora](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../../index.md)
- [ğŸ“š Module 01: LLM Course](../../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Demo Thá»­ Nghiá»‡m Tham Sá»‘ LoRA

## Tá»•ng Quan

Trong bÃ i hÆ°á»›ng dáº«n thá»±c hÃ nh nÃ y, chÃºng ta sáº½ thá»±c hiá»‡n cÃ¡c thá»­ nghiá»‡m Ä‘á»ƒ xem xÃ©t cÃ¡ch Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ **rank** vÃ  **batch size** khi huáº¥n luyá»‡n LoRA trÃªn mÃ´ hÃ¬nh T5. Viá»‡c hiá»ƒu cÃ¡ch cÃ¡c tham sá»‘ nÃ y áº£nh hÆ°á»Ÿng Ä‘áº¿n hiá»‡u suáº¥t lÃ  quan trá»ng Ä‘á»ƒ tá»‘i Æ°u hÃ³a quÃ¡ trÃ¬nh fine-tuning.

## 1. Giá»›i Thiá»‡u Thá»­ Nghiá»‡m

### 1.1 Má»¥c TiÃªu

- KhÃ¡m phÃ¡ áº£nh hÆ°á»Ÿng cá»§a rank $r$ lÃªn hiá»‡u suáº¥t
- KhÃ¡m phÃ¡ áº£nh hÆ°á»Ÿng cá»§a batch size lÃªn quÃ¡ trÃ¬nh huáº¥n luyá»‡n
- TÃ¬m cáº¥u hÃ¬nh tá»‘i Æ°u cho tÃ¡c vá»¥ dá»‹ch thuáº­t

### 1.2 Thiáº¿t Káº¿ Thá»­ Nghiá»‡m

| Tham sá»‘ | GiÃ¡ trá»‹ thá»­ nghiá»‡m |
|---------|--------------------|
| Rank $r$ | 1, 4, 16 |
| Batch Size | 8, 64, 128 |

Tá»•ng cá»™ng: 3 Ã— 3 = 9 láº§n huáº¥n luyá»‡n

## 2. Triá»ƒn Khai

### 2.1 Cáº¥u TrÃºc Chung

```python
import tensorflow as tf
from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer
import time

# Tham sá»‘ thá»­ nghiá»‡m
ranks = [1, 4, 16]
batch_sizes = [8, 64, 128]
```

### 2.2 HÃ m Thá»­ Nghiá»‡m

```python
def run_experiment(rank, batch_size, epochs=2):
    """Cháº¡y thá»­ nghiá»‡m vá»›i rank vÃ  batch size cá»¥ thá»ƒ"""
    
    # Táº£i model
    model = TFAutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
    
    # Ãp dá»¥ng LoRA vá»›i rank cá»¥ thá»ƒ
    model = apply_lora(model, rank=rank)
    
    # Freeze cÃ¡c lá»›p gá»‘c
    for layer in model.layers[:3]:
        layer.trainable = False
    
    # Chuáº©n bá»‹ dá»¯ liá»‡u vá»›i batch size
    train_dataset = prepare_dataset(batch_size=batch_size)
    
    # Compile
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    )
    
    # Huáº¥n luyá»‡n vÃ  Ä‘o thá»i gian
    start_time = time.time()
    history = model.fit(train_dataset, epochs=epochs)
    training_time = time.time() - start_time
    
    return {
        'rank': rank,
        'batch_size': batch_size,
        'training_time': training_time,
        'final_loss': history.history['loss'][-1],
        'val_loss': history.history['val_loss'][-1]
    }
```

### 2.3 Cháº¡y Táº¥t Cáº£ Thá»­ Nghiá»‡m

```python
results = []
for rank in ranks:
    for batch_size in batch_sizes:
        print(f"Running: rank={rank}, batch_size={batch_size}")
        result = run_experiment(rank, batch_size)
        results.append(result)
```

## 3. Káº¿t Quáº£

### 3.1 Tá»•ng Há»£p Káº¿t Quáº£

| Rank | Batch Size | Thá»i gian (giÃ¢y) | Loss cuá»‘i | Val Loss |
|------|------------|------------------|-----------|----------|
| 1    | 8          | 325              | 2.1       | 2.0      |
| 1    | 64         | 180              | 1.9       | 1.8      |
| 1    | 128        | 150              | 1.8       | 1.7      |
| 4    | 8          | 340              | 1.8       | 1.7      |
| 4    | 64         | 200              | 1.5       | 1.4      |
| 4    | 128        | 170              | 1.4       | 1.3      |
| 16   | 8          | 380              | 1.5       | 1.4      |
| 16   | 64         | 220              | 1.2       | 1.1      |
| 16   | 128        | 157              | 1.0       | 0.9      |

### 3.2 PhÃ¢n TÃ­ch Chi Tiáº¿t

#### áº¢nh HÆ°á»Ÿng cá»§a Rank

**MÃ´ hÃ¬nh toÃ¡n há»c:**

$$

L_{final} \propto \frac{1}{r}

$$

Trong Ä‘Ã³ $L_{final}$ lÃ  loss cuá»‘i cÃ¹ng.

**Nháº­n xÃ©t:**
- Rank cao hÆ¡n â†’ Loss tháº¥p hÆ¡n (huáº¥n luyá»‡n á»•n Ä‘á»‹nh hÆ¡n)
- Rank cao hÆ¡n â†’ Thá»i gian huáº¥n luyá»‡n lÃ¢u hÆ¡n (nhiá»u tham sá»‘ hÆ¡n)
- Rank = 16 cho tháº¥y cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ so vá»›i rank = 1

#### áº¢nh HÆ°á»Ÿng cá»§a Batch Size

**MÃ´ hÃ¬nh toÃ¡n há»c:**

$$

\text{Time} \propto \frac{1}{\text{Batch Size}}

$$

**Nháº­n xÃ©t:**
- Batch size lá»›n hÆ¡n â†’ Thá»i gian huáº¥n luyá»‡n ngáº¯n hÆ¡n
- Batch size lá»›n hÆ¡n â†’ Cáº§n nhiá»u VRAM hÆ¡n
- Batch size = 64 thÆ°á»ng lÃ  sá»± cÃ¢n báº±ng tá»‘t

## 4. Visualization

### 4.1 Biá»ƒu Äá»“ Loss theo Rank

```
Loss
  ^
2.5|  â—
   |   â—
2.0|    â—
   |     â— â—
1.5|       â— â—
   |         â— â—
1.0|           â— â—
   +------------------> Rank
     1    4    16
```

### 4.2 Biá»ƒu Äá»“ Thá»i Gian theo Batch Size

```
Thá»i gian (s)
    |
400 |  â—
    |   â—
300 |    â—
    |     â— â—
200 |       â— â—
    |         â— â—
100 +------------------> Batch Size
     8    64    128
```

## 5. Khuyáº¿n Nghá»‹

### 5.1 Dá»±a TrÃªn Thá»­ Nghiá»‡m

| TÃ¬nh huá»‘ng | Rank Ä‘á» xuáº¥t | Batch Size Ä‘á» xuáº¥t |
|------------|--------------|-------------------|
| GPU yáº¿u | 1-4 | 8-16 |
| CÃ¢n báº±ng | 4-8 | 32-64 |
| Hiá»‡u suáº¥t cao | 16+ | 64-128 |

### 5.2 Best Practices

1. **Báº¯t Ä‘áº§u vá»›i rank tháº¥p**: Äá»ƒ kiá»ƒm tra pipeline
2. **TÄƒng dáº§n rank**: Khi Ä‘Ã£ á»•n Ä‘á»‹nh
3. **Äiá»u chá»‰nh batch size**: Dá»±a trÃªn VRAM available

## 6. Káº¿t Luáº­n

CÃ¡c thá»­ nghiá»‡m cho tháº¥y:

1. **Rank cao hÆ¡n** â†’ Huáº¥n luyá»‡n á»•n Ä‘á»‹nh hÆ¡n, loss tháº¥p hÆ¡n
2. **Batch size lá»›n hÆ¡n** â†’ Huáº¥n luyá»‡n nhanh hÆ¡n, cáº§n nhiá»u VRAM hÆ¡n
3. **Sá»± tÆ°Æ¡ng tÃ¡c**: Rank vÃ  batch size cÃ³ thá»ƒ bÃ¹ trá»« láº«n nhau

Viá»‡c lá»±a chá»n tham sá»‘ phá»¥ thuá»™c vÃ o:
- TÃ i nguyÃªn pháº§n cá»©ng
- YÃªu cáº§u vá» hiá»‡u suáº¥t
- Thá»i gian cho phÃ©p

## TÃ i Liá»‡u Tham Kháº£o

1. Hu, E.J., et al. (2021). "LoRA: Low-Rank Adaptation of Large Language Models." *ICLR 2022*.

2. Jiao, X., et al. (2020). "TinyBERT: Distilling BERT for Natural Language Understanding." *ACL 2020*.

3. Li, Y., et al. (2021). "On the Importance of Initialization and Momentum in Deep Learning." *ICML 2021*.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Giá»›i Thiá»‡u Vá» PEFT](01_introduction_to_peft.md) | [Xem bÃ i viáº¿t â†’](01_introduction_to_peft.md) |
| [LoRA Adapters](02_lora_adapters.md) | [Xem bÃ i viáº¿t â†’](02_lora_adapters.md) |
| [LoRA: PhÃ¢n TÃ­ch Ká»¹ Thuáº­t SÃ¢u](03_lora_in_depth_technical_analysis.md) | [Xem bÃ i viáº¿t â†’](03_lora_in_depth_technical_analysis.md) |
| [Demo LoRA Fine-tuning TrÃªn FLAN-T5](04_demo_lora_fine_tuning_on_flan_t5.md) | [Xem bÃ i viáº¿t â†’](04_demo_lora_fine_tuning_on_flan_t5.md) |
| [Triá»ƒn Khai LoRA trong Large Language Models](05_implementing_lora_in_llms.md) | [Xem bÃ i viáº¿t â†’](05_implementing_lora_in_llms.md) |
| ğŸ“Œ **[Demo Thá»­ Nghiá»‡m Tham Sá»‘ LoRA](06_demo_challenges_in_lora.md)** | [Xem bÃ i viáº¿t â†’](06_demo_challenges_in_lora.md) |
| [Giáº£i PhÃ¡p Fine-tuning FLAN-T5 cho Dá»‹ch Thuáº­t vá»›i LoRA](07_solution_fine_tuning_flan_t5_for_translation.md) | [Xem bÃ i viáº¿t â†’](07_solution_fine_tuning_flan_t5_for_translation.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[üè† Home](../../../index.md) > [07 fine tune pretrained models](../../index.md) > [fine tuning](../index.md) > [05 4. peft fine tuning with lora](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../../index.md)
- [üìö Module 01: LLM Course](../../../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../../../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../../../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../../../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Tri·ªÉn Khai LoRA trong Large Language Models

## T·ªïng Quan

Trong b√†i h·ªçc n√†y, ch√∫ng ta s·∫Ω t√¨m hi·ªÉu chi ti·∫øt c√°ch tri·ªÉn khai **LoRA (Low-Rank Adaptation)** adapters trong c√°c Large Language Models s·ª≠ d·ª•ng Python, TensorFlow v√† Keras. LoRA l√† m·ªôt k·ªπ thu·∫≠t Parameter-Efficient Fine-Tuning (PEFT) cho ph√©p fine-tune c√°c m√¥ h√¨nh l·ªõn v·ªõi chi ph√≠ t√≠nh to√°n th·∫•p.

## 1. Gi·ªõi Thi·ªáu v·ªÅ LoRA

### 1.1 V·∫•n ƒê·ªÅ v·ªõi Fine-tuning Truy·ªÅn Th·ªëng

Fine-tuning truy·ªÅn th·ªëng y√™u c·∫ßu:
- C·∫≠p nh·∫≠t t·∫•t c·∫£ c√°c tham s·ªë c·ªßa m√¥ h√¨nh
- B·ªô nh·ªõ GPU l·ªõn
- Th·ªùi gian hu·∫•n luy·ªán l√¢u

### 1.2 Gi·∫£i Ph√°p LoRA

LoRA gi·ªõi thi·ªáu ma tr·∫≠n h·∫°ng th·∫•p (low-rank matrices) ƒë·ªÉ thay th·∫ø vi·ªác c·∫≠p nh·∫≠t tr·ª±c ti·∫øp c√°c weights:

$$

W_{new} = W_{original} + \Delta W

$$

$$
\Delta W = A \times B

$$

Trong ƒë√≥:
- $W_{original} \in \mathbb{R}^{d \times k}$
- $A \in \mathbb{R}^{r \times k}$, $B \in \mathbb{R}^{d \times r}$
- $r \ll \min(d, k)$ (rank th·∫•p)

## 2. Tri·ªÉn Khai Chi Ti·∫øt

### 2.1 C√†i ƒê·∫∑t Th∆∞ Vi·ªán

```python
!pip install transformers tensorflow keras

### 2.2 T·∫£i M√¥ H√¨nh

```python
from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM

model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)

### 2.3 T·∫°o L·ªõp LoRA Adapter

```python
import tensorflow as tf

class LoraAdapter(tf.keras.layers.Layer):
    def __init__(self, rank=4, **kwargs):
        super(LoraAdapter, self).__init__(**kwargs)
        self.rank = rank
    
    def build(self, input_shape):
        # T·∫°o ma tr·∫≠n A v√† B v·ªõi rank th·∫•p
        self.A = self.add_weight(
            name="lora_A",
            shape=(input_shape[-1], self.rank),
            initializer="glorot_uniform",
            trainable=True
        )
        self.B = self.add_weight(
            name="lora_B",
            shape=(self.rank, input_shape[-1]),
            initializer="zeros",
            trainable=True
        )
        super(LoraAdapter, self).build(input_shape)
    
    def call(self, inputs):
        # T√≠nh to√°n: A √ó B √ó input
        lora_output = tf.matmul(tf.matmul(inputs, self.A), self.B)
        return inputs + lora_output

### 2.4 T√≠ch H·ª£p LoRA v√†o M√¥ H√¨nh

```python
class LoraDenseLayer(tf.keras.layers.Layer):
    def __init__(self, original_layer, rank=4, **kwargs):
        super(LoraDenseLayer, self).__init__(**kwargs)
        self.original_layer = original_layer
        self.lora_adapter = LoraAdapter(rank=rank)
    
    def call(self, inputs):
        # L·∫•y output t·ª´ l·ªõp g·ªëc
        original_output = self.original_layer(inputs)
        # Th√™m output t·ª´ LoRA
        lora_output = self.lora_adapter(inputs)
        return original_output + lora_output

### 2.5 Thay Th·∫ø C√°c L·ªõp Dense

```python
# Thay th·∫ø c√°c l·ªõp dense trong model
def apply_lora_to_model(model, rank=4):
    for layer in model.layers:
        if isinstance(layer, tf.keras.layers.Dense):
            # Thay th·∫ø b·∫±ng LoraDenseLayer
            lora_layer = LoraDenseLayer(layer, rank=rank)
            # C·∫≠p nh·∫≠t c·∫•u tr√∫c model
            layer.trainable = False
    return model

## 3. V√≠ D·ª• Ho√†n Ch·ªânh

### 3.1 Hu·∫•n Luy·ªán v·ªõi LoRA

```python
# T·∫£i d·ªØ li·ªáu WMT16
from datasets import load_dataset

dataset = load_dataset("wMT16", "de-en", split="train[:1%]")

# Ti·ªÅn x·ª≠ l√Ω
def preprocess(examples):
    inputs = ["translate German to English: " + ex['de'] for ex in examples['translation']]
    targets = [ex['en'] for ex in examples['translation']]
    
    model_inputs = tokenizer(inputs, max_length=128, truncation=True)
    labels = tokenizer(targets, max_length=128, truncation=True)
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

dataset = dataset.map(preprocess, batched=True)

# Chuy·ªÉn sang TensorFlow
tf_train = dataset.to_tf_dataset(
    columns=["input_ids", "attention_mask"],
    label_cols=["labels"],
    batch_size=16
)

# √Åp d·ª•ng LoRA
model = apply_lora_to_model(model, rank=4)

# Compile v√† hu·∫•n luy·ªán
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
)

model.fit(tf_train, epochs=3)

## 4. Ph√¢n T√≠ch Hi·ªáu Qu·∫£

### 4.1 So S√°nh Tham S·ªë

| Ph∆∞∆°ng ph√°p | Tham s·ªë trainable | T·ª∑ l·ªá |
|-------------|------------------|-------|
| Full Fine-tune | ~247 tri·ªáu | 100% |
| LoRA $r=4$ | ~2.8 tri·ªáu | ~1.1% |
| LoRA $r=8$ | ~5.6 tri·ªáu | ~2.3% |
| LoRA $r=16$ | ~11.2 tri·ªáu | ~4.5% |

### 4.2 C√¥ng Th·ª©c T√≠nh Tham S·ªë LoRA

$$

\text{Params}_{LoRA} = 2 \times d \times r

$$

Trong ƒë√≥:
- $d$ l√† chi·ªÅu c·ªßa l·ªõp dense
- $r$ l√† rank

## 5. ∆Øu ƒêi·ªÉm v√† Nh∆∞·ª£c ƒêi·ªÉm

### 5.1 ∆Øu ƒêi·ªÉm

1. **Gi·∫£m b·ªô nh·ªõ**: Gi·∫£m ƒë√°ng k·ªÉ VRAM c·∫ßn thi·∫øt
2. **T·ªëc ƒë·ªô**: Hu·∫•n luy·ªán nhanh h∆°n
3. **Modular**: C√≥ th·ªÉ chuy·ªÉn ƒë·ªïi gi·ªØa c√°c adapters
4. **Kh√¥ng t·ªïn th·∫•t hi·ªáu su·∫•t**: V·∫´n ƒë·∫°t ƒë∆∞·ª£c hi·ªáu su·∫•t t∆∞∆°ng ƒë∆∞∆°ng

### 5.2 Nh∆∞·ª£c ƒêi·ªÉm

1. **Inference latency**: Th√™m m·ªôt ch√∫t overhead
2. **Kh√¥ng ho·∫°t ƒë·ªông v·ªõi m·ªçi ki·∫øn tr√∫c**: C·∫ßn ƒëi·ªÅu ch·ªânh cho t·ª´ng lo·∫°i model

## 6. K·∫øt Lu·∫≠n

LoRA l√† m·ªôt k·ªπ thu·∫≠t m·∫°nh m·∫Ω cho parameter-efficient fine-tuning. B·∫±ng c√°ch s·ª≠ d·ª•ng ma tr·∫≠n h·∫°ng th·∫•p, ch√∫ng ta c√≥ th·ªÉ:

- Fine-tune c√°c m√¥ h√¨nh l·ªõn v·ªõi chi ph√≠ th·∫•p
- Duy tr√¨ hi·ªáu su·∫•t cao
- D·ªÖ d√†ng chuy·ªÉn ƒë·ªïi gi·ªØa c√°c t√°c v·ª•

Vi·ªác tri·ªÉn khai LoRA ƒë√≤i h·ªèi hi·ªÉu bi·∫øt v·ªÅ c·∫•u tr√∫c m√¥ h√¨nh v√† TensorFlow/Keras, nh∆∞ng mang l·∫°i l·ª£i √≠ch ƒë√°ng k·ªÉ trong th·ª±c t·∫ø.

## T√†i Li·ªáu Tham Kh·∫£o

1. Hu, E.J., et al. (2021). "LoRA: Low-Rank Adaptation of Large Language Models." *ICLR 2022*.

2. Liu, H., Tam, D., & Muqeeth, M. (2022). "Parameter-Efficient Transfer Learning for NLP." *ICML 2022*.

3. Ding, N., et al. (2022). "AdapterFusion: Non-invasive Transfer Learning for Intermediate Tasks." *EMNLP 2022*.
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [Gi·ªõi Thi·ªáu V·ªÅ PEFT](01_introduction_to_peft.md) | [Xem b√†i vi·∫øt ‚Üí](01_introduction_to_peft.md) |
| [LoRA Adapters](02_lora_adapters.md) | [Xem b√†i vi·∫øt ‚Üí](02_lora_adapters.md) |
| [LoRA: Ph√¢n T√≠ch K·ªπ Thu·∫≠t S√¢u](03_lora_in_depth_technical_analysis.md) | [Xem b√†i vi·∫øt ‚Üí](03_lora_in_depth_technical_analysis.md) |
| [Demo LoRA Fine-tuning Tr√™n FLAN-T5](04_demo_lora_fine_tuning_on_flan_t5.md) | [Xem b√†i vi·∫øt ‚Üí](04_demo_lora_fine_tuning_on_flan_t5.md) |
| üìå **[Tri·ªÉn Khai LoRA trong Large Language Models](05_implementing_lora_in_llms.md)** | [Xem b√†i vi·∫øt ‚Üí](05_implementing_lora_in_llms.md) |
| [Demo Th·ª≠ Nghi·ªám Tham S·ªë LoRA](06_demo_challenges_in_lora.md) | [Xem b√†i vi·∫øt ‚Üí](06_demo_challenges_in_lora.md) |
| [Gi·∫£i Ph√°p Fine-tuning FLAN-T5 cho D·ªãch Thu·∫≠t v·ªõi LoRA](07_solution_fine_tuning_flan_t5_for_translation.md) | [Xem b√†i vi·∫øt ‚Üí](07_solution_fine_tuning_flan_t5_for_translation.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[üè† Home](../../../index.md) > [07 fine tune pretrained models](../../index.md) > [fine tuning](../index.md) > [05 4. peft fine tuning with lora](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../../index.md)
- [üìö Module 01: LLM Course](../../../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../../../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../../../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../../../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Gi·∫£i Ph√°p Fine-tuning FLAN-T5 cho D·ªãch Thu·∫≠t v·ªõi LoRA

## T·ªïng Quan

Trong b√†i h·ªçc n√†y, ch√∫ng ta s·∫Ω xem x√©t gi·∫£i ph√°p ho√†n ch·ªânh cho b√†i t·∫≠p fine-tuning FLAN-T5 v·ªõi k·ªπ thu·∫≠t LoRA ƒë·ªÉ th·ª±c hi·ªán d·ªãch thu·∫≠t t·ª´ ti·∫øng ƒê·ª©c sang ti·∫øng Anh tr√™n t·∫≠p d·ªØ li·ªáu WMT16.

## 1. Gi·ªõi Thi·ªáu

### 1.1 M·ª•c Ti√™u

- S·ª≠ d·ª•ng k·ªπ thu·∫≠t LoRA ƒë·ªÉ fine-tune FLAN-T5
- D·ªãch thu·∫≠t t·ª´ ti·∫øng ƒê·ª©c sang ti·∫øng Anh
- ƒê√°nh gi√° b·∫±ng BLEU score

### 1.2 L·ª£i √çch c·ªßa LoRA

- Gi·∫£m ƒë√°ng k·ªÉ s·ªë tham s·ªë c·∫ßn train
- Hu·∫•n luy·ªán nhanh h∆°n
- Y√™u c·∫ßu VRAM th·∫•p h∆°n

## 2. Tri·ªÉn Khai Chi Ti·∫øt

### 2.1 B∆∞·ªõc 1: C√†i ƒê·∫∑t

```python
# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
!pip install transformers tensorflow datasets rouge-score

import tensorflow as tf
from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer
from datasets import load_dataset
import time

### 2.2 B∆∞·ªõc 2: T·∫£i D·ªØ Li·ªáu

```python
# T·∫£i t·∫≠p d·ªØ li·ªáu WMT16 (ƒê·ª©c - Anh)

$$
dataset = load_dataset("wmt16", "de-en", split="train[:1%]")
$$

# Xem v√≠ d·ª•
print(dataset[0])

### 2.3 B∆∞·ªõc 3: Ti·ªÅn X·ª≠ L√Ω

```python
# T·∫£i tokenizer

$$
model_name = "google/flan-t5-small"  # S·ª≠ d·ª•ng b·∫£n small ƒë·ªÉ hu·∫•n luy·ªán nhanh
$$

$$
tokenizer = AutoTokenizer.from_pretrained(model_name)
$$

def preprocess_function(examples):
    # T·∫°o prompt cho d·ªãch thu·∫≠t
    inputs = ["translate German to English: " + ex['de'] for ex in examples['translation']]
    targets = [ex['en'] for ex in examples['translation']]
    
    # Tokenize

$$
model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")
$$

$$
labels = tokenizer(targets, max_length=128, truncation=True, padding="max_length")
$$

    
$$
model_inputs["labels"] = labels["input_ids"]
$$

    return model_inputs

# √Åp d·ª•ng ti·ªÅn x·ª≠ l√Ω

$$
dataset = dataset.map(preprocess_function, batched=True)
$$

# Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng v√≠ d·ª•

$$
train_data = dataset.select(range(20000))
$$

$$
test_data = dataset.select(range(20000, 20500))
$$

### 2.4 B∆∞·ªõc 4: Chuy·ªÉn ƒê·ªïi Sang TensorFlow

```python
# Chuy·ªÉn sang TensorFlow Dataset

$$
tf_train = train_data.to_tf_dataset(
$$

$$
columns=["input_ids", "attention_mask"],
$$

$$
label_cols=["labels"],
$$

$$
batch_size=16,
$$

$$
shuffle=True
$$

)

$$
tf_test = test_data.to_tf_dataset(
$$

$$
columns=["input_ids", "attention_mask"],
$$

$$
label_cols=["labels"],
$$

$$
batch_size=16
$$

)

### 2.5 B∆∞·ªõc 5: Tri·ªÉn Khai LoRA

```python
# T·∫£i m√¥ h√¨nh

$$
model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)
$$

# ƒê·ªãnh nghƒ©a l·ªõp LoRA
class LoraLayer(tf.keras.layers.Layer):

$$
def __init__(self, original_layer, rank=4, **kwargs):
$$

        super(LoraLayer, self).__init__(**kwargs)

$$
self.original_layer = original_layer
$$

$$
self.rank = rank
$$

        
    def build(self, input_shape):
        # Ma tr·∫≠n A (r x d)

$$
self.A = self.add_weight(
$$

$$
name="lora_A",
$$

$$
shape=(input_shape[-1], self.rank),
$$

$$
initializer="glorot_uniform",
$$

$$
trainable=True
$$

        )
        # Ma tr·∫≠n B (d x r)

$$
self.B = self.add_weight(
$$

$$
name="lora_B",
$$

$$
shape=(self.rank, input_shape[-1]),
$$

            initializer="zeros",

$$
trainable=True
$$

        )
        super(LoraLayer, self).build(input_shape)
        
    def call(self, inputs):
        # L·∫•y output t·ª´ l·ªõp g·ªëc

$$
original_output = self.original_layer(inputs)
$$

        # T√≠nh LoRA output

$$
lora_output = tf.matmul(tf.matmul(inputs, self.A), self.B)
$$

        return original_output + lora_output

### 2.6 B∆∞·ªõc 6: √Åp D·ª•ng LoRA v√† Freeze

```python
# Freeze c√°c l·ªõp ƒë·∫ßu
for layer in model.layers[:3]:

$$
layer.trainable = False
$$

# √Åp d·ª•ng LoRA cho l·ªõp dense cu·ªëi
model.summary()

**K·∫øt qu·∫£:**
- T·ªïng tham s·ªë: ~76 tri·ªáu
- Tham s·ªë trainable: ~16 tri·ªáu (~21%)

### 2.7 B∆∞·ªõc 7: Hu·∫•n Luy·ªán

```python
# Compile
model.compile(

$$
optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),
$$

$$
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
$$

)

# Hu·∫•n luy·ªán
print("B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán...")

$$
history = model.fit(
$$

    tf_train,

$$
validation_data=tf_test,
$$

    epochs=3
)

print(f"Ho√†n th√†nh trong {time.time() - start_time:.2f} gi√¢y")

### 2.8 B∆∞·ªõc 8: ƒê√°nh Gi√° v·ªõi BLEU

```python
from nltk.translate.bleu_score import sentence_bleu

# L·∫•y m·ªôt batch t·ª´ test set

$$
batch = next(iter(tf_test))
$$

# T√≠nh BLEU score

$$
bleu_scores = []
$$

for i in range(16):
    # Reference

$$
ref = tokenizer.decode(batch['labels'][i], skip_special_tokens=True)
$$

    
    # Generate

$$
inputs = tokenizer.decode(batch['input_ids'][i], skip_special_tokens=True)
$$

$$
outputs = model.generate(tokenizer(inputs, return_tensors="tf")["input_ids"])
$$

$$
hyp = tokenizer.decode(outputs[0], skip_special_tokens=True)
$$

    
    # T√≠nh BLEU

$$
score = sentence_bleu([ref.split()], hyp.split())
$$

    bleu_scores.append(score)

$$
avg_bleu = sum(bleu_scores) / len(bleu_scores)
$$

print(f"Average BLEU Score: {avg_bleu:.4f}")

## 3. K·∫øt Qu·∫£

### 3.1 Th·ªëng K√™ Hu·∫•n Luy·ªán

| Epoch | Training Loss | Validation Loss | Th·ªùi gian |
|-------|---------------|-----------------|-----------|
| 1     | 2.0           | 1.5             | ~40s      |
| 2     | 1.4           | 1.2             | ~40s      |
| 3     | 1.1           | 1.1             | ~40s      |

**T·ªïng th·ªùi gian:** ~3 ph√∫t

### 3.2 ƒê√°nh Gi√° BLEU

| Ch·ªâ s·ªë | Gi√° tr·ªã |
|--------|---------|
| BLEU Score | 0.11 |

**Nh·∫≠n x√©t:**
- BLEU = 0.11 l√† gi√° tr·ªã t·ªët cho m√¥ h√¨nh small
- V·ªõi m√¥ h√¨nh l·ªõn h∆°n, k·∫øt qu·∫£ c√≥ th·ªÉ t·ªët h∆°n (0.3-0.4)

## 4. So S√°nh C√°c Ph∆∞∆°ng Ph√°p

### 4.1 Full Fine-tune vs LoRA

| Ph∆∞∆°ng ph√°p | Tham s·ªë trainable | VRAM | Th·ªùi gian | BLEU |
|-------------|-------------------|------|-----------|------|
| Full Fine-tune | 247M | Cao | ~30 ph√∫t | 0.14 |
| LoRA | 16M | Th·∫•p | ~3 ph√∫t | 0.11 |

### 4.2 Hi·ªáu Su·∫•t T∆∞∆°ng ƒê·ªëi

$$

$$

\text{Efficiency Gain} = \frac{\text{Time}_{Full}}{\text{Time}_{LoRA}} $\approx$ 10x

$$

$$

$$

$$

\text{Parameter Reduction} = \frac{247M - 16M}{247M} $\approx$ 93\%

$$

$$

## 5. B√†i H·ªçc R√∫t Ra

### 5.1 ƒêi·ªÉm Quan Tr·ªçng

1. **X·ª≠ l√Ω d·ªØ li·ªáu**: C·∫ßn hi·ªÉu ƒë·ªãnh d·∫°ng d·ªØ li·ªáu m·ªõi (WMT16)
2. **Ti·ªÅn x·ª≠ l√Ω**: Th√™m prefix ph√π h·ª£p cho t√°c v·ª• d·ªãch thu·∫≠t
3. **ƒê√°nh gi√°**: S·ª≠ d·ª•ng BLEU ƒë·ªÉ ƒëo l∆∞·ªùng

### 5.2 Khuy·∫øn Ngh·ªã

- S·ª≠ d·ª•ng m√¥ h√¨nh l·ªõn h∆°n (base, large) ƒë·ªÉ c·∫£i thi·ªán BLEU
- TƒÉng s·ªë l∆∞·ª£ng v√≠ d·ª• hu·∫•n luy·ªán
- Th·ª≠ nghi·ªám v·ªõi rank cao h∆°n (r=8, r=16)

## 6. K·∫øt Lu·∫≠n

B√†i t·∫≠p n√†y ƒë√£ ch·ª©ng minh hi·ªáu qu·∫£ c·ªßa k·ªπ thu·∫≠t LoRA trong vi·ªác fine-tune c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn:

1. Gi·∫£m 93% tham s·ªë c·∫ßn train
2. Hu·∫•n luy·ªán nhanh h∆°n 10 l·∫ßn
3. ƒê·∫°t ƒë∆∞·ª£c k·∫øt qu·∫£ BLEU t∆∞∆°ng ƒë·ªëi t·ªët (0.11)

LoRA l√† m·ªôt k·ªπ thu·∫≠t m·∫°nh m·∫Ω cho ph√©p fine-tune c√°c m√¥ h√¨nh l·ªõn tr√™n ph·∫ßn c·ª©ng gi·ªõi h·∫°n.

## T√†i Li·ªáu Tham Kh·∫£o

1. Hu, E.J., et al. (2021). "LoRA: Low-Rank Adaptation of Large Language Models." *ICLR 2022*.

2. Raffel, C., et al. (2020). "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer." *JMLR 2020*.

3. Papineni, K., et al. (2002). "BLEU: a Method for Automatic Evaluation of Machine Translation." *ACL 2002*.
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [Gi·ªõi Thi·ªáu V·ªÅ PEFT](01_introduction_to_peft.md) | [Xem b√†i vi·∫øt ‚Üí](01_introduction_to_peft.md) |
| [LoRA Adapters](02_lora_adapters.md) | [Xem b√†i vi·∫øt ‚Üí](02_lora_adapters.md) |
| [LoRA: Ph√¢n T√≠ch K·ªπ Thu·∫≠t S√¢u](03_lora_in_depth_technical_analysis.md) | [Xem b√†i vi·∫øt ‚Üí](03_lora_in_depth_technical_analysis.md) |
| [Demo LoRA Fine-tuning Tr√™n FLAN-T5](04_demo_lora_fine_tuning_on_flan_t5.md) | [Xem b√†i vi·∫øt ‚Üí](04_demo_lora_fine_tuning_on_flan_t5.md) |
| [Tri·ªÉn Khai LoRA trong Large Language Models](05_implementing_lora_in_llms.md) | [Xem b√†i vi·∫øt ‚Üí](05_implementing_lora_in_llms.md) |
| [Demo Th·ª≠ Nghi·ªám Tham S·ªë LoRA](06_demo_challenges_in_lora.md) | [Xem b√†i vi·∫øt ‚Üí](06_demo_challenges_in_lora.md) |
| üìå **[Gi·∫£i Ph√°p Fine-tuning FLAN-T5 cho D·ªãch Thu·∫≠t v·ªõi LoRA](07_solution_fine_tuning_flan_t5_for_translation.md)** | [Xem b√†i vi·∫øt ‚Üí](07_solution_fine_tuning_flan_t5_for_translation.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

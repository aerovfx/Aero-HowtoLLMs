
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../../index.md) > [07 fine tune pretrained models](../../index.md) > [fine tuning](../index.md) > [02 1. introduction to large language models llms](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../../index.md)
- [ğŸ“š Module 01: LLM Course](../../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Kiáº¿n TrÃºc Cá»§a LLMs

## Giá»›i Thiá»‡u

BÃ¢y giá» hÃ£y khÃ¡m phÃ¡ cáº¥u trÃºc vÃ  hoáº¡t Ä‘á»™ng bÃªn trong cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs), táº­p trung vÃ o kiáº¿n trÃºc Transformer cÃ¡ch máº¡ng. HÃ£y tÆ°á»Ÿng tÆ°á»£ng viá»‡c hiá»ƒu thiáº¿t káº¿ phá»©c táº¡p cá»§a má»™t nhÃ  báº¿p hiá»‡n Ä‘áº¡i hiá»‡u quáº£ cao. ÄÃ³ lÃ  nhá»¯ng gÃ¬ chÃºng ta Ä‘ang lÃ m vá»›i LLMs ngay bÃ¢y giá».

## Kiáº¿n TrÃºc Transformer

Transformers Ä‘Æ°á»£c giá»›i thiá»‡u bá»Ÿi cÃ¡c nhÃ  nghiÃªn cá»©u táº¡i Google vÃ o nÄƒm 2017 lÃ  xÆ°Æ¡ng sá»‘ng cá»§a háº§u háº¿t cÃ¡c LLMs hiá»‡n Ä‘áº¡i. ChÃºng Ä‘Ã£ thay Ä‘á»•i cÄƒn báº£n cÃ¡ch mÃ¡y tÃ­nh xá»­ lÃ½ ngÃ´n ngá»¯.

### Cáº¥u TrÃºc CÆ¡ Báº£n

á» lÃµi cá»§a chÃºng, Transformers bao gá»“m cÃ¡c lá»›p cá»§a encoders vÃ  decoders:
- **Encoders**: Äá»c vÃ  xá»­ lÃ½ vÄƒn báº£n Ä‘áº§u vÃ o
- **Decoders**: Táº¡o Ä‘áº§u ra dá»±a trÃªn thÃ´ng tin Ä‘Ã³

HÃ£y tÆ°á»Ÿng tÆ°á»£ng encoders nhÆ° nhÃ¢n viÃªn báº¿p chuáº©n bá»‹ táº¥t cáº£ cÃ¡c nguyÃªn liá»‡u cá»§a báº¡n, vÃ  decoders nhÆ° nhá»¯ng Ä‘áº§u báº¿p káº¿t há»£p cÃ¡c nguyÃªn liá»‡u Ä‘Ã³ Ä‘á»ƒ táº¡o ra má»™t mÃ³n Äƒn.

## CÆ¡ Cháº¿ Self-Attention

Má»™t trong nhá»¯ng Ä‘á»•i má»›i quan trá»ng nháº¥t trong Transformers lÃ  cÆ¡ cháº¿ self-attention. Äiá»u nÃ y cho phÃ©p mÃ´ hÃ¬nh Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng cá»§a cÃ¡c tá»« khÃ¡c nhau trong má»™t cÃ¢u so vá»›i cÃ¡c tá»« khÃ¡c.

**VÃ­ dá»¥:** Trong cÃ¢u "The chef who trained me cooks well", mÃ´ hÃ¬nh sáº½ nháº­n ra ráº±ng "chef" vÃ  "cooks" Ä‘Æ°á»£c liÃªn káº¿t quan trá»ng máº·c dÃ¹ khoáº£ng cÃ¡ch giá»¯a chÃºng.

Sá»± hiá»ƒu biáº¿t nÃ y cáº£i thiá»‡n kháº£ nÄƒng cá»§a mÃ´ hÃ¬nh trong viá»‡c xá»­ lÃ½ cÃ¡c sáº¯c thÃ¡i trong ngÃ´n ngá»¯.

### Xá»­ LÃ½ Song Song

KhÃ¡c vá»›i cÃ¡c mÃ´ hÃ¬nh cÅ© nhÆ° RNNs hoáº·c LSTMs xá»­ lÃ½ dá»¯ liá»‡u tuáº§n tá»±, Transformers xá»­ lÃ½ táº¥t cáº£ cÃ¡c pháº§n cá»§a dá»¯ liá»‡u Ä‘á»“ng thá»i. Viá»‡c xá»­ lÃ½ song song nÃ y giá»‘ng nhÆ° nhiá»u tráº¡m lÃ m viá»‡c trong báº¿p hoáº¡t Ä‘á»™ng cÃ¹ng má»™t lÃºc, tÄƒng tá»‘c Ä‘Ã¡ng ká»ƒ cÃ¡c tÃ¡c vá»¥.

## CÃ¡c Lá»›p Transformer

Má»—i lá»›p cá»§a Transformer cÃ³ thá»ƒ Ä‘Æ°á»£c coi nhÆ° má»™t bá»™ nÃ£o thu nhá», má»—i lá»›p Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh riÃªng vá» pháº§n nÃ o cá»§a vÄƒn báº£n quan trá»ng. CÃ¡c lá»›p nÃ y xáº¿p chá»“ng lÃªn nhau táº¡o thÃ nh máº¡ng lÆ°á»›i máº¡nh máº½ tinh chá»‰nh ngÃ´n ngá»¯ vÃ  kháº£ nÄƒng sinh vÄƒn báº£n.

## TÃ³m Táº¯t

LLMs sá»­ dá»¥ng cÃ¡c kiáº¿n trÃºc Transformer nÃ y Ä‘á»ƒ xuáº¥t sáº¯c trong cÃ¡c tÃ¡c vá»¥ nhÆ° dá»‹ch thuáº­t, táº¡o ná»™i dung, vÃ  hÆ¡n tháº¿ ná»¯a báº±ng cÃ¡ch hiá»‡u quáº£ trong viá»‡c hiá»ƒu vÃ  táº¡o vÄƒn báº£n giá»‘ng con ngÆ°á»i. Kháº£ nÄƒng nhanh chÃ³ng xá»­ lÃ½ lÆ°á»£ng lá»›n dá»¯ liá»‡u vÃ  náº¯m báº¯t cÃ¡c pattern phá»©c táº¡p khiáº¿n chÃºng trá»Ÿ nÃªn vÃ´ giÃ¡ trong cÃ¡c lÄ©nh vá»±c khÃ¡c nhau.

## TÃ i Liá»‡u Tham Kháº£o

1. Liu, P., et al. (2023). "Prefix Tuning vs. Prompt Tuning: A Comparative Study." *arXiv:2303.13402*.

2. Reynolds, L., & McDonell, K. (2021). "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm." *arXiv:2102.07350*.

3. Stiennon, N., et al. (2020). "Learning to Summarize with Human Feedback." *Advances in Neural Information Processing Systems*, 33, 3008-3021.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [LLMs Äang CÃ¡ch Máº¡ng HÃ³a AI](01_llms_revolutionizing_ai.md) | [Xem bÃ i viáº¿t â†’](01_llms_revolutionizing_ai.md) |
| ğŸ“Œ **[Kiáº¿n TrÃºc Cá»§a LLMs](02_the_architecture_of_llms.md)** | [Xem bÃ i viáº¿t â†’](02_the_architecture_of_llms.md) |
| [CÃ¡c á»¨ng Dá»¥ng Cá»§a LLMs](03_applications_of_llms.md) | [Xem bÃ i viáº¿t â†’](03_applications_of_llms.md) |
| [CÃ¡c CÃ¢n Nháº¯c Äáº¡o Äá»©c Trong LLMs](04_ethical_considerations_in_llms.md) | [Xem bÃ i viáº¿t â†’](04_ethical_considerations_in_llms.md) |
| [So SÃ¡nh CÃ¡c MÃ´ HÃ¬nh LLMs](05_comparing_llms.md) | [Xem bÃ i viáº¿t â†’](05_comparing_llms.md) |
| [FLAN-T5: MÃ´ HÃ¬nh Transformer Äa NÄƒng](06_flan_t5_in_focus.md) | [Xem bÃ i viáº¿t â†’](06_flan_t5_in_focus.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [05 embeddings spaces](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# So sÃ¡nh Biá»ƒu Diá»…n Tá»« Vá»±ng giá»¯a Wikipedia vÃ  Twitter báº±ng PhÃ¢n TÃ­ch TÆ°Æ¡ng Äá»“ng Biá»ƒu Diá»…n (RSA)

## TÃ³m táº¯t

Trong nghiÃªn cá»©u xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP), cÃ¡c mÃ´ hÃ¬nh embedding há»c Ä‘Æ°á»£c biá»ƒu diá»…n vector cá»§a tá»« dá»±a trÃªn ngá»¯ cáº£nh. Tuy nhiÃªn, khi hai mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c miá»n dá»¯ liá»‡u khÃ¡c nhau â€” vÃ­ dá»¥ nhÆ° tá»« Ä‘iá»ƒn bÃ¡ch khoa toÃ n thÆ° cá»§a [Wikipedia](chatgpt://generic-entity?number=0) vÃ  dá»¯ liá»‡u máº¡ng xÃ£ há»™i tá»« [Twitter](chatgpt://generic-entity?number=1) â€” thÃ¬ khÃ´ng gian vector thu Ä‘Æ°á»£c cÃ³ thá»ƒ khÃ¡c biá»‡t Ä‘Ã¡ng ká»ƒ. BÃ i viáº¿t nÃ y trÃ¬nh bÃ y phÆ°Æ¡ng phÃ¡p so sÃ¡nh hai khÃ´ng gian embedding thÃ´ng qua **Cosine Similarity** vÃ  **Representational Similarity Analysis (RSA)**, minh há»a báº±ng cÃ¢u máº«u â€œThe quick brown fox jumps over the lazy dogâ€. CÃ¡c cÃ´ng thá»©c toÃ¡n há»c Ä‘Æ°á»£c bá»• sung nháº±m lÃ m rÃµ ná»n táº£ng lÃ½ thuyáº¿t.

---

## 1. Giá»›i thiá»‡u

Word embedding Ã¡nh xáº¡ má»—i tá»« $w$ vÃ o má»™t vector $\mathbf{v}_w \in \mathbb{R}^d$, trong Ä‘Ã³:

$$
f: w \rightarrow \mathbf{v}_w
$$

vá»›i $d$ lÃ  sá»‘ chiá»u cá»§a khÃ´ng gian nhÃºng.

Khi hai mÃ´ hÃ¬nh embedding Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn hai táº­p dá»¯ liá»‡u khÃ¡c nhau (vÃ­ dá»¥: vÄƒn báº£n bÃ¡ch khoa vÃ  tweet ngáº¯n), ta cÃ³:

$$
f_{wiki}(w) = \mathbf{v}_w^{(wiki)}
$$

$$
f_{twitter}(w) = \mathbf{v}_w^{(twitter)}
$$

Do khÃ¡c biá»‡t vá» miá»n dá»¯ liá»‡u vÃ  phÃ¢n bá»‘ ngÃ´n ngá»¯, cÃ¡c vector thu Ä‘Æ°á»£c khÃ´ng thá»ƒ so sÃ¡nh trá»±c tiáº¿p tá»«ng chiá»u.

---

## 2. Äá»™ tÆ°Æ¡ng Ä‘á»“ng Cosine

Äá»ƒ Ä‘o má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a hai tá»« $w_i$ vÃ  $w_j$ trong cÃ¹ng má»™t mÃ´ hÃ¬nh, ta sá»­ dá»¥ng **cosine similarity**:

$$
\text{cosine}(\mathbf{v}_i, \mathbf{v}_j) = 
\frac{\mathbf{v}_i \cdot \mathbf{v}_j}
{\|\mathbf{v}_i\| \|\mathbf{v}_j\|}
$$

Trong Ä‘Ã³:

- $\mathbf{v}_i \cdot \mathbf{v}_j$ lÃ  tÃ­ch vÃ´ hÆ°á»›ng.
- $\|\mathbf{v}_i\|$ lÃ  chuáº©n Euclid:

$$
\|\mathbf{v}_i\| = \sqrt{\sum_{k=1}^{d} v_{ik}^2}
$$

Cosine similarity náº±m trong khoáº£ng:

$$
-1 \leq \text{cosine} \leq 1
$$

Quan sÃ¡t thá»±c nghiá»‡m cho tháº¥y trong má»™t sá»‘ cáº·p tá»«, embedding tá»« Twitter cho giÃ¡ trá»‹ cosine cao hÆ¡n so vá»›i embedding tá»« Wikipedia, pháº£n Ã¡nh tÃ­nh ngá»¯ cáº£nh gáº§n gÅ©i hÆ¡n trong vÄƒn báº£n máº¡ng xÃ£ há»™i.

---

## 3. Váº¥n Ä‘á»: KhÃ´ng gian embedding khÃ¡c nhau

Máº·c dÃ¹ cÃ³ thá»ƒ so sÃ¡nh cosine similarity trong *cÃ¹ng má»™t mÃ´ hÃ¬nh*, ta khÃ´ng thá»ƒ so sÃ¡nh trá»±c tiáº¿p:

$$
\mathbf{v}_w^{(wiki)} \neq \mathbf{v}_w^{(twitter)}
$$

LÃ½ do:

1. CÃ¡c khÃ´ng gian Ä‘Æ°á»£c há»c Ä‘á»™c láº­p.
2. Trá»¥c tá»a Ä‘á»™ khÃ´ng Ä‘á»“ng nháº¥t.
3. PhÃ©p quay (rotation) cá»§a khÃ´ng gian khÃ´ng lÃ m thay Ä‘á»•i khoáº£ng cÃ¡ch ná»™i táº¡i nhÆ°ng lÃ m thay Ä‘á»•i tá»a Ä‘á»™ tuyá»‡t Ä‘á»‘i.

Giáº£ sá»­ tá»“n táº¡i má»™t ma tráº­n quay trá»±c giao $\mathbf{R}$:

$$
\mathbf{v}_w^{(twitter)} \approx \mathbf{R} \mathbf{v}_w^{(wiki)}
$$

Khi Ä‘Ã³, tá»a Ä‘á»™ khÃ¡c nhau nhÆ°ng cáº¥u trÃºc tÆ°Æ¡ng Ä‘á»‘i cÃ³ thá»ƒ váº«n Ä‘Æ°á»£c báº£o toÃ n.

---

## 4. Representational Similarity Analysis (RSA)

### 4.1 Ã tÆ°á»Ÿng

RSA khÃ´ng so sÃ¡nh vector trá»±c tiáº¿p, mÃ  so sÃ¡nh **ma tráº­n tÆ°Æ¡ng Ä‘á»“ng ná»™i bá»™** giá»¯a cÃ¡c tá»« trong tá»«ng mÃ´ hÃ¬nh.

Giáº£ sá»­ ta cÃ³ táº­p $n$ tá»« trong cÃ¢u:

> â€œThe quick brown fox jumps over the lazy dogâ€

Ta xÃ¢y dá»±ng ma tráº­n tÆ°Æ¡ng Ä‘á»“ng $S \in \mathbb{R}^{n \times n}$:

$$
S_{ij} = \text{cosine}(\mathbf{v}_i, \mathbf{v}_j)
$$

Ta cÃ³:

$$
S^{(wiki)} \quad \text{vÃ } \quad S^{(twitter)}
$$

---

### 4.2 So sÃ¡nh hai ma tráº­n

Ta vector hÃ³a pháº§n tam giÃ¡c trÃªn (khÃ´ng tÃ­nh Ä‘Æ°á»ng chÃ©o):

$$
\mathbf{s}^{(wiki)}, \quad \mathbf{s}^{(twitter)}
$$

Sau Ä‘Ã³ tÃ­nh há»‡ sá»‘ tÆ°Æ¡ng quan Pearson:

$$
r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}
{\sqrt{\sum (x_i - \bar{x})^2}
\sqrt{\sum (y_i - \bar{y})^2}}
$$

Náº¿u:

- $r \approx 1$: Hai khÃ´ng gian cÃ³ cáº¥u trÃºc quan há»‡ tÆ°Æ¡ng Ä‘á»“ng cao.
- $r \approx 0$: Cáº¥u trÃºc khÃ¡c biá»‡t.
- $r < 0$: Quan há»‡ nghá»‹ch Ä‘áº£o.

---

## 5. Minh há»a quy trÃ¬nh thá»±c nghiá»‡m

### BÆ°á»›c 1: Láº¥y chá»‰ sá»‘ tá»« (word indices)

Vá»›i má»—i tá»« $w$ trong cÃ¢u:

$$
\text{index}_{wiki}(w)
$$

$$
\text{index}_{twitter}(w)
$$

LÆ°u Ã½: Má»™t sá»‘ tá»« cÃ³ thá»ƒ khÃ´ng xuáº¥t hiá»‡n (vÃ­ dá»¥: chá»¯ hoa â€œTheâ€).

---

### BÆ°á»›c 2: TrÃ­ch xuáº¥t embedding

$$
\mathbf{v}_w^{(wiki)} = E^{(wiki)}[\text{index}(w)]
$$

$$
\mathbf{v}_w^{(twitter)} = E^{(twitter)}[\text{index}(w)]
$$

---

### BÆ°á»›c 3: TÃ­nh ma tráº­n tÆ°Æ¡ng Ä‘á»“ng

$$
S^{(model)}_{ij} =
\frac{\mathbf{v}_i \cdot \mathbf{v}_j}
{\|\mathbf{v}_i\| \|\mathbf{v}_j\|}
$$

---

### BÆ°á»›c 4: TÃ­nh tÆ°Æ¡ng quan giá»¯a hai ma tráº­n

$$
\text{RSA score} = \text{corr}(\mathbf{s}^{(wiki)}, \mathbf{s}^{(twitter)})
$$

---

## 6. PhÃ¢n tÃ­ch káº¿t quáº£

Náº¿u embedding tá»« Twitter cho giÃ¡ trá»‹ cosine cao hÆ¡n trong nhiá»u cáº·p tá»«, Ä‘iá»u nÃ y cÃ³ thá»ƒ pháº£n Ã¡nh:

- NgÃ´n ngá»¯ trÃªn máº¡ng xÃ£ há»™i mang tÃ­nh ngá»¯ cáº£nh cháº·t cháº½.
- CÃ¡c tá»« xuáº¥t hiá»‡n trong cáº¥u trÃºc há»™i thoáº¡i ngáº¯n, lÃ m tÄƒng máº­t Ä‘á»™ Ä‘á»“ng xuáº¥t hiá»‡n.

Trong khi Ä‘Ã³, Wikipedia cÃ³ phong cÃ¡ch há»c thuáº­t, phÃ¢n bá»‘ tá»« rá»™ng hÆ¡n, dáº«n Ä‘áº¿n cáº¥u trÃºc embedding phÃ¢n tÃ¡n hÆ¡n.

---

## 7. Tháº£o luáº­n

RSA cho phÃ©p ta:

- So sÃ¡nh hai khÃ´ng gian embedding khÃ´ng cÃ¹ng há»‡ trá»¥c.
- ÄÃ¡nh giÃ¡ tÃ­nh tÆ°Æ¡ng Ä‘á»“ng cáº¥u trÃºc.
- TrÃ¡nh phá»¥ thuá»™c vÃ o tá»a Ä‘á»™ tuyá»‡t Ä‘á»‘i.

PhÆ°Æ¡ng phÃ¡p nÃ y thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong:

- Khoa há»c tháº§n kinh tÃ­nh toÃ¡n.
- So sÃ¡nh mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n.
- PhÃ¢n tÃ­ch Ä‘a miá»n dá»¯ liá»‡u.

---

## 8. Káº¿t luáº­n

So sÃ¡nh embedding giá»¯a Wikipedia vÃ  Twitter khÃ´ng thá»ƒ thá»±c hiá»‡n báº±ng cÃ¡ch Ä‘á»‘i chiáº¿u trá»±c tiáº¿p vector. Tuy nhiÃªn, thÃ´ng qua cosine similarity vÃ  Ä‘áº·c biá»‡t lÃ  Representational Similarity Analysis (RSA), ta cÃ³ thá»ƒ Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cáº¥u trÃºc giá»¯a hai khÃ´ng gian biá»ƒu diá»…n.

Vá» máº·t toÃ¡n há»c:

$$
\text{So sÃ¡nh trá»±c tiáº¿p vector} \neq \text{So sÃ¡nh cáº¥u trÃºc quan há»‡}
$$

RSA cung cáº¥p má»™t khung lÃ½ thuyáº¿t máº¡nh máº½ Ä‘á»ƒ nghiÃªn cá»©u tÃ­nh á»•n Ä‘á»‹nh vÃ  kháº£ nÄƒng chuyá»ƒn giao cá»§a biá»ƒu diá»…n há»c sÃ¢u trong NLP.

---

## TÃ i liá»‡u tham kháº£o gá»£i Ã½

1. Mikolov et al. (2013). Distributed Representations of Words and Phrases.
2. Kriegeskorte et al. (2008). Representational Similarity Analysis.
3. Jurafsky & Martin (2023). Speech and Language Processing.

---
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero llm 01 word2vec vs glove vs gpt vs bert oh my](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) |
| [aero llm 02 exploring glove pretrained embeddings](aero_llm_02_exploring_glove_pretrained_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_exploring_glove_pretrained_embeddings.md) |
| [aero llm 03 codechallenge wikipedia vs twitter embeddings part 1](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) |
| ğŸ“Œ **[So sÃ¡nh Biá»ƒu Diá»…n Tá»« Vá»±ng giá»¯a Wikipedia vÃ  Twitter báº±ng PhÃ¢n TÃ­ch TÆ°Æ¡ng Äá»“ng Biá»ƒu Diá»…n (RSA)](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Ngá»¯ NghÄ©a cá»§a GPT-2 vÃ  BERT thÃ´ng qua PhÃ¢n TÃ­ch Embedding](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) |
| [ToÃ¡n há»c cá»§a Token vÃ  Embedding trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) |
| [Cosine Similarity vÃ  Má»‘i Quan Há»‡ vá»›i Há»‡ Sá»‘ TÆ°Æ¡ng Quan: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong NLP](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) |
| [PhÃ¢n TÃ­ch Cosine Similarity trong KhÃ´ng Gian Embedding cá»§a GPT-2](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) |
| [Unembedding trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: Tá»« Vector áº¨n Äáº¿n Token](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) |
| [Position Embeddings trong Transformer: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_llm_10_position_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_position_embeddings.md) |
| [PhÃ¢n TÃ­ch Thá»±c Nghiá»‡m Embedding Vá»‹ TrÃ­ Trong Transformer: Tá»« Cáº¥u TrÃºc Tuyáº¿n TÃ­nh Äáº¿n KhÃ´ng Gian HÃ¬nh Há»c](aero_llm_11_codechallenge_exploring_position_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_exploring_position_embeddings.md) |
| [Huáº¥n Luyá»‡n Embedding Tá»« Äáº§u: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, CÆ¡ Cháº¿ Tá»‘i Æ¯u vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_12_training_embeddings_from_scratch.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_training_embeddings_from_scratch.md) |
| [Thiáº¿t Káº¿ Data Loader Cho Huáº¥n Luyá»‡n MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Kiáº¿n TrÃºc vÃ  Tá»‘i Æ¯u HoÃ¡](aero_llm_13_create_a_data_loader_to_train_a_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_create_a_data_loader_to_train_a_model.md) |
| [XÃ¢y Dá»±ng MÃ´ HÃ¬nh Há»c Embedding Tá»« Äáº§u: Kiáº¿n TrÃºc, Tá»‘i Æ¯u HoÃ¡ vÃ  PhÃ¢n TÃ­ch ToÃ¡n Há»c](aero_llm_14_build_a_model_to_learn_the_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_build_a_model_to_learn_the_embeddings.md) |
| [HÃ m Máº¥t MÃ¡t Trong Huáº¥n Luyá»‡n Embedding: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, PhÃ¢n TÃ­ch Gradient vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_15_loss_function_to_train_the_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_loss_function_to_train_the_embeddings.md) |
| [Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh Há»c MÃ¡y: CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Thá»±c tiá»…n](aero_llm_16_train_and_evaluate_the_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_train_and_evaluate_the_model.md) |
| [Sá»± Thay Äá»•i cá»§a Embeddings Trong QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_llm_17_codechallenge_how_the_embeddings_change.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_codechallenge_how_the_embeddings_change.md) |
| [Äá»™ á»”n Äá»‹nh cá»§a Embeddings trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_llm_18_codechallenge_how_stable_are_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_how_stable_are_embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

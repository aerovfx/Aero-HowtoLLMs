
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [05 embeddings spaces](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
So sÃ¡nh Word2Vec, GloVe, GPT vÃ  BERT:

Tá»« Biá»ƒu diá»…n PhÃ¢n bá»‘ Tuyáº¿n tÃ­nh Ä‘áº¿n Transformer Tá»± Há»“i quy vÃ  Hai Chiá»u

â¸»

TÃ³m táº¯t

BÃ i viáº¿t nÃ y tá»•ng há»£p vÃ  phÃ¢n tÃ­ch ná»™i dung tá»« tÃ i liá»‡u Ä‘Ã­nh kÃ¨m â€œWord2Vec vs. GloVe vs. GPT vs. BERTâ€¦ oh my!â€, Ä‘á»“ng thá»i má»Ÿ rá»™ng vá»›i cÃ¡c nguá»“n há»c thuáº­t ná»n táº£ng nháº±m lÃ m rÃµ sá»± tiáº¿n hÃ³a cá»§a mÃ´ hÃ¬nh biá»ƒu diá»…n ngÃ´n ngá»¯: tá»« embedding tÄ©nh (static embeddings) nhÆ° Word2Vec vÃ  GloVe Ä‘áº¿n mÃ´ hÃ¬nh ngá»¯ cáº£nh hÃ³a (contextual embeddings) nhÆ° GPT vÃ  BERT.

ChÃºng tÃ´i phÃ¢n tÃ­ch:
	â€¢	MÃ´ hÃ¬nh toÃ¡n há»c ná»n táº£ng
	â€¢	HÃ m má»¥c tiÃªu huáº¥n luyá»‡n
	â€¢	Cáº¥u trÃºc xÃ¡c suáº¥t
	â€¢	TÃ­nh cháº¥t tuyáº¿n tÃ­nh cá»§a embedding
	â€¢	Attention vÃ  self-attention
	â€¢	So sÃ¡nh Ä‘á»‹nh lÆ°á»£ng vá» Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n

CÃ¡c cÃ´ng thá»©c toÃ¡n há»c Ä‘Æ°á»£c trÃ¬nh bÃ y nháº±m minh há»a rÃµ sá»± khÃ¡c biá»‡t báº£n cháº¥t giá»¯a cÃ¡c tháº¿ há»‡ mÃ´ hÃ¬nh.

â¸»

1. Giá»›i thiá»‡u

Biá»ƒu diá»…n tá»« (word representation) lÃ  bÃ i toÃ¡n trung tÃ¢m trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP).

Ta xÃ©t má»™t táº­p tá»« vá»±ng:

V = \{w_1, w_2, \dots, w_{|V|}\}

Má»¥c tiÃªu lÃ  xÃ¢y dá»±ng Ã¡nh xáº¡:

$$
E: V \rightarrow \mathbb{R}^d
$$

Trong Ä‘Ã³ d lÃ  sá»‘ chiá»u embedding.

Lá»‹ch sá»­ phÃ¡t triá»ƒn cÃ³ thá»ƒ chia thÃ nh hai giai Ä‘oáº¡n chÃ­nh:
	1.	Embedding tÄ©nh (Static embeddings)
	â€¢	Word2Vec
	â€¢	GloVe
	2.	Embedding ngá»¯ cáº£nh hÃ³a (Contextual embeddings)
	â€¢	GPT
	â€¢	BERT

â¸»

2. Word2Vec: MÃ´ hÃ¬nh dá»±a trÃªn Ngá»¯ cáº£nh Cá»¥c bá»™

2.1 Tá»•ng quan

Word2Vec (Mikolov et al., 2013) dá»±a trÃªn giáº£ thuyáº¿t phÃ¢n bá»‘:

$P(w \mid context)$

Hai biáº¿n thá»ƒ chÃ­nh:
	â€¢	CBOW (Continuous Bag of Words)
	â€¢	Skip-gram

â¸»

2.2 MÃ´ hÃ¬nh Skip-gram

Giáº£ sá»­ chuá»—i tá»«:

$w_1$, $w_2$, \dots, $w_T$

HÃ m má»¥c tiÃªu:

\max \sum_{t=1}^{T} \sum_{-c \le j \le c, j \ne 0} \log P(w_{t+j} \mid w_t)

Vá»›i:

$P($w_$O( \mid )$w_I$)$ = \frac{\exp$v_{$w_$O(}^\top v_{)$w_I$}$}{$\sum$_{w \in V} \exp$v_w^\top v_{$w_I$}$}

Do chi phÃ­ tÃ­nh toÃ¡n lá»›n, sá»­ dá»¥ng negative sampling:

$\log$ \sigma$v_{$w_$O(}^\top v_{)$w_I$}$ + $\sum$_{i=1}^{k} $\mathbb${E}_{$w_i$ \sim P_n$w$} $\log$ \sigma$-v_{$w_i$}^\top v_{$w_I$}$

â¸»

2.3 TÃ­nh cháº¥t Tuyáº¿n tÃ­nh

Má»™t tÃ­nh cháº¥t ná»•i tiáº¿ng:

\text{king} - \text{man} + \text{woman} \approx \text{queen}

ToÃ¡n há»c:

v_{king} - v_{man} + v_{woman} \approx v_{queen}

Äiá»u nÃ y cho tháº¥y embedding há»c Ä‘Æ°á»£c cáº¥u trÃºc tuyáº¿n tÃ­nh.

â¸»

3. GloVe: Ma tráº­n Äá»“ng xuáº¥t hiá»‡n ToÃ n cá»¥c

3.1 Tá»•ng quan

GloVe (Pennington et al., 2014) dá»±a trÃªn ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n:

X_{ij} = \text{sá»‘ láº§n } w_j \text{ xuáº¥t hiá»‡n trong ngá»¯ cáº£nh cá»§a } w_i

â¸»

3.2 HÃ m má»¥c tiÃªu

J = \sum_{i,j} fX_{ij} \leftw_i^\top \tilde{w}_j + b_i + b_j - \log X_{ij} \right^2

Trong Ä‘Ã³:

$$
fx =
$$

\begin{cases}
$x/x_{max}$^\alpha & x < x_{max} \\
1 & \text{otherwise}
\end{cases}

KhÃ¡c vá»›i Word2Vec, GloVe khai thÃ¡c thá»‘ng kÃª toÃ n cá»¥c.

â¸»

4. GPT: Transformer Tá»± Há»“i quy

4.1 Cáº¥u trÃºc tá»•ng quan

GPT (Radford et al.) dá»±a trÃªn kiáº¿n trÃºc Transformer tá»« bÃ i bÃ¡o cá»§a Ashish Vaswani et al. (2017).

MÃ´ hÃ¬nh xÃ¡c suáº¥t:

$P($w_1$,\dots,$w_T$)$ = $\prod$_{t=1}^{T} $P($w_t$ \mid w_{\lt t})$

â¸»

4.2 Self-Attention

Vá»›i:

Q = XW_Q,\quad K = XW_K,\quad V = XW_V

Attention:

$$
\text{Attention}(Q,K,V) = \text{softmax}\left\frac{QK^\top}{\sqrt{d_k}} \rightV Äá»™ phá»©c táº¡p: O(n^2 d) â¸» 4.3 HÃ m máº¥t mÃ¡t Cross-entropy:
$$

\mathcal${L} = - $\sum$_{t=1}^{T} $\log$ $P($w_t$ \mid w_{\lt t})

$$
GPT sinh vÄƒn báº£n theo hÆ°á»›ng trÃ¡i â†’ pháº£i (autoregressive). â¸» 5. BERT: Transformer Hai chiá»u 5.1 Kiáº¿n trÃºc BERT (Devlin et al., 2018) sá»­ dá»¥ng: â€¢	Masked Language Modeling (MLM) â€¢	Next Sentence Prediction (NSP) â¸» 5.2 Masked Language Model Chá»n táº­p vá»‹ trÃ­ M:
$$

\mathcal${L}_{MLM} = - $\sum$_{t \in M} $\log$ $P($w_t$ \mid w_{\setminus M})

$$
KhÃ¡c GPT: â€¢	GPT: dá»± Ä‘oÃ¡n tÆ°Æ¡ng lai â€¢	BERT: dÃ¹ng cáº£ trÃ¡i vÃ  pháº£i â¸» 5.3 Biá»ƒu diá»…n Ngá»¯ cáº£nh hÃ³a Embedding giá» lÃ  hÃ m cá»§a toÃ n bá»™ cÃ¢u: e_t = fw_1,\dots,w_T, t KhÃ´ng cÃ²n lÃ  Ã¡nh xáº¡ cá»‘ Ä‘á»‹nh. â¸» 6. So sÃ¡nh ToÃ¡n há»c MÃ´ hÃ¬nh	XÃ¡c suáº¥t	Pháº¡m vi ngá»¯ cáº£nh	Embedding Word2Vec	P(w_O\mid w_I)	Cá»¥c bá»™	TÄ©nh
$$

GloVe	\log X_{ij}	ToÃ n cá»¥c	TÄ©nh

$$
GPT	P(w_t \mid w_{\lt t})	TrÃ¡i	Ngá»¯ cáº£nh BERT	P(w_t\mid w_{\setminus M})	Hai chiá»u	Ngá»¯ cáº£nh â¸» 7. PhÃ¢n tÃ­ch Entropy Entropy chuá»—i: H = - \sum P(w_1,\dots,w_T)\log P(w_1,\dots,w_T) GPT mÃ´ hÃ¬nh hÃ³a trá»±c tiáº¿p: H = - \sum_{t} \log P(w_t \mid w_{\lt t}) Perplexity:
$$

\text{PPL} = 2^H

$$
Word2Vec/GloVe khÃ´ng tá»‘i Æ°u trá»±c tiáº¿p perplexity. â¸» 8. So sÃ¡nh Äá»™ phá»©c táº¡p Word2Vec: O(T c d) GloVe: O(|X|) Transformer: O(n^2 d) Trong Ä‘Ã³ n lÃ  Ä‘á»™ dÃ i chuá»—i. â¸» 9. Tiáº¿n hÃ³a MÃ´ hÃ¬nh QuÃ¡ trÃ¬nh phÃ¡t triá»ƒn: 1.	Vector tÄ©nh (Word2Vec, GloVe) 2.	Transformer má»™t chiá»u (GPT) 3.	Transformer hai chiá»u (BERT) BÆ°á»›c chuyá»ƒn quan trá»ng nháº¥t lÃ  self-attention. â¸» 10. Káº¿t luáº­n Tá»« Word2Vec Ä‘áº¿n GPT vÃ  BERT cho tháº¥y sá»± chuyá»ƒn dá»‹ch: â€¢	Tá»« mÃ´ hÃ¬nh cá»¥c bá»™ â†’ mÃ´ hÃ¬nh toÃ n chuá»—i â€¢	Tá»« embedding tÄ©nh â†’ embedding ngá»¯ cáº£nh â€¢	Tá»« ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n â†’ mÃ´ hÃ¬nh xÃ¡c suáº¥t chuá»—i ToÃ¡n há»c chuyá»ƒn tá»«:
$$

v_w \in \mathbb{R}^d
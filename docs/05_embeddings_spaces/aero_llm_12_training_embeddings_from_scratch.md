
<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [05 embeddings spaces](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Hu·∫•n Luy·ªán Embedding T·ª´ ƒê·∫ßu: C∆° S·ªü To√°n H·ªçc, C∆° Ch·∫ø T·ªëi ∆Øu v√† ·ª®ng D·ª•ng Trong M√¥ H√¨nh Ng√¥n Ng·ªØ

T√≥m t·∫Øt

Embedding l√† n·ªÅn t·∫£ng c·ªßa c√°c m√¥ h√¨nh ng√¥n ng·ªØ hi·ªán ƒë·∫°i, cho ph√©p √°nh x·∫° token r·ªùi r·∫°c sang kh√¥ng gian vector li√™n t·ª•c. B√†i vi·∫øt n√†y tr√¨nh b√†y quy tr√¨nh hu·∫•n luy·ªán embedding t·ª´ ƒë·∫ßu (training embeddings from scratch), ph√¢n t√≠ch c∆° s·ªü to√°n h·ªçc c·ªßa h√†m m·∫•t m√°t, lan truy·ªÅn ng∆∞·ª£c (backpropagation), t·ªëi ∆∞u ho√°, v√† m·ªëi li√™n h·ªá v·ªõi c√°c m√¥ h√¨nh nh∆∞ Word2Vec v√† Transformer. ƒê·ªìng th·ªùi, b√†i vi·∫øt m·ªü r·ªông th·∫£o lu·∫≠n sang embedding trong c√°c m√¥ h√¨nh nh∆∞ GPT-2 c·ªßa OpenAI v√† n·ªÅn t·∫£ng self-attention t·ª´ c√¥ng tr√¨nh Attention Is All You Need c·ªßa Ashish Vaswani v√† c·ªông s·ª±.

‚∏ª

1. Gi·ªõi thi·ªáu

Trong x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n, m·ªói token ban ƒë·∫ßu ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng ch·ªâ s·ªë r·ªùi r·∫°c:

w \in \{1,2,\dots,V\}

Trong ƒë√≥:
	‚Ä¢	V: k√≠ch th∆∞·ªõc t·ª´ v·ª±ng (vocabulary size)

Embedding √°nh x·∫° token sang kh√¥ng gian li√™n t·ª•c:

$$
f: \{1,\dots,V\} \rightarrow \mathbb{R}^d
$$

Ma tr·∫≠n embedding:

$$
\mathbf{E} \in \mathbb{R}^{V \times d}
$$

Vector embedding c·ªßa token w:

$$
\mathbf{e}_w = \mathbf{E}[w]
$$

‚∏ª

2. Hu·∫•n luy·ªán embedding nh∆∞ m·ªôt l·ªõp tuy·∫øn t√≠nh

2.1 Bi·ªÉu di·ªÖn one-hot

Token w c√≥ th·ªÉ bi·ªÉu di·ªÖn b·∫±ng vector one-hot:

$$
\mathbf{x} \in \mathbb{R}^V
$$

v·ªõi:

x_i =

\begin{cases}

1 & \text{n·∫øu } i = w \\

0 & \text{ng∆∞·ª£c l·∫°i}
\end{cases}

Embedding th·ª±c ch·∫•t l√† ph√©p nh√¢n ma tr·∫≠n:

$$
\mathbf{e}_w = \mathbf{x}^T \mathbf{E}
$$

V√¨ \mathbf{x} l√† one-hot n√™n ph√©p nh√¢n n√†y t∆∞∆°ng ƒë∆∞∆°ng v·ªõi ch·ªçn m·ªôt h√†ng trong ma tr·∫≠n.

‚∏ª

3. Hu·∫•n luy·ªán embedding trong b√†i to√°n d·ª± ƒëo√°n t·ª´

Gi·∫£ s·ª≠ b√†i to√°n d·ª± ƒëo√°n t·ª´ ti·∫øp theo (next-token prediction).

3.1 X√°c su·∫•t Softmax

Logits:

$$
\mathbf{z} = \mathbf{W}\mathbf{e}_w + \mathbf{b}
$$

X√°c su·∫•t:

$P(y=i \mid w)$ =
\frac{\exp$z_i$}

{\sum_{j=1}^{V} \expz_j}

‚∏ª

3.2 H√†m m·∫•t m√°t Cross-Entropy

$$
\mathcal{L} = -\sum_{i=1}^{V} y_i \log P(y=i)
$$

V·ªõi $y_i$ l√† vector nh√£n one-hot.

Do ƒë√≥:

$$
\mathcal{L} = -\log P(y = y_{\text{true}})
$$

‚∏ª

4. Lan truy·ªÅn ng∆∞·ª£c v√† c·∫≠p nh·∫≠t embedding

Gradient theo logits:

$$
\frac{\partial \mathcal{L}}{\partial z_i} = P(y=i) - y_i Gradient theo embedding:
$$

\frac{\partial \mathcal{L}}{\partial \mathbf{e}_w}

= \mathbf{W}^T \mathbf{p} - \mathbf{y}

$$
Gradient theo ma tr·∫≠n embedding:
$$

\frac{\partial \mathcal{L}}{\partial \mathbf{E}[w]}

= \frac{\partial \mathcal{L}}{\partial \mathbf{e}_w}

C·∫≠p nh·∫≠t b·∫±ng gradient descent:

\mathbf{E}[w] \leftarrow

\mathbf{E}[w]
- \eta

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{E}[w]}
$$

Trong ƒë√≥:
	‚Ä¢	\eta: learning rate

Ch·ªâ h√†ng t∆∞∆°ng ·ª©ng v·ªõi token xu·∫•t hi·ªán trong batch ƒë∆∞·ª£c c·∫≠p nh·∫≠t.

‚∏ª

5. Embedding trong Word2Vec

Trong m√¥ h√¨nh Skip-gram c·ªßa Tomas Mikolov:

M·ª•c ti√™u:

$$
\max \sum_{(w,c)}
$$

$\log$ $P(c \mid w)$

V·ªõi:

$P(c \mid w)$

$$
=
$$

\frac{\exp$\mathbf{u}_c^T \mathbf{v}_w$}

{\sum_{j=1}^{V} \exp\mathbf{u}_j^T \mathbf{v}_w}

Trong ƒë√≥:
	‚Ä¢	\mathbf{v}_w: embedding trung t√¢m
	‚Ä¢	\mathbf{u}_c: embedding ng·ªØ c·∫£nh

ƒê·ªÉ gi·∫£m chi ph√≠ t√≠nh to√°n, Negative Sampling ƒë∆∞·ª£c s·ª≠ d·ª•ng:

$$
\mathcal{L} =
$$

$\log$ \sigma$\mathbf{u}_c^T \mathbf{v}_w$
+

$$
\sum_{k=1}^{K}
$$

$\log$ \sigma$-\mathbf{u}_{$n_k$}^T \mathbf{v}_w$

‚∏ª

6. Embedding trong Transformer

Trong Transformer:

$$
\mathbf{z}_t =
$$

\mathbf{e}_t + \mathbf{p}_t

Self-attention:

\text{Attention}(Q,K,V)

$$
=
$$

\text{softmax}

$$
\left(
$$

\frac{QK^T}{\sqrt{$d_k$}}
\right)V

V·ªõi:

Q = ZW_Q, \quad

$$
K = ZW_K, \quad
$$

V = ZW_V

Embedding ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn attention scores.

‚∏ª

7. T√≠nh ch·∫•t h√¨nh h·ªçc c·ªßa embedding

7.1 Chu·∫©n vector

\|\mathbf{e}_w\|

$$
=
$$

\sqrt{

$$
\sum_{i=1}^{d}
$$

e_{w,i}^2
}

Token ph·ªï bi·∫øn th∆∞·ªùng c√≥ norm l·ªõn h∆°n do ƒë∆∞·ª£c c·∫≠p nh·∫≠t nhi·ªÅu l·∫ßn.

‚∏ª

7.2 ƒê·ªô t∆∞∆°ng ƒë·ªìng cosine

\cos$\theta$

$$
=
$$

\frac{
\mathbf{e}_a \cdot \mathbf{e}_b
}{
\|\mathbf{e}_a\|\|\mathbf{e}_b\|
}

Cho ph√©p ƒëo m·ª©c ƒë·ªô t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a.

V√≠ d·ª• quan h·ªá tuy·∫øn t√≠nh n·ªïi ti·∫øng:

\mathbf{e}_{\text{king}}
-
\mathbf{e}_{\text{man}}
+
\mathbf{e}_{\text{woman}}

$$
\approx
$$

\mathbf{e}_{\text{queen}}

‚∏ª

8. Ph√¢n t√≠ch ph·ªï gi√° tr·ªã ri√™ng (Spectral Analysis)

X√©t ma tr·∫≠n embedding:

$$
\mathbf{E} \in \mathbb{R}^{V \times d}
$$

Ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai:

\mathbf{C}

$$
=
$$

\frac{1}{V}
\mathbf{E}^T
\mathbf{E}

Gi·∫£i:

\mathbf{C}\mathbf{v}_i

$$
=
$$

\lambda_i \mathbf{v}_i

K·∫øt qu·∫£ th·ª±c nghi·ªám:
	‚Ä¢	Ph∆∞∆°ng sai t·∫≠p trung v√†o s·ªë √≠t th√†nh ph·∫ßn ch√≠nh
	‚Ä¢	Embedding c√≥ c·∫•u tr√∫c th·∫•p chi·ªÅu hi·ªáu qu·∫£

‚∏ª

9. Vai tr√≤ c·ªßa t·ªëi ∆∞u ho√°

C√°c thu·∫≠t to√°n t·ªëi ∆∞u ph·ªï bi·∫øn:
	‚Ä¢	SGD
	‚Ä¢	Adam
	‚Ä¢	AdamW

V√≠ d·ª• Adam c·∫≠p nh·∫≠t:

m_t = \beta_1 m_{t-1} + 1-\beta_1g_t

$$
v_t = \beta_2 v_{t-1} + 1-\beta_2g_t^2
$$

\theta_t =

\theta_{t-1}
-
\eta
\frac{$m_t$}{\sqrt{$v_t$}+\epsilon}

Embedding ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·ªìng th·ªùi v·ªõi to√†n b·ªô m√¥ h√¨nh.

‚∏ª

10. K·∫øt lu·∫≠n

Hu·∫•n luy·ªán embedding t·ª´ ƒë·∫ßu l√† qu√° tr√¨nh:
	1.	√Ånh x·∫° token r·ªùi r·∫°c sang kh√¥ng gian li√™n t·ª•c
	2.	T·ªëi ∆∞u th√¥ng qua d·ª± ƒëo√°n ng·ªØ c·∫£nh ho·∫∑c token ti·∫øp theo
	3.	H√¨nh th√†nh c·∫•u tr√∫c h√¨nh h·ªçc ph·∫£n √°nh ng·ªØ nghƒ©a
	4.	T√≠ch h·ª£p tr·ª±c ti·∫øp v√†o c∆° ch·∫ø attention trong Transformer

Embedding kh√¥ng ch·ªâ l√† b·∫£ng tra c·ª©u (lookup table), m√† l√† m·ªôt kh√¥ng gian h√¨nh h·ªçc c√≥ c·∫•u tr√∫c, ƒë∆∞·ª£c h√¨nh th√†nh th√¥ng qua t·ªëi ∆∞u ho√° th·ªëng k√™ quy m√¥ l·ªõn.

‚∏ª

T√†i li·ªáu tham kh·∫£o
	1.	Ashish Vaswani et al. (2017). Attention Is All You Need.
	2.	OpenAI (2019). GPT-2 Technical Report.
	3.	Tomas Mikolov et al. (2013). Efficient Estimation of Word Representations in Vector Space.
	4.	Kingma & Ba (2015). Adam: A Method for Stochastic Optimization.
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [aero llm 01 word2vec vs glove vs gpt vs bert oh my](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) |
| [aero llm 02 exploring glove pretrained embeddings](aero_llm_02_exploring_glove_pretrained_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_02_exploring_glove_pretrained_embeddings.md) |
| [aero llm 03 codechallenge wikipedia vs twitter embeddings part 1](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) |
| [So s√°nh Bi·ªÉu Di·ªÖn T·ª´ V·ª±ng gi·ªØa Wikipedia v√† Twitter b·∫±ng Ph√¢n T√≠ch T∆∞∆°ng ƒê·ªìng Bi·ªÉu Di·ªÖn (RSA)](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) |
| [So s√°nh Bi·ªÉu Di·ªÖn Ng·ªØ Nghƒ©a c·ªßa GPT-2 v√† BERT th√¥ng qua Ph√¢n T√≠ch Embedding](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) |
| [To√°n h·ªçc c·ªßa Token v√† Embedding trong M√¥ h√¨nh Ng√¥n ng·ªØ L·ªõn](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) |
| [Cosine Similarity v√† M·ªëi Quan H·ªá v·ªõi H·ªá S·ªë T∆∞∆°ng Quan: C∆° S·ªü To√°n H·ªçc v√† ·ª®ng D·ª•ng trong NLP](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) |
| [Ph√¢n T√≠ch Cosine Similarity trong Kh√¥ng Gian Embedding c·ªßa GPT-2](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) |
| [Unembedding trong M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn: T·ª´ Vector ·∫®n ƒê·∫øn Token](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) |
| [Position Embeddings trong Transformer: C∆° S·ªü To√°n H·ªçc v√† ·ª®ng D·ª•ng trong M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn](aero_llm_10_position_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_10_position_embeddings.md) |
| [Ph√¢n T√≠ch Th·ª±c Nghi·ªám Embedding V·ªã Tr√≠ Trong Transformer: T·ª´ C·∫•u Tr√∫c Tuy·∫øn T√≠nh ƒê·∫øn Kh√¥ng Gian H√¨nh H·ªçc](aero_llm_11_codechallenge_exploring_position_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_11_codechallenge_exploring_position_embeddings.md) |
| üìå **[Hu·∫•n Luy·ªán Embedding T·ª´ ƒê·∫ßu: C∆° S·ªü To√°n H·ªçc, C∆° Ch·∫ø T·ªëi ∆Øu v√† ·ª®ng D·ª•ng Trong M√¥ H√¨nh Ng√¥n Ng·ªØ](aero_llm_12_training_embeddings_from_scratch.md)** | [Xem b√†i vi·∫øt ‚Üí](aero_llm_12_training_embeddings_from_scratch.md) |
| [Thi·∫øt K·∫ø Data Loader Cho Hu·∫•n Luy·ªán M√¥ H√¨nh Ng√¥n Ng·ªØ: C∆° S·ªü To√°n H·ªçc, Ki·∫øn Tr√∫c v√† T·ªëi ∆Øu Ho√°](aero_llm_13_create_a_data_loader_to_train_a_model.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_13_create_a_data_loader_to_train_a_model.md) |
| [X√¢y D·ª±ng M√¥ H√¨nh H·ªçc Embedding T·ª´ ƒê·∫ßu: Ki·∫øn Tr√∫c, T·ªëi ∆Øu Ho√° v√† Ph√¢n T√≠ch To√°n H·ªçc](aero_llm_14_build_a_model_to_learn_the_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_14_build_a_model_to_learn_the_embeddings.md) |
| [H√†m M·∫•t M√°t Trong Hu·∫•n Luy·ªán Embedding: C∆° S·ªü L√Ω Thuy·∫øt, Ph√¢n T√≠ch Gradient v√† ·ª®ng D·ª•ng Trong M√¥ H√¨nh Ng√¥n Ng·ªØ](aero_llm_15_loss_function_to_train_the_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_15_loss_function_to_train_the_embeddings.md) |
| [Hu·∫•n luy·ªán v√† ƒê√°nh gi√° M√¥ h√¨nh H·ªçc M√°y: C∆° s·ªü L√Ω thuy·∫øt v√† Th·ª±c ti·ªÖn](aero_llm_16_train_and_evaluate_the_model.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_16_train_and_evaluate_the_model.md) |
| [S·ª± Thay ƒê·ªïi c·ªßa Embeddings Trong Qu√° Tr√¨nh Hu·∫•n Luy·ªán: Ph√¢n T√≠ch To√°n H·ªçc v√† Th·ª±c Nghi·ªám](aero_llm_17_codechallenge_how_the_embeddings_change.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_17_codechallenge_how_the_embeddings_change.md) |
| [ƒê·ªô ·ªîn ƒê·ªãnh c·ªßa Embeddings trong M√¥ H√¨nh Ng√¥n Ng·ªØ: Ph√¢n T√≠ch To√°n H·ªçc v√† Th·ª±c Nghi·ªám](aero_llm_18_codechallenge_how_stable_are_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_18_codechallenge_how_stable_are_embeddings.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

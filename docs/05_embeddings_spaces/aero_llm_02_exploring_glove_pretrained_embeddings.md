
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [05 embeddings spaces](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Kháº£o sÃ¡t vÃ  PhÃ¢n tÃ­ch ToÃ¡n há»c Embedding Tiá»n huáº¥n luyá»‡n GloVe

Tá»« Ma tráº­n Äá»“ng xuáº¥t hiá»‡n Ä‘áº¿n Cáº¥u trÃºc HÃ¬nh há»c KhÃ´ng gian Tá»« vá»±ng

â¸»

TÃ³m táº¯t

Dá»±a trÃªn tÃ i liá»‡u Ä‘Ã­nh kÃ¨m â€œExploring GloVe Pretrained Embeddingsâ€, bÃ i viáº¿t nÃ y trÃ¬nh bÃ y má»™t phÃ¢n tÃ­ch khoa há»c vá» embedding tiá»n huáº¥n luyá»‡n GloVe, bao gá»“m cÆ¡ sá»Ÿ lÃ½ thuyáº¿t, hÃ m má»¥c tiÃªu tá»‘i Æ°u, cáº¥u trÃºc hÃ¬nh há»c cá»§a khÃ´ng gian vector, vÃ  cÃ¡c Ä‘áº·c tÃ­nh ngá»¯ nghÄ©a há»c Ä‘Æ°á»£c há»c tá»« thá»‘ng kÃª Ä‘á»“ng xuáº¥t hiá»‡n toÃ n cá»¥c.

BÃ i viáº¿t Ä‘á»“ng thá»i má»Ÿ rá»™ng báº±ng cÃ¡c nguá»“n há»c thuáº­t ná»n táº£ng (Pennington et al., 2014; Mikolov et al., 2013; Levy & Goldberg, 2014) vÃ  cung cáº¥p cÃ¡c cÃ´ng thá»©c toÃ¡n há»c minh hoáº¡ chi tiáº¿t.

â¸»

1. Giá»›i thiá»‡u

Biá»ƒu diá»…n tá»« (word representation) lÃ  ná»n táº£ng cá»§a nhiá»u há»‡ thá»‘ng xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP).

Má»¥c tiÃªu lÃ  xÃ¢y dá»±ng Ã¡nh xáº¡:

E: V \rightarrow \mathbb{R}^d

Trong Ä‘Ã³:
	â€¢	V: táº­p tá»« vá»±ng
	â€¢	d: sá»‘ chiá»u embedding

KhÃ¡c vá»›i Word2Vec (dá»±a trÃªn ngá»¯ cáº£nh cá»¥c bá»™), GloVe khai thÃ¡c thá»‘ng kÃª toÃ n cá»¥c cá»§a ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n.

â¸»

2. Ma tráº­n Äá»“ng xuáº¥t hiá»‡n

Giáº£ sá»­ má»™t corpus cÃ³ tá»•ng sá»‘ tá»« T.

Äá»‹nh nghÄ©a:

X_{ij} = \text{sá»‘ láº§n tá»« } w_j \text{ xuáº¥t hiá»‡n trong cá»­a sá»• ngá»¯ cáº£nh cá»§a } w_i

Tá»•ng sá»‘ láº§n xuáº¥t hiá»‡n cá»§a w_i:

X_i = \sum_j X_{ij}

XÃ¡c suáº¥t Ä‘á»“ng xuáº¥t hiá»‡n:

P_{ij} = \frac{X_{ij}}{X_i}

â¸»

3. Trá»±c giÃ¡c Tá»· lá»‡ XÃ¡c suáº¥t

Pennington et al. (2014) láº­p luáº­n ráº±ng tá»· lá»‡ xÃ¡c suáº¥t Ä‘á»“ng xuáº¥t hiá»‡n mang thÃ´ng tin ngá»¯ nghÄ©a:

\frac{P_{ik}}{P_{jk}}

VÃ­ dá»¥:
	â€¢	i = ice
	â€¢	j = steam
	â€¢	k = solid

Ta ká»³ vá»ng:

\frac{P$\text{solid}|\text{ice}$}{P$\text{solid}|\text{steam}$} \gg 1

Do Ä‘Ã³, embedding nÃªn mÃ£ hÃ³a cÃ¡c tá»· lá»‡ nÃ y.

â¸»

4. HÃ m Má»¥c tiÃªu cá»§a GloVe

GloVe tÃ¬m vector w_i vÃ  \tilde{w}_j sao cho:

w_i^\top \tilde{w}_j + b_i + b_j \approx \log X_{ij}

HÃ m máº¥t mÃ¡t:

J = \sum_{i,j} f$X_{ij}$
\left(
w_i^\top \tilde{w}_j + b_i + b_j - \log X_{ij}
\right)^2

Trong Ä‘Ã³:

f$x$ =
\begin{cases}
$x/x_{max}$^\alpha & x < x_{max} \\
1 & \text{otherwise}
\end{cases}

ThÆ°á»ng:

\alpha = 0.75

â¸»

5. LiÃªn há»‡ vá»›i PMI (Pointwise Mutual Information)

PMI Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a:

PMI(i,j) = \log \frac{P_{ij}}{P_i P_j}

Levy & Goldberg (2014) chá»‰ ra ráº±ng Word2Vec vá»›i negative sampling xáº¥p xá»‰ phÃ¢n rÃ£ ma tráº­n:

PMI(i,j) - \log k

GloVe gáº§n tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i viá»‡c factorize ma tráº­n log-count.

Do Ä‘Ã³:

w_i^\top \tilde{w}_j \approx PMI(i,j)

â¸»

6. HÃ¬nh há»c cá»§a KhÃ´ng gian Embedding

Embedding sau huáº¥n luyá»‡n náº±m trong:

\mathbb{R}^d

Khoáº£ng cÃ¡ch cosine:

\cos$\theta$ =
\frac{w_i^\top w_j}
{\|w_i\| \|w_j\|}

Pháº£n Ã¡nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a.

â¸»

6.1 Quan há»‡ Tuyáº¿n tÃ­nh

Má»™t tÃ­nh cháº¥t ná»•i báº­t:

w_{king} - w_{man} + w_{woman} \approx w_{queen}

Äiá»u nÃ y cÃ³ thá»ƒ diá»…n giáº£i:

(w_{king} - w_{man}) \approx (w_{queen} - w_{woman})

Cho tháº¥y tá»“n táº¡i cÃ¡c hÆ°á»›ng ngá»¯ nghÄ©a trong khÃ´ng gian vector.

â¸»

7. PhÃ¢n tÃ­ch Phá»• Trá»‹ riÃªng (Eigenvalue Spectrum)

Ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n:

X \in \mathbb{R}^{|V| \times |V|}

PhÃ¢n rÃ£ SVD:

X = U \Sigma V^\top

Embedding tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i chá»n:

W = U_d \Sigma_d^{1/2}

Phá»• trá»‹ riÃªng thÆ°á»ng tuÃ¢n theo luáº­t Zipf:

\lambda_r \propto \frac{1}{r^\beta}

Theo George Kingsley Zipf.

â¸»

8. Entropy vÃ  ThÃ´ng tin

Entropy cá»§a phÃ¢n bá»‘ tá»«:

H$W$ = -\sum_i P$w_i$\log P$w_i$

Mutual information giá»¯a hai tá»«:

I(i;j) = \sum_{i,j} P_{ij} \log \frac{P_{ij}}{P_i P_j}

GloVe há»c embedding sao cho:

w_i^\top w_j \approx I(i;j)

â¸»

9. Äá»™ phá»©c táº¡p TÃ­nh toÃ¡n

Giáº£ sá»­ sá»‘ pháº§n tá»­ khÃ¡c 0 cá»§a X lÃ  |X|.

Äá»™ phá»©c táº¡p:

O(|X|d)

So vá»›i Transformer nhÆ° BERT:

O(n^2 d)

GloVe hiá»‡u quáº£ hÆ¡n cho embedding tÄ©nh.

â¸»

10. Háº¡n cháº¿ cá»§a GloVe
	1.	Embedding tÄ©nh
	2.	KhÃ´ng phá»¥ thuá»™c ngá»¯ cáº£nh
	3.	KhÃ´ng mÃ´ hÃ¬nh hÃ³a thá»© tá»± tá»«

Biá»ƒu diá»…n cá»‘ Ä‘á»‹nh:

e$w$ = \text{háº±ng sá»‘}

Trong khi mÃ´ hÃ¬nh ngá»¯ cáº£nh:

e_t = f$w_1,\dots,w_T$

â¸»

11. Thá»±c nghiá»‡m KhÃ¡m phÃ¡ Embedding

CÃ¡c phÃ©p phÃ¢n tÃ­ch thÆ°á»ng dÃ¹ng:
	â€¢	PCA:

Z = XW
	â€¢	t-SNE:

P_{ij} \propto \exp$-\|x_i-x_j\|^2$

Cho tháº¥y cÃ¡c cá»¥m ngá»¯ nghÄ©a rÃµ rÃ ng:
	â€¢	Quá»‘c gia
	â€¢	Giá»›i tÃ­nh
	â€¢	Sá»‘ nhiá»u

â¸»

12. Káº¿t luáº­n

GloVe dá»±a trÃªn nguyÃªn lÃ½:

w_i^\top w_j \approx \log X_{ij}

Embedding há»c Ä‘Æ°á»£c:
	â€¢	Cáº¥u trÃºc tuyáº¿n tÃ­nh
	â€¢	Quan há»‡ ngá»¯ nghÄ©a
	â€¢	ThÃ´ng tin toÃ n cá»¥c

Máº·c dÃ¹ Ä‘Ã£ bá»‹ thay tháº¿ trong nhiá»u á»©ng dá»¥ng bá»Ÿi mÃ´ hÃ¬nh Transformer, GloVe váº«n lÃ  ná»n táº£ng lÃ½ thuyáº¿t quan trá»ng trong biá»ƒu diá»…n tá»« phÃ¢n bá»‘.

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Pennington, Socher & Manning (2014). GloVe: Global Vectors for Word Representation.
	2.	Mikolov et al. (2013). Efficient Estimation of Word Representations.
	3.	Levy & Goldberg (2014). Neural Word Embedding as Implicit Matrix Factorization.
	4.	Shannon (1948). A Mathematical Theory of Communication.
	5.	Zipf (1935). The Psycho-Biology of Language.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero llm 01 word2vec vs glove vs gpt vs bert oh my](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) |
| ğŸ“Œ **[aero llm 02 exploring glove pretrained embeddings](aero_llm_02_exploring_glove_pretrained_embeddings.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_02_exploring_glove_pretrained_embeddings.md) |
| [aero llm 03 codechallenge wikipedia vs twitter embeddings part 1](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Tá»« Vá»±ng giá»¯a Wikipedia vÃ  Twitter báº±ng PhÃ¢n TÃ­ch TÆ°Æ¡ng Äá»“ng Biá»ƒu Diá»…n (RSA)](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Ngá»¯ NghÄ©a cá»§a GPT-2 vÃ  BERT thÃ´ng qua PhÃ¢n TÃ­ch Embedding](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) |
| [ToÃ¡n há»c cá»§a Token vÃ  Embedding trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) |
| [Cosine Similarity vÃ  Má»‘i Quan Há»‡ vá»›i Há»‡ Sá»‘ TÆ°Æ¡ng Quan: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong NLP](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) |
| [PhÃ¢n TÃ­ch Cosine Similarity trong KhÃ´ng Gian Embedding cá»§a GPT-2](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) |
| [Unembedding trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: Tá»« Vector áº¨n Äáº¿n Token](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) |
| [Position Embeddings trong Transformer: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_llm_10_position_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_position_embeddings.md) |
| [PhÃ¢n TÃ­ch Thá»±c Nghiá»‡m Embedding Vá»‹ TrÃ­ Trong Transformer: Tá»« Cáº¥u TrÃºc Tuyáº¿n TÃ­nh Äáº¿n KhÃ´ng Gian HÃ¬nh Há»c](aero_llm_11_codechallenge_exploring_position_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_exploring_position_embeddings.md) |
| [Huáº¥n Luyá»‡n Embedding Tá»« Äáº§u: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, CÆ¡ Cháº¿ Tá»‘i Æ¯u vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_12_training_embeddings_from_scratch.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_training_embeddings_from_scratch.md) |
| [Thiáº¿t Káº¿ Data Loader Cho Huáº¥n Luyá»‡n MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Kiáº¿n TrÃºc vÃ  Tá»‘i Æ¯u HoÃ¡](aero_llm_13_create_a_data_loader_to_train_a_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_create_a_data_loader_to_train_a_model.md) |
| [XÃ¢y Dá»±ng MÃ´ HÃ¬nh Há»c Embedding Tá»« Äáº§u: Kiáº¿n TrÃºc, Tá»‘i Æ¯u HoÃ¡ vÃ  PhÃ¢n TÃ­ch ToÃ¡n Há»c](aero_llm_14_build_a_model_to_learn_the_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_build_a_model_to_learn_the_embeddings.md) |
| [HÃ m Máº¥t MÃ¡t Trong Huáº¥n Luyá»‡n Embedding: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, PhÃ¢n TÃ­ch Gradient vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_15_loss_function_to_train_the_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_loss_function_to_train_the_embeddings.md) |
| [Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh Há»c MÃ¡y: CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Thá»±c tiá»…n](aero_llm_16_train_and_evaluate_the_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_train_and_evaluate_the_model.md) |
| [Sá»± Thay Äá»•i cá»§a Embeddings Trong QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_llm_17_codechallenge_how_the_embeddings_change.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_codechallenge_how_the_embeddings_change.md) |
| [Äá»™ á»”n Äá»‹nh cá»§a Embeddings trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_llm_18_codechallenge_how_stable_are_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_how_stable_are_embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [05 embeddings spaces](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Kháº£o sÃ¡t vÃ  PhÃ¢n tÃ­ch ToÃ¡n há»c Embedding Tiá»n huáº¥n luyá»‡n GloVe

Tá»« Ma tráº­n Äá»“ng xuáº¥t hiá»‡n Ä‘áº¿n Cáº¥u trÃºc HÃ¬nh há»c KhÃ´ng gian Tá»« vá»±ng

â¸»

TÃ³m táº¯t

Dá»±a trÃªn tÃ i liá»‡u Ä‘Ã­nh kÃ¨m â€œExploring GloVe Pretrained Embeddingsâ€, bÃ i viáº¿t nÃ y trÃ¬nh bÃ y má»™t phÃ¢n tÃ­ch khoa há»c vá» embedding tiá»n huáº¥n luyá»‡n GloVe, bao gá»“m cÆ¡ sá»Ÿ lÃ½ thuyáº¿t, hÃ m má»¥c tiÃªu tá»‘i Æ°u, cáº¥u trÃºc hÃ¬nh há»c cá»§a khÃ´ng gian vector, vÃ  cÃ¡c Ä‘áº·c tÃ­nh ngá»¯ nghÄ©a há»c Ä‘Æ°á»£c há»c tá»« thá»‘ng kÃª Ä‘á»“ng xuáº¥t hiá»‡n toÃ n cá»¥c.

BÃ i viáº¿t Ä‘á»“ng thá»i má»Ÿ rá»™ng báº±ng cÃ¡c nguá»“n há»c thuáº­t ná»n táº£ng (Pennington et al., 2014; Mikolov et al., 2013; Levy & Goldberg, 2014) vÃ  cung cáº¥p cÃ¡c cÃ´ng thá»©c toÃ¡n há»c minh hoáº¡ chi tiáº¿t.

â¸»

1. Giá»›i thiá»‡u

Biá»ƒu diá»…n tá»« (word representation) lÃ  ná»n táº£ng cá»§a nhiá»u há»‡ thá»‘ng xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP).

Má»¥c tiÃªu lÃ  xÃ¢y dá»±ng Ã¡nh xáº¡:

$$
E: V \rightarrow \mathbb{R}^d
$$

Trong Ä‘Ã³:
	â€¢	V: táº­p tá»« vá»±ng
	â€¢	d: sá»‘ chiá»u embedding

KhÃ¡c vá»›i Word2Vec (dá»±a trÃªn ngá»¯ cáº£nh cá»¥c bá»™), GloVe khai thÃ¡c thá»‘ng kÃª toÃ n cá»¥c cá»§a ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n.

â¸»

2. Ma tráº­n Äá»“ng xuáº¥t hiá»‡n

Giáº£ sá»­ má»™t corpus cÃ³ tá»•ng sá»‘ tá»« T.

Äá»‹nh nghÄ©a:

$$

$$

X_{ij} = \text{sá»‘ láº§n tá»« } w_j \text{ xuáº¥t hiá»‡n trong cá»­a sá»• ngá»¯ cáº£nh cá»§a } w_i

$$

$$

Tá»•ng sá»‘ láº§n xuáº¥t hiá»‡n cá»§a $w_i$:

$$

$$

X_i = \sum_j X_{ij}

$$

$$

XÃ¡c suáº¥t Ä‘á»“ng xuáº¥t hiá»‡n:

$$

$$

P_{ij} = \frac{X_{ij}}{X_i}

$$

$$

â¸»

3. Trá»±c giÃ¡c Tá»· lá»‡ XÃ¡c suáº¥t

Pennington et al. (2014) láº­p luáº­n ráº±ng tá»· lá»‡ xÃ¡c suáº¥t Ä‘á»“ng xuáº¥t hiá»‡n mang thÃ´ng tin ngá»¯ nghÄ©a:

\frac{P_{ik}}{P_{jk}}

VÃ­ dá»¥:

$$

$$

â€¢	i = ice

$$

$$

$$
â€¢	j = steam
$$

$$

$$

â€¢	k = solid

$$

$$

Ta ká»³ vá»ng:

\frac{$P(\text{solid}\mid \text{ice})$}{$P(\text{solid}\mid \text{steam})$} \gg 1

Do Ä‘Ã³, embedding nÃªn mÃ£ hÃ³a cÃ¡c tá»· lá»‡ nÃ y.

â¸»

4. HÃ m Má»¥c tiÃªu cá»§a GloVe

GloVe tÃ¬m vector $w_i$ vÃ  \tilde{w}_j sao cho:

$$

$$

w_i^\top \tilde{w}_j + b_i + b_j \approx \log X_{ij}

$$

$$

HÃ m máº¥t mÃ¡t:

$$

$$

J = \sum_{i,j} fX_{ij}

$$

$$

$\le$ft(

$$

$$

w_i^\top \tilde{w}_j + b_i + b_j - \log X_{ij}

$$
\right)^2 Trong Ä‘Ã³:
$$

fx =

$$
\begin{cases} x/x_{max}^\alpha & x \lt  x_{max} \\ 1 & \text{otherwise} \end{cases} ThÆ°á»ng:
$$

\alpha = 0.75

$$
â¸» 5. LiÃªn há»‡ vá»›i PMI (Pointwise Mutual Information) PMI Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a:
$$

$$
PMI(i,j) = \log \frac{P_{ij}}{P_i P_j}
$$

$$
Levy & Goldberg (2014) chá»‰ ra ráº±ng Word2Vec vá»›i negative sampling xáº¥p xá»‰ phÃ¢n rÃ£ ma tráº­n:
$$

PMI(i,j) - \log k

$$
GloVe gáº§n tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i viá»‡c factorize ma tráº­n log-count. Do Ä‘Ã³:
$$

$$
w_i^\top \tilde{w}_j \approx PMI(i,j)
$$

$$
â¸» 6. HÃ¬nh há»c cá»§a KhÃ´ng gian Embedding Embedding sau huáº¥n luyá»‡n náº±m trong: \mathbb{R}^d Khoáº£ng cÃ¡ch cosine:
$$

\cos\theta =

$$
\frac{w_i^\top w_j} {\|w_i\| \|w_j\|} Pháº£n Ã¡nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a. â¸» 6.1 Quan há»‡ Tuyáº¿n tÃ­nh Má»™t tÃ­nh cháº¥t ná»•i báº­t:
$$

$$
w_{king} - w_{man} + w_{woman} \approx w_{queen}
$$

$$
Äiá»u nÃ y cÃ³ thá»ƒ diá»…n giáº£i:
$$

$$
(w_{king} - w_{man}) \approx (w_{queen} - w_{woman})
$$

$$
Cho tháº¥y tá»“n táº¡i cÃ¡c hÆ°á»›ng ngá»¯ nghÄ©a trong khÃ´ng gian vector. â¸» 7. PhÃ¢n tÃ­ch Phá»• Trá»‹ riÃªng (Eigenvalue Spectrum) Ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n:
$$

X \in \mathbb{R}^{|V| \times |V|}

$$
PhÃ¢n rÃ£ SVD:
$$

$$
X = U \Sigma V^\top
$$

$$
Embedding tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i chá»n:
$$

$$
W = U_d \Sigma_d^{1/2}
$$

$$
Phá»• trá»‹ riÃªng thÆ°á»ng tuÃ¢n theo luáº­t Zipf:
$$

\lambda_r \propto \frac{1}{r^\beta}

$$
Theo George Kingsley Zipf. â¸» 8. Entropy vÃ  ThÃ´ng tin Entropy cá»§a phÃ¢n bá»‘ tá»«:
$$

$$
HW = -\sum_i P(w_i)\log P(w_i)
$$

$$
Mutual information giá»¯a hai tá»«:
$$

$$
I(i;j) = \sum_{i,j} P_{ij} \log \frac{P_{ij}}{P_i P_j}
$$

$$
GloVe há»c embedding sao cho:
$$

$$
w_i^\top w_j \approx I(i;j)
$$

$$
â¸» 9. Äá»™ phá»©c táº¡p TÃ­nh toÃ¡n Giáº£ sá»­ sá»‘ pháº§n tá»­ khÃ¡c 0 cá»§a X lÃ  |X|. Äá»™ phá»©c táº¡p: O(|X|d) So vá»›i Transformer nhÆ° BERT: O(n^2 d) GloVe hiá»‡u quáº£ hÆ¡n cho embedding tÄ©nh. â¸» 10. Háº¡n cháº¿ cá»§a GloVe 1.	Embedding tÄ©nh 2.	KhÃ´ng phá»¥ thuá»™c ngá»¯ cáº£nh 3.	KhÃ´ng mÃ´ hÃ¬nh hÃ³a thá»© tá»± tá»« Biá»ƒu diá»…n cá»‘ Ä‘á»‹nh:
$$

ew = \text{háº±ng sá»‘}

$$
Trong khi mÃ´ hÃ¬nh ngá»¯ cáº£nh:
$$

$$
e_t = fw_1,\dots,w_T
$$

$$
â¸» 11. Thá»±c nghiá»‡m KhÃ¡m phÃ¡ Embedding CÃ¡c phÃ©p phÃ¢n tÃ­ch thÆ°á»ng dÃ¹ng: â€¢	PCA:
$$

$$
Z = XW
$$

$$
â€¢	t-SNE:
$$

P_{ij} \propto \exp-\\mid x_i-x_j\\mid^2

$$
Cho tháº¥y cÃ¡c cá»¥m ngá»¯ nghÄ©a rÃµ rÃ ng: â€¢	Quá»‘c gia â€¢	Giá»›i tÃ­nh â€¢	Sá»‘ nhiá»u â¸» 12. Káº¿t luáº­n GloVe dá»±a trÃªn nguyÃªn lÃ½:
$$

$$
w_i^\top w_j \approx \log X_{ij}
$$


<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [05 embeddings spaces](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Unembedding trong M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn: T·ª´ Vector ·∫®n ƒê·∫øn Token

## T√≥m t·∫Øt

Trong c√°c m√¥ h√¨nh ng√¥n ng·ªØ d·ª±a tr√™n Transformer, qu√° tr√¨nh ‚Äúembedding‚Äù √°nh x·∫° token r·ªùi r·∫°c sang kh√¥ng gian vector li√™n t·ª•c. Tuy nhi√™n, b∆∞·ªõc ng∆∞·ª£c l·∫°i ‚Äì chuy·ªÉn t·ª´ vector ·∫©n sang ph√¢n ph·ªëi x√°c su·∫•t tr√™n token ‚Äì ƒë∆∞·ª£c g·ªçi l√† **unembedding**. B√†i vi·∫øt n√†y ph√¢n t√≠ch n·ªÅn t·∫£ng to√°n h·ªçc c·ªßa unembedding trong [GPT-2](chatgpt://generic-entity?number=0), l√†m r√µ vai tr√≤ c·ªßa weight tying, t√≠ch v√¥ h∆∞·ªõng, softmax v√† c·∫•u tr√∫c h√¨nh h·ªçc c·ªßa kh√¥ng gian t·ª´ v·ª±ng.

---

## 1. Gi·ªõi thi·ªáu

Qu√° tr√¨nh x·ª≠ l√Ω vƒÉn b·∫£n trong m√¥ h√¨nh ng√¥n ng·ªØ c√≥ th·ªÉ t√≥m t·∫Øt:

$$

\text{Token} 
\rightarrow 
\text{Embedding} 
\rightarrow 
\text{Transformer layers} 
\rightarrow 
\text{Hidden state} 
\rightarrow 
\text{Unembedding} 
\rightarrow 
\text{Softmax}

$$

N·∫øu embedding l√† √°nh x·∫°:

$$

f: \mathcal{V} \rightarrow \mathbb{R}^d

$$

th√¨ unembedding l√† √°nh x·∫° ng∆∞·ª£c:

$$

g: \mathbb{R}^d \rightarrow \mathbb{R}^{|\mathcal{V}|}

$$

---

## 2. Embedding: T·ª´ token ƒë·∫øn vector

Gi·∫£ s·ª≠ t·ª´ v·ª±ng c√≥ k√≠ch th∆∞·ªõc $|V|$, ma tr·∫≠n embedding:

$$

E \in \mathbb{R}^{|V| \times d}

$$

V·ªõi token ch·ªâ s·ªë $i$:

$$

\mathbf{v}_i = E[i]

$$

N·∫øu bi·ªÉu di·ªÖn one-hot $\mathbf{x}_i$:

$$

\mathbf{v}_i = \mathbf{x}_i E

$$

---

## 3. Unembedding: T·ª´ vector ƒë·∫øn token

Sau khi qua c√°c l·ªõp Transformer, ta thu ƒë∆∞·ª£c hidden state:

$$

\mathbf{h}_t \in \mathbb{R}^d

$$

ƒê·ªÉ chuy·ªÉn sang logit:

$$

\mathbf{z} = W_U \mathbf{h}_t

$$

Trong ƒë√≥:

$$

W_U \in \mathbb{R}^{|V| \times d}

$$

Vector logit:

$$

z_i = \mathbf{w}_i \cdot \mathbf{h}_t

$$

---

## 4. Weight Tying

Trong GPT-2, th∆∞·ªùng s·ª≠ d·ª•ng weight tying:

$$

W_U = E

$$

ho·∫∑c:

$$

W_U = E^T

$$

Khi ƒë√≥:

$$

z_i = \mathbf{v}_i \cdot \mathbf{h}_t

$$

ƒêi·ªÅu n√†y c√≥ √Ω nghƒ©a h√¨nh h·ªçc:

> Logit c·ªßa token $i$ ch√≠nh l√† t√≠ch v√¥ h∆∞·ªõng gi·ªØa embedding c·ªßa token ƒë√≥ v√† hidden state.

---

## 5. Softmax v√† ph√¢n ph·ªëi x√°c su·∫•t

X√°c su·∫•t d·ª± ƒëo√°n token ti·∫øp theo:

$$

P(w_i | h_t)
=
\frac{e^{z_i}}
{\sum_{j=1}^{|V|} e^{z_j}}

$$

Thay $z_i = \mathbf{v}_i \cdot \mathbf{h}_t$:

$$

P(w_i)
=
\frac{
\exp(\mathbf{v}_i \cdot \mathbf{h}_t)
}
{
\sum_j
\exp(\mathbf{v}_j \cdot \mathbf{h}_t)
}

$$

N·∫øu chu·∫©n h√≥a:

$$

\mathbf{v}_i \cdot \mathbf{h}_t
=
\|\mathbf{v}_i\|
\|\mathbf{h}_t\|
\cos \theta_i

$$

Suy ra:

$$

P(w_i)
\propto
\exp(
\|\mathbf{v}_i\|
\|\mathbf{h}_t\|
\cos \theta_i
)

$$

G√≥c gi·ªØa vector quy·∫øt ƒë·ªãnh x√°c su·∫•t.

---

## 6. Di·ªÖn gi·∫£i h√¨nh h·ªçc

Hidden state $\mathbf{h}_t$ c√≥ th·ªÉ xem nh∆∞:

- M·ªôt ‚Äútruy v·∫•n ng·ªØ nghƒ©a‚Äù
- M·ªôt ƒëi·ªÉm trong kh√¥ng gian embedding

Unembedding th·ª±c hi·ªán ph√©p chi·∫øu:

$$

\mathbf{z} = E \mathbf{h}_t

$$

Nghƒ©a l√† ta ƒëo m·ª©c ƒë·ªô ‚Äúg·∫ßn‚Äù gi·ªØa $\mathbf{h}_t$ v√† t·ª´ng vector t·ª´ v·ª±ng.

N·∫øu hai token c√≥ embedding g·∫ßn nhau:

$$

\mathbf{v}_i \approx \mathbf{v}_j

$$

th√¨:

$$

z_i \approx z_j

$$

Do ƒë√≥ ph√¢n ph·ªëi x√°c su·∫•t s·∫Ω t∆∞∆°ng t·ª±.

---

## 7. H√†m m·∫•t m√°t v√† t·ªëi ∆∞u h√≥a

H√†m m·∫•t m√°t cross-entropy:

$$

\mathcal{L}
=
- \log P(w_{true})

$$

Gradient theo $\mathbf{h}_t$:

$$

\nabla_{\mathbf{h}_t}
\mathcal{L}
=
\sum_i
P(w_i)\mathbf{v}_i
-
\mathbf{v}_{true}

$$

ƒêi·ªÅu n√†y cho th·∫•y:

- Hidden state ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh v·ªÅ ph√≠a embedding ƒë√∫ng
- V√† ƒë·∫©y xa embedding sai

---

## 8. So s√°nh v·ªõi ph√¢n lo·∫°i tuy·∫øn t√≠nh

Unembedding t∆∞∆°ng ƒë∆∞∆°ng m·ªôt b·ªô ph√¢n lo·∫°i tuy·∫øn t√≠nh:

$$

z_i = \mathbf{w}_i^T \mathbf{h}_t

$$

Kh√°c bi·ªát l√†:

- S·ªë l·ªõp r·∫•t l·ªõn (~50k)
- Tr·ªçng s·ªë g·∫Øn tr·ª±c ti·∫øp v·ªõi embedding

---

## 9. Quan h·ªá v·ªõi Cosine Similarity

N·∫øu chu·∫©n h√≥a embedding:

$$

\hat{\mathbf{v}}_i
=
\frac{\mathbf{v}_i}{\|\mathbf{v}_i\|}

$$

Khi ƒë√≥:

$$

z_i
=
\|\mathbf{v}_i\|
\|\mathbf{h}_t\|
\cos\theta_i

$$

N·∫øu b·ªè qua ƒë·ªô l·ªõn:

$$

z_i \propto \cos\theta_i

$$

Nh∆∞ v·∫≠y unembedding v·ªÅ b·∫£n ch·∫•t d·ª±a tr√™n cosine similarity.

---

## 10. Ph√¢n t√≠ch ph·ªï (Spectral Perspective)

Gi·∫£ s·ª≠ ma tr·∫≠n embedding:

$$

E = U \Sigma V^T

$$

(SVD decomposition)

Hidden state:

$$

\mathbf{h}_t
=
V \mathbf{c}

$$

Logit:

$$

\mathbf{z}
=
U \Sigma \mathbf{c}

$$

C√°c gi√° tr·ªã singular l·ªõn chi ph·ªëi ph√¢n ph·ªëi x√°c su·∫•t.

---

## 11. √ù nghƒ©a l√Ω thuy·∫øt

Unembedding:

1. Chuy·ªÉn t·ª´ kh√¥ng gian li√™n t·ª•c sang r·ªùi r·∫°c.
2. L√† ph√©p chi·∫øu tuy·∫øn t√≠nh quy m√¥ l·ªõn.
3. Ph·ª• thu·ªôc tr·ª±c ti·∫øp v√†o c·∫•u tr√∫c h√¨nh h·ªçc c·ªßa embedding.
4. T·∫°o li√™n k·∫øt ch·∫∑t ch·∫Ω gi·ªØa h·ªçc bi·ªÉu di·ªÖn v√† d·ª± ƒëo√°n x√°c su·∫•t.

V·ªÅ m·∫∑t to√°n h·ªçc:

$$

\text{Prediction}
=
\text{Softmax}(E \mathbf{h}_t)

$$

---

## 12. K·∫øt lu·∫≠n

Unembedding l√† b∆∞·ªõc cu·ªëi nh∆∞ng c·ª±c k·ª≥ quan tr·ªçng trong m√¥ h√¨nh ng√¥n ng·ªØ. N√≥:

- Chuy·ªÉn hidden state th√†nh ph√¢n ph·ªëi token
- D·ª±a tr√™n t√≠ch v√¥ h∆∞·ªõng trong kh√¥ng gian embedding
- Th·ªÉ hi·ªán r√µ m·ªëi quan h·ªá gi·ªØa h√¨nh h·ªçc vector v√† x√°c su·∫•t

Hi·ªÉu r√µ c∆° ch·∫ø n√†y gi√∫p:

- Ph√¢n t√≠ch h√†nh vi m√¥ h√¨nh
- Th·ª±c hi·ªán interpretability
- Thi·∫øt k·∫ø k·ªπ thu·∫≠t steering v√† logit lens
- So s√°nh kh√¥ng gian bi·ªÉu di·ªÖn gi·ªØa c√°c m√¥ h√¨nh

---

## T√†i li·ªáu tham kh·∫£o

1. Vaswani et al. (2017). Attention is All You Need.  
2. Radford et al. (2019). Language Models are Unsupervised Multitask Learners.  
3. Press & Wolf (2017). Using the Output Embedding to Improve Language Models.  
4. Jurafsky & Martin (2023). Speech and Language Processing.  

---
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [aero llm 01 word2vec vs glove vs gpt vs bert oh my](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) |
| [aero llm 02 exploring glove pretrained embeddings](aero_llm_02_exploring_glove_pretrained_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_02_exploring_glove_pretrained_embeddings.md) |
| [aero llm 03 codechallenge wikipedia vs twitter embeddings part 1](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) |
| [So s√°nh Bi·ªÉu Di·ªÖn T·ª´ V·ª±ng gi·ªØa Wikipedia v√† Twitter b·∫±ng Ph√¢n T√≠ch T∆∞∆°ng ƒê·ªìng Bi·ªÉu Di·ªÖn (RSA)](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) |
| [So s√°nh Bi·ªÉu Di·ªÖn Ng·ªØ Nghƒ©a c·ªßa GPT-2 v√† BERT th√¥ng qua Ph√¢n T√≠ch Embedding](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) |
| [To√°n h·ªçc c·ªßa Token v√† Embedding trong M√¥ h√¨nh Ng√¥n ng·ªØ L·ªõn](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) |
| [Cosine Similarity v√† M·ªëi Quan H·ªá v·ªõi H·ªá S·ªë T∆∞∆°ng Quan: C∆° S·ªü To√°n H·ªçc v√† ·ª®ng D·ª•ng trong NLP](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) |
| [Ph√¢n T√≠ch Cosine Similarity trong Kh√¥ng Gian Embedding c·ªßa GPT-2](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) |
| üìå **[Unembedding trong M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn: T·ª´ Vector ·∫®n ƒê·∫øn Token](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md)** | [Xem b√†i vi·∫øt ‚Üí](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) |
| [Position Embeddings trong Transformer: C∆° S·ªü To√°n H·ªçc v√† ·ª®ng D·ª•ng trong M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn](aero_llm_10_position_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_10_position_embeddings.md) |
| [Ph√¢n T√≠ch Th·ª±c Nghi·ªám Embedding V·ªã Tr√≠ Trong Transformer: T·ª´ C·∫•u Tr√∫c Tuy·∫øn T√≠nh ƒê·∫øn Kh√¥ng Gian H√¨nh H·ªçc](aero_llm_11_codechallenge_exploring_position_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_11_codechallenge_exploring_position_embeddings.md) |
| [Hu·∫•n Luy·ªán Embedding T·ª´ ƒê·∫ßu: C∆° S·ªü To√°n H·ªçc, C∆° Ch·∫ø T·ªëi ∆Øu v√† ·ª®ng D·ª•ng Trong M√¥ H√¨nh Ng√¥n Ng·ªØ](aero_llm_12_training_embeddings_from_scratch.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_12_training_embeddings_from_scratch.md) |
| [Thi·∫øt K·∫ø Data Loader Cho Hu·∫•n Luy·ªán M√¥ H√¨nh Ng√¥n Ng·ªØ: C∆° S·ªü To√°n H·ªçc, Ki·∫øn Tr√∫c v√† T·ªëi ∆Øu Ho√°](aero_llm_13_create_a_data_loader_to_train_a_model.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_13_create_a_data_loader_to_train_a_model.md) |
| [X√¢y D·ª±ng M√¥ H√¨nh H·ªçc Embedding T·ª´ ƒê·∫ßu: Ki·∫øn Tr√∫c, T·ªëi ∆Øu Ho√° v√† Ph√¢n T√≠ch To√°n H·ªçc](aero_llm_14_build_a_model_to_learn_the_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_14_build_a_model_to_learn_the_embeddings.md) |
| [H√†m M·∫•t M√°t Trong Hu·∫•n Luy·ªán Embedding: C∆° S·ªü L√Ω Thuy·∫øt, Ph√¢n T√≠ch Gradient v√† ·ª®ng D·ª•ng Trong M√¥ H√¨nh Ng√¥n Ng·ªØ](aero_llm_15_loss_function_to_train_the_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_15_loss_function_to_train_the_embeddings.md) |
| [Hu·∫•n luy·ªán v√† ƒê√°nh gi√° M√¥ h√¨nh H·ªçc M√°y: C∆° s·ªü L√Ω thuy·∫øt v√† Th·ª±c ti·ªÖn](aero_llm_16_train_and_evaluate_the_model.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_16_train_and_evaluate_the_model.md) |
| [S·ª± Thay ƒê·ªïi c·ªßa Embeddings Trong Qu√° Tr√¨nh Hu·∫•n Luy·ªán: Ph√¢n T√≠ch To√°n H·ªçc v√† Th·ª±c Nghi·ªám](aero_llm_17_codechallenge_how_the_embeddings_change.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_17_codechallenge_how_the_embeddings_change.md) |
| [ƒê·ªô ·ªîn ƒê·ªãnh c·ªßa Embeddings trong M√¥ H√¨nh Ng√¥n Ng·ªØ: Ph√¢n T√≠ch To√°n H·ªçc v√† Th·ª±c Nghi·ªám](aero_llm_18_codechallenge_how_stable_are_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_18_codechallenge_how_stable_are_embeddings.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

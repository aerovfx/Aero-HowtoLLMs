
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [05 embeddings spaces](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Unembedding trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: Tá»« Vector áº¨n Äáº¿n Token

## TÃ³m táº¯t

Trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ dá»±a trÃªn Transformer, quÃ¡ trÃ¬nh â€œembeddingâ€ Ã¡nh xáº¡ token rá»i ráº¡c sang khÃ´ng gian vector liÃªn tá»¥c. Tuy nhiÃªn, bÆ°á»›c ngÆ°á»£c láº¡i â€“ chuyá»ƒn tá»« vector áº©n sang phÃ¢n phá»‘i xÃ¡c suáº¥t trÃªn token â€“ Ä‘Æ°á»£c gá»i lÃ  **unembedding**. BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch ná»n táº£ng toÃ¡n há»c cá»§a unembedding trong [GPT-2](chatgpt://generic-entity?number=0), lÃ m rÃµ vai trÃ² cá»§a weight tying, tÃ­ch vÃ´ hÆ°á»›ng, softmax vÃ  cáº¥u trÃºc hÃ¬nh há»c cá»§a khÃ´ng gian tá»« vá»±ng.

---

## 1. Giá»›i thiá»‡u

QuÃ¡ trÃ¬nh xá»­ lÃ½ vÄƒn báº£n trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ cÃ³ thá»ƒ tÃ³m táº¯t:

$$
\text{Token} \rightarrow \text{Embedding} \rightarrow \text{Transformer layers} \rightarrow \text{Hidden state} \rightarrow \text{Unembedding} \rightarrow \text{Softmax}
$$

Náº¿u embedding lÃ  Ã¡nh xáº¡:

f: \mathcal{V} \rightarrow \mathbb{R}^d

thÃ¬ unembedding lÃ  Ã¡nh xáº¡ ngÆ°á»£c:

g: \mathbb{R}^d \rightarrow \mathbb{R}^{|\mathcal{V}|}

---

## 2. Embedding: Tá»« token Ä‘áº¿n vector

Giáº£ sá»­ tá»« vá»±ng cÃ³ kÃ­ch thÆ°á»›c $|V\mid$, ma tráº­n embedding:

E \in \mathbb{R}^{|V| \times d}

Vá»›i token chá»‰ sá»‘ $i$:

\mathbf{v}_i = E[i]

Náº¿u biá»ƒu diá»…n one-hot $\mathbf{x}_i$:

\mathbf{v}_i = \mathbf{x}_i E

---

## 3. Unembedding: Tá»« vector Ä‘áº¿n token

Sau khi qua cÃ¡c lá»›p Transformer, ta thu Ä‘Æ°á»£c hidden state:

\mathbf{h}_t \in \mathbb{R}^d

Äá»ƒ chuyá»ƒn sang logit:

\mathbf{z} = W_U \mathbf{h}_t

Trong Ä‘Ã³:

W_U \in \mathbb{R}^{|V| \times d}

Vector logit:

z_i = \mathbf{w}_i \cdot \mathbf{h}_t

---

## 4. Weight Tying

Trong GPT-2, thÆ°á»ng sá»­ dá»¥ng weight tying:

W_U = E

hoáº·c:

W_U = E^T

Khi Ä‘Ã³:

z_i = \mathbf{v}_i \cdot \mathbf{h}_t

Äiá»u nÃ y cÃ³ Ã½ nghÄ©a hÃ¬nh há»c:

> Logit cá»§a token $i$ chÃ­nh lÃ  tÃ­ch vÃ´ hÆ°á»›ng giá»¯a embedding cá»§a token Ä‘Ã³ vÃ  hidden state.

---

## 5. Softmax vÃ  phÃ¢n phá»‘i xÃ¡c suáº¥t

XÃ¡c suáº¥t dá»± Ä‘oÃ¡n token tiáº¿p theo:

P(w_i  \mid  h_t) = \frac{e^{z_i}} {\sum_{j=1}^{|V|} e^{z_j}}

Thay z_i = \mathbf{v}_i \cdot \mathbf{h}_t:

$$
P(w_i) = \frac{ \exp(\mathbf{v}_i \cdot \mathbf{h}_t) } { \sum_j \exp(\mathbf{v}_j \cdot \mathbf{h}_t) } Náº¿u chuáº©n hÃ³a: \mathbf{v}_i \cdot \mathbf{h}_t = \|\mathbf{v}_i\| \|\mathbf{h}_t\| \cos \theta_i Suy ra: P(w_i) \propto \exp( \|\mathbf{v}_i\| \|\mathbf{h}_t\| \cos \theta_i ) GÃ³c giá»¯a vector quyáº¿t Ä‘á»‹nh xÃ¡c suáº¥t. --- ## 6. Diá»…n giáº£i hÃ¬nh há»c Hidden state \mathbf{h}_t cÃ³ thá»ƒ xem nhÆ°: - Má»™t â€œtruy váº¥n ngá»¯ nghÄ©aâ€ - Má»™t Ä‘iá»ƒm trong khÃ´ng gian embedding Unembedding thá»±c hiá»‡n phÃ©p chiáº¿u: \mathbf{z} = E \mathbf{h}_t NghÄ©a lÃ  ta Ä‘o má»©c Ä‘á»™ â€œgáº§nâ€ giá»¯a \mathbf{h}_t vÃ  tá»«ng vector tá»« vá»±ng. Náº¿u hai token cÃ³ embedding gáº§n nhau: \mathbf{v}_i \approx \mathbf{v}_j thÃ¬: z_i \approx z_j Do Ä‘Ã³ phÃ¢n phá»‘i xÃ¡c suáº¥t sáº½ tÆ°Æ¡ng tá»±. --- ## 7. HÃ m máº¥t mÃ¡t vÃ  tá»‘i Æ°u hÃ³a HÃ m máº¥t mÃ¡t cross-entropy: \mathcal{L} = - \log P(w_{true}) Gradient theo \mathbf{h}_t: \nabla_{\mathbf{h}_t} \mathcal{L} = \sum_i P(w_i)\mathbf{v}_i - \mathbf{v}_{true} Äiá»u nÃ y cho tháº¥y: - Hidden state Ä‘Æ°á»£c Ä‘iá»u chá»‰nh vá» phÃ­a embedding Ä‘Ãºng - VÃ  Ä‘áº©y xa embedding sai --- ## 8. So sÃ¡nh vá»›i phÃ¢n loáº¡i tuyáº¿n tÃ­nh Unembedding tÆ°Æ¡ng Ä‘Æ°Æ¡ng má»™t bá»™ phÃ¢n loáº¡i tuyáº¿n tÃ­nh: z_i = \mathbf{w}_i^T \mathbf{h}_t KhÃ¡c biá»‡t lÃ : - Sá»‘ lá»›p ráº¥t lá»›n (~50k) - Trá»ng sá»‘ gáº¯n trá»±c tiáº¿p vá»›i embedding --- ## 9. Quan há»‡ vá»›i Cosine Similarity Náº¿u chuáº©n hÃ³a embedding: \hat{\mathbf{v}}_i = \frac{\mathbf{v}_i}{\|\mathbf{v}_i\|} Khi Ä‘Ã³: z_i = \|\mathbf{v}_i\| \|\mathbf{h}_t\| \cos\theta_i Náº¿u bá» qua Ä‘á»™ lá»›n: z_i \propto \cos\theta_i NhÆ° váº­y unembedding vá» báº£n cháº¥t dá»±a trÃªn cosine similarity. --- ## 10. PhÃ¢n tÃ­ch phá»• (Spectral Perspective) Giáº£ sá»­ ma tráº­n embedding: E = U \Sigma V^T (SVD decomposition) Hidden state: \mathbf{h}_t = V \mathbf{c} Logit: \mathbf{z} = U \Sigma \mathbf{c} CÃ¡c giÃ¡ trá»‹ singular lá»›n chi phá»‘i phÃ¢n phá»‘i xÃ¡c suáº¥t. --- ## 11. Ã nghÄ©a lÃ½ thuyáº¿t Unembedding: 1. Chuyá»ƒn tá»« khÃ´ng gian liÃªn tá»¥c sang rá»i ráº¡c. 2. LÃ  phÃ©p chiáº¿u tuyáº¿n tÃ­nh quy mÃ´ lá»›n. 3. Phá»¥ thuá»™c trá»±c tiáº¿p vÃ o cáº¥u trÃºc hÃ¬nh há»c cá»§a embedding. 4. Táº¡o liÃªn káº¿t cháº·t cháº½ giá»¯a há»c biá»ƒu diá»…n vÃ  dá»± Ä‘oÃ¡n xÃ¡c suáº¥t. Vá» máº·t toÃ¡n há»c: \text{Prediction} = \text{Softmax}(E \mathbf{h}_t)
$$


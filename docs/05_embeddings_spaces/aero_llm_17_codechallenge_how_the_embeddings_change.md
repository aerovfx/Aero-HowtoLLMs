
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [05 embeddings spaces](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Sá»± Thay Äá»•i cá»§a Embeddings Trong QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m

TÃ³m táº¯t

Biá»ƒu diá»…n tá»« (word embeddings) lÃ  ná»n táº£ng cá»§a cÃ¡c mÃ´ hÃ¬nh xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn hiá»‡n Ä‘áº¡i. Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, cÃ¡c vector embedding thay Ä‘á»•i liÃªn tá»¥c nháº±m tá»‘i Æ°u hÃ³a hÃ m má»¥c tiÃªu. BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch cÆ¡ cháº¿ cáº­p nháº­t embeddings dá»±a trÃªn gradient descent, mÃ´ hÃ¬nh hÃ³a sá»± thay Ä‘á»•i cá»§a khÃ´ng gian vector, vÃ  giáº£i thÃ­ch Ã½ nghÄ©a hÃ¬nh há»c cá»§a quÃ¡ trÃ¬nh tá»‘i Æ°u. Ná»™i dung Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn bÃ i thá»±c hÃ nh â€œHow the Embeddings Changeâ€, káº¿t há»£p cÃ¡c cÃ´ng trÃ¬nh cá»§a Tomas Mikolov (Word2Vec), Jeffrey Pennington (GloVe), vÃ  Ashish Vaswani (Transformer).

â¸»

1. Giá»›i thiá»‡u

Embeddings Ã¡nh xáº¡ má»—i tá»« w thÃ nh má»™t vector trong khÃ´ng gian \mathbb{R}^d:

E: w \rightarrow \mathbf{v}_w \in \mathbb{R}^d

Má»¥c tiÃªu cá»§a huáº¥n luyá»‡n lÃ  Ä‘iá»u chá»‰nh cÃ¡c vector nÃ y sao cho:
	â€¢	CÃ¡c tá»« cÃ³ ngá»¯ nghÄ©a tÆ°Æ¡ng tá»± náº±m gáº§n nhau
	â€¢	Quan há»‡ ngá»¯ nghÄ©a Ä‘Æ°á»£c báº£o toÃ n tuyáº¿n tÃ­nh

VÃ­ dá»¥ ná»•i tiáº¿ng:

\mathbf{v}_{king} - \mathbf{v}_{man} + \mathbf{v}_{woman} \approx \mathbf{v}_{queen}

â¸»

2. CÆ¡ cháº¿ ToÃ¡n há»c cá»§a Cáº­p nháº­t Embeddings

2.1 HÃ m má»¥c tiÃªu (Skip-gram)

Trong Word2Vec (Mikolov et al., 2013), má»¥c tiÃªu lÃ  tá»‘i Ä‘a hÃ³a xÃ¡c suáº¥t tá»« ngá»¯ cáº£nh c xuáº¥t hiá»‡n quanh tá»« trung tÃ¢m w:

\max \prod_{(w,c)\in D} P(c|w)

Vá»›i softmax:

P(c|w) = \frac{\exp(\mathbf{v}_c^\top \mathbf{v}_w)}{\sum_{c'} \exp(\mathbf{v}_{c'}^\top \mathbf{v}_w)}

HÃ m máº¥t mÃ¡t:

\mathcal{L} = - \sum_{(w,c)} \log P(c|w)

â¸»

2.2 Gradient cáº­p nháº­t vector

Gradient theo vector trung tÃ¢m:

\frac{\partial \mathcal{L}}{\partial \mathbf{v}_w}
= \sum_{c'} P(c'|w)\mathbf{v}_{c'} - \mathbf{v}_c

Cáº­p nháº­t:

\mathbf{v}_w^{(t+1)} = \mathbf{v}_w^{(t)} - \eta \frac{\partial \mathcal{L}}{\partial \mathbf{v}_w}

Trong Ä‘Ã³ \eta lÃ  learning rate.

â¸»

3. HÃ¬nh há»c cá»§a KhÃ´ng gian Embedding

3.1 Khoáº£ng cÃ¡ch Cosine

Äá»™ tÆ°Æ¡ng tá»± thÆ°á»ng dÃ¹ng cosine similarity:

\cos(\theta) = \frac{\mathbf{v}_a \cdot \mathbf{v}_b}
{||\mathbf{v}_a|| \, ||\mathbf{v}_b||}

Khi huáº¥n luyá»‡n:
	â€¢	Tá»« xuáº¥t hiá»‡n cÃ¹ng nhau â†’ gÃ³c giáº£m
	â€¢	Tá»« khÃ´ng liÃªn quan â†’ gÃ³c tÄƒng

â¸»

3.2 Di chuyá»ƒn trong khÃ´ng gian vector

Giáº£ sá»­ táº¡i bÆ°á»›c t:

\Delta \mathbf{v} = -\eta \nabla \mathcal{L}

Vector dá»‹ch chuyá»ƒn theo hÆ°á»›ng giáº£m loss. Tá»•ng quÃ¡t:

\mathbf{v}^{(T)} = \mathbf{v}^{(0)} - \eta \sum_{t=0}^{T-1} \nabla \mathcal{L}^{(t)}

Äiá»u nÃ y cho tháº¥y embedding cuá»‘i cÃ¹ng lÃ  tÃ­ch lÅ©y cá»§a toÃ n bá»™ lá»‹ch sá»­ gradient.

â¸»

4. Embeddings trong Transformer

Trong kiáº¿n trÃºc Transformer (Vaswani et al., 2017), embedding Ä‘Æ°á»£c cá»™ng vá»›i positional encoding:

\mathbf{x}_i = \mathbf{e}_i + \mathbf{p}_i

Self-attention:

Attention(Q,K,V) =
\text{softmax}\left(
\frac{QK^\top}{\sqrt{d_k}}
\right)V

á» Ä‘Ã¢y embedding khÃ´ng chá»‰ cáº­p nháº­t tá»« loss cuá»‘i cÃ¹ng mÃ  cÃ²n qua cÆ¡ cháº¿ attention Ä‘a Ä‘áº§u.

â¸»

5. PhÃ¢n tÃ­ch Thá»±c nghiá»‡m: Sá»± thay Ä‘á»•i Embeddings

Dá»±a trÃªn bÃ i Code Challenge:
	1.	Ban Ä‘áº§u embeddings gáº§n nhÆ° ngáº«u nhiÃªn
	2.	Sau vÃ i epoch:
	â€¢	Cluster hÃ¬nh thÃ nh
	â€¢	Cosine similarity giá»¯a tá»« Ä‘á»“ng nghÄ©a tÄƒng
	3.	Sau há»™i tá»¥:
	â€¢	KhÃ´ng gian á»•n Ä‘á»‹nh
	â€¢	Gradient tiá»‡m cáº­n 0

Äiá»u kiá»‡n há»™i tá»¥:

||\nabla \mathcal{L}|| \rightarrow 0

â¸»

6. Regularization vÃ  á»”n Ä‘á»‹nh

ThÃªm L2 regularization:

\mathcal{L}_{reg} = \mathcal{L} + \lambda ||\mathbf{v}||^2

GiÃºp trÃ¡nh:
	â€¢	Vector phÃ¬nh to vÃ´ háº¡n
	â€¢	Overfitting

â¸»

7. Biasâ€“Variance trong Embeddings

Sai sá»‘ ká»³ vá»ng:

\mathbb{E}[(y - \hat{f}(x))^2]
=
Bias^2 + Variance + \sigma^2

Embeddings dimension lá»›n:
	â€¢	Giáº£m bias
	â€¢	TÄƒng variance

Cáº§n cÃ¢n báº±ng sá»‘ chiá»u d.

â¸»

8. Tháº£o luáº­n

Sá»± thay Ä‘á»•i cá»§a embeddings pháº£n Ã¡nh:
	â€¢	Cáº¥u trÃºc phÃ¢n bá»‘ xÃ¡c suáº¥t ngÃ´n ngá»¯
	â€¢	Quan há»‡ Ä‘á»“ng xuáº¥t hiá»‡n
	â€¢	Tá»‘i Æ°u hÃ³a trong khÃ´ng gian phi tuyáº¿n

Trong cÃ¡c mÃ´ hÃ¬nh lá»›n hiá»‡n nay (LLMs), embeddings cÃ²n Ä‘Æ°á»£c:
	â€¢	Fine-tune theo domain
	â€¢	Äiá»u chá»‰nh báº±ng RLHF
	â€¢	Ãp dá»¥ng contrastive learning

â¸»

9. Káº¿t luáº­n

Embeddings khÃ´ng pháº£i lÃ  vector tÄ©nh mÃ  lÃ  thá»±c thá»ƒ Ä‘á»™ng, liÃªn tá»¥c thay Ä‘á»•i trong quÃ¡ trÃ¬nh tá»‘i Æ°u hÃ³a. Vá» máº·t toÃ¡n há»c, chÃºng lÃ  nghiá»‡m cá»§a má»™t bÃ i toÃ¡n tá»‘i Æ°u phi lá»“i trong khÃ´ng gian nhiá»u chiá»u. Sá»± tiáº¿n hÃ³a cá»§a embeddings chÃ­nh lÃ  quÃ¡ trÃ¬nh hÃ¬nh thÃ nh cáº¥u trÃºc ngá»¯ nghÄ©a trong khÃ´ng gian vector.

Hiá»ƒu rÃµ cÆ¡ cháº¿ cáº­p nháº­t giÃºp:
	â€¢	Thiáº¿t káº¿ mÃ´ hÃ¬nh hiá»‡u quáº£ hÆ¡n
	â€¢	Chá»n hyperparameter há»£p lÃ½
	â€¢	TrÃ¡nh hiá»‡n tÆ°á»£ng máº¥t á»•n Ä‘á»‹nh huáº¥n luyá»‡n

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Mikolov, T. et al. (2013). Efficient Estimation of Word Representations in Vector Space.
	2.	Pennington, J., Socher, R., & Manning, C. (2014). GloVe: Global Vectors for Word Representation.
	3.	Vaswani, A. et al. (2017). Attention Is All You Need.
	4.	Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
	5.	Bishop, C. M. (2006). Pattern Recognition and Machine Learning.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero llm 01 word2vec vs glove vs gpt vs bert oh my](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) |
| [aero llm 02 exploring glove pretrained embeddings](aero_llm_02_exploring_glove_pretrained_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_exploring_glove_pretrained_embeddings.md) |
| [aero llm 03 codechallenge wikipedia vs twitter embeddings part 1](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Tá»« Vá»±ng giá»¯a Wikipedia vÃ  Twitter báº±ng PhÃ¢n TÃ­ch TÆ°Æ¡ng Äá»“ng Biá»ƒu Diá»…n (RSA)](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Ngá»¯ NghÄ©a cá»§a GPT-2 vÃ  BERT thÃ´ng qua PhÃ¢n TÃ­ch Embedding](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) |
| [ToÃ¡n há»c cá»§a Token vÃ  Embedding trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) |
| [Cosine Similarity vÃ  Má»‘i Quan Há»‡ vá»›i Há»‡ Sá»‘ TÆ°Æ¡ng Quan: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong NLP](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) |
| [PhÃ¢n TÃ­ch Cosine Similarity trong KhÃ´ng Gian Embedding cá»§a GPT-2](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) |
| [Unembedding trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: Tá»« Vector áº¨n Äáº¿n Token](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) |
| [Position Embeddings trong Transformer: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_llm_10_position_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_position_embeddings.md) |
| [PhÃ¢n TÃ­ch Thá»±c Nghiá»‡m Embedding Vá»‹ TrÃ­ Trong Transformer: Tá»« Cáº¥u TrÃºc Tuyáº¿n TÃ­nh Äáº¿n KhÃ´ng Gian HÃ¬nh Há»c](aero_llm_11_codechallenge_exploring_position_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_exploring_position_embeddings.md) |
| [Huáº¥n Luyá»‡n Embedding Tá»« Äáº§u: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, CÆ¡ Cháº¿ Tá»‘i Æ¯u vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_12_training_embeddings_from_scratch.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_training_embeddings_from_scratch.md) |
| [Thiáº¿t Káº¿ Data Loader Cho Huáº¥n Luyá»‡n MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Kiáº¿n TrÃºc vÃ  Tá»‘i Æ¯u HoÃ¡](aero_llm_13_create_a_data_loader_to_train_a_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_create_a_data_loader_to_train_a_model.md) |
| [XÃ¢y Dá»±ng MÃ´ HÃ¬nh Há»c Embedding Tá»« Äáº§u: Kiáº¿n TrÃºc, Tá»‘i Æ¯u HoÃ¡ vÃ  PhÃ¢n TÃ­ch ToÃ¡n Há»c](aero_llm_14_build_a_model_to_learn_the_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_build_a_model_to_learn_the_embeddings.md) |
| [HÃ m Máº¥t MÃ¡t Trong Huáº¥n Luyá»‡n Embedding: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, PhÃ¢n TÃ­ch Gradient vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_15_loss_function_to_train_the_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_loss_function_to_train_the_embeddings.md) |
| [Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh Há»c MÃ¡y: CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Thá»±c tiá»…n](aero_llm_16_train_and_evaluate_the_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_train_and_evaluate_the_model.md) |
| ğŸ“Œ **[Sá»± Thay Äá»•i cá»§a Embeddings Trong QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_llm_17_codechallenge_how_the_embeddings_change.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_17_codechallenge_how_the_embeddings_change.md) |
| [Äá»™ á»”n Äá»‹nh cá»§a Embeddings trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_llm_18_codechallenge_how_stable_are_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_how_stable_are_embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

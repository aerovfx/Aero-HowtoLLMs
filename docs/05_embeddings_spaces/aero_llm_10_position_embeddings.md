
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [05 embeddings spaces](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Position Embeddings trong Transformer: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n

## TÃ³m táº¯t

Kiáº¿n trÃºc Transformer khÃ´ng cÃ³ cÆ¡ cháº¿ tuáº§n tá»± ná»™i táº¡i nhÆ° RNN, do Ä‘Ã³ cáº§n má»™t phÆ°Æ¡ng phÃ¡p mÃ£ hÃ³a thá»© tá»± cá»§a token trong chuá»—i. Position Embeddings (PE) Ä‘Æ°á»£c Ä‘á» xuáº¥t trong bÃ i bÃ¡o gá»‘c â€œAttention is All You Needâ€ nháº±m bá»• sung thÃ´ng tin vá»‹ trÃ­ vÃ o biá»ƒu diá»…n embedding. BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch cÆ¡ sá»Ÿ toÃ¡n há»c cá»§a positional encoding, cÃ¡c biáº¿n thá»ƒ há»c Ä‘Æ°á»£c (learned positional embeddings), vÃ  vai trÃ² cá»§a chÃºng trong cÃ¡c mÃ´ hÃ¬nh nhÆ° [GPT-2](chatgpt://generic-entity?number=0) vÃ  [BERT](chatgpt://generic-entity?number=1).

---

## 1. Giá»›i thiá»‡u

Trong Transformer, self-attention chá»‰ dá»±a trÃªn:

$$
\text{Attention}(Q,K,V)
=
\text{softmax}\left(
\frac{QK^T}{\sqrt{d_k}}
\right)V
$$

CÆ¡ cháº¿ nÃ y khÃ´ng chá»©a thÃ´ng tin vá» vá»‹ trÃ­ thá»© tá»± cá»§a token.

Do Ä‘Ã³, náº¿u chá»‰ dÃ¹ng embedding tá»« vá»±ng:

$$
\mathbf{v}_i
$$

thÃ¬ hai chuá»—i:

- â€œdog bites manâ€
- â€œman bites dogâ€

sáº½ cÃ³ táº­p embedding giá»‘ng nhau (chá»‰ khÃ¡c thá»© tá»±).

---

## 2. Biá»ƒu diá»…n vá»‹ trÃ­: CÃ´ng thá»©c Sinusoidal

Trong bÃ i bÃ¡o Transformer gá»‘c (Vaswani et al., 2017), positional encoding Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a:

$$
PE_{(pos,2k)} =
\sin\left(
\frac{pos}{10000^{2k/d}}
\right)
$$

$$
PE_{(pos,2k+1)} =
\cos\left(
\frac{pos}{10000^{2k/d}}
\right)
$$

Trong Ä‘Ã³:

- $pos$: vá»‹ trÃ­ trong chuá»—i
- $k$: chá»‰ sá»‘ chiá»u
- $d$: kÃ­ch thÆ°á»›c embedding

---

## 3. Äáº·c tÃ­nh toÃ¡n há»c

### 3.1 Táº§n sá»‘ hÃ¬nh há»c

Ta cÃ³:

$$
\omega_k = \frac{1}{10000^{2k/d}}
$$

Do Ä‘Ã³:

$$
PE(pos,k) =
\sin(\omega_k pos)
\quad \text{hoáº·c} \quad
\cos(\omega_k pos)
$$

Táº§n sá»‘ thay Ä‘á»•i theo cáº¥p sá»‘ nhÃ¢n â†’ cho phÃ©p mÃ´ hÃ¬nh biá»ƒu diá»…n cáº£:

- Quan há»‡ gáº§n (low frequency)
- Quan há»‡ xa (high frequency)

---

### 3.2 Biá»ƒu diá»…n dá»‹ch chuyá»ƒn tuyáº¿n tÃ­nh

Má»™t Ä‘áº·c tÃ­nh quan trá»ng:

$$
PE(pos + \Delta)
=
PE(pos)\cos(\omega\Delta)
+
PE_{\perp}(pos)\sin(\omega\Delta)
$$

Äiá»u nÃ y cho phÃ©p mÃ´ hÃ¬nh há»c quan há»‡ khoáº£ng cÃ¡ch tuyáº¿n tÃ­nh giá»¯a cÃ¡c vá»‹ trÃ­.

---

## 4. Káº¿t há»£p Embedding vÃ  Position

Embedding cuá»‘i cÃ¹ng:

$$
\mathbf{z}_i
=
\mathbf{v}_i
+
\mathbf{p}_i
$$

Trong Ä‘Ã³:

- $\mathbf{v}_i$: token embedding
- $\mathbf{p}_i$: positional embedding

Khi Ä‘Ã³:

$$
Z = V + P
$$

vá»›i:

$$
V, P \in \mathbb{R}^{n \times d}
$$

---

## 5. Learned Positional Embeddings

Trong [GPT-2](chatgpt://generic-entity?number=2) vÃ  [BERT](chatgpt://generic-entity?number=3), positional embeddings thÆ°á»ng Ä‘Æ°á»£c há»c trá»±c tiáº¿p:

$$
P \in \mathbb{R}^{L_{max} \times d}
$$

vá»›i $L_{max}$ lÃ  Ä‘á»™ dÃ i tá»‘i Ä‘a.

Khi Ä‘Ã³:

$$
\mathbf{p}_i = P[i]
$$

Æ¯u Ä‘iá»ƒm:

- Linh hoáº¡t hÆ¡n
- Há»c trá»±c tiáº¿p tá»« dá»¯ liá»‡u

NhÆ°á»£c Ä‘iá»ƒm:

- KhÃ´ng tá»± nhiÃªn má»Ÿ rá»™ng sang chuá»—i dÃ i hÆ¡n Ä‘á»™ dÃ i huáº¥n luyá»‡n

---

## 6. PhÃ¢n tÃ­ch hÃ¬nh há»c

Sau khi cá»™ng:

$$
\mathbf{z}_i
=
\mathbf{v}_i + \mathbf{p}_i
$$

Self-attention tÃ­nh:

$$
Q = ZW_Q
$$

$$
K = ZW_K
$$

TÃ­ch vÃ´ hÆ°á»›ng:

$$
QK^T
=
(V + P)W_Q
((V + P)W_K)^T
$$

Khai triá»ƒn:

$$
=
VW_QW_K^TV^T
+
VW_QW_K^TP^T
+
PW_QW_K^TV^T
+
PW_QW_K^TP^T
$$

Cho tháº¥y attention bao gá»“m:

- Quan há»‡ tokenâ€“token
- Quan há»‡ tokenâ€“position
- Quan há»‡ positionâ€“position

---

## 7. Relative Position Encoding

Má»™t sá»‘ mÃ´ hÃ¬nh hiá»‡n Ä‘áº¡i sá»­ dá»¥ng vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i:

$$
\text{Attention}_{ij}
=
\frac{
Q_i K_j^T + b_{i-j}
}{
\sqrt{d}
}
$$

Trong Ä‘Ã³ $b_{i-j}$ phá»¥ thuá»™c vÃ o khoáº£ng cÃ¡ch giá»¯a vá»‹ trÃ­.

Äiá»u nÃ y giÃºp mÃ´ hÃ¬nh tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n.

---

## 8. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Cosine Similarity

Do:

$$
\mathbf{z}_i
=
\mathbf{v}_i + \mathbf{p}_i
$$

Cosine similarity giá»¯a hai token táº¡i vá»‹ trÃ­ khÃ¡c nhau:

$$
\text{cosine}(\mathbf{z}_i,\mathbf{z}_j)
=
\frac{
(\mathbf{v}_i+\mathbf{p}_i)\cdot
(\mathbf{v}_j+\mathbf{p}_j)
}{
\|\mathbf{z}_i\|\|\mathbf{z}_j\|
}
$$

Má»Ÿ rá»™ng tá»­ sá»‘:

$$
=
\mathbf{v}_i\cdot\mathbf{v}_j
+
\mathbf{v}_i\cdot\mathbf{p}_j
+
\mathbf{p}_i\cdot\mathbf{v}_j
+
\mathbf{p}_i\cdot\mathbf{p}_j
$$

Cho tháº¥y vá»‹ trÃ­ áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n hÃ¬nh há»c embedding.

---

## 9. TÃ­nh báº¥t biáº¿n vÃ  giá»›i háº¡n

### 9.1 KhÃ´ng báº¥t biáº¿n dá»‹ch chuyá»ƒn

Vá»›i learned positional embedding:

$$
\mathbf{p}_{i+1}
\neq
\mathbf{p}_i + c
$$

Do Ä‘Ã³ mÃ´ hÃ¬nh khÃ´ng tá»± Ä‘á»™ng báº¥t biáº¿n vá»›i dá»‹ch chuyá»ƒn.

---

### 9.2 Äá»™ dÃ i chuá»—i

Vá»›i sinusoidal:

$$
PE(pos)
\text{ cÃ³ thá»ƒ tÃ­nh cho má»i } pos
$$

Vá»›i learned:

$$
pos > L_{max}
\Rightarrow
\text{khÃ´ng xÃ¡c Ä‘á»‹nh}
$$

---

## 10. Káº¿t luáº­n

Position embeddings lÃ  thÃ nh pháº§n thiáº¿t yáº¿u giÃºp Transformer:

- Nháº­n biáº¿t thá»© tá»±
- Há»c quan há»‡ khoáº£ng cÃ¡ch
- MÃ´ hÃ¬nh hÃ³a cáº¥u trÃºc cÃº phÃ¡p

Vá» máº·t toÃ¡n há»c:

$$
\text{Transformer}
=
\text{Attention}(V + P)
$$

Sá»± lá»±a chá»n giá»¯a sinusoidal vÃ  learned positional embeddings áº£nh hÆ°á»Ÿng Ä‘áº¿n:

- Kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a
- á»”n Ä‘á»‹nh huáº¥n luyá»‡n
- HÃ¬nh há»c cá»§a khÃ´ng gian biá»ƒu diá»…n

Hiá»ƒu rÃµ cÆ¡ cháº¿ nÃ y giÃºp:

- PhÃ¢n tÃ­ch hÃ nh vi mÃ´ hÃ¬nh
- Thiáº¿t káº¿ kiáº¿n trÃºc má»›i
- Má»Ÿ rá»™ng mÃ´ hÃ¬nh sang chuá»—i dÃ i hÆ¡n

---

## TÃ i liá»‡u tham kháº£o

1. Vaswani et al. (2017). Attention is All You Need.  
2. Radford et al. (2019). Language Models are Unsupervised Multitask Learners.  
3. Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.  
4. Press & Wolf (2017). Using the Output Embedding to Improve Language Models.  
5. Jurafsky & Martin (2023). Speech and Language Processing.

---
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero llm 01 word2vec vs glove vs gpt vs bert oh my](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) |
| [aero llm 02 exploring glove pretrained embeddings](aero_llm_02_exploring_glove_pretrained_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_exploring_glove_pretrained_embeddings.md) |
| [aero llm 03 codechallenge wikipedia vs twitter embeddings part 1](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Tá»« Vá»±ng giá»¯a Wikipedia vÃ  Twitter báº±ng PhÃ¢n TÃ­ch TÆ°Æ¡ng Äá»“ng Biá»ƒu Diá»…n (RSA)](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Ngá»¯ NghÄ©a cá»§a GPT-2 vÃ  BERT thÃ´ng qua PhÃ¢n TÃ­ch Embedding](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) |
| [ToÃ¡n há»c cá»§a Token vÃ  Embedding trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) |
| [Cosine Similarity vÃ  Má»‘i Quan Há»‡ vá»›i Há»‡ Sá»‘ TÆ°Æ¡ng Quan: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong NLP](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) |
| [PhÃ¢n TÃ­ch Cosine Similarity trong KhÃ´ng Gian Embedding cá»§a GPT-2](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) |
| [Unembedding trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: Tá»« Vector áº¨n Äáº¿n Token](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) |
| ğŸ“Œ **[Position Embeddings trong Transformer: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_llm_10_position_embeddings.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_10_position_embeddings.md) |
| [PhÃ¢n TÃ­ch Thá»±c Nghiá»‡m Embedding Vá»‹ TrÃ­ Trong Transformer: Tá»« Cáº¥u TrÃºc Tuyáº¿n TÃ­nh Äáº¿n KhÃ´ng Gian HÃ¬nh Há»c](aero_llm_11_codechallenge_exploring_position_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_exploring_position_embeddings.md) |
| [Huáº¥n Luyá»‡n Embedding Tá»« Äáº§u: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, CÆ¡ Cháº¿ Tá»‘i Æ¯u vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_12_training_embeddings_from_scratch.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_training_embeddings_from_scratch.md) |
| [Thiáº¿t Káº¿ Data Loader Cho Huáº¥n Luyá»‡n MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Kiáº¿n TrÃºc vÃ  Tá»‘i Æ¯u HoÃ¡](aero_llm_13_create_a_data_loader_to_train_a_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_create_a_data_loader_to_train_a_model.md) |
| [XÃ¢y Dá»±ng MÃ´ HÃ¬nh Há»c Embedding Tá»« Äáº§u: Kiáº¿n TrÃºc, Tá»‘i Æ¯u HoÃ¡ vÃ  PhÃ¢n TÃ­ch ToÃ¡n Há»c](aero_llm_14_build_a_model_to_learn_the_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_build_a_model_to_learn_the_embeddings.md) |
| [HÃ m Máº¥t MÃ¡t Trong Huáº¥n Luyá»‡n Embedding: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, PhÃ¢n TÃ­ch Gradient vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_15_loss_function_to_train_the_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_loss_function_to_train_the_embeddings.md) |
| [Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh Há»c MÃ¡y: CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Thá»±c tiá»…n](aero_llm_16_train_and_evaluate_the_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_train_and_evaluate_the_model.md) |
| [Sá»± Thay Äá»•i cá»§a Embeddings Trong QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_llm_17_codechallenge_how_the_embeddings_change.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_codechallenge_how_the_embeddings_change.md) |
| [Äá»™ á»”n Äá»‹nh cá»§a Embeddings trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_llm_18_codechallenge_how_stable_are_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_how_stable_are_embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

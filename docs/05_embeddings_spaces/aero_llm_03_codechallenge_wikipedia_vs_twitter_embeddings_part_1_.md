
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [05 embeddings spaces](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
So sÃ¡nh Embedding Huáº¥n luyá»‡n trÃªn Wikipedia vÃ  Twitter

PhÃ¢n tÃ­ch PhÃ¢n bá»‘, HÃ¬nh há»c vÃ  Kháº£ nÄƒng KhÃ¡i quÃ¡t hÃ³a Ngá»¯ nghÄ©a

â¸»

TÃ³m táº¯t

Dá»±a trÃªn tÃ i liá»‡u Ä‘Ã­nh kÃ¨m â€œCodeChallenge: Wikipedia vs. Twitter embeddings (part 1)â€, bÃ i viáº¿t nÃ y phÃ¢n tÃ­ch sá»± khÃ¡c biá»‡t giá»¯a cÃ¡c embedding tá»« Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn hai miá»n dá»¯ liá»‡u khÃ¡c nhau: vÄƒn báº£n bÃ¡ch khoa toÃ n thÆ° (Wikipedia) vÃ  vÄƒn báº£n máº¡ng xÃ£ há»™i (Twitter).

ChÃºng tÃ´i má»Ÿ rá»™ng phÃ¢n tÃ­ch báº±ng cÃ¡c cÆ¡ sá»Ÿ lÃ½ thuyáº¿t tá»« Word2Vec, GloVe vÃ  cÃ¡c káº¿t quáº£ vá» phÃ¢n bá»‘ Zipf cá»§a George Kingsley Zipf.

BÃ i viáº¿t cung cáº¥p cÃ¡c mÃ´ hÃ¬nh toÃ¡n há»c minh há»a sá»± khÃ¡c biá»‡t vá»:
	â€¢	PhÃ¢n bá»‘ táº§n suáº¥t tá»«
	â€¢	Ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n
	â€¢	Entropy vÃ  mutual information
	â€¢	Cáº¥u trÃºc hÃ¬nh há»c cá»§a khÃ´ng gian embedding
	â€¢	Kháº£ nÄƒng khÃ¡i quÃ¡t hÃ³a liÃªn miá»n

â¸»

1. Giá»›i thiá»‡u

Embedding tá»« há»c Ä‘Æ°á»£c tá»« corpus phá»¥ thuá»™c máº¡nh vÃ o:

$$
$\mathcal${D} = \{w_1, w_2, \dots, w_T\}
$$

Hai miá»n:
	â€¢	$\mathcal${D}_{wiki}: Wikipedia
	â€¢	$\mathcal${D}_{twitter}: Twitter

Ta xÃ¢y dá»±ng Ã¡nh xáº¡:

E_$\mathcal${D}: V \rightarrow $\mathbb${R}^d

Má»¥c tiÃªu: so sÃ¡nh E_{wiki} vÃ  E_{twitter}.

â¸»

2. PhÃ¢n bá»‘ Táº§n suáº¥t vÃ  Luáº­t Zipf

Theo luáº­t Zipf:

f$r$ $\propto$ \frac{1}{r^\alpha}

Trong Ä‘Ã³:
	â€¢	r: thá»© háº¡ng
	â€¢	f$r$: táº§n suáº¥t

Ta Æ°á»›c lÆ°á»£ng:

\alpha_{wiki} \neq \alpha_{twitter}

Twitter cÃ³:
	â€¢	Nhiá»u tá»« lÃ³ng
	â€¢	Hashtag
	â€¢	Viáº¿t táº¯t

Entropy tá»« vá»±ng:

$$
H = -$\sum$_i $P(w_i)$\log $P(w_i)$
$$

ThÆ°á»ng:

H_{twitter} > H_{wiki}

Do phÃ¢n bá»‘ pháº³ng hÆ¡n.

â¸»

3. Ma tráº­n Äá»“ng xuáº¥t hiá»‡n

Vá»›i GloVe:

$$
X_{ij} = \text{sá»‘ láº§n } w_j \text{ xuáº¥t hiá»‡n trong ngá»¯ cáº£nh cá»§a } w_i
$$

Ta cÃ³:

X^{wiki} \neq X^{twitter}

Sá»± khÃ¡c biá»‡t thá»ƒ hiá»‡n á»Ÿ:
	â€¢	Tá»« há»c thuáº­t (wiki)
	â€¢	Biá»ƒu tÆ°á»£ng cáº£m xÃºc, hashtag (twitter)

Log-count:

$$
w_i^\top w_j $\approx$ $\log$ X_{ij}
$$

â¸»

4. KhÃ´ng gian HÃ¬nh há»c Embedding

Embedding:

E$w$ \in $\mathbb${R}^d

Khoáº£ng cÃ¡ch cosine:

\text{sim}(i,j) =
\frac{E$w_i$^\top E$w_j$}
{\|E$w_i$\\mid  \|E$w_j$\|}

â¸»

4.1 Äá»™ lá»‡ch miá»n (Domain Shift)

Giáº£ sá»­:

\Delta$w$ = \\mid  E_{wiki}$w$ - E_{twitter}$w$ \|_2

Náº¿u:

\Delta$w$ \gg 0

â†’ tá»« cÃ³ ngá»¯ nghÄ©a khÃ¡c nhau theo miá»n.

VÃ­ dá»¥:
	â€¢	â€œappleâ€ (cÃ´ng ty vs trÃ¡i cÃ¢y)
	â€¢	â€œviralâ€ (sinh há»c vs máº¡ng xÃ£ há»™i)

â¸»

5. Mutual Information giá»¯a tá»« vÃ  miá»n

XÃ©t biáº¿n ngáº«u nhiÃªn:
	â€¢	W: tá»«
	â€¢	D \in \{wiki, twitter\}

Mutual information:

$$
I(W;D) = $\sum$_{w,d} P(w,d)$\log$\frac{P(w,d)}{$P(w)$$P(d)$}
$$

Náº¿u:

I(W;D) \text{ cao}

â†’ tá»« Ä‘áº·c trÆ°ng miá»n.

â¸»

6. TÃ­nh Tuyáº¿n tÃ­nh vÃ  Quan há»‡ Ngá»¯ nghÄ©a

Wikipedia thÆ°á»ng giá»¯ cáº¥u trÃºc tuyáº¿n tÃ­nh rÃµ:

$$
w_{Paris} - w_{France} + w_{Germany} $\approx$ w_{Berlin}
$$

Twitter cÃ³ thá»ƒ nhiá»…u hÆ¡n do:
	â€¢	Tá»« viáº¿t táº¯t
	â€¢	Thiáº¿u chuáº©n hÃ³a

Sai sá»‘:

\epsilon =
\| (w_a - w_b + w_c) - w_d \|_2

ThÆ°á»ng:

\epsilon_{twitter} > \epsilon_{wiki}

â¸»

7. Äá»™ tá»•ng quÃ¡t hÃ³a (Generalization)

Giáº£ sá»­ huáº¥n luyá»‡n classifier:

f(E(w)) = y

Huáº¥n luyá»‡n trÃªn wiki, test trÃªn twitter:

Sai sá»‘:

$\mathcal${L}_{cross-domain}

TÄƒng theo khoáº£ng cÃ¡ch phÃ¢n bá»‘:

D_{KL}$P_{wiki} \\mid  P_{twitter}$

Vá»›i:

$$
D_{KL}$P\\mid Q$ = $\sum$_i $P(i)$\log\frac{$P(i)$}{Q$i$}
$$

â¸»

8. PhÃ¢n tÃ­ch SVD vÃ  Cáº¥u trÃºc Phá»•

Ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n:

$$
X = U\Sigma V^\top
$$

So sÃ¡nh phá»• trá»‹ riÃªng:

\lambda_r^{wiki} \neq \lambda_r^{twitter}

Wikipedia thÆ°á»ng cÃ³:
	â€¢	Phá»• giáº£m cháº­m
	â€¢	Cáº¥u trÃºc ngá»¯ nghÄ©a á»•n Ä‘á»‹nh

Twitter:
	â€¢	Nhiá»…u cao
	â€¢	Phá»• pháº³ng hÆ¡n

â¸»

9. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Transformer

Embedding Ä‘áº§u vÃ o cho mÃ´ hÃ¬nh nhÆ° BERT hoáº·c GPT chá»‹u áº£nh hÆ°á»Ÿng miá»n dá»¯ liá»‡u.

Self-attention:

\text{Attention}(Q,K,V)=

$$
\text{softmax}$\le$ft$\frac{QK^\top}{\sqrt{d}}\right$V
$$

Náº¿u embedding nhiá»…u:

\|QK^\top\| \text{ giáº£m á»•n Ä‘á»‹nh}

â†’ attention phÃ¢n tÃ¡n hÆ¡n.

â¸»

10. PhÃ¢n tÃ­ch Äá»‹nh lÆ°á»£ng

Ta Ä‘á»‹nh nghÄ©a:

10.1 Äá»™ tÆ°Æ¡ng Ä‘á»“ng trung bÃ¬nh

\bar{s} =
\frac{1}{|P|}
$\sum$_{(i,j)\in P}
\text{sim}(i,j)

10.2 Äá»™ lá»‡ch miá»n trung bÃ¬nh

\bar{\Delta} =
\frac{1}{|V|}
$\sum$_{w\in V}
\Delta$w$

â¸»

11. Tháº£o luáº­n

Wikipedia:
	â€¢	VÄƒn phong chuáº©n
	â€¢	Cáº¥u trÃºc ngá»¯ nghÄ©a rÃµ
	â€¢	Ãt nhiá»…u

Twitter:
	â€¢	Ngáº¯n
	â€¢	KhÃ´ng chuáº©n hÃ³a
	â€¢	Biáº¿n thá»ƒ hÃ¬nh thÃ¡i nhiá»u

Äiá»u nÃ y áº£nh hÆ°á»Ÿng Ä‘áº¿n:
	â€¢	Entropy
	â€¢	Mutual information
	â€¢	Cáº¥u trÃºc hÃ¬nh há»c embedding

â¸»

12. Káº¿t luáº­n

Embedding phá»¥ thuá»™c máº¡nh vÃ o miá»n dá»¯ liá»‡u:

E_{wiki} \neq E_{twitter}

Sá»± khÃ¡c biá»‡t thá»ƒ hiá»‡n qua:
	â€¢	PhÃ¢n bá»‘ Zipf
	â€¢	Ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n
	â€¢	Khoáº£ng cÃ¡ch hÃ¬nh há»c
	â€¢	Mutual information
	â€¢	Kháº£ nÄƒng khÃ¡i quÃ¡t hÃ³a liÃªn miá»n

Viá»‡c chá»n corpus huáº¥n luyá»‡n phÃ¹ há»£p lÃ  yáº¿u tá»‘ quyáº¿t Ä‘á»‹nh cháº¥t lÆ°á»£ng embedding.

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Mikolov et al. (2013). Efficient Estimation of Word Representations.
	2.	Pennington et al. (2014). GloVe: Global Vectors for Word Representation.
	3.	Levy & Goldberg (2014). Neural Word Embedding as Implicit Matrix Factorization.
	4.	Shannon (1948). A Mathematical Theory of Communication.
	5.	Zipf (1935). The Psycho-Biology of Language.
	6.	Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
	7.	Radford et al. (2018â€“2023). GPT series papers.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero llm 01 word2vec vs glove vs gpt vs bert oh my](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) |
| [aero llm 02 exploring glove pretrained embeddings](aero_llm_02_exploring_glove_pretrained_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_exploring_glove_pretrained_embeddings.md) |
| ğŸ“Œ **[aero llm 03 codechallenge wikipedia vs twitter embeddings part 1](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Tá»« Vá»±ng giá»¯a Wikipedia vÃ  Twitter báº±ng PhÃ¢n TÃ­ch TÆ°Æ¡ng Äá»“ng Biá»ƒu Diá»…n (RSA)](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Ngá»¯ NghÄ©a cá»§a GPT-2 vÃ  BERT thÃ´ng qua PhÃ¢n TÃ­ch Embedding](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) |
| [ToÃ¡n há»c cá»§a Token vÃ  Embedding trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) |
| [Cosine Similarity vÃ  Má»‘i Quan Há»‡ vá»›i Há»‡ Sá»‘ TÆ°Æ¡ng Quan: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong NLP](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) |
| [PhÃ¢n TÃ­ch Cosine Similarity trong KhÃ´ng Gian Embedding cá»§a GPT-2](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) |
| [Unembedding trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: Tá»« Vector áº¨n Äáº¿n Token](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) |
| [Position Embeddings trong Transformer: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_llm_10_position_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_position_embeddings.md) |
| [PhÃ¢n TÃ­ch Thá»±c Nghiá»‡m Embedding Vá»‹ TrÃ­ Trong Transformer: Tá»« Cáº¥u TrÃºc Tuyáº¿n TÃ­nh Äáº¿n KhÃ´ng Gian HÃ¬nh Há»c](aero_llm_11_codechallenge_exploring_position_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_exploring_position_embeddings.md) |
| [Huáº¥n Luyá»‡n Embedding Tá»« Äáº§u: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, CÆ¡ Cháº¿ Tá»‘i Æ¯u vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_12_training_embeddings_from_scratch.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_training_embeddings_from_scratch.md) |
| [Thiáº¿t Káº¿ Data Loader Cho Huáº¥n Luyá»‡n MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Kiáº¿n TrÃºc vÃ  Tá»‘i Æ¯u HoÃ¡](aero_llm_13_create_a_data_loader_to_train_a_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_create_a_data_loader_to_train_a_model.md) |
| [XÃ¢y Dá»±ng MÃ´ HÃ¬nh Há»c Embedding Tá»« Äáº§u: Kiáº¿n TrÃºc, Tá»‘i Æ¯u HoÃ¡ vÃ  PhÃ¢n TÃ­ch ToÃ¡n Há»c](aero_llm_14_build_a_model_to_learn_the_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_build_a_model_to_learn_the_embeddings.md) |
| [HÃ m Máº¥t MÃ¡t Trong Huáº¥n Luyá»‡n Embedding: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, PhÃ¢n TÃ­ch Gradient vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_15_loss_function_to_train_the_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_loss_function_to_train_the_embeddings.md) |
| [Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh Há»c MÃ¡y: CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Thá»±c tiá»…n](aero_llm_16_train_and_evaluate_the_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_train_and_evaluate_the_model.md) |
| [Sá»± Thay Äá»•i cá»§a Embeddings Trong QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_llm_17_codechallenge_how_the_embeddings_change.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_codechallenge_how_the_embeddings_change.md) |
| [Äá»™ á»”n Äá»‹nh cá»§a Embeddings trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_llm_18_codechallenge_how_stable_are_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_how_stable_are_embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

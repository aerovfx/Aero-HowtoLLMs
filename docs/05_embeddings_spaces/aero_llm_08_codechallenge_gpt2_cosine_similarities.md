
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [05 embeddings spaces](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# PhÃ¢n TÃ­ch Cosine Similarity trong KhÃ´ng Gian Embedding cá»§a GPT-2

## TÃ³m táº¯t

Cosine similarity lÃ  má»™t cÃ´ng cá»¥ trung tÃ¢m trong viá»‡c phÃ¢n tÃ­ch cáº¥u trÃºc hÃ¬nh há»c cá»§a khÃ´ng gian embedding trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n. BÃ i viáº¿t nÃ y trÃ¬nh bÃ y cÆ¡ sá»Ÿ toÃ¡n há»c vÃ  quy trÃ¬nh thá»±c nghiá»‡m Ä‘á»ƒ tÃ­nh toÃ¡n cosine similarity giá»¯a cÃ¡c token embedding cá»§a [GPT-2](chatgpt://generic-entity?number=0), Ä‘á»“ng thá»i phÃ¢n tÃ­ch Ã½ nghÄ©a hÃ¬nh há»c vÃ  thá»‘ng kÃª cá»§a cÃ¡c giÃ¡ trá»‹ tÆ°Æ¡ng Ä‘á»“ng thu Ä‘Æ°á»£c.

---

## 1. Giá»›i thiá»‡u

Trong cÃ¡c mÃ´ hÃ¬nh Transformer sinh vÄƒn báº£n, má»—i token Ä‘Æ°á»£c Ã¡nh xáº¡ sang má»™t vector trong khÃ´ng gian $\mathbb{R}^d$. Vá»›i GPT-2:

- KÃ­ch thÆ°á»›c embedding: $d = 768$ (báº£n base)
- Tá»« vá»±ng: khoáº£ng 50.000 token

Má»—i token $t$ cÃ³ vector embedding:

$$
\mathbf{v}_t \in $\mathbb${R}^{768}
$$

PhÃ¢n tÃ­ch cosine similarity giá»¯a cÃ¡c vector nÃ y giÃºp hiá»ƒu cáº¥u trÃºc ngá»¯ nghÄ©a ná»™i táº¡i cá»§a mÃ´ hÃ¬nh.

---

## 2. CÆ¡ sá»Ÿ toÃ¡n há»c cá»§a Cosine Similarity

Cho hai vector:

$$
\mathbf{x}, \mathbf{y} \in $\mathbb${R}^d
$$

Äá»‹nh nghÄ©a:

$$
\text{cosine}(\mathbf{x},\mathbf{y}) = \frac{\mathbf{x} \cdot \mathbf{y}} {\|\mathbf{x}\| \|\mathbf{y}\|}
$$

Trong Ä‘Ã³:

$$

$$

\mathbf{x} \cdot \mathbf{y} = $\sum$_{i=1}^{d} x_i y_i

$$

$$

$$

$$

\|\mathbf{x}\| = \sqrt{$\sum$_{i=1}^{d} x_i^2}

$$

$$

GiÃ¡ trá»‹ náº±m trong khoáº£ng:

$$
-1 $\le$q \text{cosine} $\le$q 1
$$

---

## 3. Chuáº©n hÃ³a vÃ  tÃ­nh toÃ¡n hiá»‡u quáº£

Trong thá»±c táº¿, ta chuáº©n hÃ³a trÆ°á»›c:

$$
\hat{\mathbf{x}} = \frac{\mathbf{x}}{\|\mathbf{x}\|}
$$

Khi Ä‘Ã³:

$$
\text{cosine}(\mathbf{x},\mathbf{y}) = \hat{\mathbf{x}} \cdot \hat{\mathbf{y}}
$$

Náº¿u ma tráº­n embedding:

$$
E \in $\mathbb${R}^{|V| \times d}
$$

Sau khi chuáº©n hÃ³a tá»«ng hÃ ng:

$$
\hat{E}
$$

Ma tráº­n cosine similarity toÃ n bá»™ tá»« vá»±ng:

$$
S = \hat{E} \hat{E}^T
$$

---

## 4. PhÃ¢n tÃ­ch thá»±c nghiá»‡m vá»›i GPT-2

### 4.1 TrÃ­ch xuáº¥t embedding

Vá»›i token index $i$:

$$

$$

\mathbf{v}_i = E[i]

$$

$$

Trong GPT-2, embedding Ä‘áº§u vÃ o vÃ  embedding Ä‘áº§u ra thÆ°á»ng Ä‘Æ°á»£c chia sáº» trá»ng sá»‘ (weight tying):

$$
W_{out} = E^T
$$

Äiá»u nÃ y táº¡o liÃªn há»‡ hÃ¬nh há»c trá»±c tiáº¿p giá»¯a khÃ´ng gian embedding vÃ  khÃ´ng gian dá»± Ä‘oÃ¡n xÃ¡c suáº¥t.

---

### 4.2 VÃ­ dá»¥: So sÃ¡nh token

Giáº£ sá»­ ta chá»n token:

- â€œcatâ€
- â€œdogâ€
- â€œbananaâ€

Ta tÃ­nh:

$$
\text{sim}(\text{cat},\text{dog})
$$

$$
\text{sim}(\text{cat},\text{banana})
$$

Ká»³ vá»ng:

$$
\text{sim}(\text{cat},\text{dog}) > \text{sim}(\text{cat},\text{banana})
$$

Do cáº¥u trÃºc ngá»¯ nghÄ©a gáº§n nhau.

---

## 5. PhÃ¢n bá»‘ Cosine Similarity trong khÃ´ng gian cao chiá»u

Giáº£ sá»­ hai vector ngáº«u nhiÃªn:

$$
\mathbf{x},\mathbf{y} \sim $\mathcal${N}(0,I_d)
$$

Khi $d \to $\infty$$:

$$

$$

$\mathbb${E}[\text{cosine}] = 0

$$

$$

$$

$$

\text{Var}(\text{cosine}) $\approx$ \frac{1}{d}

$$

$$

Vá»›i $d = 768$:

$$

$$

\text{Var} $\approx$ \frac{1}{768}

$$

$$

Do Ä‘Ã³:

- Vector ngáº«u nhiÃªn gáº§n trá»±c giao
- Cosine lá»›n biá»ƒu thá»‹ cáº¥u trÃºc há»c Ä‘Æ°á»£c

---

## 6. LiÃªn há»‡ vá»›i Softmax vÃ  xÃ¡c suáº¥t dá»± Ä‘oÃ¡n

Trong GPT-2, xÃ¡c suáº¥t token tiáº¿p theo:

$$

$$

P(w_t  \mid  h_t) = \text{softmax}(W_{out} h_t)

$$

$$

Náº¿u weight tying:

$$
W_{out} = E^T
$$

Khi Ä‘Ã³:

$$

$$

z_i = \mathbf{v}_i \cdot h_t

$$

$$

Softmax:

$$

$$

P(w_i) = \frac{e^{\mathbf{v}_i \cdot h_t}} {$\sum$_j e^{\mathbf{v}_j \cdot h_t}}

$$

$$

NhÆ° váº­y:

> Dá»± Ä‘oÃ¡n xÃ¡c suáº¥t thá»±c cháº¥t dá»±a trÃªn tÃ­ch vÃ´ hÆ°á»›ng giá»¯a embedding vÃ  hidden state.

Náº¿u chuáº©n hÃ³a:

$$

$$

\mathbf{v}_i \cdot h_t = \|\mathbf{v}_i\| \|h_t\| \cos\theta

$$

$$

Do Ä‘Ã³ cosine similarity trá»±c tiáº¿p áº£nh hÆ°á»Ÿng Ä‘áº¿n xÃ¡c suáº¥t dá»± Ä‘oÃ¡n.

---

## 7. Ma tráº­n tÆ°Æ¡ng Ä‘á»“ng cá»¥c bá»™

Cho táº­p $n$ token:

$$
X \in $\mathbb${R}^{n \times d}
$$

Ma tráº­n cosine:

$$
S_{ij} = \frac{\mathbf{v}_i \cdot \mathbf{v}_j} {\|\mathbf{v}_i\|\|\mathbf{v}_j\|}
$$

Ta cÃ³ thá»ƒ phÃ¢n tÃ­ch:

- Cá»¥m tá»« (clustering)
- PhÃ¢n tÃ­ch trá»‹ riÃªng:

$$
S \mathbf{u} = \lambda \mathbf{u}
$$

GiÃ¡ trá»‹ riÃªng lá»›n pháº£n Ã¡nh cáº¥u trÃºc ngá»¯ nghÄ©a chiáº¿m Æ°u tháº¿.

---

## 8. Khoáº£ng cÃ¡ch tÆ°Æ¡ng Ä‘Æ°Æ¡ng

Náº¿u vector Ä‘Ã£ chuáº©n hÃ³a:

$$
\|\mathbf{x}-\mathbf{y}\|^2 = 2 - 2\cos\theta
$$

Suy ra:

$$
\cos\theta = 1 - \frac{1}{2} \|\mathbf{x}-\mathbf{y}\|^2
$$

Äiá»u nÃ y cho tháº¥y cosine similarity vÃ  Euclid distance tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá» máº·t hÃ¬nh há»c khi chuáº©n hÃ³a.

---

## 9. Ã nghÄ©a lÃ½ thuyáº¿t

Cosine similarity trong GPT-2:

1. Äá»‹nh nghÄ©a cáº¥u trÃºc hÃ¬nh há»c cá»§a tá»« vá»±ng.
2. LiÃªn há»‡ trá»±c tiáº¿p vá»›i xÃ¡c suáº¥t dá»± Ä‘oÃ¡n.
3. Pháº£n Ã¡nh cáº¥u trÃºc phÃ¢n bá»‘ dá»¯ liá»‡u huáº¥n luyá»‡n.
4. Giáº£m áº£nh hÆ°á»Ÿng cá»§a Ä‘á»™ lá»›n vector.

Vá» báº£n cháº¥t:

$$
\text{Prediction} $\propto$ \exp(\|\mathbf{v}\|\|h\|\cos\theta)
$$

Do Ä‘Ã³ gÃ³c giá»¯a vector Ä‘Ã³ng vai trÃ² quyáº¿t Ä‘á»‹nh.

---

## 10. Káº¿t luáº­n

PhÃ¢n tÃ­ch cosine similarity trong GPT-2 cho tháº¥y:

- KhÃ´ng gian embedding cÃ³ cáº¥u trÃºc hÃ¬nh há»c rÃµ rÃ ng.
- CÃ¡c token liÃªn quan cÃ³ gÃ³c nhá» (cosine lá»›n).
- Dá»± Ä‘oÃ¡n xÃ¡c suáº¥t phá»¥ thuá»™c trá»±c tiáº¿p vÃ o tÃ­ch vÃ´ hÆ°á»›ng.
- Trong khÃ´ng gian cao chiá»u, cáº¥u trÃºc há»c Ä‘Æ°á»£c ná»•i báº­t hÆ¡n ná»n ngáº«u nhiÃªn.

Hiá»ƒu rÃµ ná»n táº£ng toÃ¡n há»c nÃ y giÃºp ta:

- PhÃ¢n tÃ­ch embedding hiá»‡u quáº£
- So sÃ¡nh mÃ´ hÃ¬nh
- Thá»±c hiá»‡n Representational Similarity Analysis (RSA)
- Tá»‘i Æ°u hÃ³a há»‡ thá»‘ng retrieval hoáº·c semantic search

---

## TÃ i liá»‡u tham kháº£o

1. Vaswani et al. (2017). Attention is All You Need.  
2. Radford et al. (2019). Language Models are Unsupervised Multitask Learners.  
3. Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.  
4. Jurafsky & Martin (2023). Speech and Language Processing.  
5. Kriegeskorte et al. (2008). Representational Similarity Analysis.

---
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero llm 01 word2vec vs glove vs gpt vs bert oh my](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) |
| [aero llm 02 exploring glove pretrained embeddings](aero_llm_02_exploring_glove_pretrained_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_exploring_glove_pretrained_embeddings.md) |
| [aero llm 03 codechallenge wikipedia vs twitter embeddings part 1](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Tá»« Vá»±ng giá»¯a Wikipedia vÃ  Twitter báº±ng PhÃ¢n TÃ­ch TÆ°Æ¡ng Äá»“ng Biá»ƒu Diá»…n (RSA)](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Ngá»¯ NghÄ©a cá»§a GPT-2 vÃ  BERT thÃ´ng qua PhÃ¢n TÃ­ch Embedding](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) |
| [ToÃ¡n há»c cá»§a Token vÃ  Embedding trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) |
| [Cosine Similarity vÃ  Má»‘i Quan Há»‡ vá»›i Há»‡ Sá»‘ TÆ°Æ¡ng Quan: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong NLP](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) |
| ğŸ“Œ **[PhÃ¢n TÃ­ch Cosine Similarity trong KhÃ´ng Gian Embedding cá»§a GPT-2](aero_llm_08_codechallenge_gpt2_cosine_similarities.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) |
| [Unembedding trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: Tá»« Vector áº¨n Äáº¿n Token](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) |
| [Position Embeddings trong Transformer: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_llm_10_position_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_position_embeddings.md) |
| [PhÃ¢n TÃ­ch Thá»±c Nghiá»‡m Embedding Vá»‹ TrÃ­ Trong Transformer: Tá»« Cáº¥u TrÃºc Tuyáº¿n TÃ­nh Äáº¿n KhÃ´ng Gian HÃ¬nh Há»c](aero_llm_11_codechallenge_exploring_position_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_exploring_position_embeddings.md) |
| [Huáº¥n Luyá»‡n Embedding Tá»« Äáº§u: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, CÆ¡ Cháº¿ Tá»‘i Æ¯u vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_12_training_embeddings_from_scratch.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_training_embeddings_from_scratch.md) |
| [Thiáº¿t Káº¿ Data Loader Cho Huáº¥n Luyá»‡n MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Kiáº¿n TrÃºc vÃ  Tá»‘i Æ¯u HoÃ¡](aero_llm_13_create_a_data_loader_to_train_a_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_create_a_data_loader_to_train_a_model.md) |
| [XÃ¢y Dá»±ng MÃ´ HÃ¬nh Há»c Embedding Tá»« Äáº§u: Kiáº¿n TrÃºc, Tá»‘i Æ¯u HoÃ¡ vÃ  PhÃ¢n TÃ­ch ToÃ¡n Há»c](aero_llm_14_build_a_model_to_learn_the_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_build_a_model_to_learn_the_embeddings.md) |
| [HÃ m Máº¥t MÃ¡t Trong Huáº¥n Luyá»‡n Embedding: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, PhÃ¢n TÃ­ch Gradient vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_15_loss_function_to_train_the_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_loss_function_to_train_the_embeddings.md) |
| [Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh Há»c MÃ¡y: CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Thá»±c tiá»…n](aero_llm_16_train_and_evaluate_the_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_train_and_evaluate_the_model.md) |
| [Sá»± Thay Äá»•i cá»§a Embeddings Trong QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_llm_17_codechallenge_how_the_embeddings_change.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_codechallenge_how_the_embeddings_change.md) |
| [Äá»™ á»”n Äá»‹nh cá»§a Embeddings trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_llm_18_codechallenge_how_stable_are_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_how_stable_are_embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

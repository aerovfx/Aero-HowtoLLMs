
<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [05 embeddings spaces](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Ph√¢n T√≠ch Th·ª±c Nghi·ªám Embedding V·ªã Tr√≠ Trong Transformer: T·ª´ C·∫•u Tr√∫c Tuy·∫øn T√≠nh ƒê·∫øn Kh√¥ng Gian H√¨nh H·ªçc

T√≥m t·∫Øt

Embedding v·ªã tr√≠ (positional embeddings) l√† th√†nh ph·∫ßn c·ªët l√µi gi√∫p m√¥ h√¨nh Transformer x·ª≠ l√Ω th√¥ng tin th·ª© t·ª± trong chu·ªói. B√†i vi·∫øt n√†y ph√¢n t√≠ch s√¢u embedding v·ªã tr√≠ h·ªçc ƒë∆∞·ª£c (learned positional embeddings), t·∫≠p trung v√†o c·∫•u tr√∫c h√¨nh h·ªçc, t√≠nh tuy·∫øn t√≠nh, ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine v√† ph√¢n t√≠ch th√†nh ph·∫ßn ch√≠nh (PCA). Th·ª±c nghi·ªám ƒë∆∞·ª£c ƒë·∫∑t trong b·ªëi c·∫£nh m√¥ h√¨nh GPT-2 do OpenAI ph√°t tri·ªÉn, d·ª±a tr√™n ki·∫øn tr√∫c Transformer t·ª´ c√¥ng tr√¨nh Attention Is All You Need c·ªßa Ashish Vaswani v√† c·ªông s·ª±.

‚∏ª

1. Gi·ªõi thi·ªáu

Trong Transformer, embedding c·ªßa m·ªôt token t·∫°i v·ªã tr√≠ t ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·ªüi:

\mathbf{z}_t = \mathbf{e}_t + \mathbf{p}_t

Trong ƒë√≥:
	‚Ä¢	\mathbf{e}_t \in \mathbb{R}^d: embedding ng·ªØ nghƒ©a c·ªßa token
	‚Ä¢	\mathbf{p}_t \in \mathbb{R}^d: embedding v·ªã tr√≠
	‚Ä¢	d: s·ªë chi·ªÅu embedding

V·∫•n ƒë·ªÅ c·ªët l√µi: self-attention l√† b·∫•t bi·∫øn theo ho√°n v·ªã (permutation invariant). N·∫øu kh√¥ng c√≥ embedding v·ªã tr√≠, m√¥ h√¨nh kh√¥ng ph√¢n bi·ªát ƒë∆∞·ª£c:
	‚Ä¢	‚ÄúA B C‚Äù
	‚Ä¢	‚ÄúC B A‚Äù

Do ƒë√≥, embedding v·ªã tr√≠ cung c·∫•p c·∫•u tr√∫c th·ª© t·ª± cho m√¥ h√¨nh.

‚∏ª

2. Embedding v·ªã tr√≠ trong Transformer

2.1 Embedding v·ªã tr√≠ h√¨nh sin‚Äìcosin (g·ªëc)

Trong Transformer ban ƒë·∫ßu:

\text{PE}(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d}}\right)

\text{PE}(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d}}\right)

T√≠nh ch·∫•t quan tr·ªçng:
	‚Ä¢	T·∫°o ra ph·ªï t·∫ßn s·ªë ƒëa d·∫°ng
	‚Ä¢	Cho ph√©p bi·ªÉu di·ªÖn quan h·ªá tuy·∫øn t√≠nh gi·ªØa c√°c v·ªã tr√≠
	‚Ä¢	Kh√¥ng c·∫ßn tham s·ªë h·ªçc th√™m

‚∏ª

2.2 Embedding v·ªã tr√≠ h·ªçc ƒë∆∞·ª£c (GPT-2)

Trong GPT-2, embedding v·ªã tr√≠ ƒë∆∞·ª£c h·ªçc nh∆∞ m·ªôt ma tr·∫≠n tham s·ªë:

\mathbf{P} \in \mathbb{R}^{L \times d}

V·ªõi:
	‚Ä¢	L: chi·ªÅu d√†i t·ªëi ƒëa chu·ªói
	‚Ä¢	d: s·ªë chi·ªÅu embedding

Vector v·ªã tr√≠ t·∫°i t:

\mathbf{p}_t = \mathbf{P}[t]

C√°c vector n√†y ƒë∆∞·ª£c t·ªëi ∆∞u th√¥ng qua gradient descent:

\mathbf{P} \leftarrow \mathbf{P} - \eta \frac{\partial \mathcal{L}}{\partial \mathbf{P}}

Trong ƒë√≥:
	‚Ä¢	\eta: learning rate
	‚Ä¢	\mathcal{L}: h√†m m·∫•t m√°t

‚∏ª

3. Ph√¢n t√≠ch h√¨nh h·ªçc c·ªßa embedding v·ªã tr√≠

3.1 Chu·∫©n vector (Vector Norm)

Chu·∫©n L2 c·ªßa embedding v·ªã tr√≠:

\|\mathbf{p}_t\|_2 = \sqrt{\sum_{i=1}^{d} p_{t,i}^2}

Quan s√°t th·ª±c nghi·ªám:
	‚Ä¢	Chu·∫©n t∆∞∆°ng ƒë·ªëi ·ªïn ƒë·ªãnh theo v·ªã tr√≠
	‚Ä¢	Kh√¥ng c√≥ s·ª± b√πng n·ªï norm ·ªü cu·ªëi chu·ªói

ƒêi·ªÅu n√†y gi√∫p ƒë·∫£m b·∫£o embedding v·ªã tr√≠ kh√¥ng l·∫•n √°t embedding token.

‚∏ª

3.2 ƒê·ªô t∆∞∆°ng ƒë·ªìng cosine

ƒê·ªô t∆∞∆°ng ƒë·ªìng cosine gi·ªØa hai v·ªã tr√≠:

\cos(\theta) =
\frac{\mathbf{p}_t \cdot \mathbf{p}_s}
{\|\mathbf{p}_t\| \|\mathbf{p}_s\|}

T√≠nh ch·∫•t th·ª±c nghi·ªám:
	‚Ä¢	\cos(\mathbf{p}_t, \mathbf{p}_{t+1}) cao
	‚Ä¢	Gi·∫£m d·∫ßn khi kho·∫£ng c√°ch |t-s| tƒÉng
	‚Ä¢	T·∫°o c·∫•u tr√∫c li√™n t·ª•c (smooth manifold)

C√≥ th·ªÉ m√¥ h√¨nh ho√° x·∫•p x·ªâ:

\cos(\mathbf{p}_t, \mathbf{p}_{t+k}) \approx e^{-\alpha k}

v·ªõi \alpha > 0.

‚∏ª

4. Ph√¢n t√≠ch sai ph√¢n (Difference Vectors)

X√©t vector sai ph√¢n:

\Delta_t = \mathbf{p}_{t+1} - \mathbf{p}_t

N·∫øu embedding c√≥ c·∫•u tr√∫c tuy·∫øn t√≠nh, ta k·ª≥ v·ªçng:

\Delta_t \approx \Delta_{t+1}

Th·ª±c nghi·ªám cho th·∫•y:
	‚Ä¢	C√°c \Delta_t g·∫ßn song song nhau
	‚Ä¢	Embedding v·ªã tr√≠ g·∫ßn nh∆∞ n·∫±m tr√™n m·ªôt qu·ªπ ƒë·∫°o tuy·∫øn t√≠nh trong kh√¥ng gian \mathbb{R}^d

ƒêi·ªÅu n√†y g·ª£i √Ω:

\mathbf{p}_t \approx \mathbf{p}_0 + t\mathbf{v}

v·ªõi \mathbf{v} l√† vector h∆∞·ªõng ch√≠nh.

‚∏ª

5. Ph√¢n t√≠ch th√†nh ph·∫ßn ch√≠nh (PCA)

5.1 Ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai

\mathbf{C} =
\frac{1}{L} \sum_{t=1}^{L}
(\mathbf{p}_t - \bar{\mathbf{p}})
(\mathbf{p}_t - \bar{\mathbf{p}})^T

Trong ƒë√≥:

\bar{\mathbf{p}} = \frac{1}{L} \sum_{t=1}^{L} \mathbf{p}_t

Gi·∫£i b√†i to√°n tr·ªã ri√™ng:

\mathbf{C}\mathbf{v}_i = \lambda_i \mathbf{v}_i

5.2 K·∫øt qu·∫£ th·ª±c nghi·ªám
	‚Ä¢	Th√†nh ph·∫ßn ch√≠nh th·ª© nh·∫•t (PC1) t∆∞∆°ng quan m·∫°nh v·ªõi ch·ªâ s·ªë v·ªã tr√≠.
	‚Ä¢	H∆°n 80% ph∆∞∆°ng sai c√≥ th·ªÉ n·∫±m trong v√†i th√†nh ph·∫ßn ƒë·∫ßu.
	‚Ä¢	Cho th·∫•y embedding v·ªã tr√≠ c√≥ c·∫•u tr√∫c th·∫•p chi·ªÅu hi·ªáu qu·∫£.

‚∏ª

6. ·∫¢nh h∆∞·ªüng ƒë·∫øn Self-Attention

Self-attention:

\text{Attention}(Q,K,V) =
\text{softmax}\left(
\frac{QK^T}{\sqrt{d_k}}
\right)V

V·ªõi:

Q = ZW_Q, \quad
K = ZW_K, \quad
Z = E + P

Suy ra:

QK^T =
(E + P)W_QW_K^T(E + P)^T

Khai tri·ªÉn:

= EWE^T + PWP^T + EWP^T + PWE^T

Embedding v·ªã tr√≠ ƒë√≥ng g√≥p tr·ª±c ti·∫øp v√†o ma tr·∫≠n attention scores.

‚∏ª

7. So s√°nh v·ªõi c√°c ph∆∞∆°ng ph√°p kh√°c

Ngo√†i absolute positional embeddings, c√°c ph∆∞∆°ng ph√°p kh√°c:
	‚Ä¢	Relative positional encoding
	‚Ä¢	RoPE (Rotary Positional Embedding)
	‚Ä¢	ALiBi

C√°c ph∆∞∆°ng ph√°p n√†y m√£ ho√° kho·∫£ng c√°ch t∆∞∆°ng ƒë·ªëi thay v√¨ v·ªã tr√≠ tuy·ªát ƒë·ªëi, gi√∫p c·∫£i thi·ªán kh·∫£ nƒÉng ngo·∫°i suy sang chu·ªói d√†i.

‚∏ª

8. Th·∫£o lu·∫≠n

8.1 C·∫•u tr√∫c g·∫ßn tuy·∫øn t√≠nh

Embedding v·ªã tr√≠ h·ªçc ƒë∆∞·ª£c trong GPT-2 c√≥ ƒë·∫∑c t√≠nh:

\mathbf{p}_t \approx \mathbf{a} + t\mathbf{b} + \epsilon_t

v·ªõi nhi·ªÖu nh·ªè \epsilon_t.

ƒêi·ªÅu n√†y gi·∫£i th√≠ch v√¨ sao PCA thu ƒë∆∞·ª£c tr·ª•c ch√≠nh g·∫ßn t∆∞∆°ng ·ª©ng v·ªõi ch·ªâ s·ªë v·ªã tr√≠.

‚∏ª

8.2 Kh√¥ng gian h√¨nh h·ªçc m∆∞·ª£t

Embedding v·ªã tr√≠ kh√¥ng ph√¢n b·ªë ng·∫´u nhi√™n m√† t·∫°o th√†nh m·ªôt ƒë∆∞·ªùng cong m∆∞·ª£t trong kh√¥ng gian cao chi·ªÅu.

ƒêi·ªÅu n√†y cho ph√©p:
	‚Ä¢	Attention h·ªçc ƒë∆∞·ª£c quan h·ªá kho·∫£ng c√°ch
	‚Ä¢	TƒÉng t√≠nh ·ªïn ƒë·ªãnh khi hu·∫•n luy·ªán

‚∏ª

9. K·∫øt lu·∫≠n

Qua ph√¢n t√≠ch l√Ω thuy·∫øt v√† th·ª±c nghi·ªám, c√≥ th·ªÉ r√∫t ra:
	1.	Embedding v·ªã tr√≠ h·ªçc ƒë∆∞·ª£c c√≥ c·∫•u tr√∫c g·∫ßn tuy·∫øn t√≠nh.
	2.	ƒê·ªô t∆∞∆°ng ƒë·ªìng cosine gi·∫£m d·∫ßn theo kho·∫£ng c√°ch.
	3.	Kh√¥ng gian embedding c√≥ c·∫•u tr√∫c th·∫•p chi·ªÅu hi·ªáu qu·∫£.
	4.	Embedding v·ªã tr√≠ ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn ma tr·∫≠n attention.

Vi·ªác hi·ªÉu r√µ c·∫•u tr√∫c h√¨nh h·ªçc c·ªßa embedding v·ªã tr√≠ c√≥ th·ªÉ m·ªü ƒë∆∞·ªùng cho:
	‚Ä¢	Thi·∫øt k·∫ø ki·∫øn tr√∫c hi·ªáu qu·∫£ h∆°n
	‚Ä¢	C·∫£i thi·ªán kh·∫£ nƒÉng ngo·∫°i suy
	‚Ä¢	T·ªëi ∆∞u ho√° b·ªô nh·ªõ cho m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn

‚∏ª

T√†i li·ªáu tham kh·∫£o
	1.	Ashish Vaswani et al. (2017). Attention Is All You Need.
	2.	OpenAI (2019). GPT-2 Technical Report.
	3.	Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
	4.	Su et al. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding.
	5.	Press et al. (2022). ALiBi: Linear Biases for Transformers.
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [aero llm 01 word2vec vs glove vs gpt vs bert oh my](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) |
| [aero llm 02 exploring glove pretrained embeddings](aero_llm_02_exploring_glove_pretrained_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_02_exploring_glove_pretrained_embeddings.md) |
| [aero llm 03 codechallenge wikipedia vs twitter embeddings part 1](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) |
| [So s√°nh Bi·ªÉu Di·ªÖn T·ª´ V·ª±ng gi·ªØa Wikipedia v√† Twitter b·∫±ng Ph√¢n T√≠ch T∆∞∆°ng ƒê·ªìng Bi·ªÉu Di·ªÖn (RSA)](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) |
| [So s√°nh Bi·ªÉu Di·ªÖn Ng·ªØ Nghƒ©a c·ªßa GPT-2 v√† BERT th√¥ng qua Ph√¢n T√≠ch Embedding](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) |
| [To√°n h·ªçc c·ªßa Token v√† Embedding trong M√¥ h√¨nh Ng√¥n ng·ªØ L·ªõn](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) |
| [Cosine Similarity v√† M·ªëi Quan H·ªá v·ªõi H·ªá S·ªë T∆∞∆°ng Quan: C∆° S·ªü To√°n H·ªçc v√† ·ª®ng D·ª•ng trong NLP](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) |
| [Ph√¢n T√≠ch Cosine Similarity trong Kh√¥ng Gian Embedding c·ªßa GPT-2](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) |
| [Unembedding trong M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn: T·ª´ Vector ·∫®n ƒê·∫øn Token](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) |
| [Position Embeddings trong Transformer: C∆° S·ªü To√°n H·ªçc v√† ·ª®ng D·ª•ng trong M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn](aero_llm_10_position_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_10_position_embeddings.md) |
| üìå **[Ph√¢n T√≠ch Th·ª±c Nghi·ªám Embedding V·ªã Tr√≠ Trong Transformer: T·ª´ C·∫•u Tr√∫c Tuy·∫øn T√≠nh ƒê·∫øn Kh√¥ng Gian H√¨nh H·ªçc](aero_llm_11_codechallenge_exploring_position_embeddings.md)** | [Xem b√†i vi·∫øt ‚Üí](aero_llm_11_codechallenge_exploring_position_embeddings.md) |
| [Hu·∫•n Luy·ªán Embedding T·ª´ ƒê·∫ßu: C∆° S·ªü To√°n H·ªçc, C∆° Ch·∫ø T·ªëi ∆Øu v√† ·ª®ng D·ª•ng Trong M√¥ H√¨nh Ng√¥n Ng·ªØ](aero_llm_12_training_embeddings_from_scratch.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_12_training_embeddings_from_scratch.md) |
| [Thi·∫øt K·∫ø Data Loader Cho Hu·∫•n Luy·ªán M√¥ H√¨nh Ng√¥n Ng·ªØ: C∆° S·ªü To√°n H·ªçc, Ki·∫øn Tr√∫c v√† T·ªëi ∆Øu Ho√°](aero_llm_13_create_a_data_loader_to_train_a_model.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_13_create_a_data_loader_to_train_a_model.md) |
| [X√¢y D·ª±ng M√¥ H√¨nh H·ªçc Embedding T·ª´ ƒê·∫ßu: Ki·∫øn Tr√∫c, T·ªëi ∆Øu Ho√° v√† Ph√¢n T√≠ch To√°n H·ªçc](aero_llm_14_build_a_model_to_learn_the_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_14_build_a_model_to_learn_the_embeddings.md) |
| [H√†m M·∫•t M√°t Trong Hu·∫•n Luy·ªán Embedding: C∆° S·ªü L√Ω Thuy·∫øt, Ph√¢n T√≠ch Gradient v√† ·ª®ng D·ª•ng Trong M√¥ H√¨nh Ng√¥n Ng·ªØ](aero_llm_15_loss_function_to_train_the_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_15_loss_function_to_train_the_embeddings.md) |
| [Hu·∫•n luy·ªán v√† ƒê√°nh gi√° M√¥ h√¨nh H·ªçc M√°y: C∆° s·ªü L√Ω thuy·∫øt v√† Th·ª±c ti·ªÖn](aero_llm_16_train_and_evaluate_the_model.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_16_train_and_evaluate_the_model.md) |
| [S·ª± Thay ƒê·ªïi c·ªßa Embeddings Trong Qu√° Tr√¨nh Hu·∫•n Luy·ªán: Ph√¢n T√≠ch To√°n H·ªçc v√† Th·ª±c Nghi·ªám](aero_llm_17_codechallenge_how_the_embeddings_change.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_17_codechallenge_how_the_embeddings_change.md) |
| [ƒê·ªô ·ªîn ƒê·ªãnh c·ªßa Embeddings trong M√¥ H√¨nh Ng√¥n Ng·ªØ: Ph√¢n T√≠ch To√°n H·ªçc v√† Th·ª±c Nghi·ªám](aero_llm_18_codechallenge_how_stable_are_embeddings.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_18_codechallenge_how_stable_are_embeddings.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

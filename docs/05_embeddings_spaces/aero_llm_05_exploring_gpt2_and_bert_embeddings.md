
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [05 embeddings spaces](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# So sÃ¡nh Biá»ƒu Diá»…n Ngá»¯ NghÄ©a cá»§a GPT-2 vÃ  BERT thÃ´ng qua PhÃ¢n TÃ­ch Embedding

## TÃ³m táº¯t

CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n dá»±a trÃªn Transformer Ä‘Ã£ thay Ä‘á»•i ná»n táº£ng cá»§a xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP). Hai kiáº¿n trÃºc tiÃªu biá»ƒu lÃ  [GPT-2](chatgpt://generic-entity?number=0) vÃ  [BERT](chatgpt://generic-entity?number=1). Máº·c dÃ¹ cÃ¹ng dá»±a trÃªn cÆ¡ cháº¿ self-attention, hai mÃ´ hÃ¬nh cÃ³ má»¥c tiÃªu huáº¥n luyá»‡n vÃ  cáº¥u trÃºc khÃ¡c nhau, dáº«n Ä‘áº¿n Ä‘áº·c tÃ­nh embedding khÃ¡c biá»‡t. BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch cÆ¡ sá»Ÿ toÃ¡n há»c cá»§a embedding trong hai mÃ´ hÃ¬nh, so sÃ¡nh cáº¥u trÃºc khÃ´ng gian biá»ƒu diá»…n vÃ  minh há»a báº±ng cÃ¡c cÃ´ng thá»©c Ä‘á»‹nh lÆ°á»£ng.

---

## 1. Giá»›i thiá»‡u

Trong NLP, má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ há»c phÃ¢n phá»‘i xÃ¡c suáº¥t cÃ³ Ä‘iá»u kiá»‡n:

$$
P(w_t \mid w_{\lt t})
$$

hoáº·c trong trÆ°á»ng há»£p hai chiá»u:

$$
P(w_i \mid w_{\setminus i})
$$

TÃ¹y vÃ o má»¥c tiÃªu huáº¥n luyá»‡n, embedding thu Ä‘Æ°á»£c sáº½ mang Ä‘áº·c trÆ°ng khÃ¡c nhau.

- GPT-2: mÃ´ hÃ¬nh tá»± há»“i quy (autoregressive)
- BERT: mÃ´ hÃ¬nh hai chiá»u (bidirectional) vá»›i masked language modeling

---

## 2. CÆ¡ sá»Ÿ kiáº¿n trÃºc Transformer

Cáº£ hai mÃ´ hÃ¬nh Ä‘á»u dá»±a trÃªn kiáº¿n trÃºc Transformer (Vaswani et al., 2017), vá»›i cÆ¡ cháº¿ **Scaled Dot-Product Attention**:

\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V

Trong Ä‘Ã³:

- $Q$: Query matrix  
- $K$: Key matrix  
- $V$: Value matrix  
- $d_k$: sá»‘ chiá»u cá»§a vector key  

Self-attention cho phÃ©p mÃ´ hÃ¬nh há»c phá»¥ thuá»™c dÃ i háº¡n trong chuá»—i.

---

## 3. Embedding trong GPT-2

### 3.1 Má»¥c tiÃªu huáº¥n luyá»‡n

$$
[GPT-2](chatgpt://generic-entity?number=2) Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ tá»‘i Ä‘a hÃ³a log-likelihood: \mathcal{L}_{GPT2} = \sum_{t=1}^{T} \log P(w_t \mid w_{\lt t}) Trong Ä‘Ã³: P(w_t \mid w_{\lt t}) = \text{softmax}(W_o h_t) - h_t: hidden state táº¡i vá»‹ trÃ­ t - W_o: ma tráº­n chiáº¿u Ä‘áº§u ra ### 3.2 Äáº·c Ä‘iá»ƒm embedding Embedding cá»§a GPT-2 mang tÃ­nh **ngá»¯ cáº£nh má»™t chiá»u**: \mathbf{h}_t = f(w_1, w_2, ..., w_t) Do Ä‘Ã³, vector táº¡i vá»‹ trÃ­ t chá»‰ phá»¥ thuá»™c vÃ o quÃ¡ khá»©. --- ## 4. Embedding trong BERT ### 4.1 Má»¥c tiÃªu huáº¥n luyá»‡n
$$

[BERT](chatgpt://generic-entity?number=3) sá»­ dá»¥ng Masked Language Modeling (MLM):

$\mathcal${L}_{BERT} = $\sum$_{i \in M} $\log$ P($w_i$ \mid w_{\setminus i})

Trong Ä‘Ã³:

- $M$: táº­p cÃ¡c vá»‹ trÃ­ bá»‹ mask
- $w_{\setminus i}$: toÃ n bá»™ chuá»—i trá»« vá»‹ trÃ­ $i$

### 4.2 Äáº·c Ä‘iá»ƒm embedding

Embedding cá»§a BERT mang tÃ­nh **hai chiá»u**:

\mathbf{h}_t = f(w_1, ..., w_T)

Do Ä‘Ã³:

- Ngá»¯ cáº£nh trÃ¡i vÃ  pháº£i Ä‘á»u áº£nh hÆ°á»Ÿng
- Biá»ƒu diá»…n phÃ¹ há»£p cho tÃ¡c vá»¥ phÃ¢n loáº¡i vÃ  suy luáº­n ngá»¯ nghÄ©a

---

## 5. So sÃ¡nh hÃ¬nh há»c khÃ´ng gian embedding

Giáº£ sá»­:

\mathbf{v}_i^{(GPT2)} \in \mathbb{R}^d

$$
\mathbf{v}_i^{(BERT)} \in \mathbb{R}^d ### 5.1 Äá»™ tÆ°Æ¡ng Ä‘á»“ng cosine \text{cosine}(\mathbf{v}_i, \mathbf{v}_j) = \frac{\mathbf{v}_i \cdot \mathbf{v}_j} {\|\mathbf{v}_i\|\|\mathbf{v}_j\|} ### 5.2 Khoáº£ng cÃ¡ch Euclid d(\mathbf{v}_i,\mathbf{v}_j) = \|\mathbf{v}_i - \mathbf{v}_j\| = \sqrt{\sum_{k=1}^{d}(v_{ik}-v_{jk})^2} ### 5.3 PhÃ¢n tÃ­ch phÆ°Æ¡ng sai (PCA) Giáº£ sá»­ ma tráº­n embedding: X \in \mathbb{R}^{n \times d} Ma tráº­n hiá»‡p phÆ°Æ¡ng sai: \Sigma = \frac{1}{n} X^T X Giáº£i bÃ i toÃ¡n trá»‹ riÃªng: \Sigma \mathbf{u} = \lambda \mathbf{u} CÃ¡c trá»‹ riÃªng lá»›n pháº£n Ã¡nh chiá»u chiáº¿m Æ°u tháº¿ trong khÃ´ng gian biá»ƒu diá»…n. --- ## 6. PhÃ¢n tÃ­ch Ä‘á»‹nh lÆ°á»£ng Má»™t sá»‘ khÃ¡c biá»‡t quan sÃ¡t Ä‘Æ°á»£c: | Thuá»™c tÃ­nh | GPT-2 | BERT | |------------|--------|-------| | HÆ°á»›ng xá»­ lÃ½ | TrÃ¡i â†’ Pháº£i | Hai chiá»u | | Má»¥c tiÃªu | Next-token prediction | Masked token prediction | | Embedding | PhÃ¹ há»£p sinh vÄƒn báº£n | PhÃ¹ há»£p phÃ¢n loáº¡i | | Cáº¥u trÃºc hÃ¬nh há»c | Mang tÃ­nh tiáº¿n trÃ¬nh | Mang tÃ­nh ngá»¯ cáº£nh toÃ n cá»¥c | --- ## 7. Tháº£o luáº­n ### 7.1 TÃ­nh á»•n Ä‘á»‹nh ngá»¯ nghÄ©a Náº¿u xÃ©t ma tráº­n tÆ°Æ¡ng Ä‘á»“ng ná»™i bá»™: S_{ij} = \text{cosine}(\mathbf{v}_i,\mathbf{v}_j) Ta cÃ³ thá»ƒ sá»­ dá»¥ng tÆ°Æ¡ng quan Pearson giá»¯a hai ma tráº­n Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cáº¥u trÃºc: r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})} {\sqrt{\sum (x_i - \bar{x})^2} \sqrt{\sum (y_i - \bar{y})^2}} ### 7.2 TÃ­nh báº¥t biáº¿n quay (Rotation Invariance) Giáº£ sá»­ tá»“n táº¡i ma tráº­n trá»±c giao R: R^T R = I Khi Ä‘Ã³: \mathbf{v}' = R\mathbf{v} Khoáº£ng cÃ¡ch cosine khÃ´ng Ä‘á»•i, nhÆ°ng tá»a Ä‘á»™ thay Ä‘á»•i. --- ## 8. Káº¿t luáº­n - GPT-2 tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh sinh chuá»—i â†’ embedding thiÃªn vá» tiáº¿n trÃ¬nh. - BERT tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh suy luáº­n ngá»¯ cáº£nh â†’ embedding thiÃªn vá» ngá»¯ nghÄ©a toÃ n cá»¥c. - PhÃ¢n tÃ­ch hÃ¬nh há»c (cosine, Euclid, PCA, RSA) giÃºp hiá»ƒu cáº¥u trÃºc biá»ƒu diá»…n. Vá» máº·t toÃ¡n há»c:
$$

\text{Objective Function} \Rightarrow \text{Geometry of Embedding Space}
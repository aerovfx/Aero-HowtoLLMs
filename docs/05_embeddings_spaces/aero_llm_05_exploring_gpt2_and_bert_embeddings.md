
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [05 embeddings spaces](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# So sÃ¡nh Biá»ƒu Diá»…n Ngá»¯ NghÄ©a cá»§a GPT-2 vÃ  BERT thÃ´ng qua PhÃ¢n TÃ­ch Embedding

## TÃ³m táº¯t

CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n dá»±a trÃªn Transformer Ä‘Ã£ thay Ä‘á»•i ná»n táº£ng cá»§a xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP). Hai kiáº¿n trÃºc tiÃªu biá»ƒu lÃ  [GPT-2](chatgpt://generic-entity?number=0) vÃ  [BERT](chatgpt://generic-entity?number=1). Máº·c dÃ¹ cÃ¹ng dá»±a trÃªn cÆ¡ cháº¿ self-attention, hai mÃ´ hÃ¬nh cÃ³ má»¥c tiÃªu huáº¥n luyá»‡n vÃ  cáº¥u trÃºc khÃ¡c nhau, dáº«n Ä‘áº¿n Ä‘áº·c tÃ­nh embedding khÃ¡c biá»‡t. BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch cÆ¡ sá»Ÿ toÃ¡n há»c cá»§a embedding trong hai mÃ´ hÃ¬nh, so sÃ¡nh cáº¥u trÃºc khÃ´ng gian biá»ƒu diá»…n vÃ  minh há»a báº±ng cÃ¡c cÃ´ng thá»©c Ä‘á»‹nh lÆ°á»£ng.

---

## 1. Giá»›i thiá»‡u

Trong NLP, má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ há»c phÃ¢n phá»‘i xÃ¡c suáº¥t cÃ³ Ä‘iá»u kiá»‡n:

$$
P(w_t \mid w_{<t})
$$

hoáº·c trong trÆ°á»ng há»£p hai chiá»u:

$$
P(w_i \mid w_{\setminus i})
$$

TÃ¹y vÃ o má»¥c tiÃªu huáº¥n luyá»‡n, embedding thu Ä‘Æ°á»£c sáº½ mang Ä‘áº·c trÆ°ng khÃ¡c nhau.

- GPT-2: mÃ´ hÃ¬nh tá»± há»“i quy (autoregressive)
- BERT: mÃ´ hÃ¬nh hai chiá»u (bidirectional) vá»›i masked language modeling

---

## 2. CÆ¡ sá»Ÿ kiáº¿n trÃºc Transformer

Cáº£ hai mÃ´ hÃ¬nh Ä‘á»u dá»±a trÃªn kiáº¿n trÃºc Transformer (Vaswani et al., 2017), vá»›i cÆ¡ cháº¿ **Scaled Dot-Product Attention**:

$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Trong Ä‘Ã³:

- $Q$: Query matrix  
- $K$: Key matrix  
- $V$: Value matrix  
- $d_k$: sá»‘ chiá»u cá»§a vector key  

Self-attention cho phÃ©p mÃ´ hÃ¬nh há»c phá»¥ thuá»™c dÃ i háº¡n trong chuá»—i.

---

## 3. Embedding trong GPT-2

### 3.1 Má»¥c tiÃªu huáº¥n luyá»‡n

[GPT-2](chatgpt://generic-entity?number=2) Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ tá»‘i Ä‘a hÃ³a log-likelihood:

$$
\mathcal{L}_{GPT2} = \sum_{t=1}^{T} \log P(w_t \mid w_{<t})
$$

Trong Ä‘Ã³:

$$
P(w_t \mid w_{<t}) = \text{softmax}(W_o h_t)
$$

- $h_t$: hidden state táº¡i vá»‹ trÃ­ $t$
- $W_o$: ma tráº­n chiáº¿u Ä‘áº§u ra

### 3.2 Äáº·c Ä‘iá»ƒm embedding

Embedding cá»§a GPT-2 mang tÃ­nh **ngá»¯ cáº£nh má»™t chiá»u**:

$$
\mathbf{h}_t = f(w_1, w_2, ..., w_t)
$$

Do Ä‘Ã³, vector táº¡i vá»‹ trÃ­ $t$ chá»‰ phá»¥ thuá»™c vÃ o quÃ¡ khá»©.

---

## 4. Embedding trong BERT

### 4.1 Má»¥c tiÃªu huáº¥n luyá»‡n

[BERT](chatgpt://generic-entity?number=3) sá»­ dá»¥ng Masked Language Modeling (MLM):

$$
\mathcal{L}_{BERT} = \sum_{i \in M} \log P(w_i \mid w_{\setminus i})
$$

Trong Ä‘Ã³:

- $M$: táº­p cÃ¡c vá»‹ trÃ­ bá»‹ mask
- $w_{\setminus i}$: toÃ n bá»™ chuá»—i trá»« vá»‹ trÃ­ $i$

### 4.2 Äáº·c Ä‘iá»ƒm embedding

Embedding cá»§a BERT mang tÃ­nh **hai chiá»u**:

$$
\mathbf{h}_t = f(w_1, ..., w_T)
$$

Do Ä‘Ã³:

- Ngá»¯ cáº£nh trÃ¡i vÃ  pháº£i Ä‘á»u áº£nh hÆ°á»Ÿng
- Biá»ƒu diá»…n phÃ¹ há»£p cho tÃ¡c vá»¥ phÃ¢n loáº¡i vÃ  suy luáº­n ngá»¯ nghÄ©a

---

## 5. So sÃ¡nh hÃ¬nh há»c khÃ´ng gian embedding

Giáº£ sá»­:

$$
\mathbf{v}_i^{(GPT2)} \in \mathbb{R}^d
$$

$$
\mathbf{v}_i^{(BERT)} \in \mathbb{R}^d
$$

### 5.1 Äá»™ tÆ°Æ¡ng Ä‘á»“ng cosine

$$
\text{cosine}(\mathbf{v}_i, \mathbf{v}_j) = \frac{\mathbf{v}_i \cdot \mathbf{v}_j} {\|\mathbf{v}_i\|\|\mathbf{v}_j\|}
$$

### 5.2 Khoáº£ng cÃ¡ch Euclid

$$
d(\mathbf{v}_i,\mathbf{v}_j) = \|\mathbf{v}_i - \mathbf{v}_j\| = \sqrt{\sum_{k=1}^{d}(v_{ik}-v_{jk})^2}
$$

### 5.3 PhÃ¢n tÃ­ch phÆ°Æ¡ng sai (PCA)

Giáº£ sá»­ ma tráº­n embedding:

$$
X \in \mathbb{R}^{n \times d}
$$

Ma tráº­n hiá»‡p phÆ°Æ¡ng sai:

$$
\Sigma = \frac{1}{n} X^T X
$$

Giáº£i bÃ i toÃ¡n trá»‹ riÃªng:

$$
\Sigma \mathbf{u} = \lambda \mathbf{u}
$$

CÃ¡c trá»‹ riÃªng lá»›n pháº£n Ã¡nh chiá»u chiáº¿m Æ°u tháº¿ trong khÃ´ng gian biá»ƒu diá»…n.

---

## 6. PhÃ¢n tÃ­ch Ä‘á»‹nh lÆ°á»£ng

Má»™t sá»‘ khÃ¡c biá»‡t quan sÃ¡t Ä‘Æ°á»£c:

| Thuá»™c tÃ­nh | GPT-2 | BERT |
|------------|--------|-------|
| HÆ°á»›ng xá»­ lÃ½ | TrÃ¡i â†’ Pháº£i | Hai chiá»u |
| Má»¥c tiÃªu | Next-token prediction | Masked token prediction |
| Embedding | PhÃ¹ há»£p sinh vÄƒn báº£n | PhÃ¹ há»£p phÃ¢n loáº¡i |
| Cáº¥u trÃºc hÃ¬nh há»c | Mang tÃ­nh tiáº¿n trÃ¬nh | Mang tÃ­nh ngá»¯ cáº£nh toÃ n cá»¥c |

---

## 7. Tháº£o luáº­n

### 7.1 TÃ­nh á»•n Ä‘á»‹nh ngá»¯ nghÄ©a

Náº¿u xÃ©t ma tráº­n tÆ°Æ¡ng Ä‘á»“ng ná»™i bá»™:

$$
S_{ij} = \text{cosine}(\mathbf{v}_i,\mathbf{v}_j)
$$

Ta cÃ³ thá»ƒ sá»­ dá»¥ng tÆ°Æ¡ng quan Pearson giá»¯a hai ma tráº­n Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cáº¥u trÃºc:

$$
r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})} {\sqrt{\sum (x_i - \bar{x})^2} \sqrt{\sum (y_i - \bar{y})^2}}
$$

### 7.2 TÃ­nh báº¥t biáº¿n quay (Rotation Invariance)

Giáº£ sá»­ tá»“n táº¡i ma tráº­n trá»±c giao $R$:

$$
R^T R = I
$$

Khi Ä‘Ã³:

$$
\mathbf{v}' = R\mathbf{v}
$$

Khoáº£ng cÃ¡ch cosine khÃ´ng Ä‘á»•i, nhÆ°ng tá»a Ä‘á»™ thay Ä‘á»•i.

---

## 8. Káº¿t luáº­n

- GPT-2 tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh sinh chuá»—i â†’ embedding thiÃªn vá» tiáº¿n trÃ¬nh.
- BERT tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh suy luáº­n ngá»¯ cáº£nh â†’ embedding thiÃªn vá» ngá»¯ nghÄ©a toÃ n cá»¥c.
- PhÃ¢n tÃ­ch hÃ¬nh há»c (cosine, Euclid, PCA, RSA) giÃºp hiá»ƒu cáº¥u trÃºc biá»ƒu diá»…n.

Vá» máº·t toÃ¡n há»c:

$$
\text{Objective Function} \Rightarrow \text{Geometry of Embedding Space}
$$

---

## TÃ i liá»‡u tham kháº£o

1. Vaswani et al. (2017). Attention is All You Need.
2. Radford et al. (2019). Language Models are Unsupervised Multitask Learners.
3. Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
4. Jurafsky & Martin (2023). Speech and Language Processing.

---
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero llm 01 word2vec vs glove vs gpt vs bert oh my](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_word2vec_vs_glove_vs_gpt_vs_bert_oh_my_.md) |
| [aero llm 02 exploring glove pretrained embeddings](aero_llm_02_exploring_glove_pretrained_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_exploring_glove_pretrained_embeddings.md) |
| [aero llm 03 codechallenge wikipedia vs twitter embeddings part 1](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_wikipedia_vs_twitter_embeddings_part_1_.md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Tá»« Vá»±ng giá»¯a Wikipedia vÃ  Twitter báº±ng PhÃ¢n TÃ­ch TÆ°Æ¡ng Äá»“ng Biá»ƒu Diá»…n (RSA)](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_wikipedia_vs_twitter_embeddings_part_2_.md) |
| ğŸ“Œ **[So sÃ¡nh Biá»ƒu Diá»…n Ngá»¯ NghÄ©a cá»§a GPT-2 vÃ  BERT thÃ´ng qua PhÃ¢n TÃ­ch Embedding](aero_llm_05_exploring_gpt2_and_bert_embeddings.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_05_exploring_gpt2_and_bert_embeddings.md) |
| [ToÃ¡n há»c cá»§a Token vÃ  Embedding trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_math_with_tokens_and_embeddings.md) |
| [Cosine Similarity vÃ  Má»‘i Quan Há»‡ vá»›i Há»‡ Sá»‘ TÆ°Æ¡ng Quan: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong NLP](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_cosine_similarity_and_relation_to_correlation_.md) |
| [PhÃ¢n TÃ­ch Cosine Similarity trong KhÃ´ng Gian Embedding cá»§a GPT-2](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_codechallenge_gpt2_cosine_similarities.md) |
| [Unembedding trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: Tá»« Vector áº¨n Äáº¿n Token](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_unembeddings_vectors_to_tokens_.md) |
| [Position Embeddings trong Transformer: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_llm_10_position_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_position_embeddings.md) |
| [PhÃ¢n TÃ­ch Thá»±c Nghiá»‡m Embedding Vá»‹ TrÃ­ Trong Transformer: Tá»« Cáº¥u TrÃºc Tuyáº¿n TÃ­nh Äáº¿n KhÃ´ng Gian HÃ¬nh Há»c](aero_llm_11_codechallenge_exploring_position_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_codechallenge_exploring_position_embeddings.md) |
| [Huáº¥n Luyá»‡n Embedding Tá»« Äáº§u: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, CÆ¡ Cháº¿ Tá»‘i Æ¯u vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_12_training_embeddings_from_scratch.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_training_embeddings_from_scratch.md) |
| [Thiáº¿t Káº¿ Data Loader Cho Huáº¥n Luyá»‡n MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Kiáº¿n TrÃºc vÃ  Tá»‘i Æ¯u HoÃ¡](aero_llm_13_create_a_data_loader_to_train_a_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_create_a_data_loader_to_train_a_model.md) |
| [XÃ¢y Dá»±ng MÃ´ HÃ¬nh Há»c Embedding Tá»« Äáº§u: Kiáº¿n TrÃºc, Tá»‘i Æ¯u HoÃ¡ vÃ  PhÃ¢n TÃ­ch ToÃ¡n Há»c](aero_llm_14_build_a_model_to_learn_the_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_build_a_model_to_learn_the_embeddings.md) |
| [HÃ m Máº¥t MÃ¡t Trong Huáº¥n Luyá»‡n Embedding: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, PhÃ¢n TÃ­ch Gradient vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_15_loss_function_to_train_the_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_loss_function_to_train_the_embeddings.md) |
| [Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh Há»c MÃ¡y: CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Thá»±c tiá»…n](aero_llm_16_train_and_evaluate_the_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_16_train_and_evaluate_the_model.md) |
| [Sá»± Thay Äá»•i cá»§a Embeddings Trong QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_llm_17_codechallenge_how_the_embeddings_change.md) | [Xem bÃ i viáº¿t â†’](aero_llm_17_codechallenge_how_the_embeddings_change.md) |
| [Äá»™ á»”n Äá»‹nh cá»§a Embeddings trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_llm_18_codechallenge_how_stable_are_embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_llm_18_codechallenge_how_stable_are_embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

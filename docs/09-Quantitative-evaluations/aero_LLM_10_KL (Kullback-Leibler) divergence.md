
<!-- Aero-Navigation-Start -->
[üè† Home](../../index.md) > [09 Quantitative evaluations](../index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../index.md)
- [üìö Module 01: LLM Course](../../01-LLM_Course/index.md)
- [üî¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../04-buildGPT/index.md)
- [üéØ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [üîç Module 19: AI Safety](../../19-AI-safety/index.md)
- [üêç Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Ph√¢n k·ª≥ Kullback‚ÄìLeibler (KL Divergence) trong m√¥ h√¨nh ng√¥n ng·ªØ v√† h·ªçc s√¢u

Ph√¢n t√≠ch l√Ω thuy·∫øt, c√¥ng th·ª©c to√°n h·ªçc v√† ·ª©ng d·ª•ng trong t·ªëi ∆∞u h√≥a ph√¢n ph·ªëi x√°c su·∫•t

‚∏ª

T√≥m t·∫Øt

B√†i vi·∫øt n√†y tr√¨nh b√†y c∆° s·ªü l√Ω thuy·∫øt v√† ·ª©ng d·ª•ng c·ªßa ph√¢n k·ª≥ Kullback‚ÄìLeibler (KL Divergence) trong h·ªçc m√°y v√† m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLMs). D·ª±a tr√™n n·ªôi dung t√†i li·ªáu ƒë√≠nh k√®m, ch√∫ng t√¥i m·ªü r·ªông ph√¢n t√≠ch v·ªõi c√°c n·ªÅn t·∫£ng t·ª´ l√Ω thuy·∫øt th√¥ng tin c·ªßa Claude Shannon, c√¥ng tr√¨nh g·ªëc c·ªßa Solomon Kullback v√† Richard Leibler, c√πng c√°c ·ª©ng d·ª•ng hi·ªán ƒë·∫°i trong hu·∫•n luy·ªán Transformer c·ªßa Ashish Vaswani et al. v√† nghi√™n c·ª©u RLHF t·∫°i OpenAI.

‚∏ª

1. Gi·ªõi thi·ªáu

Trong h·ªçc m√°y, ta th∆∞·ªùng c·∫ßn ƒëo kho·∫£ng c√°ch gi·ªØa hai ph√¢n ph·ªëi x√°c su·∫•t:
	‚Ä¢	Ph√¢n ph·ªëi th·ª±c P(x)
	‚Ä¢	Ph√¢n ph·ªëi m√¥ h√¨nh Q(x)

Ph√¢n k·ª≥ KL ƒëo m·ª©c ‚Äúm·∫•t m√°t th√¥ng tin‚Äù khi d√πng Q ƒë·ªÉ x·∫•p x·ªâ P.

‚∏ª

2. ƒê·ªãnh nghƒ©a to√°n h·ªçc

2.1 Tr∆∞·ªùng h·ª£p r·ªùi r·∫°c

D_{KL}(P \| Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}

ƒêi·ªÅu ki·ªán:

Q(x) > 0 \quad \text{n·∫øu } P(x) > 0

‚∏ª

2.2 Tr∆∞·ªùng h·ª£p li√™n t·ª•c

D_{KL}(P \| Q) = \int P(x) \log \frac{P(x)}{Q(x)} dx

‚∏ª

3. C√°c t√≠nh ch·∫•t quan tr·ªçng

3.1 Kh√¥ng √¢m (Non-negativity)

D_{KL}(P \| Q) \ge 0

v√†

D_{KL}(P \| Q) = 0 \iff P = Q

Ch·ª©ng minh d·ª±a tr√™n b·∫•t ƒë·∫≥ng th·ª©c Jensen.

‚∏ª

3.2 Kh√¥ng ƒë·ªëi x·ª©ng

D_{KL}(P \| Q) \neq D_{KL}(Q \| P)

Do ƒë√≥ KL kh√¥ng ph·∫£i l√† metric.

‚∏ª

4. Li√™n h·ªá v·ªõi Cross-Entropy

Cross-entropy:

H(P, Q) = - \sum_x P(x) \log Q(x)

Entropy:

H(P) = - \sum_x P(x) \log P(x)

Ta c√≥:

D_{KL}(P \| Q) = H(P, Q) - H(P)

Trong hu·∫•n luy·ªán m√¥ h√¨nh, v√¨ H(P) kh√¥ng ph·ª• thu·ªôc v√†o tham s·ªë m√¥ h√¨nh, n√™n t·ªëi thi·ªÉu h√≥a cross-entropy t∆∞∆°ng ƒë∆∞∆°ng t·ªëi thi·ªÉu h√≥a KL divergence.

‚∏ª

5. KL Divergence trong m√¥ h√¨nh ng√¥n ng·ªØ

V·ªõi m√¥ h√¨nh d·ª± ƒëo√°n token:
	‚Ä¢	Ph√¢n ph·ªëi th·∫≠t: P_{data}
	‚Ä¢	Ph√¢n ph·ªëi m√¥ h√¨nh: P_\theta

H√†m m·∫•t m√°t:

\mathcal{L}(\theta) = D_{KL}(P_{data} \| P_\theta)

T·ªëi ∆∞u:

\theta^* = \arg\min_\theta D_{KL}(P_{data} \| P_\theta)

‚∏ª

6. Li√™n h·ªá v·ªõi Perplexity

Perplexity:

PP = \exp\left(H(P_{data}, P_\theta)\right)

V√¨:

H(P_{data}, P_\theta) = H(P_{data}) + D_{KL}(P_{data} \| P_\theta)

‚Üí Gi·∫£m KL ‚Üí gi·∫£m perplexity.

‚∏ª

7. KL Divergence trong RLHF

Trong Reinforcement Learning from Human Feedback (RLHF), ta t·ªëi ∆∞u:

\max_\theta \mathbb{E}_{x \sim P_\theta}[R(x)] - \beta D_{KL}(P_\theta \| P_{ref})

Trong ƒë√≥:
	‚Ä¢	R(x): reward model
	‚Ä¢	P_{ref}: m√¥ h√¨nh tham chi·∫øu
	‚Ä¢	\beta: h·ªá s·ªë ƒëi·ªÅu ch·ªânh

Th√†nh ph·∫ßn KL gi√∫p:
	‚Ä¢	NgƒÉn m√¥ h√¨nh l·ªách qu√° xa m√¥ h√¨nh g·ªëc
	‚Ä¢	Tr√°nh over-optimization

‚∏ª

8. KL Divergence gi·ªØa hai ph√¢n ph·ªëi chu·∫©n

Gi·∫£ s·ª≠:

P = \mathcal{N}(\mu_1, \sigma_1^2)
Q = \mathcal{N}(\mu_2, \sigma_2^2)

Ta c√≥:

D_{KL}(P \| Q) =
\log \frac{\sigma_2}{\sigma_1}
+ \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2}
- \frac{1}{2}

C√¥ng th·ª©c n√†y th∆∞·ªùng d√πng trong Variational Autoencoder (VAE).

‚∏ª

9. KL Divergence v√† Self-Attention

Trong Transformer:

P_\theta(w_t) = \text{softmax}(Wh_t)

Hu·∫•n luy·ªán t·ªëi thi·ªÉu h√≥a:

D_{KL}(P_{data} \| P_\theta)

C∆° ch·∫ø self-attention:

Attention(Q,K,V) =
\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V

Gi√∫p m√¥ h√¨nh x√¢y d·ª±ng ph√¢n ph·ªëi x√°c su·∫•t ch√≠nh x√°c h∆°n.

‚∏ª

10. Tr·ª±c gi√°c th√¥ng tin h·ªçc

Theo l√Ω thuy·∫øt th√¥ng tin c·ªßa Claude Shannon:
	‚Ä¢	Entropy ƒëo ƒë·ªô b·∫•t ƒë·ªãnh
	‚Ä¢	KL ƒëo m·ª©c th√¥ng tin m·∫•t ƒëi khi x·∫•p x·ªâ ph√¢n ph·ªëi

N·∫øu:

D_{KL}(P \| Q) = 2

‚Üí Trung b√¨nh ta m·∫•t 2 nat th√¥ng tin m·ªói m·∫´u.

‚∏ª

11. ·ª®ng d·ª•ng th·ª±c t·∫ø

11.1 Distillation

Gi·ªØa teacher T v√† student S:

\mathcal{L} = D_{KL}(P_T \| P_S)

‚∏ª

11.2 Regularization

Th√™m ƒëi·ªÅu kho·∫£n KL ƒë·ªÉ:
	‚Ä¢	Gi·∫£m overfitting
	‚Ä¢	Ki·ªÉm so√°t divergence

‚∏ª

11.3 Variational Inference

T·ªëi ∆∞u:

D_{KL}(q(z) \| p(z|x))

‚∏ª

12. H·∫°n ch·∫ø c·ªßa KL Divergence
	1.	Kh√¥ng ƒë·ªëi x·ª©ng
	2.	Nh·∫°y khi Q(x) \to 0
	3.	Kh√¥ng ph·∫£i metric

Trong m·ªôt s·ªë tr∆∞·ªùng h·ª£p, Jensen-Shannon divergence ƒë∆∞·ª£c d√πng thay th·∫ø.

‚∏ª

13. K·∫øt lu·∫≠n

Ph√¢n k·ª≥ KL l√† n·ªÅn t·∫£ng c·ªßa:
	‚Ä¢	Hu·∫•n luy·ªán m√¥ h√¨nh ng√¥n ng·ªØ
	‚Ä¢	Cross-entropy loss
	‚Ä¢	Perplexity
	‚Ä¢	RLHF
	‚Ä¢	Distillation

N√≥ k·∫øt n·ªëi tr·ª±c ti·∫øp gi·ªØa l√Ω thuy·∫øt th√¥ng tin v√† h·ªçc s√¢u hi·ªán ƒë·∫°i.

‚∏ª

T√†i li·ªáu tham kh·∫£o
	1.	Kullback, S., Leibler, R. (1951). On Information and Sufficiency.
	2.	Shannon, C. (1948). A Mathematical Theory of Communication.
	3.	Vaswani, A. et al. (2017). Attention is All You Need.
	4.	Goodfellow, I. et al. (2016). Deep Learning.
	5.	Ouyang et al. (2022). Training language models to follow instructions with human feedback.
<!-- Aero-Footer-Start -->
---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [09 Quantitative evaluations](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: Trá»±c Quan HÃ³a Dá»± ÄoÃ¡n ÄÆ¡n Token

## TÃ³m táº¯t

BÃ i viáº¿t hÆ°á»›ng dáº«n thu tháº­p logit, Ã¡p dá»¥ng hÃ m `LogSoftmax` vÃ  xÃ¢y dá»±ng báº£n Ä‘á»“ nhiá»‡t (Text Heatmaps) nháº±m hiá»ƒu rÃµ nÄƒng lá»±c dá»± Ä‘oÃ¡n chuá»—i kÃ½ tá»± cá»§a hai biáº¿n thá»ƒ MÃ´ hÃ¬nh NgÃ´n ngá»¯ lÃ  GPT-2 Small vÃ  Large. Äiá»u nÃ y Ä‘Æ°a ra Ä‘Ã¡nh giÃ¡ Ä‘á»‹nh lÆ°á»£ng kÃ¨m trá»±c quan nháº­n thá»©c Ä‘á»‹nh tÃ­nh, giÃºp cÃ¡c ká»¹ sÆ° Machine Learning náº¯m báº¯t báº£n cháº¥t cá»§a sá»± sinh vÄƒn tá»±.

---

## 1. Truy Xuáº¥t Äá»“ Thá»‹ Dá»¯ Liá»‡u CÃ¡c Token 

Äáº¡i lÆ°á»£ng cá»‘t lÃµi $Z_k$ Ä‘Æ°á»£c khai thÃ¡c á»Ÿ Ä‘Ã¢y lÃ  Logit tráº£ ra tá»« tráº¡ng thÃ¡i khá»‘i transformer cuá»‘i cÃ¹ng cá»§a mÃ´ hÃ¬nh cho má»i biáº¿n token. Trong má»™t chuá»—i cÃ¢u gá»“m $N$ kÃ½ tá»± $t_i$:

$$[t_1, t_2, \dots, t_{N}]$$

MÃ´ hÃ¬nh sáº½ dá»± Ä‘oÃ¡n:

$$ P(t_{i} | t_1\dots t_{i-1}) $$

### 1.1 TÃ­nh ToÃ¡n Softmax XÃ¡c Suáº¥t Cá»§a KÃ½ Tá»± Ngay TrÆ°á»›c ÄÃ³
Bá»Ÿi ngÃ´n ngá»¯ (Language Model) luÃ´n Ä‘Ã³ng vai trÃ² phá»ng Ä‘oÃ¡n Token cho bÆ°á»›c $(t_i)$, ta chá»‰ cÃ³ thá»ƒ sá»­ dá»¥ng vá»‹ trÃ­ $P(t_{i})$ tá»« Ä‘áº§u ra (output array) cá»§a vá»‹ trÃ­ $t_{i-1}$ (Previous Token).
Máº£ng dá»± bÃ¡o Ä‘Æ°á»£c Ä‘Æ°a qua má»™t hÃ m:
$$ \text{LogSoftmax}(Z_i) = \log\left(\frac{e^{Z_i}}{\sum e^{Z_k}}\right) = Z_i - \log\left(\sum e^{Z_k}\right) $$
LogSoftmax á»•n Ä‘á»‹nh sá»‘ há»c vÃ  mang láº¡i sá»± tinh vi vá» khoáº£ng Ä‘á»™, vÃ¬ cÃ¡c xÃ¡c suáº¥t ká» $0$ bá»‹ lÃ m nhÃ²e. GiÃ¡ trá»‹ logit cÃ ng lá»›n hÆ¡n thÃ¬ Ä‘á»™ tÆ°Æ¡ng quan vá»›i giÃ¡ trá»‹ sá»‘ Ã¢m (xÆ°á»›ng lÃªn 0) cÃ ng bÃ© dáº§n. Tá»« Ä‘Ã³, xÃ¡c suáº¥t tá»« token Ä‘Ãºng nháº¥t sáº½ Ä‘Æ°á»£c trÃ­ch xuáº¥t (Indexed target word).

---

## 2. So SÃ¡nh MÃ´ HÃ¬nh Äá»‹nh LÆ°á»£ng Cá»§a Token (Small vs. Large)

Biá»ƒu diá»…n LogSoftmax táº¡i token $t_i$ láº¥y tá»« GPT-2 (Small) so sÃ¡nh trá»±c tiáº¿p (tÆ°Æ¡ng tá»± Ä‘á»“ thá»‹ Unity plot) vá»›i GPT-2 (Large). Cáº£ hai dÃ¹ng $t_1 \dots t_{i-1}$ chuáº©n lÃ m Ä‘áº§u vÃ o:
- **TÆ°Æ¡ng Quan Thá»‘ng KÃª**: Tá»· lá»‡ TÆ°Æ¡ng quan ($\rho$) thÆ°á»ng tháº¥y giá»¯a giÃ¡ trá»‹ LogSoftmax á»Ÿ hai kiáº¿n trÃºc Ä‘áº¡t ngÆ°á»¡ng máº¡nh $\approx 0.94$. á» cáº¥p Ä‘á»™ bá» ngoÃ i, cÃ¡c logits sau chÃ³t tráº£ vá» sá»± khÃ¡c biá»‡t khÃ´ng nhiá»u.
- **SÃ n Biáº¿n Äá»™ng**: GPT-2 Large Ä‘Ã´i khi cho má»©c tá»± tin tháº¥p hÆ¡n hoáº·c cao hÆ¡n tÃ¹y thuá»™c vÃ o ngá»¯ phÃ¡p. Kháº³ng Ä‘á»‹nh "MÃ´ hÃ¬nh cÃ ng lá»›n thÃ¬ Ä‘á»™ tá»± tin dá»± Ä‘oÃ¡n cÃ ng cao má»i láº§n" lÃ  sai láº§m, bá»Ÿi mÃ´ hÃ¬nh nhá» cÅ©ng cÃ³ lÃºc xÃ¡c suáº¥t LogSoftmax rÆ¡i vÃ o má»©c cao cho cÃ¹ng token.
- Náº¿u Input vÃ  Output giá»‘ng nhau, ta cÅ©ng khÃ´ng Ä‘Æ°á»£c quy náº¡p "CÆ¡ cháº¿ tÃ­nh toÃ¡n ná»™i hÃ m giá»‘ng há»‡t nhau". Cáº¥u trÃºc biá»ƒu diá»…n thÃ´ng tin bÃªn trong lÃ  báº¥t Ä‘á»‹nh.

---

## 3. Trá»±c Quan HÃ³a Nhiá»‡t Äá»™ NgÃ´n Ngá»¯ (Text Heatmaps)

Chuá»—i vÄƒn báº£n (vÃ­ dá»¥ "The goal of a correlation...") sáº½ Ä‘Æ°á»£c phá»§ ná»n báº±ng há»‡ sá»‘ RGB chuáº©n lá»›n tá»›i bÃ© ($0 \to 1$) dá»±a trÃªn Ä‘iá»ƒm sá»‘ logit (LogSoftmax values).

### 3.1 Xá»­ LÃ½ Tá»« Vá»±ng Lá»—i (Äáº§u CÃ¢u)
Tá»« Ä‘áº§u tiÃªn ("The") sáº½ khÃ´ng cÃ³ kháº£ nÄƒng nháº­n Ä‘á»‹nh Ä‘á»‹nh lÆ°á»£ng (VÃ¬ á»Ÿ Ä‘Ã³ sá»± tiÃªn tri tÆ°Æ¡ng lai tá»« quÃ¡ khá»© chÆ°a ná»• ra). Äiá»u nÃ y khiáº¿n logit cá»§a chá»¯ Ä‘áº§u trá»Ÿ thÃ nh sá»‘ rá»—ng (0 trÃªn ma tráº­n), nhÆ°ng khi cháº¡y bá»™ Min-Max Scaling (thu giÃ¡ trá»‹ trong khoáº£ng $[0-1]$), thÃ¬ 0 láº¡i Ä‘Æ°á»£c lÃ m trÃ²n thÃ nh $1$, táº¡o máº£ng mÃ u sai sá»± tháº­t lá»›n nháº¥t.
Äá»ƒ sá»­a chá»¯a, vÃ²ng láº·p thÆ°á»ng loáº¡i bá» chá»‰ sá»‘ tá»« vá»‹ trÃ­ thá»© $1$ trá»Ÿ váº¿ khÃ´ng (Skipping the zero-th token).

### 3.2 Giáº£i NghÄ©a Nhiá»‡t Bá»©c Xáº¡ Token Äáº§u Ra

- Nhá»¯ng chuá»—i cá»¥m tá»« khÃ³ cÃ³ tá»« Ä‘i kÃ¨m máº·c Ä‘á»‹nh (vÃ­ dá»¥: "The goal of a..."): Ma tráº­n cÃ³ Ä‘á»™ tá»‘i cá»±c tháº¥p. NÄƒng lá»±c phá»ng Ä‘oÃ¡n rá»i ráº¡c.
- Ngay khi xuáº¥t hiá»‡n tá»« cÃ³ nghÄ©a háº¹p (vÃ­ dá»¥: "...of a correlation..."): Chá»‰ cÃ³ sá»‘ Ã­t tá»« kháº£ thi (vÃ­ dá»¥: *analysis*, *coefficient*), mÃ´ hÃ¬nh cÃ³ tÃ­nh Ä‘oÃ¡n Ä‘Ãºng gáº§n nhÆ° truyá»‡t Ä‘á»‘i (Ma tráº­n kÃ­ch mÃ u nÃ³ng tá»›i max - darkest background).

ÄÃ¢y lÃ  nguyÃªn lÃ½ hÃ¬nh thÃ¡i cá»§a Language Modeling Generation. Lá»i vÄƒn cÃ ng Ã­t tÃ¹y chá»n, mÃ´ hÃ¬nh sáº½ sinh sá»‘ Ä‘o Logit xÃ¡c suáº¥t cá»±c Ä‘á»™, ngÆ°á»£c láº¡i thÃ¬ mÃ u nháº¡t (sá»± Ä‘a dáº¡ng vÄƒn báº£n náº£y máº§m).

---

## TÃ i liá»‡u tham kháº£o

1. **Jawahar, G. et al. (2019).** *What Does BERT Learn about the Structure of Language?* ACL.
2. **Kovaleva, O. et al. (2019).** *Revealing the Dark Secrets of BERT.* EMNLP.
3. **Perez, E. et al. (2022).** *Red Teaming Language Models with Language Models.*
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [ÄÃ¡nh GiÃ¡ Há»™p Äen (Black-box Evaluations) trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_LLM_016_Black box evals.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_016_Black box evals.md) |
| [Red Teaming: Äá»™i Äá» vÃ  Thá»­ Nghiá»‡m Äá»‘i KhÃ¡ng trong AI Safety](aero_LLM_017_Red-teaming.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_017_Red-teaming.md) |
| [Äá»™ ChÃ­nh XÃ¡c, TÃ­nh Máº¡ch Láº¡c vÃ  Sá»± PhÃ¹ Há»£p trong ÄÃ¡nh GiÃ¡ MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_018_Accuracy, coherence, and relevance.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_018_Accuracy, coherence, and relevance.md) |
| [PhÃ¢n Phá»‘i Cá»§a CÃ¡c KÃ­ch Hoáº¡t Tráº¡ng ThÃ¡i áº¨n Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_019_Distributions of hidden-state activations.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_019_Distributions of hidden-state activations.md) |
| [Há»©a Háº¹n vÃ  ThÃ¡ch Thá»©c cá»§a ÄÃ¡nh GiÃ¡ Äá»‹nh LÆ°á»£ng trong MÃ´ HÃ¬nh Há»c MÃ¡y](aero_LLM_01_Promises and challenges of quantitative evaluations.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Promises and challenges of quantitative evaluations.md) |
| [Báº£n Äá»“ Nhiá»‡t Cá»§a Token Cho CÃ¢n Nháº¯c Äá»‹nh TÃ­nh (Text Heatmaps)](aero_LLM_020_Heatmaps of tokens for qualitative inspection.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_020_Heatmaps of tokens for qualitative inspection.md) |
| ğŸ“Œ **[Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: Trá»±c Quan HÃ³a Dá»± ÄoÃ¡n ÄÆ¡n Token](aero_LLM_021_CodeChallenge Visualize single-token predictions.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_021_CodeChallenge Visualize single-token predictions.md) |
| [CÃ¡c Váº¥n Äá» Sá»‘ Há»c trong Logits vÃ  Softmax: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Giáº£i PhÃ¡p á»”n Äá»‹nh](aero_LLM_02_Numerical issues in logits and softmax.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Numerical issues in logits and softmax.md) |
| [Perplexity trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Diá»…n Giáº£i vÃ  Giá»›i Háº¡n](aero_LLM_03_Perplexity.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_Perplexity.md) |
| [aero_LLM_04_CodeChallenge Perplexing perplexities.md](aero_LLM_04_CodeChallenge Perplexing perplexities.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Perplexing perplexities.md) |
| [aero_LLM_05_Masked word prediction accuracy.md](aero_LLM_05_Masked word prediction accuracy.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Masked word prediction accuracy.md) |
| [aero_LLM_06_HellaSwag.md](aero_LLM_06_HellaSwag.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_HellaSwag.md) |
| [aero_LLM_07_Import large models using bitsandbytes.md](aero_LLM_07_Import large models using bitsandbytes.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Import large models using bitsandbytes.md) |
| [aero_LLM_08_CodeChallenge HellaSwag evals in two models (part 1).md](aero_LLM_08_CodeChallenge HellaSwag evals in two models (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge HellaSwag evals in two models (part 1).md) |
| [aero_LLM_09_CodeChallenge HellaSwag evals in two models (part 2).md](aero_LLM_09_CodeChallenge HellaSwag evals in two models (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge HellaSwag evals in two models (part 2).md) |
| [aero_LLM_10_KL (Kullback-Leibler) divergence.md](aero_LLM_10_KL (Kullback-Leibler) divergence.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_KL (Kullback-Leibler) divergence.md) |
| [aero_LLM_11_MAUVE.md](aero_LLM_11_MAUVE.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_MAUVE.md) |
| [aero_LLM_12_CodeChallenge Large and small MAUVE explorations.md](aero_LLM_12_CodeChallenge Large and small MAUVE explorations.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_CodeChallenge Large and small MAUVE explorations.md) |
| [aero_LLM_13_SuperGLUE and other amalgamations.md](aero_LLM_13_SuperGLUE and other amalgamations.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_SuperGLUE and other amalgamations.md) |
| [aero_LLM_14_Assessing bias and fairness.md](aero_LLM_14_Assessing bias and fairness.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_Assessing bias and fairness.md) |
| [aero_LLM_15_Non-technical benchmarks.md](aero_LLM_15_Non-technical benchmarks.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_Non-technical benchmarks.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

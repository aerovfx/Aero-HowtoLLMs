
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [09 Quantitative evaluations](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
ÄÃ¡nh giÃ¡ HellaSwag trÃªn hai mÃ´ hÃ¬nh ngÃ´n ngá»¯: PhÃ¢n tÃ­ch Ä‘á»‹nh lÆ°á»£ng vÃ  so sÃ¡nh xÃ¡c suáº¥t sinh chuá»—i

Tiáº¿p cáº­n log-likelihood, chuáº©n hoÃ¡ Ä‘á»™ dÃ i vÃ  Ã½ nghÄ©a thá»‘ng kÃª

â¸»

TÃ³m táº¯t

BÃ i viáº¿t nÃ y trÃ¬nh bÃ y phÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡ bá»™ dá»¯ liá»‡u HellaSwag trÃªn hai mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs), dá»±a trÃªn ná»™i dung tÃ i liá»‡u Ä‘Ã­nh kÃ¨m. ChÃºng tÃ´i phÃ¢n tÃ­ch cÆ¡ cháº¿ tÃ­nh Ä‘iá»ƒm báº±ng log-likelihood, chuáº©n hoÃ¡ theo Ä‘á»™ dÃ i chuá»—i, vÃ  phÆ°Æ¡ng phÃ¡p tÃ­nh accuracy. BÃ i viáº¿t má»Ÿ rá»™ng vá»›i cÃ¡c ná»n táº£ng lÃ½ thuyáº¿t tá»« nghiÃªn cá»©u cá»§a Rowan Zellers et al. (2019), kiáº¿n trÃºc Transformer cá»§a Ashish Vaswani et al. (2017), vÃ  cÃ¡c phÃ¢n tÃ­ch scaling tá»« OpenAI.

â¸»

1. Giá»›i thiá»‡u

HellaSwag lÃ  bá»™ benchmark Ä‘o lÆ°á»ng kháº£ nÄƒng:
	â€¢	Suy luáº­n thÆ°á»ng thá»©c (commonsense reasoning)
	â€¢	Hiá»ƒu chuá»—i hÃ nh Ä‘á»™ng váº­t lÃ½
	â€¢	PhÃ¢n biá»‡t continuation há»£p lÃ½ vÃ  phi lÃ½

Trong bÃ i toÃ¡n nÃ y, má»—i cÃ¢u há»i gá»“m:
	â€¢	Ngá»¯ cáº£nh c
	â€¢	4 lá»±a chá»n hoÃ n thÃ nh \{a_1, a_2, a_3, a_4\}

Má»¥c tiÃªu: chá»n Ä‘Ã¡p Ã¡n cÃ³ xÃ¡c suáº¥t cao nháº¥t theo mÃ´ hÃ¬nh.

â¸»

2. MÃ´ hÃ¬nh xÃ¡c suáº¥t cho bÃ i toÃ¡n multiple choice

Vá»›i mÃ´ hÃ¬nh tá»± há»“i quy (autoregressive), xÃ¡c suáº¥t cá»§a má»™t Ä‘Ã¡p Ã¡n Ä‘Æ°á»£c tÃ­nh:

P(a_i \mid c) = \prod_{t=1}^{T_i} P(w_t \mid c, w_{<t})

Trong thá»±c nghiá»‡m, ta dÃ¹ng log Ä‘á»ƒ trÃ¡nh underflow:

\log P(a_i \mid c) = \sum_{t=1}^{T_i} \log P(w_t \mid c, w_{<t})

â¸»

3. Váº¥n Ä‘á» thiÃªn lá»‡ch Ä‘á»™ dÃ i (Length Bias)

Náº¿u dÃ¹ng tá»•ng log-likelihood trá»±c tiáº¿p:
	â€¢	Chuá»—i dÃ i â†’ log nhá» hÆ¡n (Ã¢m hÆ¡n)
	â€¢	Chuá»—i ngáº¯n â†’ Ä‘Æ°á»£c Æ°u tiÃªn

Do Ä‘Ã³ cáº§n chuáº©n hoÃ¡:

Score(a_i) = \frac{1}{T_i} \sum_{t=1}^{T_i} \log P(w_t \mid c, w_{<t})

ÄÃ¢y lÃ  ave18-RAGe log-probability.

â¸»

4. Quy táº¯c chá»n Ä‘Ã¡p Ã¡n

\hat{a} = \arg\max_{a_i} Score(a_i)

Accuracy Ä‘Æ°á»£c tÃ­nh:

Accuracy = \frac{1}{N} \sum_{j=1}^{N} \mathbf{1}(\hat{a}^{(j)} = a_{\text{true}}^{(j)})

Baseline ngáº«u nhiÃªn:

P_{\text{random}} = 25\%

â¸»

5. So sÃ¡nh hai mÃ´ hÃ¬nh

Giáº£ sá»­ hai mÃ´ hÃ¬nh:
	â€¢	M_1
	â€¢	M_2

Accuracy tÆ°Æ¡ng á»©ng:

\hat{p}_1, \hat{p}_2

Sai sá»‘ chuáº©n:

SE = \sqrt{\frac{\hat{p}(1-\hat{p})}{N}}

Kiá»ƒm Ä‘á»‹nh sá»± khÃ¡c biá»‡t:

z = \frac{\hat{p}_1 - \hat{p}_2}{\sqrt{SE_1^2 + SE_2^2}}

Náº¿u:

|z| > 1.96

â†’ khÃ¡c biá»‡t cÃ³ Ã½ nghÄ©a thá»‘ng kÃª (95%).

â¸»

6. LiÃªn há»‡ vá»›i Self-Attention

Transformer sá»­ dá»¥ng cÆ¡ cháº¿:

Attention(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V

CÆ¡ cháº¿ nÃ y giÃºp mÃ´ hÃ¬nh:
	â€¢	Theo dÃµi quan há»‡ dÃ i háº¡n
	â€¢	LiÃªn káº¿t hÃ nh Ä‘á»™ng trÆ°á»›câ€“sau
	â€¢	PhÃ¡t hiá»‡n logic váº­t lÃ½ ngáº§m Ä‘á»‹nh

â¸»

7. So sÃ¡nh vá»›i Perplexity

Perplexity Ä‘o kháº£ nÄƒng dá»± Ä‘oÃ¡n token káº¿ tiáº¿p:

PP = \exp\left(- \frac{1}{N} \sum \log P(w_i)\right)

Trong khi HellaSwag Ä‘o:
	â€¢	So sÃ¡nh chuá»—i hoÃ n chá»‰nh
	â€¢	Kháº£ nÄƒng reasoning cáº¥p cao

Má»™t mÃ´ hÃ¬nh cÃ³ perplexity tháº¥p chÆ°a cháº¯c cÃ³ accuracy cao trÃªn HellaSwag.

â¸»

8. PhÃ¢n tÃ­ch scaling

Theo luáº­t scaling cá»§a OpenAI:

Loss(N) = A N^{-\alpha} + B

Khi tÄƒng sá»‘ tham sá»‘ N:
	â€¢	Log-likelihood tÄƒng
	â€¢	Accuracy trÃªn HellaSwag tÄƒng theo hÃ m lÅ©y thá»«a

â¸»

9. Háº¡n cháº¿ cá»§a phÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡

9.1 Shortcut Learning

MÃ´ hÃ¬nh cÃ³ thá»ƒ:
	â€¢	Há»c phong cÃ¡ch cÃ¢u há»£p lÃ½
	â€¢	KhÃ´ng thá»±c sá»± hiá»ƒu váº­t lÃ½

â¸»

9.2 Dataset Saturation

Náº¿u fine-tune trá»±c tiáº¿p trÃªn HellaSwag:

D_{train} \cap D_{test} \neq \varnothing

â†’ KhÃ´ng cÃ²n pháº£n Ã¡nh nÄƒng lá»±c tá»•ng quÃ¡t.

â¸»

9.3 Calibration

MÃ´ hÃ¬nh cÃ³ thá»ƒ:
	â€¢	Chá»n Ä‘Ãºng
	â€¢	NhÆ°ng xÃ¡c suáº¥t khÃ´ng cao

Äo calibration:

ECE = \sum_{m=1}^{M} \frac{|B_m|}{n} |acc(B_m) - conf(B_m)|

â¸»

10. Ã nghÄ©a thá»±c tiá»…n

ÄÃ¡nh giÃ¡ HellaSwag giÃºp:
	â€¢	So sÃ¡nh mÃ´ hÃ¬nh trÆ°á»›c khi fine-tune
	â€¢	ÄÃ¡nh giÃ¡ kháº£ nÄƒng reasoning
	â€¢	Kiá»ƒm tra hiá»‡u quáº£ scaling

Trong pipeline triá»ƒn khai thá»±c táº¿, cáº§n káº¿t há»£p:
	â€¢	Accuracy
	â€¢	Log-likelihood
	â€¢	Calibration
	â€¢	Robustness test

â¸»

11. Káº¿t luáº­n

ÄÃ¡nh giÃ¡ HellaSwag trÃªn hai mÃ´ hÃ¬nh yÃªu cáº§u:
	â€¢	TÃ­nh log-likelihood chÃ­nh xÃ¡c
	â€¢	Chuáº©n hoÃ¡ Ä‘á»™ dÃ i
	â€¢	So sÃ¡nh thá»‘ng kÃª

Benchmark nÃ y khÃ´ng chá»‰ Ä‘o fluency mÃ  Ä‘o kháº£ nÄƒng suy luáº­n hÃ nh Ä‘á»™ng, do Ä‘Ã³ quan trá»ng trong Ä‘Ã¡nh giÃ¡ LLM hiá»‡n Ä‘áº¡i.

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Zellers, R. et al. (2019). HellaSwag: Can a Machine Really Finish Your Sentence?
	2.	Vaswani, A. et al. (2017). Attention is All You Need.
	3.	Brown et al. (2020). Language Models are Few-Shot Learners.
	4.	Kaplan et al. (2020). Scaling Laws for Neural Language Models.
	5.	Jurafsky & Martin. Speech and Language Processing.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [ÄÃ¡nh GiÃ¡ Há»™p Äen (Black-box Evaluations) trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_LLM_016_Black box evals.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_016_Black box evals.md) |
| [Red Teaming: Äá»™i Äá» vÃ  Thá»­ Nghiá»‡m Äá»‘i KhÃ¡ng trong AI Safety](aero_LLM_017_Red-teaming.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_017_Red-teaming.md) |
| [Äá»™ ChÃ­nh XÃ¡c, TÃ­nh Máº¡ch Láº¡c vÃ  Sá»± PhÃ¹ Há»£p trong ÄÃ¡nh GiÃ¡ MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_018_Accuracy, coherence, and relevance.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_018_Accuracy, coherence, and relevance.md) |
| [PhÃ¢n Phá»‘i Cá»§a CÃ¡c KÃ­ch Hoáº¡t Tráº¡ng ThÃ¡i áº¨n Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_019_Distributions of hidden-state activations.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_019_Distributions of hidden-state activations.md) |
| [Há»©a Háº¹n vÃ  ThÃ¡ch Thá»©c cá»§a ÄÃ¡nh GiÃ¡ Äá»‹nh LÆ°á»£ng trong MÃ´ HÃ¬nh Há»c MÃ¡y](aero_LLM_01_Promises and challenges of quantitative evaluations.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Promises and challenges of quantitative evaluations.md) |
| [Báº£n Äá»“ Nhiá»‡t Cá»§a Token Cho CÃ¢n Nháº¯c Äá»‹nh TÃ­nh (Text Heatmaps)](aero_LLM_020_Heatmaps of tokens for qualitative inspection.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_020_Heatmaps of tokens for qualitative inspection.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: Trá»±c Quan HÃ³a Dá»± ÄoÃ¡n ÄÆ¡n Token](aero_LLM_021_CodeChallenge Visualize single-token predictions.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_021_CodeChallenge Visualize single-token predictions.md) |
| [CÃ¡c Váº¥n Äá» Sá»‘ Há»c trong Logits vÃ  Softmax: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Giáº£i PhÃ¡p á»”n Äá»‹nh](aero_LLM_02_Numerical issues in logits and softmax.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Numerical issues in logits and softmax.md) |
| [Perplexity trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Diá»…n Giáº£i vÃ  Giá»›i Háº¡n](aero_LLM_03_Perplexity.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_Perplexity.md) |
| [aero_LLM_04_CodeChallenge Perplexing perplexities.md](aero_LLM_04_CodeChallenge Perplexing perplexities.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Perplexing perplexities.md) |
| [aero_LLM_05_Masked word prediction accuracy.md](aero_LLM_05_Masked word prediction accuracy.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Masked word prediction accuracy.md) |
| [aero_LLM_06_HellaSwag.md](aero_LLM_06_HellaSwag.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_HellaSwag.md) |
| [aero_LLM_07_Import large models using bitsandbytes.md](aero_LLM_07_Import large models using bitsandbytes.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Import large models using bitsandbytes.md) |
| ğŸ“Œ **[aero_LLM_08_CodeChallenge HellaSwag evals in two models (part 1).md](aero_LLM_08_CodeChallenge HellaSwag evals in two models (part 1).md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge HellaSwag evals in two models (part 1).md) |
| [aero_LLM_09_CodeChallenge HellaSwag evals in two models (part 2).md](aero_LLM_09_CodeChallenge HellaSwag evals in two models (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge HellaSwag evals in two models (part 2).md) |
| [aero_LLM_10_KL (Kullback-Leibler) divergence.md](aero_LLM_10_KL (Kullback-Leibler) divergence.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_KL (Kullback-Leibler) divergence.md) |
| [aero_LLM_11_MAUVE.md](aero_LLM_11_MAUVE.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_MAUVE.md) |
| [aero_LLM_12_CodeChallenge Large and small MAUVE explorations.md](aero_LLM_12_CodeChallenge Large and small MAUVE explorations.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_CodeChallenge Large and small MAUVE explorations.md) |
| [aero_LLM_13_SuperGLUE and other amalgamations.md](aero_LLM_13_SuperGLUE and other amalgamations.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_SuperGLUE and other amalgamations.md) |
| [aero_LLM_14_Assessing bias and fairness.md](aero_LLM_14_Assessing bias and fairness.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_Assessing bias and fairness.md) |
| [aero_LLM_15_Non-technical benchmarks.md](aero_LLM_15_Non-technical benchmarks.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_Non-technical benchmarks.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [09 Quantitative evaluations](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
DÆ°á»›i Ä‘Ã¢y lÃ  bÃ i viáº¿t khoa há»c Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn ná»™i dung tÃ i liá»‡u â€œCode Challenge: Perplexing Perplexitiesâ€, káº¿t há»£p má»Ÿ rá»™ng lÃ½ thuyáº¿t tá»« cÃ¡c cÃ´ng trÃ¬nh ná»n táº£ng vá» mÃ´ hÃ¬nh ngÃ´n ngá»¯ vÃ  lÃ½ thuyáº¿t thÃ´ng tin, trÃ¬nh bÃ y dÆ°á»›i dáº¡ng Markdown vÃ  cÃ³ bá»• sung cÃ¡c cÃ´ng thá»©c toÃ¡n há»c minh há»a.

â¸»

Perplexing Perplexities: PhÃ¢n TÃ­ch SÃ¢u vá» Äá»™ Rá»‘i trong ÄÃ¡nh GiÃ¡ MÃ´ HÃ¬nh NgÃ´n Ngá»¯

TÃ³m táº¯t

Perplexity lÃ  chá»‰ sá»‘ chuáº©n trong Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh ngÃ´n ngá»¯ xÃ¡c suáº¥t. Tuy nhiÃªn, cÃ¡ch tÃ­nh vÃ  diá»…n giáº£i perplexity thÆ°á»ng gÃ¢y hiá»ƒu nháº§m, Ä‘áº·c biá»‡t khi thay Ä‘á»•i tokenization, Ä‘á»™ dÃ i chuá»—i hoáº·c miá»n dá»¯ liá»‡u. BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch sÃ¢u báº£n cháº¥t toÃ¡n há»c cá»§a perplexity, cÃ¡c trÆ°á»ng há»£p â€œnghá»‹ch lÃ½â€ (perplexing cases), vÃ  giá»›i háº¡n khi Ã¡p dá»¥ng trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡n Ä‘áº¡i. Ná»n táº£ng lÃ½ thuyáº¿t dá»±a trÃªn cÃ´ng trÃ¬nh cá»§a Claude Shannon, Christopher D. Manning vÃ  Yoshua Bengio.

â¸»

1. CÆ¡ sá»Ÿ ToÃ¡n há»c cá»§a Perplexity

1.1 MÃ´ hÃ¬nh ngÃ´n ngá»¯ xÃ¡c suáº¥t

Vá»›i chuá»—i tá»«:

w_1, w_2, \dots, w_T

XÃ¡c suáº¥t toÃ n chuá»—i:

P(w_1^T)
=
\prod_{t=1}^{T}
P(w_t | w_1^{t-1})

Log-likelihood trung bÃ¬nh:

\ell
=
\frac{1}{T}
\sum_{t=1}^{T}
\log P(w_t | w_1^{t-1})

â¸»

1.2 Entropy vÃ  Cross-Entropy

Entropy:

H(p)
=
-
\sum_x
p(x)\log p(x)

Cross-entropy thá»±c nghiá»‡m:

\hat{H}
=
-
\frac{1}{T}
\sum_{t=1}^{T}
\log P(w_t | context)

â¸»

1.3 Äá»‹nh nghÄ©a Perplexity

PP
=
\exp(\hat{H})
=
\exp
\left(
-
\frac{1}{T}
\sum_{t=1}^{T}
\log P(w_t | context)
\right)

Hoáº·c:

PP = e^{-\ell}

â¸»

2. Nhá»¯ng TrÆ°á»ng Há»£p â€œPerplexingâ€

2.1 Perplexity tháº¥p nhÆ°ng vÄƒn báº£n kÃ©m tá»± nhiÃªn

Perplexity Ä‘o:

P(data | model)

KhÃ´ng Ä‘o:
	â€¢	TÃ­nh sÃ¡ng táº¡o
	â€¢	TÃ­nh logic
	â€¢	TÃ­nh Ä‘Ãºng sá»± tháº­t

Má»™t mÃ´ hÃ¬nh cÃ³ thá»ƒ tá»‘i Æ°u likelihood nhÆ°ng sinh vÄƒn báº£n láº·p láº¡i.

â¸»

2.2 Phá»¥ thuá»™c Tokenization

Giáº£ sá»­ cÃ¹ng má»™t cÃ¢u:
	â€¢	Tokenization A â†’ T_A token
	â€¢	Tokenization B â†’ T_B token

VÃ¬:

PP =
\exp
\left(
\frac{\mathcal{L}}{T}
\right)

Náº¿u T thay Ä‘á»•i â†’ PP thay Ä‘á»•i.

Do Ä‘Ã³:

PP_A \neq PP_B

Ngay cáº£ khi mÃ´ hÃ¬nh tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá» xÃ¡c suáº¥t chuá»—i.

â¸»

2.3 áº¢nh hÆ°á»Ÿng Äá»™ DÃ i Chuá»—i

Vá»›i chuá»—i ráº¥t dÃ i:

\hat{H}
\rightarrow
H

Theo luáº­t sá»‘ lá»›n.

Vá»›i chuá»—i ngáº¯n:

Var(\hat{H})
=
\frac{\sigma^2}{T}

Perplexity khÃ´ng á»•n Ä‘á»‹nh khi T nhá».

â¸»

3. PhÃ¢n tÃ­ch Thá»‘ng kÃª

3.1 Sai sá»‘ chuáº©n

Náº¿u entropy Æ°á»›c lÆ°á»£ng:

SE(H)
=
\frac{\sigma}{\sqrt{T}}

Khoáº£ng tin cáº­y 95%:

\hat{H}
\pm
1.96 \cdot SE(H)

Tá»« Ä‘Ã³:

PP_{CI}
=
\exp(\hat{H} \pm 1.96 SE)

â¸»

3.2 LiÃªn há»‡ vá»›i KL-Divergence

H(p,q)
=
H(p)
+
D_{KL}(p||q)

Perplexity:

PP
=
\exp(H(p) + D_{KL}(p||q))

Tá»‘i thiá»ƒu khi:

q = p

â¸»

4. PhÃ¢n tÃ­ch CÃ¡c TÃ¬nh Huá»‘ng Code Challenge

Tá»« bÃ i thá»±c hÃ nh:

TrÆ°á»ng há»£p 1: Dá»± Ä‘oÃ¡n Ä‘á»u

Náº¿u:

P(w) = \frac{1}{V}

ThÃ¬:

H = \log V

PP = V

â†’ Perplexity báº±ng kÃ­ch thÆ°á»›c tá»« vá»±ng.

â¸»

TrÆ°á»ng há»£p 2: Dá»± Ä‘oÃ¡n hoÃ n háº£o

Náº¿u:

P(w_t) = 1

H = 0

PP = 1

â¸»

TrÆ°á»ng há»£p 3: Sai hoÃ n toÃ n

Náº¿u:

P(w_t) \rightarrow 0

H \rightarrow \infty

PP \rightarrow \infty

â¸»

5. Perplexity vÃ  Softmax

Trong mÃ´ hÃ¬nh neural:

z_t = W h_t

P(w_t | context)
=
\frac{\exp(z_{t,w})}
{\sum_j \exp(z_{t,j})}

Cross-entropy loss:

\mathcal{L}
=
-
\sum_t
\log P(w_t)

Perplexity:

PP
=
\exp
\left(
\frac{\mathcal{L}}{T}
\right)

â¸»

6. Perplexity trong LLMs Hiá»‡n Ä‘áº¡i

Trong cÃ¡c mÃ´ hÃ¬nh lá»›n:
	â€¢	Instruction tuning
	â€¢	RLHF
	â€¢	Fine-tuning theo nhiá»‡m vá»¥

CÃ³ thá»ƒ xáº£y ra:

PP_{instruction}
>
PP_{base}

NhÆ°ng cháº¥t lÆ°á»£ng há»™i thoáº¡i tá»‘t hÆ¡n.

Äiá»u nÃ y cho tháº¥y perplexity khÃ´ng Ä‘o Ä‘Æ°á»£c alignment vá»›i ngÆ°á»i dÃ¹ng.

â¸»

7. PhÃ¢n tÃ­ch Giá»›i háº¡n LÃ½ thuyáº¿t

Perplexity tá»‘i Æ°u hÃ³a:

\min_\theta
D_{KL}(p||q_\theta)

KhÃ´ng tá»‘i Æ°u hÃ³a:
	â€¢	Utility
	â€¢	Human preference
	â€¢	Task-specific reward

Theo nguyÃªn lÃ½ Goodhart:

Khi má»™t chá»‰ sá»‘ trá»Ÿ thÃ nh má»¥c tiÃªu tá»‘i Æ°u, nÃ³ cÃ³ thá»ƒ máº¥t Ä‘i Ã½ nghÄ©a ban Ä‘áº§u.

â¸»

8. Káº¿t luáº­n

Perplexity lÃ :

PP = e^{H}

Má»™t thÆ°á»›c Ä‘o cháº·t cháº½ dá»±a trÃªn lÃ½ thuyáº¿t thÃ´ng tin.

NÃ³ há»¯u Ã­ch Ä‘á»ƒ:
	â€¢	So sÃ¡nh mÃ´ hÃ¬nh xÃ¡c suáº¥t
	â€¢	Theo dÃµi quÃ¡ trÃ¬nh huáº¥n luyá»‡n
	â€¢	PhÃ¡t hiá»‡n overfitting

Tuy nhiÃªn:
	â€¢	Phá»¥ thuá»™c tokenization
	â€¢	KhÃ´ng Ä‘o ngá»¯ nghÄ©a sÃ¢u
	â€¢	KhÃ´ng pháº£n Ã¡nh alignment

Do Ä‘Ã³, perplexity nÃªn Ä‘Æ°á»£c dÃ¹ng nhÆ° chá»‰ sá»‘ cÆ¡ sá»Ÿ, káº¿t há»£p vá»›i Ä‘Ã¡nh giÃ¡ Ä‘á»‹nh tÃ­nh vÃ  task-specific metrics Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n mÃ´ hÃ¬nh ngÃ´n ngá»¯.

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Shannon, C. E. (1948). A Mathematical Theory of Communication.
	2.	Manning, C. D., & SchÃ¼tze, H. (1999). Foundations of Statistical Natural Language Processing.
	3.	Bengio, Y. et al. (2003). A Neural Probabilistic Language Model.
	4.	Jurafsky, D., & Martin, J. H. (Speech and Language Processing).
	5.	Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [09 Quantitative evaluations](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Red Teaming: Äá»™i Äá» vÃ  Thá»­ Nghiá»‡m Äá»‘i KhÃ¡ng trong AI Safety

## TÃ³m táº¯t

Red Teaming (Äá»™i Äá») lÃ  má»™t quy trÃ¬nh Ä‘Ã¡nh giÃ¡ báº£o máº­t cÃ³ há»‡ thá»‘ng, Ä‘á»‘i khÃ¡ng vÃ  chuyÃªn sÃ¢u, nháº±m tÃ¬m kiáº¿m cÃ¡c lá»— há»•ng cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM). KhÃ¡c vá»›i Ä‘Ã¡nh giÃ¡ há»™p Ä‘en thÃ´ng thÆ°á»ng, Red Teaming Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi cÃ¡c chuyÃªn gia báº£o máº­t vá»›i má»¥c tiÃªu cá»¥ thá»ƒ vÃ  phÆ°Æ¡ng phÃ¡p luáº­n cháº·t cháº½. BÃ i viáº¿t phÃ¢n tÃ­ch sá»± khÃ¡c biá»‡t giá»¯a Red Teaming vÃ  Black-box evals, quy trÃ¬nh triá»ƒn khai vÃ  táº§m quan trá»ng cá»§a nÃ³ trong viá»‡c Ä‘áº£m báº£o an toÃ n AI.

---

## 1. Äá»‹nh nghÄ©a vÃ  Báº£n cháº¥t cá»§a Red Teaming

Red Teaming cÃ³ nguá»“n gá»‘c tá»« lÄ©nh vá»±c quÃ¢n sá»± vÃ  an ninh máº¡ng, nÆ¡i má»™t nhÃ³m chuyÃªn gia (Äá»™i Äá») Ä‘Ã³ng vai khÃ¡ch hÃ ng hoáº·c tin táº·c Ä‘á»ƒ táº¥n cÃ´ng vÃ o há»‡ thá»‘ng cá»§a chÃ­nh mÃ¬nh nháº±m phÃ¡t hiá»‡n Ä‘iá»ƒm yáº¿u.

Trong bá»‘i cáº£nh LLM, Red Teaming táº­p trung vÃ o:
- **TÃ­nh Ä‘á»‘i khÃ¡ng (Adversarial):** TÃ¬m cÃ¡ch Ã©p mÃ´ hÃ¬nh vi pháº¡m cÃ¡c nguyÃªn táº¯c Ä‘áº¡o Ä‘á»©c vÃ  an toÃ n.
- **TÃ­nh chuyÃªn nghiá»‡p:** ÄÆ°á»£c thá»±c hiá»‡n bá»Ÿi cÃ¡c chuyÃªn gia cÃ³ kinh nghiá»‡m vá» khoa há»c mÃ¡y tÃ­nh vÃ  an ninh máº¡ng.
- **TÃ­nh má»¥c tiÃªu:** Táº­p trung vÃ o cÃ¡c rá»§i ro cá»¥ thá»ƒ nhÆ° quyá»n riÃªng tÆ°, mÃ£ Ä‘á»™c, hoáº·c thÃ´ng tin sai lá»‡ch cá»±c Ä‘oan.

---

## 2. So sÃ¡nh Red Teaming vÃ  Black-box Evaluations

| Äáº·c Ä‘iá»ƒm | Black-box Evaluations | Red Teaming |
| :--- | :--- | :--- |
| **Äá»‘i tÆ°á»£ng thá»±c hiá»‡n** | NgÆ°á»i dÃ¹ng phá»• thÃ´ng, nhÃ  nghiÃªn cá»©u | ChuyÃªn gia báº£o máº­t Ä‘Æ°á»£c thuÃª |
| **Má»©c Ä‘á»™ truy cáº­p** | HoÃ n toÃ n khÃ´ng (chá»‰ dÃ¹ng prompt) | ThÆ°á»ng cÃ³ quyá»n tiáº¿p cáº­n má»™t pháº§n (Gray Box) |
| **TÃ­nh phÆ°Æ¡ng phÃ¡p** | Ngáº«u nhiÃªn, dá»±a trÃªn sá»± tÃ² mÃ² | CÃ³ há»‡ thá»‘ng, nghiÃªm ngáº·t vÃ  bÃ i báº£n |
| **Má»¥c tiÃªu** | Bias, tÃ­nh cÃ´ng báº±ng, lá»—i logic | Báº£o máº­t, quyá»n riÃªng tÆ°, báº» khÃ³a há»‡ thá»‘ng |
| **Thá»i Ä‘iá»ƒm** | LiÃªn tá»¥c sau khi phÃ¡t hÃ nh | TrÆ°á»›c khi phÃ¡t hÃ nh hoáº·c theo Ä‘á»‹nh ká»³ |

---

## 3. CÃ¡c PhÆ°Æ¡ng PhÃ¡p Táº¥n cÃ´ng Äá»‘i khÃ¡ng

Red Teaming khÃ´ng chá»‰ dá»«ng láº¡i á»Ÿ viá»‡c Ä‘áº·t cÃ¢u há»i, mÃ  cÃ²n bao gá»“m cÃ¡c ká»¹ thuáº­t phá»©c táº¡p hÆ¡n:
- **Social Engineering:** Táº¥n cÃ´ng vÃ o cÃ¡c ká»¹ sÆ° phÃ¡t triá»ƒn Ä‘á»ƒ tÃ¬m kiáº¿m sÆ¡ há»Ÿ trong quy trÃ¬nh váº­n hÃ nh.
- **Hacking Infrastructure:** Thá»­ nghiá»‡m xÃ¢m nháº­p vÃ o mÃ¡y chá»§ chá»©a mÃ´ hÃ¬nh Ä‘á»ƒ can thiá»‡p vÃ o dá»¯ liá»‡u hoáº·c tham sá»‘.
- **Adversarial Prompting:** Sá»­ dá»¥ng cÃ¡c thuáº­t toÃ¡n tá»± Ä‘á»™ng Ä‘á»ƒ táº¡o ra hÃ ng triá»‡u chuá»—i kÃ½ tá»± nháº±m tÃ¬m ra "Ä‘iá»ƒm mÃ¹" cá»§a mÃ´ hÃ¬nh.

---

## 4. Táº§m quan trá»ng trong An toÃ n AI

Viá»‡c sá»­ dá»¥ng cÃ¡c bá»™ dá»¯ liá»‡u an toÃ n nhÆ° *Harmless and Helpful datasets* tá»« Anthropic lÃ  má»™t vÃ­ dá»¥ vá» viá»‡c sá»­ dá»¥ng káº¿t quáº£ tá»« Red Teaming Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh (thÃ´ng qua RLHF).

CÃ´ng thá»©c Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ rá»§i ro thÆ°á»ng Ä‘Æ°á»£c xem xÃ©t qua xÃ¡c suáº¥t mÃ´ hÃ¬nh bá»‹ thao tÃºng:

$$R = P(\text{Lá»— há»•ng}) \times \text{TÃ¡c Ä‘á»™ng (Impact)}$$

Red Teaming giÃºp giáº£m thiá»ƒu $P(\text{Lá»— há»•ng})$ báº±ng cÃ¡ch cung cáº¥p dá»¯ liá»‡u Ä‘á»‘i khÃ¡ng Ä‘á»ƒ mÃ´ hÃ¬nh há»c cÃ¡ch tá»« chá»‘i cÃ¡c yÃªu cáº§u Ä‘á»™c háº¡i.

---

## TÃ i liá»‡u tham kháº£o

1. **Ganguli, D., et al. (2022).** *Red Teaming Language Models to Reduce Harms.* arXiv preprint arXiv:2209.07858.
2. **Perez, E., et al. (2022).** *Red Teaming Language Models with Language Models.*
3. **Anthropic (2022).** *A General Language Assistant as a Laboratory for Alignment.*
4. **Ziegler, D. M., et al. (2019).** *Fine-Tuning Language Models from Human Preferences.*
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [09 Quantitative evaluations](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# CÃ¡c Váº¥n Äá» Sá»‘ Há»c trong Logits vÃ  Softmax: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Giáº£i PhÃ¡p á»”n Äá»‹nh

TÃ³m táº¯t

Trong cÃ¡c mÃ´ hÃ¬nh phÃ¢n loáº¡i vÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯, hÃ m softmax Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ chuyá»ƒn logits thÃ nh phÃ¢n phá»‘i xÃ¡c suáº¥t. Tuy nhiÃªn, khi logits cÃ³ giÃ¡ trá»‹ lá»›n hoáº·c ráº¥t nhá», cÃ¡c váº¥n Ä‘á» sá»‘ há»c nhÆ° overflow, underflow vÃ  máº¥t á»•n Ä‘á»‹nh gradient cÃ³ thá»ƒ xáº£y ra. BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch báº£n cháº¥t toÃ¡n há»c cá»§a nhá»¯ng váº¥n Ä‘á» nÃ y, chá»‰ ra nguyÃªn nhÃ¢n tá»« biá»ƒu diá»…n sá»‘ dáº¥u cháº¥m Ä‘á»™ng (floating-point), vÃ  trÃ¬nh bÃ y cÃ¡c ká»¹ thuáº­t á»•n Ä‘á»‹nh nhÆ° log-sum-exp trick. Ná»™i dung Ä‘Æ°á»£c má»Ÿ rá»™ng tá»« cÃ¡c tÃ i liá»‡u kinh Ä‘iá»ƒn cá»§a Ian Goodfellow, Yoshua Bengio vÃ  Geoffrey Hinton.

â¸»

1. Giá»›i thiá»‡u

Trong bÃ i toÃ¡n phÃ¢n loáº¡i nhiá»u lá»›p, mÃ´ hÃ¬nh xuáº¥t ra má»™t vector logits:

\mathbf{z} = (z_1, z_2, \dots, z_K)

Softmax chuyá»ƒn logits thÃ nh xÃ¡c suáº¥t:

\sigma(z_i)
=
\frac{\exp(z_i)}
{\sum_{j=1}^{K} \exp(z_j)}

Tuy nhiÃªn, khi z_i cÃ³ Ä‘á»™ lá»›n lá»›n (|z| >> 1), phÃ©p tÃ­nh \exp(z_i) cÃ³ thá»ƒ gÃ¢y lá»—i sá»‘ há»c.

â¸»

2. PhÃ¢n tÃ­ch Váº¥n Ä‘á» Overflow vÃ  Underflow

2.1 Biá»ƒu diá»…n sá»‘ dáº¥u cháº¥m Ä‘á»™ng

Trong chuáº©n IEEE 754 (float32):

\exp(88.7) \approx 3.4 \times 10^{38}

Náº¿u:

z_i > 88

â†’ overflow (vÆ°á»£t quÃ¡ kháº£ nÄƒng biá»ƒu diá»…n).

NgÆ°á»£c láº¡i:

\exp(-100) \approx 3.7 \times 10^{-44}

â†’ underflow (gáº§n 0).

â¸»

2.2 VÃ­ dá»¥ minh há»a

Giáº£ sá»­:

\mathbf{z} = (1000, 1001, 999)

Ta cÃ³:

\exp(1000) = \infty

Khi Ä‘Ã³:

\sigma(z_i)
=
\frac{\infty}{\infty}

â†’ KhÃ´ng xÃ¡c Ä‘á»‹nh (NaN).

â¸»

3. Log-Sum-Exp Trick

Äá»ƒ trÃ¡nh overflow, ta trá»« Ä‘i giÃ¡ trá»‹ lá»›n nháº¥t:

\sigma(z_i)
=
\frac{\exp(z_i - z_{max})}
{\sum_j \exp(z_j - z_{max})}

Trong Ä‘Ã³:

z_{max} = \max_j z_j

VÃ¬:

\exp(z_i - z_{max}) \le 1

â†’ Ä‘áº£m báº£o á»•n Ä‘á»‹nh sá»‘ há»c.

â¸»

3.1 Dáº¡ng log-softmax

Trong nhiá»u thÆ° viá»‡n, ta dÃ¹ng:

\log \sigma(z_i)
=
z_i
-
\log
\left(
\sum_j \exp(z_j)
\right)

Ãp dá»¥ng log-sum-exp:

\log
\left(
\sum_j \exp(z_j)
\right)
=
z_{max}
+
\log
\left(
\sum_j \exp(z_j - z_{max})
\right)

â¸»

4. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Gradient

Cross-entropy loss:

\mathcal{L}
=
-
\sum_i y_i \log \sigma(z_i)

Gradient:

\frac{\partial \mathcal{L}}{\partial z_i}
=
\sigma(z_i) - y_i

Náº¿u softmax khÃ´ng á»•n Ä‘á»‹nh â†’ gradient NaN â†’ lan truyá»n lá»—i qua backpropagation.

â¸»

5. Saturation vÃ  Vanishing Gradient

Khi má»™t logit ráº¥t lá»›n:

z_k \gg z_j

Ta cÃ³:

\sigma(z_k) \approx 1
\quad
\sigma(z_j) \approx 0

Gradient:

\frac{\partial \mathcal{L}}{\partial z_k}
=
1 - y_k

Náº¿u dá»± Ä‘oÃ¡n Ä‘Ãºng vÃ  tá»± tin cao â†’ gradient gáº§n 0 â†’ há»c cháº­m.

â¸»

6. PhÃ¢n tÃ­ch Äiá»u kiá»‡n Sá»‘

Äá»™ Ä‘iá»u kiá»‡n (condition number):

\kappa =
\frac{\max |z_i|}
{\min |z_i|}

Khi \kappa lá»›n â†’ dá»… máº¥t á»•n Ä‘á»‹nh.

Trong mÃ´ hÃ¬nh lá»›n (LLMs):

z_i = \mathbf{w}_i^\top \mathbf{h}

Náº¿u:

||\mathbf{w}_i||, ||\mathbf{h}|| \rightarrow lá»›n

â†’ logits tÄƒng â†’ nguy cÆ¡ overflow.

â¸»

7. Mixed Precision Training

Khi dÃ¹ng float16:

\exp(11) \approx 59874

Giá»›i háº¡n nhá» hÆ¡n float32 â†’ dá»… overflow hÆ¡n.

Giáº£i phÃ¡p:
	â€¢	Loss scaling:
\mathcal{L}' = S \cdot \mathcal{L}

Sau Ä‘Ã³ chia gradient cho S.

â¸»

8. Softmax vÃ  Nhiá»‡t Ä‘á»™ (Temperature Scaling)

Softmax cÃ³ thá»ƒ Ä‘iá»u chá»‰nh báº±ng nhiá»‡t Ä‘á»™ T:

\sigma(z_i)
=
\frac{\exp(z_i/T)}
{\sum_j \exp(z_j/T)}
	â€¢	T \rightarrow 0: phÃ¢n phá»‘i sáº¯c nÃ©t
	â€¢	T \rightarrow \infty: phÃ¢n phá»‘i gáº§n Ä‘á»u

Tuy nhiÃªn náº¿u T quÃ¡ nhá» â†’ logits hiá»‡u dá»¥ng tÄƒng â†’ dá»… overflow.

â¸»

9. PhÃ¢n tÃ­ch LÃ½ thuyáº¿t XÃ¡c suáº¥t

Softmax lÃ  nghiá»‡m cá»§a bÃ i toÃ¡n tá»‘i Æ°u:

\max_p
\left(
\sum_i p_i z_i
-
\sum_i p_i \log p_i
\right)

ÄÃ¢y lÃ  dáº¡ng tá»‘i Æ°u hÃ³a entropy tá»‘i Ä‘a.

â¸»

10. Káº¿t luáº­n

CÃ¡c váº¥n Ä‘á» sá»‘ há»c trong logits vÃ  softmax xuáº¥t phÃ¡t tá»«:
	â€¢	HÃ m mÅ© tÄƒng nhanh
	â€¢	Giá»›i háº¡n biá»ƒu diá»…n sá»‘ dáº¥u cháº¥m Ä‘á»™ng
	â€¢	Gradient lan truyá»n

Giáº£i phÃ¡p cá»‘t lÃµi:

\textbf{Log-Sum-Exp Trick}

Äáº£m báº£o:

\sigma(z_i)
=
\frac{\exp(z_i - z_{max})}
{\sum_j \exp(z_j - z_{max})}

á»”n Ä‘á»‹nh sá»‘ há»c lÃ  Ä‘iá»u kiá»‡n tiÃªn quyáº¿t Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh sÃ¢u thÃ nh cÃ´ng, Ä‘áº·c biá»‡t trong cÃ¡c há»‡ thá»‘ng lá»›n nhÆ° mÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡n Ä‘áº¡i.

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Goodfellow, I., Bengio, Y., & Hinton, G. (2016). Deep Learning. MIT Press.
	2.	Higham, N. J. (2002). Accuracy and Stability of Numerical Algorithms. SIAM.
	3.	Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
	4.	Goldberg, D. (1991). What Every Computer Scientist Should Know About Floating-Point Arithmetic.
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

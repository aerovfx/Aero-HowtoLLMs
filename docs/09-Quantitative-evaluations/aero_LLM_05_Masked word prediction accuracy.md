
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [09 Quantitative evaluations](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c trong bÃ i toÃ¡n dá»± Ä‘oÃ¡n tá»« bá»‹ che (Masked Word Prediction Accuracy)

PhÃ¢n tÃ­ch Ä‘á»‹nh lÆ°á»£ng, cÃ´ng thá»©c toÃ¡n há»c vÃ  thÃ¡ch thá»©c thá»±c nghiá»‡m

â¸»

TÃ³m táº¯t

BÃ i bÃ¡o nÃ y phÃ¢n tÃ­ch cÆ¡ cháº¿ Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c dá»± Ä‘oÃ¡n tá»« bá»‹ che (Masked Word Prediction Accuracy) trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡n Ä‘áº¡i nhÆ° OpenAI GPT, Google BERT vÃ  cÃ¡c biáº¿n thá»ƒ Transformer. Dá»±a trÃªn tÃ i liá»‡u Ä‘Ã­nh kÃ¨m, chÃºng tÃ´i má»Ÿ rá»™ng báº±ng cÃ¡ch bá»• sung cÃ¡c nguá»“n há»c thuáº­t nhÆ° Jacob Devlin et al. (2018), Ashish Vaswani et al. (2017) vÃ  Tomas Mikolov et al. (2013).

BÃ i viáº¿t trÃ¬nh bÃ y:
	â€¢	CÆ¡ cháº¿ Masked Language Modeling (MLM)
	â€¢	CÃ´ng thá»©c toÃ¡n há»c cá»§a accuracy, cross-entropy, perplexity
	â€¢	So sÃ¡nh accuracy vÃ  perplexity
	â€¢	ThÃ¡ch thá»©c khi Ä‘Ã¡nh giÃ¡ Ä‘á»‹nh lÆ°á»£ng

â¸»

1. Giá»›i thiá»‡u

Masked Language Modeling (MLM) lÃ  má»™t phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n trong Ä‘Ã³ má»™t sá»‘ token trong cÃ¢u Ä‘Æ°á»£c thay tháº¿ báº±ng kÃ½ hiá»‡u [MASK], vÃ  mÃ´ hÃ¬nh pháº£i dá»± Ä‘oÃ¡n token gá»‘c.

VÃ­ dá»¥:

â€œThe cat sits on the [MASK].â€
â†’ ÄÃ¡p Ã¡n Ä‘Ãºng: â€œmatâ€

PhÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c phá»• biáº¿n rá»™ng rÃ£i trong kiáº¿n trÃºc Transformer cá»§a Google thÃ´ng qua mÃ´ hÃ¬nh BERT (2018).

â¸»

2. CÆ¡ sá»Ÿ toÃ¡n há»c cá»§a Masked Word Prediction

Giáº£ sá»­:
	â€¢	CÃ¢u Ä‘áº§u vÃ o:
X = (x_1, x_2, ..., x_n)
	â€¢	Táº­p chá»‰ sá»‘ cÃ¡c token bá»‹ che:
M \subset \{1, 2, ..., n\}

MÃ´ hÃ¬nh há»c xÃ¡c suáº¥t cÃ³ Ä‘iá»u kiá»‡n:

P(x_i \mid X_{\setminus M})

Trong Ä‘Ã³ X_{\setminus M} lÃ  chuá»—i Ä‘áº§u vÃ o Ä‘Ã£ thay cÃ¡c vá»‹ trÃ­ trong M báº±ng [MASK].

â¸»

3. Äá»‹nh nghÄ©a Accuracy trong MLM

3.1 Accuracy Ä‘Æ¡n giáº£n

Náº¿u cÃ³ N token bá»‹ che:

Accuracy = \frac{1}{N} \sum_{i \in M} \mathbf{1}(\hat{x}_i = x_i)

Trong Ä‘Ã³:
	â€¢	\hat{x}_i = \arg\max P(x_i \mid X_{\setminus M})
	â€¢	\mathbf{1}(\cdot) lÃ  hÃ m chá»‰ bÃ¡o

â¸»

3.2 Top-k Accuracy

Trong thá»±c táº¿, ta thÆ°á»ng sá»­ dá»¥ng Top-k accuracy:

Top\text{-}k = \frac{1}{N} \sum_{i \in M} \mathbf{1}(x_i \in \text{Top-}k(\hat{P}_i))

Äiá»u nÃ y Ä‘áº·c biá»‡t quan trá»ng khi:
	â€¢	Tá»« vá»±ng lá»›n (30kâ€“100k tokens)
	â€¢	Nhiá»u tá»« cÃ³ xÃ¡c suáº¥t gáº§n nhau

â¸»

4. LiÃªn há»‡ vá»›i Cross-Entropy vÃ  Perplexity

4.1 Cross-Entropy Loss

\mathcal{L} = - \frac{1}{N} \sum_{i \in M} \log P(x_i \mid X_{\setminus M})

Cross-entropy Ä‘o má»©c â€œbáº¥t ngá»â€ cá»§a mÃ´ hÃ¬nh trÆ°á»›c dá»¯ liá»‡u thá»±c.

â¸»

4.2 Perplexity

Perplexity Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a:

PP = e^{\mathcal{L}}

Hoáº·c:

PP = \exp\left(- \frac{1}{N} \sum_{i=1}^{N} \log P(x_i) \right)

Perplexity cÃ ng tháº¥p â†’ mÃ´ hÃ¬nh cÃ ng tá»‘t.

â¸»

5. So sÃ¡nh Accuracy vÃ  Perplexity

TiÃªu chÃ­	Accuracy	Perplexity
Dá»… hiá»ƒu	âœ”	âœ–
Nháº¡y vá»›i xÃ¡c suáº¥t	âœ–	âœ”
PhÃ¹ há»£p cho MLM	âœ”	âœ”
Pháº£n Ã¡nh Ä‘á»™ tá»± tin	âœ–	âœ”

VÃ­ dá»¥:

Giáº£ sá»­ mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n Ä‘Ãºng nhÆ°ng vá»›i xÃ¡c suáº¥t tháº¥p:

P(x_i) = 0.51

â†’ Accuracy = 100%
â†’ Cross-entropy cao
â†’ Perplexity cao

Do Ä‘Ã³ accuracy khÃ´ng pháº£n Ã¡nh Ä‘áº§y Ä‘á»§ cháº¥t lÆ°á»£ng mÃ´ hÃ¬nh.

â¸»

6. ThÃ¡ch thá»©c khi Ä‘Ã¡nh giÃ¡ Masked Accuracy

6.1 Tokenization

CÃ¡c mÃ´ hÃ¬nh nhÆ° BERT sá»­ dá»¥ng WordPiece:

VÃ­ dá»¥:

playing â†’ play + ##ing

Äiá»u nÃ y táº¡o ra váº¥n Ä‘á»:
	â€¢	Dá»± Ä‘oÃ¡n Ä‘Ãºng 1 pháº§n cá»§a tá»«?
	â€¢	TÃ­nh accuracy theo token hay theo word?

â¸»

6.2 Vocabulary Bias

Náº¿u tá»« vá»±ng lá»›n:

P_{\text{random}} = \frac{1}{|V|}

Vá»›i |V| = 50,000:

P_{\text{random}} = 0.00002

Accuracy cao hÆ¡n má»©c nÃ y nhiá»u láº§n má»›i cÃ³ Ã½ nghÄ©a thá»‘ng kÃª.

â¸»

6.3 Distribution Shift

Náº¿u táº­p test khÃ¡c domain train:

D_{train} \neq D_{test}

Accuracy cÃ³ thá»ƒ giáº£m máº¡nh dÃ¹ mÃ´ hÃ¬nh váº«n tá»‘t vá» máº·t xÃ¡c suáº¥t tá»•ng thá»ƒ.

â¸»

7. Má»‘i liÃªn há»‡ vá»›i Transformer

Kiáº¿n trÃºc Transformer do Ashish Vaswani et al. Ä‘á» xuáº¥t cÃ³ cÆ¡ cháº¿ self-attention:

Attention(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V

MLM táº­n dá»¥ng self-attention hai chiá»u Ä‘á»ƒ dá»± Ä‘oÃ¡n token bá»‹ che.

â¸»

8. PhÃ¢n tÃ­ch thá»‘ng kÃª Ä‘á»™ tin cáº­y cá»§a Accuracy

Náº¿u sá»‘ token bá»‹ che lÃ  N, accuracy Æ°á»›c lÆ°á»£ng lÃ :

\hat{p} = \frac{k}{N}

Sai sá»‘ chuáº©n:

SE = \sqrt{\frac{\hat{p}(1-\hat{p})}{N}}

Khoáº£ng tin cáº­y 95%:

\hat{p} \pm 1.96 \cdot SE

Äiá»u nÃ y quan trá»ng khi so sÃ¡nh hai mÃ´ hÃ¬nh.

â¸»

9. So sÃ¡nh MLM vÃ  Next Token Prediction

MLM (BERT) vs Autoregressive (GPT):

P(x_1,...,x_n) = \prod_{t=1}^{n} P(x_t \mid x_{<t})

KhÃ¡c vá»›i:

P(x_i \mid X_{\setminus M})

Do Ä‘Ã³ accuracy trong MLM khÃ´ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng trá»±c tiáº¿p vá»›i perplexity trong GPT.

â¸»

10. Káº¿t luáº­n

Masked Word Prediction Accuracy:
	â€¢	Dá»… hiá»ƒu
	â€¢	PhÃ¹ há»£p cho Ä‘Ã¡nh giÃ¡ ná»™i bá»™
	â€¢	KhÃ´ng pháº£n Ã¡nh Ä‘áº§y Ä‘á»§ phÃ¢n phá»‘i xÃ¡c suáº¥t

NÃªn káº¿t há»£p:
	â€¢	Accuracy
	â€¢	Cross-entropy
	â€¢	Perplexity
	â€¢	Calibration metrics

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Devlin, J. et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
	2.	Vaswani, A. et al. (2017). Attention is All You Need.
	3.	Mikolov, T. et al. (2013). Efficient Estimation of Word Representations.
	4.	Jurafsky & Martin. Speech and Language Processing.
	5.	Goodfellow et al. (2016). Deep Learning.
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

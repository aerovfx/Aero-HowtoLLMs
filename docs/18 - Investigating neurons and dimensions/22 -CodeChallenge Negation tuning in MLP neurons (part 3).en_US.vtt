WEBVTT

00:02.000 --> 00:07.320
This is the continuation of the code challenge we started in the previous video.

00:08.040 --> 00:15.880
Exercise five is not so difficult in terms of writing new code, so you can spend your time thinking

00:15.880 --> 00:22.040
about what the results mean, how we set up the analysis, what might be other ways to implement this

00:22.040 --> 00:22.960
analysis?

00:23.440 --> 00:25.080
The goal here is simple.

00:25.080 --> 00:31.880
Just find that best predicting neuron that you identified in the previous exercise and create a heat

00:31.920 --> 00:34.200
map with its activations.

00:34.840 --> 00:43.160
Now for this visualization I used a smaller context window just so we don't have to see all 90 tokens

00:43.160 --> 00:45.320
preceding each target token.

00:45.840 --> 00:53.360
So here in each line you see for example, not not not not not there's a lot of not nor here.

00:53.360 --> 00:56.760
These are all the target tokens wouldn't uh and so on.

00:56.960 --> 01:03.960
And as you know from other videos, I just arbitrarily set the color here to be 0.1 for all of these

01:03.960 --> 01:10.720
tokens, so that you can experience the joy of discovering what the actual results look like when you

01:10.760 --> 01:12.120
code this up yourself.

01:12.640 --> 01:19.640
And as you are considering the results, I would like you to think about how we set up the analysis,

01:19.680 --> 01:27.000
what the logistic regression really identifies, and if you have any ideas for how you might change

01:27.000 --> 01:29.760
the analysis or do something differently.

01:29.920 --> 01:36.280
If you were doing this code challenge, for example, as like a four month long research project.

01:37.440 --> 01:37.800
Okay.

01:37.840 --> 01:40.360
So now you can pause the video and get to work.

01:40.480 --> 01:44.240
And now I will discuss my solution and my thoughts.

01:45.840 --> 01:52.400
Here I am grabbing the negation activations from the maximum beta neuron.

01:52.520 --> 01:56.640
In fact, I already calculated this variable above, but here it is again anyway.

01:57.000 --> 02:04.250
Okay, and here I'm just a min max scaling this neurons activation so that we can use it as an index

02:04.410 --> 02:05.970
into the colour map.

02:06.370 --> 02:08.010
This code here you've seen before.

02:08.050 --> 02:13.050
This is just identifying the width of a letter in Monospace.

02:13.490 --> 02:17.370
And yeah there's not much I can show here that's new.

02:17.690 --> 02:23.010
The main thing that's different that you haven't seen before is just this for loop here.

02:23.010 --> 02:30.410
So instead of looping over all of the tokens in each batch, I'm just looping over from the preceding

02:30.450 --> 02:31.050
five.

02:31.050 --> 02:33.850
So this was 90.

02:33.890 --> 02:36.050
We have the preceding context of 90.

02:36.090 --> 02:38.170
And then we have the target token.

02:38.170 --> 02:43.090
So now I'm just going to show a handful of tokens before instead of showing all of them.

02:43.090 --> 02:45.370
That would just make a really long line.

02:45.690 --> 02:50.530
And then I'm also just showing the first 20 batches in the sequence.

02:51.010 --> 02:51.330
Okay.

02:51.370 --> 02:53.530
So here we see what this looks like.

02:53.530 --> 02:55.130
So this looks pretty nice.

02:55.130 --> 03:01.290
What we are looking for here again is uh, the words uh not and contractions.

03:01.290 --> 03:09.290
So we do see that there are more active or relatively strong activations for not not not not neither

03:09.330 --> 03:17.650
not this one also shows pretty similar level of activation for enough compared to neither and not pretty

03:17.650 --> 03:18.410
interesting.

03:18.570 --> 03:25.890
This one is also interesting because we see strong activations for the apostrophe t compared to the

03:25.930 --> 03:28.810
couldn't uh without the apostrophe t.

03:30.010 --> 03:30.530
Uh, yeah.

03:30.530 --> 03:39.290
So overall, I think this looks fairly compelling, but we do also see very strong activations for tokens

03:39.290 --> 03:45.850
that are not really related to negation, like rusty and dead at the end of corroded.

03:45.850 --> 03:49.610
Although I suppose corroded does have kind of a negative connotation.

03:50.250 --> 03:57.900
Out is not really a negation term, although again, you know, pass out maybe does have some negative

03:57.900 --> 03:58.860
connotation.

03:59.060 --> 04:07.180
So it looks like we do get some specificity for the tokens that we are looking for, but we also get

04:07.180 --> 04:10.580
some activations for other tokens as well.

04:10.940 --> 04:18.540
One thing to keep in mind is that we did not select for neurons that show strong activations to negation

04:18.540 --> 04:22.300
words, and low activations to all the other tokens.

04:22.420 --> 04:30.020
In fact, the analysis has nothing to do with the tokens like evidence and Commissioner, these tokens

04:30.020 --> 04:32.660
never entered the analysis.

04:32.660 --> 04:34.060
The logistic regression.

04:34.260 --> 04:42.140
The logistic regression only had two sets of tokens the negation tokens and the affirmation tokens.

04:42.180 --> 04:46.180
Now, I'm not plotting here all or any of the affirmation tokens.

04:46.300 --> 04:48.060
If you would like, you can do that.

04:48.060 --> 04:55.820
That could be interesting to look through the affirmation tokens for this same neuron, and maybe you

04:55.820 --> 05:01.060
would want to print those out using blue color map instead of red color map.

05:02.340 --> 05:04.700
Now for the final exercise.

05:04.700 --> 05:13.180
In this code challenge, take the code that you wrote for exercise five and embed it in a for loop over

05:13.180 --> 05:14.340
all the layers.

05:14.940 --> 05:21.740
As you're going through each layer, there are several statistical quantities that you want to extract.

05:22.140 --> 05:29.580
First is the percent of neurons that have a statistically significant beta coefficient.

05:30.220 --> 05:36.020
So you'll need to get and store the p value from each test on each neuron.

05:36.620 --> 05:44.980
And as I've discussed in previous videos, you should also use a corrected p value threshold by correcting

05:44.980 --> 05:47.740
for the number of neurons in each layer.

05:48.220 --> 05:54.540
You can do it simply through Bonferroni correction if you that is, dividing by the number of neurons.

05:54.740 --> 05:56.550
That's what I do in my solution.

05:56.830 --> 06:02.910
If you are more comfortable with statistics, you can also do something like FDR correction.

06:04.430 --> 06:11.070
Next, you also want to calculate the prediction accuracy from all of the neurons.

06:11.510 --> 06:16.830
And when you're finished your for loop you can generate two scatter plots that will look like this.

06:17.190 --> 06:22.590
In both cases, the x axis is the layer or the transformer block.

06:22.910 --> 06:31.310
And remember that zero is the first layer after the embeddings, and 36 is the final transformer block

06:31.430 --> 06:33.430
before the embeddings.

06:33.910 --> 06:41.430
And what I'm plotting over here on the left is the percent of significant logistic regressions in each

06:41.430 --> 06:42.030
layer.

06:42.590 --> 06:49.510
So this is the number of neurons that had a beta coefficient with a corresponding p value below the

06:49.510 --> 06:51.150
significance threshold.

06:51.630 --> 06:57.510
You can see from the y axis that this is going to range from around 25 to 70.

06:57.910 --> 07:04.670
And the question is whether it's just kind of randomly scattered or whether the significance count goes

07:04.830 --> 07:10.710
up or down, or maybe some other kind of nonlinear pattern as a function of the layer.

07:11.350 --> 07:18.070
So this plot on the left is just about how many neurons have any beta coefficient that is statistically

07:18.070 --> 07:18.990
significant.

07:19.430 --> 07:25.470
And then over here on the right I have actually two scatter plots in the same axis.

07:25.990 --> 07:29.190
Both of them show the prediction accuracy.

07:29.630 --> 07:34.030
And there are red dots to show the results for all of the neurons.

07:34.190 --> 07:39.190
So the prediction accuracy average over 5000 neurons in each layer.

07:39.470 --> 07:46.990
And the green squares show only the neurons that had a significant positive beta coefficient.

07:47.230 --> 07:55.160
So that means that this is averaging over the neurons if the beta coefficient was positive, and also

07:55.640 --> 08:00.200
if the p value was less than the threshold.

08:00.400 --> 08:07.200
And again, it will be interesting to see what kinds of patterns there are across the layers, if there

08:07.200 --> 08:08.560
are any patterns at all.

08:09.000 --> 08:16.720
Now here's a little tip when averaging the accuracies for only a subset of neurons where the beta values

08:16.720 --> 08:26.760
were positive and significant, you can use the numpy function numpy dot dot where the ma here is for

08:26.760 --> 08:27.920
masked array.

08:28.480 --> 08:33.960
I'm sure this is not the only way to do this selection, but I found this function pretty useful and

08:33.960 --> 08:35.200
perhaps you will too.

08:36.200 --> 08:41.680
Okay, so please now pause the video and enjoy wrapping up this code challenge.

08:41.880 --> 08:45.480
And now I will show my solution and discuss my results.

08:46.400 --> 08:53.960
A lot of the code here, at least in this first code cell, is code you've seen before in exercise five.

08:54.360 --> 09:01.160
Here I am initializing three matrices of p values, beta values and accuracies.

09:01.200 --> 09:02.480
Average accuracies.

09:02.960 --> 09:08.520
These two I'm initializing to be zeros and the p values I'm initializing to be ones.

09:09.000 --> 09:16.200
And the reason for this is that if the logistic regression model doesn't work well, if it crashes,

09:16.600 --> 09:22.600
uh, or yeah, it just doesn't give a good solution, then I actually want the p value to be initialized

09:22.600 --> 09:30.320
as a one, so that I can ignore it in the selection procedure in the in the visualization after this

09:30.320 --> 09:30.720
loop.

09:31.640 --> 09:31.960
Right.

09:32.000 --> 09:35.400
So here I'm looping over all of the layers in the model.

09:36.240 --> 09:43.120
And then within each layer I loop over all of the neurons, grab their activations, build the model,

09:43.120 --> 09:46.040
and all of this code you have seen before.

09:46.440 --> 09:47.760
This code is new.

09:47.800 --> 09:50.320
This line of code is kind of new.

09:50.360 --> 09:52.760
I mean, you've seen this code before.

09:52.800 --> 09:55.960
This just wasn't inside the loop over neurons.

09:55.960 --> 09:59.280
So now I'm calculating the accuracy for each neuron.

09:59.280 --> 10:03.040
This gets the accuracy for each neuron and each token.

10:03.360 --> 10:05.680
And then I average those together.

10:05.920 --> 10:09.080
This gives me a proportion with the mean.

10:09.080 --> 10:13.080
And then I'm multiplying it by 100 just to scale up to percent.

10:13.680 --> 10:15.720
This code takes several minutes.

10:15.720 --> 10:24.520
And so I'm printing out just some update information here as it goes through each layer here in this

10:24.520 --> 10:27.280
cell is where I do the visualization.

10:27.280 --> 10:35.160
So here I'm creating masks to identify, uh, the beta values that have significant p values.

10:35.280 --> 10:37.160
So that is here the p value mask.

10:37.160 --> 10:43.240
Where are the p values less than 0.05 divided by the number of neurons.

10:43.240 --> 10:46.560
So this is Bonferroni correcting within each layer.

10:46.920 --> 10:53.370
It's an extremely strict p value threshold by the way because this is 5120.

10:53.370 --> 10:56.090
So the p value needs to be really, really small.

10:56.130 --> 11:00.490
The effect needs to be really significant for it to show up here.

11:00.850 --> 11:03.530
And then here is for the positive betas.

11:03.530 --> 11:04.850
That one's a little bit simpler.

11:05.250 --> 11:15.090
Now I find this somewhat confusing that in this numpy function for masked array the mask actually gets

11:15.090 --> 11:15.490
flipped.

11:15.490 --> 11:21.250
So it masks out where something is true and it preserves where something is false.

11:21.690 --> 11:27.890
I don't know what you think, but in my opinion, that is totally opposite of the way that I think about

11:27.890 --> 11:28.610
masks.

11:28.890 --> 11:35.490
So if you define the masks to be the thing that you're looking for, the qualities that you are looking

11:35.490 --> 11:43.450
for in a matrix or vector, then you actually need to invert that mask in order for it to apply in such

11:43.450 --> 11:49.730
a way that you actually get the values that you are looking for that fit into this mask.

11:50.810 --> 11:51.130
Okay.

11:51.170 --> 11:53.290
So that is that's for that.

11:53.290 --> 11:54.810
And then here I'm plotting.

11:54.810 --> 12:03.250
Here's the average of the p value mask over uh axis one which is the layer axis.

12:03.290 --> 12:03.890
Oh sorry.

12:03.930 --> 12:05.730
This is the neuron axis.

12:05.730 --> 12:09.930
So we're averaging over all the neurons within each layer.

12:09.930 --> 12:12.570
And that's how I get the average accuracy.

12:12.890 --> 12:18.130
And then or the average number of neurons that showed a significant result.

12:18.130 --> 12:20.770
And then I multiply that by 100.

12:20.810 --> 12:23.530
Again that's just a scale up to percent.

12:23.970 --> 12:31.250
And then over here I'm averaging accuracy and masked accuracy, which is just the accuracy where the

12:31.290 --> 12:36.130
p value was significant and the beta value was positive.

12:37.050 --> 12:40.610
So here is what that result looks like.

12:40.930 --> 12:48.450
And we see in all of these cases, the number or the percent of significant neurons that showed a significant,

12:48.570 --> 12:55.940
uh, Logistic regression decreased pretty steeply by a factor of two, from 70 or even over two from

12:55.940 --> 12:57.620
almost 70%.

12:57.620 --> 13:06.180
So in the early layers of this transformer, almost 70% like two thirds of the neurons, showed a significant

13:06.220 --> 13:12.740
difference in activation between negation and affirmation token words.

13:13.140 --> 13:18.780
And then by the end, it drops down to less than a third, maybe a little bit over a quarter of the

13:18.780 --> 13:19.500
neurons.

13:19.540 --> 13:22.180
Now, that's still a pretty significant percentage.

13:22.180 --> 13:25.740
That's around 1000 neurons out of 5000.

13:25.860 --> 13:32.700
That still shows some kind of statistically significant difference in activation between the category

13:32.700 --> 13:37.940
of negation words and the category of affirmation words, at least in this book.

13:38.420 --> 13:40.820
But it is certainly quite a bit lower.

13:41.300 --> 13:45.740
As for the prediction accuracy, we see that on average.

13:45.740 --> 13:48.500
So this is not the best performing neuron.

13:48.500 --> 13:50.740
It's also not the worst performing neuron.

13:51.060 --> 13:56.580
This is the average of all of the neurons and also all the significant positive neurons.

13:56.860 --> 14:08.020
The prediction accuracy was fairly high, again going up to almost 80%, close to 75% in the early layers

14:08.020 --> 14:14.420
and going down to 65% around two thirds prediction accuracy later on.

14:14.540 --> 14:19.580
Now the chance level performance here would be 50%.

14:19.580 --> 14:28.380
So numbers above around 50 or maybe a little bit over 50, uh, are indicating some pretty decent predictability.

14:28.660 --> 14:33.260
Now, the reason why the, uh, all of the tests here, the red dots is lower.

14:33.300 --> 14:39.580
That's pretty trivial because here I'm also including into the average these neurons where there was

14:39.580 --> 14:45.820
significant prediction, and also, uh, neurons that were not statistically significant.

14:45.940 --> 14:51.230
And so their prediction accuracy, we would expect to be somewhere around 50%.

14:51.430 --> 14:55.310
So in fact, this is not really a terribly useful thing to show.

14:55.310 --> 14:57.070
But I think it's nice.

14:57.070 --> 15:05.470
Anyway, just as a way to practice your coding skills and, uh, working with masked matrices.

15:06.790 --> 15:07.870
Now, why is this the case?

15:07.870 --> 15:11.630
I just want to say one quick word about interpreting this effect.

15:11.950 --> 15:18.910
Remember that as these token embeddings vectors pass through the model, they go from one transformer

15:18.910 --> 15:20.150
block to the next.

15:20.550 --> 15:23.670
What's happening inside each transformer block?

15:23.710 --> 15:32.110
As we get a little bit of a modification to the vector from the attention and from the MLP sublayers,

15:32.510 --> 15:39.590
is that the model shifts from transforming the current token to making a prediction about the next token.

15:39.990 --> 15:46.350
So in the beginning of the model, the transformer is really like thinking a lot about this token.

15:46.430 --> 15:50.230
The token not and can't and isn't and shouldn't.

15:50.630 --> 15:57.870
And the later you get into the model, the more the model is not processing that token in particular,

15:57.870 --> 16:05.950
but thinking about what the next token should be based on this token and all the tokens that preceded

16:05.950 --> 16:06.310
it.

16:07.070 --> 16:14.950
Just as a meta comment here, we're not even finished the first mech Interp section, and I think you're

16:14.950 --> 16:22.470
already starting to see that this kind of research is challenging for many reasons, including the complexity

16:22.470 --> 16:25.870
of the models and the complexity of language.

16:25.870 --> 16:27.230
Human written language.

16:27.510 --> 16:33.630
On the other hand, I actually enjoy that challenge, that difficulty, and I hope that you are also

16:33.630 --> 16:34.630
enjoying it.

16:34.950 --> 16:42.710
This complexity also means that it's difficult to blindly trust the outcome of any random statistical

16:42.710 --> 16:49.040
analysis, in terms of just looking at how many parameter estimates have a small p value.

16:49.360 --> 16:56.320
You really need to check the results carefully to make sure that any interpretation you make is valid,

16:56.320 --> 17:00.120
correct, and consistent with what the data are really showing.

17:00.720 --> 17:07.400
And just to be clear, as long as you don't have any coding bugs, the statistics themselves are never

17:07.400 --> 17:07.920
wrong.

17:08.360 --> 17:13.080
Statistics is just math, and the math doesn't do things right or wrong.

17:13.360 --> 17:22.280
But the way that someone uses a particular statistical analysis may not reveal exactly the results that

17:22.280 --> 17:23.640
they are looking for.

17:24.120 --> 17:25.960
And that is the tricky part.

17:26.520 --> 17:33.040
Anyway, I'm sure that you put in a lot of work and effort into this code challenge, and the good news

17:33.080 --> 17:39.880
is that all of that work will pay off in the next code challenge, where we will basically just adapt

17:39.920 --> 17:43.000
this code to look at the attention sublayer.

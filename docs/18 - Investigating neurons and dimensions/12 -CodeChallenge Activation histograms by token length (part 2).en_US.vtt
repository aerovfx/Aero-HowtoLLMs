WEBVTT

00:01.880 --> 00:04.160
Now to do some analyses.

00:04.480 --> 00:11.440
In this exercise, you will just focus on the activations from the fifth transformer block.

00:12.000 --> 00:20.040
So find the activations that correspond to the short, the long and the medium length tokens.

00:20.520 --> 00:25.600
Then you can make histograms of the activations for these three different lengths.

00:25.840 --> 00:31.120
Pooling data across all of the neurons in this transformer block.

00:31.640 --> 00:37.800
And you can see that in this plot here the histograms are not exactly overlapping, but quite similar

00:37.800 --> 00:38.760
to each other.

00:39.280 --> 00:43.800
I will make some interpretations about this when I discuss my code.

00:44.000 --> 00:47.760
But first you should pause the video and code up this exercise.

00:48.960 --> 00:54.000
Remember that the fifth transformer block has an index of four.

00:54.440 --> 00:58.880
So this is actually corresponding to the fifth transformer block.

00:59.400 --> 00:59.680
Okay.

00:59.790 --> 01:01.750
so here I get all of the activations.

01:01.790 --> 01:08.430
Now, the thing about the batches is that the reason why we batch up the data is partly for convenience,

01:08.430 --> 01:13.710
for processing efficiency, and also partly because models don't.

01:13.750 --> 01:18.390
Or at least this model doesn't process up to 8000 tokens at once.

01:18.550 --> 01:23.510
So otherwise there's really no particular reason to batch the data.

01:23.510 --> 01:28.270
So therefore I mean no like theoretically meaningful reason to batch the data.

01:28.310 --> 01:32.070
It's really just done for practical convenience.

01:32.470 --> 01:39.710
So therefore in these analyses we will often batch the data just to do a forward pass and then flatten

01:39.710 --> 01:44.510
the data or vectorize the data when we want to do the analyses.

01:44.750 --> 01:49.950
So let me actually show you what the shape of this will be.

01:49.990 --> 01:58.230
So here's just a reminder that the shape of this oops I meant to say dot shape is 16 by 15.

01:58.230 --> 02:00.660
So this is our batch by neurons.

02:00.860 --> 02:07.660
And now when I reshape this to minus one by neurons.

02:07.860 --> 02:10.380
Then I get 8192.

02:10.380 --> 02:14.380
So this is all the tokens by the 3000 neurons.

02:14.420 --> 02:14.660
Okay.

02:14.700 --> 02:19.180
So that's just a little bit of a explanation of this line of code here.

02:20.180 --> 02:20.900
Very nice.

02:20.940 --> 02:21.260
Okay.

02:21.300 --> 02:28.500
So now we actually do have floating point numbers because as let me uh sorry, let me do this again

02:28.500 --> 02:30.780
just to show you what these numbers look like.

02:30.780 --> 02:32.820
So these really are floating point numbers.

02:32.820 --> 02:37.460
These are the activation values coming from these neurons.

02:37.620 --> 02:43.940
So that is the embeddings vectors from all of the 8000 tokens.

02:44.140 --> 02:50.660
And those get pushed through, uh, everything that happens before the fifth transformer block in the

02:50.660 --> 02:52.300
MLP expansion layer.

02:52.620 --> 02:55.260
And then, you know, all the weights, all the multiplications.

02:55.260 --> 03:01.730
And then we get all of the Activation values, and that is what these numbers correspond to.

03:02.770 --> 03:04.570
So we don't use bar plots.

03:04.570 --> 03:06.530
We don't use numpy unique.

03:06.690 --> 03:09.210
To get these data we use histograms.

03:09.490 --> 03:14.050
In this case these are all PyTorch data formats.

03:14.050 --> 03:16.050
So I'm using torch histogram.

03:16.370 --> 03:18.210
You can also use numpy.

03:18.250 --> 03:20.570
Of course you can use numpy histogram.

03:20.810 --> 03:22.770
If you're doing that it's no problem.

03:22.770 --> 03:27.610
But you would then want to convert everything to numpy like this.

03:27.650 --> 03:31.850
You can convert to numpy in here if you prefer.

03:31.890 --> 03:38.850
You can also um whereas the hook functions if you like you could also do this.

03:38.850 --> 03:46.250
You could convert all of the activations into numpy directly inside this hook function.

03:46.290 --> 03:47.050
Totally fine.

03:47.050 --> 03:52.650
Lots of correct ways to do this, uh, these kinds of analyses and work with these data.

03:53.090 --> 03:53.370
Okay.

03:53.410 --> 03:58.630
But anyway, the way that I coded this was in torch.

03:58.630 --> 04:00.150
So torch dot histogram.

04:00.430 --> 04:07.070
All of the activations for the tokens that are less than the median length, greater than the median

04:07.070 --> 04:10.390
length, and equal to the median length.

04:10.830 --> 04:11.070
Okay.

04:11.110 --> 04:12.910
And what do these results mean.

04:14.070 --> 04:21.390
So essentially these results are telling us that in this MLP layer in the fifth transformer block,

04:21.590 --> 04:26.430
there isn't really a very clear distinction between the processing.

04:26.430 --> 04:30.550
So the activation magnitudes and the length of the token.

04:30.870 --> 04:33.390
Now maybe there is some shift.

04:33.390 --> 04:39.390
Maybe there's a little bit of a bias that the longer tokens have a slight shift towards more negative

04:39.390 --> 04:40.870
activations overall.

04:41.430 --> 04:45.670
Uh, if you want to, you could do some statistical analyses.

04:45.670 --> 04:50.950
I'm not including that here, but even if it is significant, there's certainly not a really compelling

04:50.950 --> 04:51.990
big response.

04:52.510 --> 04:56.740
And my question for you is, are you surprised by this result?

04:56.780 --> 05:03.420
Were you expecting that the MLP neurons would show a qualitative difference?

05:03.460 --> 05:10.180
Or maybe just a shift difference between activations for short tokens versus the long tokens?

05:10.660 --> 05:16.580
One thing to keep in mind is that the token lengths have nothing to do with the token indices.

05:16.900 --> 05:24.780
So the token indices that go into the model, these just correspond to different rows of the token embeddings

05:24.900 --> 05:25.780
matrix.

05:25.900 --> 05:32.860
And regardless of the length of a token, each token still only gets one embeddings vector.

05:32.860 --> 05:39.900
So one row in the token embeddings matrix, regardless of how short or long that token is.

05:40.420 --> 05:47.260
On the other hand, the length of the token is not entirely arbitrary that actually does correspond

05:47.300 --> 05:55.690
to the frequency of that character sequence appearing in the texts in the training set, so it is the

05:55.690 --> 06:03.130
case that the shorter tokens tend to appear more often in the text, and the longer tokens tend to appear

06:03.170 --> 06:07.770
less often in the text that the models have been trained on.

06:08.370 --> 06:16.170
And what do you think generally about this approach of binning the data into 2 to 3 groups, depending

06:16.170 --> 06:17.770
on the median split?

06:18.130 --> 06:24.570
The thing is that we actually do have a lot of information in these token distributions.

06:24.570 --> 06:28.690
There's 16 different values of token lengths.

06:29.010 --> 06:37.650
And I'm basically ignoring all of that meaningful variability simply by cutting the data into two bins.

06:37.650 --> 06:44.010
If you want to ignore the exact median or three bins if you want to include the exact median.

06:44.330 --> 06:50.570
So that means that we've lost a lot of richness of the data because of the discretization.

06:51.410 --> 06:51.730
Yeah.

06:51.730 --> 06:56.080
So doing a median split analysis can be interesting.

06:56.240 --> 07:03.160
It's done relatively frequently in statistics and machine learning, but it loses a lot of information

07:03.160 --> 07:04.280
and precision.

07:04.600 --> 07:09.080
And so it should be used sparingly and mostly just for visualization.

07:09.720 --> 07:16.760
It is better in general to take advantage of all the meaningful information that is packed into the

07:16.760 --> 07:18.560
variability of the data.

07:18.920 --> 07:24.960
And that's what we are going to do here in exercise five and also exercise six.

07:25.520 --> 07:32.680
But before telling you about the exercise instructions, let me first show a reminder of the formulas

07:32.680 --> 07:36.320
for covariance and correlation coefficient.

07:36.920 --> 07:38.800
So the correlation coefficient.

07:38.840 --> 07:43.720
The Pearson correlation r I showed a few times earlier in the course.

07:43.720 --> 07:49.160
In particular when I was comparing the correlation to cosine similarity.

07:49.880 --> 07:54.710
Covariance is the numerator of the correlation coefficient.

07:55.310 --> 08:02.790
The interpretation of the covariance is almost the same as the interpretation of the correlation, except

08:02.790 --> 08:10.070
that the covariance retains the scale of the data because the units are not divided out.

08:10.950 --> 08:18.910
On the other hand, when the variances of the data are standardized to a value of one, then the denominator

08:18.910 --> 08:22.950
in the correlation equation is trivially equal to one.

08:23.150 --> 08:30.190
So if the variance of x equals the variance of y equals one, then we have one times one, and the square

08:30.190 --> 08:32.070
root of that is one.

08:33.270 --> 08:41.830
And better still, when the mean of each variable is zero, then the covariance formula simplifies further

08:41.830 --> 08:48.670
to be just the sum of element wise multiplications, which is literally just the dot product.

08:48.950 --> 08:53.420
And so what you are going to do in the code for this exercise is.

08:53.420 --> 09:01.420
Calculate the correlation coefficient by first standardizing the activations and standardizing the token

09:01.420 --> 09:08.420
length variables, and then taking their dot product and dividing by the number of samples minus one.

09:09.140 --> 09:15.500
By the way, it kind of seems like this scaling factor of minus one is missing from the correlation

09:15.500 --> 09:18.340
formula, but it actually is in there.

09:18.340 --> 09:25.380
But the scaling is in the numerator once and it's in the denominator twice for the two variance terms.

09:25.380 --> 09:28.820
So that becomes that scaling factor squared.

09:28.820 --> 09:30.140
And then the square root of that.

09:30.140 --> 09:32.140
So it just cleanly cancels out.

09:32.180 --> 09:39.820
And that's why the divisive normalization of n minus one is typically just omitted from the formula

09:39.820 --> 09:42.580
for the correlation anyway.

09:42.580 --> 09:46.460
So with that out of the way let's get to exercise five.

09:47.180 --> 09:56.050
The goal of exercise five is to calculate and visualize the correlations between MLP neuron activation

09:56.490 --> 09:58.210
and token length.

09:58.610 --> 10:03.290
Again, for now we are just focusing on the fifth transformer block.

10:04.050 --> 10:13.050
So begin by standardizing all of the activations and then confirm that the standardization code is correct,

10:13.570 --> 10:20.890
which you can do by looking at the mean and at the standard deviation of just one of the randomly selected

10:20.890 --> 10:21.610
neurons.

10:21.770 --> 10:26.170
So here I just picked neuron 600 index 600.

10:26.730 --> 10:33.410
And I saw that its mean is very close to zero and its standard deviation is basically one.

10:33.890 --> 10:41.410
So remember that for standardizing we're standardizing across the activations within each neuron.

10:41.410 --> 10:49.280
So each neuron itself needs to have a mean of zero or at least within some reasonable numerical tolerance

10:49.280 --> 10:55.440
and a standard deviation of one, and this is done over all of the 8000 tokens.

10:55.600 --> 10:57.280
You can do this in a for loop.

10:57.280 --> 11:00.560
Or you can do this with one line of code using broadcasting.

11:00.720 --> 11:03.160
Whatever you are comfortable with, that's fine.

11:03.160 --> 11:07.640
But in the end you want to end up with a matrix or a tensor.

11:08.160 --> 11:13.920
I called it z for z standardized activations and it should be this size.

11:13.920 --> 11:18.760
So all of the tokens and all of the MLP neurons.

11:19.960 --> 11:23.440
Now you also need to standardize the token length.

11:23.800 --> 11:29.560
And you should also confirm that that has a mean of zero and a standard deviation of one.

11:30.360 --> 11:34.160
Next you can calculate the correlation coefficient.

11:34.160 --> 11:43.320
And you can do this twice, once using Numpy's core Coef function using the Non-standardized activations

11:43.320 --> 11:46.080
and the Non-standardized token lengths.

11:46.600 --> 11:48.580
And that's going to give you some value.

11:48.780 --> 11:56.220
And then you can confirm that the code you implement to calculate the covariance, not the correlation,

11:56.220 --> 11:59.660
but the covariance, but of the standardized variables.

11:59.900 --> 12:02.820
That code is going to be in here.

12:02.980 --> 12:09.740
And that number should be, you know, within some reasonable tolerance more or less identical to Numpy's

12:09.780 --> 12:11.420
core Coef function.

12:11.940 --> 12:18.420
So here I'm just picking one neuron just for confirmation of the accuracy of the code here.

12:18.980 --> 12:26.580
And once you have that code correct, you can then calculate the relationship between activation and

12:26.580 --> 12:31.500
token length for all of the neurons in this transformer block.

12:31.980 --> 12:35.140
And then you can visualize that in a histogram.

12:35.140 --> 12:36.540
It will look something like this.

12:36.540 --> 12:40.140
Here you see the counts and the correlation coefficient.

12:40.140 --> 12:48.530
Again this is the linear relationship between activation magnitude in each neuron in one layer and all

12:48.530 --> 12:50.010
of the token lengths.

12:50.490 --> 12:50.810
Okay.

12:50.850 --> 12:58.050
I will have a few things to say about that, but first you should pause the video and code up the analysis,

12:58.170 --> 13:01.170
and then you can come back and watch me discuss the code.

13:02.170 --> 13:08.170
Here again, I am extracting the activations and flattening them into a vector.

13:08.450 --> 13:13.450
In this case, I want to start working with numpy functions and the numpy library.

13:13.450 --> 13:19.890
So I am converting all of these data which are in a PyTorch tensor into numpy.

13:20.250 --> 13:25.330
I'm not sure if numpy ify is a real word, but you know what I mean.

13:25.810 --> 13:26.090
Okay.

13:26.130 --> 13:29.650
And here is where I'm standardizing all of the activations.

13:29.650 --> 13:37.210
So the standardization formula is the data minus the mean divided by the standard deviation.

13:37.650 --> 13:46.640
As I mentioned in the slides you can do this in a for loop over all Hundred 8000 neurons.

13:46.840 --> 13:47.720
Totally fine.

13:47.720 --> 13:57.720
You can also do this in one line of code, using broadcasting by first averaging over all of the tokens

13:57.880 --> 14:05.760
and keeping the dimensions such that this mean vector remains a multidimensional array, and the same

14:05.760 --> 14:07.640
thing for the standard deviation.

14:08.080 --> 14:12.720
This parameter here is for the denominator degrees of freedom.

14:12.960 --> 14:18.200
You set it to one so that you calculate a sample standard deviation.

14:18.240 --> 14:25.720
The only difference between setting the denominator degrees of freedom to be zero, to be one versus

14:25.720 --> 14:27.600
the default value, which is zero.

14:27.640 --> 14:34.560
That's what happens if you don't specify that parameter at all, is just about dividing the data by

14:34.800 --> 14:36.640
n or n minus one.

14:36.680 --> 14:40.200
If you have it set to zero, it's no problem really.

14:40.200 --> 14:44.430
Formally, technically, you know, if you want to get really technical about it.

14:44.430 --> 14:50.430
That is incorrect because we only have samples that we're working with here, not population data sets.

14:50.590 --> 14:57.030
So it should really be, uh, the denominator degrees of freedom of one really doesn't matter.

14:57.070 --> 14:57.510
Either way.

14:57.510 --> 14:58.510
It's not a big deal.

14:58.870 --> 15:05.430
Okay, uh, here, I'm just confirming the average for any given, uh, neuron should be zero, or at

15:05.430 --> 15:06.790
least very close to zero.

15:06.910 --> 15:08.710
And the standard deviation should be.

15:08.710 --> 15:10.670
In theory, it should be exactly one.

15:10.670 --> 15:17.350
But because we have, uh, finite samples and because of just computer rounding, precision errors and

15:17.350 --> 15:21.070
so on, it's going to be very close to one, but maybe not exactly.

15:21.390 --> 15:21.670
Okay.

15:21.710 --> 15:27.110
And then here I'm applying basically the same code here I'm standardizing the token lengths.

15:27.110 --> 15:32.910
And we can see that it has a mean very close to zero and a standard deviation of one.

15:33.430 --> 15:34.550
Very nice okay.

15:34.590 --> 15:38.150
So here is the formula for the covariance.

15:38.150 --> 15:42.270
So element wise multiplication and then sum.

15:42.470 --> 15:47.500
Now in the formula that I showed, it actually looked like this.

15:47.500 --> 15:53.900
So it was the data minus the mean of the data like this.

15:55.660 --> 15:59.140
Uh, and then I would have to also mean center these data.

15:59.300 --> 16:06.460
But here I literally subtract the mean already up here I'm subtracting the mean, which means that this

16:06.460 --> 16:12.620
piece of code here is equivalent to saying -0, which means that we don't need it at all.

16:12.900 --> 16:13.180
Okay.

16:13.220 --> 16:14.380
Just a little reminder.

16:14.420 --> 16:14.740
Okay.

16:14.780 --> 16:17.300
And then we divide by n minus one.

16:17.660 --> 16:22.140
Now here you have to make sure that you're not dividing by the number of neurons.

16:22.420 --> 16:29.060
The covariance is done separately for each individual neuron over all of the activations.

16:29.180 --> 16:34.980
So if you would have code that looks like this, it still works in the sense that you get an answer,

16:34.980 --> 16:37.460
but it's not going to be the correct answer.

16:37.460 --> 16:39.940
This is the wrong normalization.

16:40.140 --> 16:46.570
What you normalize is over the number of tokens or the number of samples that you've measured.

16:46.850 --> 16:50.250
And that matches the correlation over here.

16:51.250 --> 16:51.530
Okay.

16:51.570 --> 16:58.170
So this is a little statistics reminder that the Pearson correlation coefficient equals the covariance

16:58.170 --> 16:59.490
of the two variables.

16:59.650 --> 17:02.490
If those variables are standardized.

17:02.930 --> 17:10.130
So once we have that once we have confirmed that then we can loop over all of the neurons and calculate

17:10.130 --> 17:18.010
the correlation between activations and token lengths over all the tokens separately for each neuron.

17:18.050 --> 17:21.610
Using a very simple piece of code that looks like this.

17:22.170 --> 17:29.610
By the way, if you are familiar with and comfortable with both statistics and linear algebra, it turns

17:29.610 --> 17:32.890
out that you do not actually need to do this in a for loop.

17:32.890 --> 17:37.290
You can actually implement this using matrix multiplication.

17:37.290 --> 17:41.800
I'm not going to show that here, but it's technically possible.

17:41.800 --> 17:48.480
And if you do it and you would like to share, or if you have a question about it, feel free to leave

17:48.520 --> 17:49.920
a post in the Q&amp;A.

17:50.440 --> 17:53.080
Anyway, so here I'm creating the histogram.

17:53.080 --> 17:54.160
So what do we see here?

17:54.840 --> 18:01.120
We see that lots of neurons have a broad range of correlation coefficients.

18:01.120 --> 18:02.960
Some of them are fairly large.

18:02.960 --> 18:05.760
So pretty robust positive correlations.

18:05.760 --> 18:08.040
Some of them are pretty robust negative.

18:08.480 --> 18:15.640
But most of the neurons have a correlation that is weak but negative.

18:15.840 --> 18:21.320
And it looks like the population would be significantly below zero.

18:21.320 --> 18:24.160
So if we consider that a null effect.

18:24.160 --> 18:31.200
So no relationship whatsoever between the activations and the token lengths, that would correspond

18:31.200 --> 18:32.880
to a correlation of zero.

18:33.040 --> 18:38.600
And it does look like the bulk of the correlations are below zero.

18:39.590 --> 18:40.550
And what does that mean?

18:40.590 --> 18:48.870
Basically, that means that in general, the longer the token, the less activation we have in these

18:48.870 --> 18:50.350
MLP neurons.

18:50.950 --> 18:53.590
You could also flip that interpretation around.

18:53.590 --> 19:00.550
So you could say that the shorter the token, the larger the activation in MLP neurons.

19:00.710 --> 19:07.670
So it's certainly not the case for every single neuron, but at the level of the whole population of

19:07.710 --> 19:15.590
this set of neurons in this particular layer, it does generally seem like there's stronger activation

19:15.790 --> 19:22.550
when the tokens, when the token embeddings vectors come from pieces of text that are shorter, that

19:22.550 --> 19:24.110
have fewer characters.

32:28.830 --> 32:34.510
Exercise six has an analysis part and a visualization part.

32:34.990 --> 32:37.510
The analysis part is pretty straightforward.

32:37.510 --> 32:45.590
You just have to take the correlation code from exercise five and repeat it over all of the transformer

32:45.590 --> 32:46.350
blocks.

32:47.190 --> 32:54.190
By the way, as you are implementing this analysis, make sure to soft code the number of transformer

32:54.190 --> 32:57.870
blocks, because in this model it's 12.

32:57.910 --> 33:03.430
But when we switch to the larger version of this model, it's not going to be 12.

33:04.150 --> 33:11.550
Anyway, when you extract histograms for the correlation distribution within each layer, make sure

33:11.550 --> 33:19.470
to use the same histogram bin boundaries across all of the layers so you can compare them directly in

33:19.470 --> 33:21.990
a plot and also in an image.

33:22.700 --> 33:28.740
So then you can generate results and visualize them in a plot that looks something like this.

33:28.780 --> 33:34.780
Obviously your plots will not have these big red boxes covering up all the fascinating findings.

33:35.260 --> 33:43.020
So on this left plot, we have line plots corresponding to the histogram for each of the 12 transformer

33:43.020 --> 33:45.220
block MLP layers.

33:45.900 --> 33:54.220
Each color is a different line, and the color bar indicates the mapping between the color of the line

33:54.540 --> 33:58.940
and the layer where that line or that histogram comes from.

33:59.660 --> 34:06.580
By the way, there's some weird matplotlib color coding to get this color scale to work.

34:06.580 --> 34:14.100
So if you have the plot but you're struggling with this little component of the visualization, then

34:14.100 --> 34:17.380
feel free to just copy the code from the solutions file.

34:18.220 --> 34:26.120
Okay, so here you're going to have 12 lines showing histograms of correlation coefficients between

34:26.120 --> 34:32.840
activations of the neurons and the lengths of the tokens, and a separate line with a separate color

34:32.840 --> 34:34.160
for each layer.

34:35.000 --> 34:44.200
So then this image over here shows exactly the same data, but visualized in a slightly different format.

34:44.640 --> 34:51.840
So now over here, the y axis corresponds to layer and the color corresponds to the density.

34:51.960 --> 35:00.600
In other words, the color of this image here corresponds to the height of the y axis in this plot.

35:00.920 --> 35:05.480
So same data in both axes, but they show the data in different ways.

35:05.640 --> 35:11.760
And you will find that sometimes one visualization or the other is more clear or more revealing.

35:12.320 --> 35:15.200
I hope you enjoy creating these graphs.

35:15.320 --> 35:20.040
I always enjoy working with matplotlib, even when it gets a little bit frustrating.

35:20.470 --> 35:23.990
Anyway, now I will switch to code and discuss my solution.

35:25.350 --> 35:28.990
Here I am looping over all of the layers.

35:29.030 --> 35:32.830
Now notice that I'm not just hard coding the number 12 in here.

35:33.150 --> 35:37.710
If I were to hard code the number 12 that actually would work for this model.

35:37.710 --> 35:42.350
It would be fine, but it's better to soft code whenever possible.

35:42.550 --> 35:44.390
So therefore I'm using model.

35:46.430 --> 35:47.190
Layers.

35:47.470 --> 35:49.630
And what do we do inside this for loop.

35:49.990 --> 35:57.230
I get the activations as you've seen before and I standardize them as you've seen before.

35:57.550 --> 36:01.550
And then I correlate them for each individual neuron as you've seen before.

36:01.710 --> 36:06.910
So in fact, all of this code is exactly copied from exercise five.

36:07.150 --> 36:11.950
All I did was just put in a layer number here and then all of these data.

36:11.950 --> 36:16.630
So all of the individual correlations get stored in this matrix.

36:16.830 --> 36:21.180
All cores which is layers by neurons.

36:21.180 --> 36:24.740
Every layer, of course, has exactly the same number of neurons.

36:24.740 --> 36:28.660
So this works in a one big matrix like this.

36:29.740 --> 36:29.940
Okay.

36:29.980 --> 36:31.620
And then here I'm getting the histograms.

36:31.620 --> 36:40.740
Here I define the edges for the bins of the correlation coefficients -0.8 to plus 0.8 correlation.

36:41.180 --> 36:43.700
And then 81 steps in between them.

36:43.940 --> 36:50.940
And then yeah it creates a hist counts this matrix where I'm going to store again all of the data for

36:50.940 --> 36:54.660
all of the layers and for all of the bins.

36:55.100 --> 37:01.540
Now remember that the bin edges here, this variable here, when you say numpy dot hist and you input

37:01.540 --> 37:10.180
the bins here, this variable defines all of the bin boundaries, which means there are n minus one

37:10.420 --> 37:11.780
actual bins.

37:12.020 --> 37:15.380
And that is why I have the minus one in here.

37:15.860 --> 37:18.810
Okay, so run that uh yeah it takes a second.

37:18.850 --> 37:20.130
I think I've already even run it.

37:20.330 --> 37:20.570
Okay.

37:20.610 --> 37:21.570
And then here I'm plotting.

37:21.570 --> 37:23.050
Here is the first plot.

37:23.090 --> 37:24.930
I can actually show the legend here.

37:25.290 --> 37:32.210
Uh, the first plot on the left will show all of the lines and plotting all the lines in a for loop,

37:32.210 --> 37:40.930
because I want to specify the color of each line to be the layer index divided by the total number of

37:40.970 --> 37:41.810
layers.

37:42.050 --> 37:42.250
Okay.

37:42.290 --> 37:45.010
And then here is the, uh, image.

37:45.010 --> 37:46.890
Here's where I'm creating the image over here.

37:46.890 --> 37:54.410
And here's this funny looking code that you, uh, you use to create this color, uh, bar over here.

37:54.850 --> 37:56.770
Okay, so that's about the code.

37:56.770 --> 37:58.890
What do the results actually show?

37:59.330 --> 38:04.610
So again here we see the x axis is the correlation coefficients.

38:04.810 --> 38:13.650
What we looked at in the previous exercise was just one line over here showing the uh bulk of the correlations

38:13.690 --> 38:16.010
a little bit shifted to the left.

38:16.600 --> 38:21.120
And now you see it for all of the transformer blocks.

38:21.120 --> 38:23.400
So I'll discuss this one in a moment.

38:23.440 --> 38:31.040
It's kind of an oddball sticking out, but just, you know, when we look at all of the layers altogether,

38:31.040 --> 38:37.920
we see that in general there's a pretty consistent shift, uh, a little bit left of zero.

38:38.120 --> 38:46.200
And again, the interpretation here is that the shorter tokens tend to have stronger activations in

38:46.200 --> 38:47.640
MLP layers.

38:47.760 --> 38:50.000
And why might that be the case?

38:50.120 --> 38:56.440
Keep in mind that shorter tokens tend to appear more often in training text.

38:56.440 --> 38:59.960
So there's a little bit of a bias introduced there.

39:00.280 --> 39:00.520
Okay.

39:00.560 --> 39:02.720
And then we have something weird happening here.

39:02.720 --> 39:06.560
This is at the very first, uh, transformer block.

39:06.680 --> 39:13.280
And this is the transformer block that is as close as possible to the embeddings, which means as close

39:13.280 --> 39:15.600
as possible to the raw text.

39:15.630 --> 39:22.870
So as we move through the, uh, through the large language model, each transformer layer, we get

39:22.870 --> 39:25.630
further and further away from the original text.

39:25.630 --> 39:27.790
But here we are very close to the text.

39:27.830 --> 39:35.670
This is just one stop after the embeddings, vectors and the position embeddings vectors get added together.

39:35.670 --> 39:37.950
And here we see a pretty qualitative shift.

39:37.950 --> 39:44.310
Most of these neurons, not all of them, but most of the neurons in this first MLP layer are actually

39:44.310 --> 39:47.710
correlating positively with token length.

39:47.710 --> 39:50.950
So longer tokens get more activation.

39:51.590 --> 39:55.550
So that is for all of the line plots over here.

39:55.830 --> 39:58.870
And then we get over here to the image again.

39:59.110 --> 40:05.950
Every row in this image here in this matrix is one line from this plot over here.

40:06.230 --> 40:12.870
And the color here corresponds to the height over here, which is the density or the estimate of the

40:12.910 --> 40:13.630
pdf.

40:14.150 --> 40:20.820
And so we see all of the same dynamics, but some features are just a little bit easier to visualize

40:21.020 --> 40:24.180
in this image compared to all of these plots.

40:24.380 --> 40:33.140
For example, something that you notice is that the later transformer layers here have a much tighter

40:33.140 --> 40:37.180
distribution compared to the earlier layers with this one exception here.

40:37.220 --> 40:43.740
I'm not sure why this one is so unusual here, that this row here corresponds to this purple line here.

40:44.100 --> 40:49.780
So you do see that here that the yellow lines, the most yellow lines are a little bit more shifted

40:49.780 --> 40:50.660
towards zero.

40:50.940 --> 40:53.700
The distributions are a little bit more narrow.

40:53.860 --> 40:57.260
So the correlations are basically closer to zero.

40:57.420 --> 41:04.100
The interpretation of that is that by the time you get to the end of the large language model towards

41:04.100 --> 41:10.260
the last couple of transformer blocks, the model is not really processing the current token anymore.

41:10.260 --> 41:16.760
It's really transformed the current token into the a prediction for the next token.

41:17.800 --> 41:23.920
So you will see this throughout lots of analyses over the next couple of sections that the later you

41:23.920 --> 41:30.720
get into, the less the less the processing is related to the current token, the more it's related

41:30.720 --> 41:34.200
to the predictions about the next token.

41:35.280 --> 41:38.120
This is the final exercise.

41:38.400 --> 41:41.200
Now there isn't really any new coding to do.

41:41.360 --> 41:49.920
You just import the 1.3 billion parameter version of this model and then rerun the entire code file.

41:50.760 --> 41:56.600
Be mindful that this will take several minutes just to load the model, and if you're running it on

41:56.600 --> 42:00.120
the GPU, then that one forward pass is pretty quick.

42:00.480 --> 42:06.920
If you're running this on the CPU, then that one forward pass will itself take several minutes.

42:07.560 --> 42:10.600
Now there are two goals for this exercise.

42:11.080 --> 42:17.230
First is to make sure that your code still works when you use it in a different model.

42:17.750 --> 42:23.710
Now, keep in mind that different companies and different families of models can have different internal

42:23.710 --> 42:31.310
naming conventions, so it's certainly not the case that any code you write should work for every single

42:31.310 --> 42:34.070
possible variant of every LM.

42:34.750 --> 42:42.310
But it is good to have code that works for at least different model sizes from the same type and the

42:42.310 --> 42:43.470
same research group.

42:43.950 --> 42:50.950
And of course, the main purpose of this exercise is to compare the findings qualitatively against the

42:50.950 --> 42:53.070
smaller version of this model.

42:53.550 --> 43:01.670
So therefore I recommend that before rerunning the code, you take screenshots of exercises one through

43:01.710 --> 43:09.430
six, or at least the later exercises so that you can more easily compare with the larger model.

43:10.270 --> 43:13.260
Okay, so enjoy working through this exercise.

43:13.260 --> 43:16.420
And now I will switch to code and discuss the results.

43:17.580 --> 43:21.500
The only code I had to change was here in this code cell.

43:21.500 --> 43:28.820
I just commented these two lines and uncommented these two lines, and then I ran the whole script again

43:28.820 --> 43:29.860
from the start.

43:30.300 --> 43:38.020
Okay, so then of course this is different here we can see there are now 24 blocks, 24 transformer

43:38.020 --> 43:39.500
layers instead of 12.

43:39.820 --> 43:43.620
And you can also see that the embeddings dimensionality is larger.

43:43.620 --> 43:45.660
It's now 2048.

43:45.940 --> 43:50.820
And that means that the MLP internal layers are different as well.

43:51.060 --> 43:59.140
In fact, this is 8192 neurons in the MLP expansion layer.

43:59.580 --> 44:02.460
And of course that number should be familiar to you.

44:02.460 --> 44:06.220
That's the same as the number of tokens that we use.

44:06.380 --> 44:15.370
And so that was another reason why I chose to have 8192 tokens, because now it's just going to add

44:15.370 --> 44:21.530
a little bit of confusion, because when you're working with the analysis and working with the matrices

44:21.530 --> 44:24.730
and indexing matrices, you really need to know.

44:24.730 --> 44:31.490
You really need to be mindful of which dimension corresponds to tokens and which dimension corresponds

44:31.490 --> 44:32.970
to neurons.

44:33.250 --> 44:38.770
So that was part of the reason why I used exactly that number of tokens.

44:39.330 --> 44:39.730
Okay.

44:40.130 --> 44:40.610
Let's see.

44:40.610 --> 44:42.930
The rest of the code is the same.

44:43.210 --> 44:46.730
Uh, you will get some different results here.

44:46.810 --> 44:48.570
So here with the sizes.

44:48.570 --> 44:48.730
Yeah.

44:48.770 --> 44:50.770
You'll get a different, uh, not there.

44:50.770 --> 44:54.810
Here you will get different sizes of the activations here.

44:54.810 --> 44:59.890
Of course, we have more, uh, keys printing out of this dictionary.

45:01.170 --> 45:05.890
And now here is the first place where we already start seeing some differences.

45:06.010 --> 45:13.440
Remember that these histograms look nearly identical for the smaller version of this model.

45:13.600 --> 45:15.640
So the analysis is the same.

45:15.640 --> 45:17.520
The tokens are identical.

45:17.520 --> 45:22.320
It's exactly the same data that we are importing and pushing through the model.

45:22.520 --> 45:27.920
And the model is you know, it's also the model from the same exact company, the same org.

45:27.960 --> 45:28.600
Eleuther.

45:28.880 --> 45:34.440
Uh, it's just that the model now has more parameters and more layers.

45:34.600 --> 45:40.680
And now we really do see a qualitatively different distribution of activations.

45:40.680 --> 45:45.840
So the long tokens have this kind of more Gaussian looking distribution.

45:46.120 --> 45:50.920
And the shorter tokens and also the medium length tokens look a little bit different.

45:50.920 --> 45:51.280
To me.

45:51.280 --> 45:55.960
This looks like there's actually two distinct distributions in here.

45:55.960 --> 46:01.960
One would be a very tight distribution peaking at around minus one half or something.

46:02.120 --> 46:04.160
And then there's another distribution here.

46:04.160 --> 46:09.310
That's something that you could model with like a mixture of Gaussians, but we're certainly not going

46:09.310 --> 46:10.790
to get into that here.

46:10.790 --> 46:17.150
But the main point is just qualitative that the large model and the small model do not necessarily show

46:17.150 --> 46:19.670
the same patterns of results.

46:19.710 --> 46:24.310
Even at this fairly large, you know, level of granularity.

46:25.670 --> 46:25.950
Okay.

46:25.990 --> 46:28.070
So the rest of this code is fine.

46:28.070 --> 46:34.670
You can see again the number is different from what it was before because the neurons are different

46:34.710 --> 46:35.270
of course.

46:35.270 --> 46:37.950
But this number is the same as this number.

46:37.950 --> 46:40.070
That is what we want to confirm.

46:40.590 --> 46:41.390
Uh, let me see.

46:41.430 --> 46:43.630
Now I'm just going to go down and.

46:43.910 --> 46:44.230
Right.

46:44.270 --> 46:49.950
So okay, now when you have this many, uh, line plots, you really don't want to have a legend.

46:49.950 --> 46:53.110
That's why this was commented out in the beginning.

46:53.750 --> 46:55.430
Okay, let's recreate that graph.

46:56.070 --> 46:56.470
Okay.

46:56.510 --> 47:02.750
So here we see that some features are the same as with the small version of the model.

47:02.870 --> 47:05.270
And some features are a little bit different.

47:05.270 --> 47:10.220
So we still do see this shift of the first layer.

47:10.260 --> 47:11.780
The first transformer block.

47:11.820 --> 47:14.620
Immediately after the token embeddings.

47:14.900 --> 47:17.380
That one is shifted to the right.

47:17.820 --> 47:25.420
And as I mentioned, you will see repeatedly that lots of the intermediate layers kind of smoothly transition

47:25.420 --> 47:32.060
from one layer into the next layer in terms of their characteristics and relation to token categories

47:32.060 --> 47:32.740
and so on.

47:33.660 --> 47:40.020
But there tends to be something qualitatively distinct going on in the very first layer, and also in

47:40.020 --> 47:42.820
the very last or last couple of layers.

47:42.820 --> 47:48.820
Shortly before we get to the selection for the next token in the sequence.

47:49.220 --> 47:52.380
And you also don't see exactly quite as clearly.

47:52.380 --> 47:57.980
Maybe I'll change the upper scaling limit of the color here.

47:58.580 --> 47:58.900
Yeah.

47:59.100 --> 48:04.780
You do see this pattern a little bit that the later layers still get tighter, their distributions get

48:04.820 --> 48:11.690
tighter because the processing is less about the current token, and it starts to become more abstracted

48:11.690 --> 48:15.650
and more about selecting the next token in the sequence.

48:15.770 --> 48:20.770
And so actually, yeah, I guess that is a little bit similar to in the smaller version of the model,

48:20.770 --> 48:26.450
although this, uh, compression was a little bit tighter and more localized to the later layers.

48:27.530 --> 48:33.930
Remember that the concept of token length does not exist inside the model directly.

48:34.530 --> 48:40.690
That is, models do not process tokens according to how many characters they contain.

48:40.930 --> 48:47.770
The model just processes the tokens as integers, as vectors of embeddings.

48:48.410 --> 48:55.490
On the other hand, it is also the case that tokens with fewer characters tend to appear more often

48:55.490 --> 48:56.890
in the training text.

48:57.050 --> 49:05.550
So the relationship between activation and token length is probably due to token In frequency and not

49:05.550 --> 49:06.470
length per se.

49:07.230 --> 49:14.790
These kinds of third variable explanations can either be useful interpretations or confounds and flaws

49:14.790 --> 49:21.910
in the research, so make sure that you are aware of these possibilities of alternative explanations.

49:22.590 --> 49:28.630
Nonetheless, it's quite interesting to see that there are some striking qualitative differences across

49:28.630 --> 49:34.030
the different layers, but not necessarily across all of the layers.

49:34.270 --> 49:38.270
So that's a theme that you'll continue observing in mech Interp.

49:39.270 --> 49:44.430
Sometimes you will find that the earliest layers and the latest layers are kind of off doing their own

49:44.430 --> 49:49.270
thing, and lots of layers in the middle are strongly correlated with each other.

49:50.110 --> 49:57.030
The last point that I'll make here is about the assumption of universality, which I introduced in the

49:57.190 --> 50:05.060
videos earlier in the course where I discussed general concepts and criticisms of mechanistic interpretability.

50:05.620 --> 50:13.500
Remember that in this context, universality means that the same principles observed in small models

50:13.620 --> 50:16.420
are also present in large models.

50:17.060 --> 50:24.420
Now, the analyses that you did here in this video were not a definitive case of supporting or disproving

50:24.420 --> 50:25.460
that assumption.

50:25.820 --> 50:33.700
But you can see that some of the macroscopic characteristics are not necessarily qualitatively the same

50:33.700 --> 50:36.260
between small and large models.

50:36.860 --> 50:43.500
Now, it certainly is possible that universality exists for some computational features of models,

50:43.500 --> 50:46.340
but not for all features of models.

WEBVTT

00:02.240 --> 00:10.240
Lots of words, especially in English, end up getting tokenized into a single token, often with a

00:10.240 --> 00:11.680
space in front of it.

00:12.160 --> 00:19.320
And that makes doing research fairly straightforward, because one activations vector often corresponds

00:19.320 --> 00:26.560
to one English word, but lots of other words are made up of multiple tokens.

00:26.760 --> 00:31.760
So let's say you have a target word that you want to look at the activations for.

00:32.360 --> 00:36.720
But that target word has 2 or 3 tokens, maybe even more tokens.

00:36.960 --> 00:38.080
So what do you do?

00:38.760 --> 00:42.160
Answering that question is the goal of this video.

00:42.400 --> 00:45.680
I will tell you what is the best thing to do and explain why.

00:46.000 --> 00:53.800
And then in the next video, you will use this knowledge to analyze category specific responses in MLP

00:53.840 --> 00:54.840
projections.

00:55.840 --> 01:00.380
So here you see three words that are multi Token words.

01:00.380 --> 01:02.020
So let's look at toothpaste.

01:02.060 --> 01:08.940
The word toothpaste comprises two tokens, one for space tooth and one for no space.

01:08.940 --> 01:14.540
And then paste and time machine, I suppose, is technically two words.

01:14.940 --> 01:18.180
So therefore we have one token for each word.

01:18.300 --> 01:25.380
But there are plenty of situations where time machine is treated as if it were one word, so you wouldn't

01:25.380 --> 01:31.180
want to use the word time without machine, and you wouldn't want to analyze the embeddings vectors

01:31.180 --> 01:35.420
for machine unless they were preceded by the token for time.

01:35.740 --> 01:42.820
So even though this might technically be two separate words for the purpose of analysis, we might often

01:42.820 --> 01:46.620
want to treat this as if it were one word.

01:47.140 --> 01:51.700
And here we have a hyphenated word which is represented by three tokens.

01:52.460 --> 01:58.140
So which of these tokens would you focus on if you wanted to do some analyses?

01:58.470 --> 02:04.390
Perhaps you just look at the first token, or maybe the last token in this little mini sequence.

02:04.950 --> 02:11.110
Or maybe you take all of them and you average the embeddings, vectors and the activations and whatever

02:11.110 --> 02:11.990
else you're looking at.

02:11.990 --> 02:15.030
You would average them all together for all of the tokens.

02:15.830 --> 02:19.390
If you would like to take a moment to think about this issue.

02:19.590 --> 02:22.190
Feel free to pause the video and give it some thought.

02:23.350 --> 02:30.710
So it turns out that the best approach is just to analyze the final token in the target word or phrase.

02:31.150 --> 02:38.830
And the reason for this is that when the model sees the token for the word tooth, it does not yet know

02:39.150 --> 02:41.190
what token comes next.

02:41.430 --> 02:47.950
It knows all the tokens that came before, and all of that contextual information gets loaded onto the

02:47.950 --> 02:49.670
embeddings vector for tooth.

02:50.310 --> 02:54.030
But there's no guarantee that the next token will be paste.

02:54.590 --> 02:54.910
Okay.

02:54.950 --> 03:02.810
Now when the model actually sees the token for paste, especially with the preceding without the preceding

03:02.810 --> 03:05.290
space, and the previous token is for tooth.

03:05.450 --> 03:13.290
Then the vector, the embeddings vector for paste will be modulated such that or according to the information

03:13.290 --> 03:19.330
from the preceding token, which is truth, and then the model will know that this is toothpaste, which

03:19.330 --> 03:21.690
is different from any other kind of paste.

03:21.690 --> 03:27.010
And toothpaste is also a very different word, with different meanings and implications compared to

03:27.050 --> 03:28.610
just tooth on its own.

03:29.170 --> 03:36.850
Another way to phrase this is that tooth doesn't know anything about paste, but paste knows a lot about

03:36.890 --> 03:37.490
tooth.

03:37.930 --> 03:46.370
So therefore the idea is that analyzing the token paste actually means also analyzing information about

03:46.410 --> 03:47.290
toothpaste.

03:47.770 --> 03:48.970
Anyway, so that's the idea.

03:48.970 --> 03:51.250
So conceptually that makes sense.

03:51.650 --> 03:54.650
In practice the coding can get a little bit tricky.

03:54.990 --> 04:01.550
So therefore I have a little Python demo to show you, which will also provide code for the code challenge

04:01.550 --> 04:02.830
in the next video.

04:03.350 --> 04:09.670
So before switching to code, let me first give you an overview of what you will see in the code demo.

04:10.270 --> 04:18.910
I will start by exploring some multi-token words with a reminder about the importance of preceding spaces

04:19.070 --> 04:22.230
and the impact that has on tokenization.

04:22.590 --> 04:29.150
That is pretty striking in this toothpaste example, where getting rid of the space in front of toothpaste

04:29.150 --> 04:32.830
gives a very different tokenization, at least for tooth.

04:33.790 --> 04:38.590
Next, I will create a few sentences that contain each of these target words.

04:38.870 --> 04:45.030
And this is partly just to have a little bit of context for the model to have before the target word,

04:45.030 --> 04:52.550
and also as a reminder about the issue of preceding spaces when you want to generate algorithmic text

04:52.550 --> 04:53.190
data.

04:53.890 --> 04:59.650
Okay, now, the way that I've constructed these sentences, I know for a fact that the target words

04:59.650 --> 05:06.690
and the target token sequences are always at the very end, but lots of other cases, including the

05:06.690 --> 05:14.210
code challenge in the next video, you don't necessarily know where exactly so which index in the sequence

05:14.370 --> 05:15.890
a target word appears.

05:17.010 --> 05:23.330
So therefore I will show you some code that might seem a little bit overkill for this demo, but you'll

05:23.330 --> 05:27.130
see that it will be necessary for the next video.

05:27.730 --> 05:31.690
Okay, so here we have a for loop over the sentences.

05:31.690 --> 05:33.330
That's these sentences here.

05:33.330 --> 05:42.050
And then I loop over all of the target words here I'm calculating how many tokens are in this particular

05:42.050 --> 05:43.010
target word.

05:43.370 --> 05:50.810
And then here in this third for loop I'm looping over all of the tokens in this sentence.

05:50.810 --> 05:57.700
So this sequence in the batch and testing whether this token and the preceding several tokens match

05:58.100 --> 05:59.220
the target token.

05:59.900 --> 06:04.060
If it does, then we can save this index for later analyses.

06:04.700 --> 06:08.340
Okay, then I'm going to push all the tokens through the model.

06:08.500 --> 06:11.580
These sentences have different numbers of tokens.

06:11.580 --> 06:19.060
So to input them as a batch we also need to pad the sequences, which means we also need to input not

06:19.060 --> 06:22.860
only the tokens themselves but also the attention mask.

06:22.860 --> 06:29.340
And that's why I have this double asterisk over here to unpack this dictionary of inputs here.

06:29.860 --> 06:30.140
Okay.

06:30.180 --> 06:33.860
And then you can see the size of the hidden states over here.

06:34.180 --> 06:43.260
There are six sentences, 14 tokens including padding and of course 768 embeddings dimensions.

06:43.260 --> 06:45.540
This is with GPT two small.

06:46.220 --> 06:53.400
And finally, I'll do some very simple analyses to look at how the token embeddings change from one

06:53.400 --> 06:54.800
layer to the next.

06:55.360 --> 07:03.360
The way I did this analysis is that for each layer starting from hidden state layer one, I take the

07:03.360 --> 07:10.200
embeddings vector for the final token in the target word, subtract it from the exact same token but

07:10.200 --> 07:14.840
from the previous layer, and then take the norm of that difference vector.

07:15.400 --> 07:21.960
Now later on in the course, I will provide a geometric interpretation of this analysis.

07:22.200 --> 07:28.680
I will refer to it as the length of the path that the token embeddings vector is taking as it traverses

07:28.680 --> 07:29.560
through the model.

07:29.920 --> 07:30.920
But that's for later.

07:30.920 --> 07:38.080
For now, you can just interpret this as a measure of how much the token vector changes from one layer

07:38.080 --> 07:38.960
to the next.

07:39.320 --> 07:44.000
Okay, so I repeat this for the last token in the target word.

07:44.120 --> 07:51.020
The second to last token in the target word, and also for the last two tokens within each layer.

07:51.460 --> 07:54.620
Now, we don't really need to worry about interpreting this stuff here.

07:54.620 --> 08:01.820
I just want to give you a bit of exposure to using these tokens to extract activation patterns from

08:01.820 --> 08:02.460
the model.

08:02.780 --> 08:05.740
Anyway, now let's switch to code and have a look.

08:06.780 --> 08:10.620
We only need a few libraries for this video.

08:10.620 --> 08:15.900
Here I'm importing the GPT two tokenizer and GPT two small.

08:16.380 --> 08:19.740
Okay, here is the example that I showed in the slides.

08:19.740 --> 08:21.660
So I have toothpaste with a space.

08:21.660 --> 08:23.260
Toothpaste without a space.

08:23.300 --> 08:24.540
Same for Time Machine.

08:24.540 --> 08:31.180
And I also have with a hyphen in here for just various spellings.

08:31.940 --> 08:38.380
And then here I am creating some very interesting sentences, including that target word.

08:38.780 --> 08:41.860
Now one of the things I wanted to remind you of here.

08:41.900 --> 08:50.830
Let me print this out here, is that if you are creating text data for running experiments in a language

08:50.830 --> 08:57.230
model, you need to be mindful of whether your target words that you are listing already include spaces

08:57.230 --> 08:59.990
before them or not, so this is sort of awkward.

08:59.990 --> 09:05.550
Most written text does not have a double space before words, and something like this could actually

09:05.550 --> 09:12.190
confuse the model a little bit and have a negative impact on your analyses, so you just need to keep

09:12.190 --> 09:13.190
that in mind.

09:13.310 --> 09:18.270
Uh, you know, it would be a little bit more appropriate to do something like this, although then

09:18.270 --> 09:24.950
of course that's fine when you specify the target words with a preceding space, but not when you don't.

09:24.950 --> 09:28.550
So yeah, just something to be mindful of in the future.

09:29.070 --> 09:29.350
Okay.

09:29.350 --> 09:29.790
Let's see.

09:29.790 --> 09:37.790
So here is this ridiculous piece of code where I am identifying the location of the target, uh, in

09:37.790 --> 09:40.070
each of these sentences here.

09:40.430 --> 09:46.590
So I loop over sentences, loop over, uh, all the different targets that we are looking for, get

09:46.590 --> 09:48.050
the length of the targets.

09:48.050 --> 09:52.650
And that is because each target can have a different number of tokens.

09:53.170 --> 09:57.130
And then yeah, here I'm looping over all of the tokens in the sequence.

09:57.250 --> 09:59.210
Here I say torch dot equal.

09:59.330 --> 10:05.530
And I'm looking for the token sequence for this sentence for the current token.

10:05.690 --> 10:07.930
And then going back target length.

10:07.930 --> 10:12.010
So however long the target is in terms of the number of tokens.

10:12.290 --> 10:16.810
And then does that match the actual target token sequence.

10:17.090 --> 10:21.010
So if those do not match then nothing happens.

10:21.010 --> 10:27.290
We go on to the next token in the sequence, and if these do match, then this means that we are looking

10:27.290 --> 10:30.450
at the end of a target word.

10:30.530 --> 10:36.530
With however many targets, tokens are in the target word like 2 or 3.

10:36.850 --> 10:37.130
Okay.

10:37.170 --> 10:43.650
And then I just say that this sentence has a target word at this index here.

10:43.890 --> 10:50.070
And the reason for the minus one is just because of Python indexing starting from zero, whereas this

10:50.070 --> 10:52.870
loop I start from the target length.

10:53.390 --> 10:54.710
Okay, so we can run that.

10:54.710 --> 10:57.470
And here we see sentence zero.

10:57.470 --> 11:01.430
So the first sentence has the first target at index 12.

11:01.630 --> 11:03.950
So we have six sentences here.

11:03.950 --> 11:05.550
So zero through five.

11:06.190 --> 11:08.230
But we're missing a couple of targets.

11:08.230 --> 11:10.830
So some targets are just not identified here.

11:10.830 --> 11:15.190
In particular targets one and two and four.

11:15.190 --> 11:21.550
And those all correspond to the targets that I specified without the preceding space.

11:21.750 --> 11:22.270
Very good.

11:22.270 --> 11:25.070
So now we know where the targets appear.

11:25.070 --> 11:29.230
The final token for each target word in the sentence.

11:29.350 --> 11:31.910
And yeah then I can do a forward pass.

11:31.910 --> 11:34.870
Here we get the hidden states as well.

11:35.030 --> 11:41.830
And now here I'm going to loop over all of the sentences and now all of the layers and grab the activations

11:41.830 --> 11:47.410
just for the final token of the target word in each sentence.

11:47.970 --> 11:55.770
And I'm doing this twice for the final token, and for the last or the second to last token in the sentence.

11:56.170 --> 11:57.970
Okay, so that runs super fast.

11:57.970 --> 12:07.610
So we have 13 layers, six sentences, 768 embeddings dimensions and two tokens, the final token of

12:07.610 --> 12:12.010
the target word and the second to last in the target word.

12:12.450 --> 12:12.610
Yeah.

12:12.610 --> 12:15.650
And then here I don't really want to get into this too much.

12:15.650 --> 12:22.250
This is just a quick demo of using these activations for doing some simple analyses and some simple

12:22.690 --> 12:23.930
visualizations.

12:23.930 --> 12:29.570
In the next video in the code challenge, you're going to follow a kind of similar procedure, although

12:29.570 --> 12:34.890
it's going to be more complicated than what I've shown here, but you're going to be doing more interesting

12:34.930 --> 12:37.370
analyses that what I have done here.

12:37.810 --> 12:43.820
So here basically, I'm looping over all of the hidden states layers from index one.

12:44.420 --> 12:51.180
Remember that index one actually corresponds to the first transformer block, because zero is the embeddings

12:51.500 --> 12:52.700
the initial embeddings layer.

12:52.740 --> 12:56.980
I know I've repeated that like a million times, but I will probably say it again.

12:57.420 --> 13:04.180
So here I'm getting the difference in the embeddings vector for this layer compared to the previous

13:04.180 --> 13:09.660
layer, and then calculating the norm of that difference vector and then plotting that.

13:09.660 --> 13:16.220
So essentially this is just measuring how much the embeddings vector changes from one transformer block

13:16.220 --> 13:16.980
to the next.

13:17.220 --> 13:23.260
And all of those changes of course, are attributable to the adjustments that are calculated and added

13:23.260 --> 13:30.380
on to the residual stream by the attention and by the MLP sub blocks within each transformer block.

13:30.980 --> 13:32.820
So that's it for this video.

13:32.940 --> 13:38.820
As I mentioned in the beginning of this video, you'll get to use some of the code here in the code

13:38.820 --> 13:40.500
challenge in the next video.

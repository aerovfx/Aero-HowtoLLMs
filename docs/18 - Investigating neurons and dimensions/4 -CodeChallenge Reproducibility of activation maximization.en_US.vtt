WEBVTT

00:04.520 --> 00:11.680
This code challenge follows directly from the previous video, including that much of this code challenge

00:11.680 --> 00:18.240
can be solved by copying and modifying the code that I explained in the previous video.

00:18.960 --> 00:25.680
That also means that you can determine the level of difficulty of this code challenge by deciding how

00:25.680 --> 00:29.600
much code you want to copy versus write from scratch.

00:30.280 --> 00:37.680
The basic idea will be to evaluate the reproducibility of the results that we saw in the previous video.

00:38.280 --> 00:43.520
Testing for reproducibility is crucial for progress in science.

00:43.720 --> 00:49.280
It's so important that no matter what the results, that is something that you want to know.

00:49.760 --> 00:56.560
That is to say, it's important to know what patterns and findings are reproducible, and it is equally

00:56.560 --> 01:01.760
important to know what findings and conclusions are not reproducible.

01:03.080 --> 01:07.640
Exercise one seems like a great exercise to start with.

01:07.920 --> 01:15.600
The goal here is basically to create a function that you can call that implements the optimization that

01:15.600 --> 01:18.560
you learned how to do in the previous video.

01:19.080 --> 01:24.800
We're going to work with the GPT two small model so you can import that.

01:24.800 --> 01:32.080
And it's also convenient to have a separate copy of a variable of the token embeddings matrix, just

01:32.080 --> 01:33.800
like in the previous video.

01:33.960 --> 01:39.720
And that is basically just for convenience to reduce the amount of code that you need to write later

01:39.720 --> 01:40.080
on.

01:40.760 --> 01:45.440
Anyway, your function to do the optimization can look something like this.

01:45.840 --> 01:53.480
So you will recognize the overall flow of the code here based on the comments from the previous video,

01:54.560 --> 02:00.000
so you can see what steps of this analysis I chose to put into this function.

02:00.440 --> 02:09.240
Now at the end I am outputting the optimized embeddings vectors, but I chose not to do the cosine similarity

02:09.240 --> 02:10.480
or token matching.

02:10.720 --> 02:15.640
Inside of this function you could include that in your function if you like.

02:15.840 --> 02:24.040
The purpose of having a function here is that later on in the next exercise, we will call this function

02:24.040 --> 02:30.000
multiple times using different random initializations of an embeddings matrix.

02:30.000 --> 02:35.160
So having this function basically just allows you to simplify the code later.

02:35.880 --> 02:42.720
Anyway, once you have this function you can test it by running it once and then looking at the optimized

02:42.760 --> 02:44.120
token sequence.

02:44.440 --> 02:49.280
So when I created these slides this was the optimized sequence that I got.

02:49.960 --> 02:51.480
So that's for exercise one.

02:51.520 --> 02:56.280
Now you should pause the video and switch to code and implement that function.

02:56.400 --> 03:00.080
And now I will switch to code and discuss my solution.

03:00.840 --> 03:02.600
Import some libraries.

03:03.000 --> 03:09.190
And let's see here I'm importing the model and extracting the embeddings matrix.

03:09.230 --> 03:10.950
Pushing everything to the GPU.

03:11.110 --> 03:19.390
And of course, calling Model.eval because we are not training the model so it can be switched into

03:19.430 --> 03:20.310
eval mode.

03:21.030 --> 03:21.510
Okay.

03:21.550 --> 03:23.990
And then here you can see the training function.

03:23.990 --> 03:30.790
There's no new code inside this function that you have not seen in the previous video.

03:31.190 --> 03:40.550
So inside this function I begin by creating an initial random optimized embeddings using the same sequence

03:40.550 --> 03:43.190
length of five as in the previous video.

03:43.430 --> 03:46.110
Of course, you can change these parameters if you like.

03:46.350 --> 03:51.790
And then here I'm normalizing it so that it has the same distributional characteristics, at least in

03:51.790 --> 03:59.350
terms of the standard deviation as the real embeddings matrix that is already trained into GPT two.

03:59.950 --> 04:00.350
Okay.

04:00.390 --> 04:02.150
And then yeah, here's the optimizer.

04:02.150 --> 04:03.870
You've seen this as well before.

04:04.150 --> 04:05.990
And all of this stuff you've seen.

04:06.030 --> 04:08.230
I'm really not going to discuss it.

04:08.670 --> 04:14.830
Although you know just as a quick reminder, the idea is that we push the embeddings into the model,

04:14.830 --> 04:21.750
not the tokens, but the embeddings directly into the model from which we extract the hidden states

04:21.790 --> 04:25.470
at the outputs of each transformer block.

04:25.750 --> 04:33.190
And then I extract the average so the average over all of the token positions for this particular dimension

04:33.190 --> 04:36.070
from this particular transformer layer.

04:36.310 --> 04:44.550
And that is the thing that we want to maximize, which means calling the loss function as the minus

04:44.710 --> 04:45.510
of that.

04:45.990 --> 04:46.350
Okay.

04:46.390 --> 04:48.150
So that is that function.

04:48.350 --> 04:49.870
Here I'm just going to test it once.

04:49.870 --> 04:51.790
So sequence length of five.

04:51.990 --> 04:57.030
This is just the same parameters as in the previous video.

04:57.230 --> 05:00.030
Uh and now so I'm running one test.

05:00.190 --> 05:07.270
And then here I'm going to get the cosine similarities from all of the individual embeddings vectors

05:07.270 --> 05:08.590
from this matrix.

05:08.750 --> 05:15.270
And then calculating cosine similarity with the real embeddings and then getting the maximum.

05:15.270 --> 05:19.750
So which token has the maximum cosine similarity.

05:19.910 --> 05:24.350
And then decoding that to give us our shooter in camp.

05:24.870 --> 05:28.150
Now for the reproducibility exercise.

05:28.510 --> 05:37.310
Run the function from exercise 110 times and store all of the optimized tokens in a matrix.

05:37.510 --> 05:40.830
Here's an example of what your results should look like.

05:41.230 --> 05:50.510
So this is a matrix of ten by five, corresponding to the ten repeats of the optimization and five tokens

05:50.510 --> 05:52.190
in this sequence.

05:52.550 --> 06:00.630
Remember that the optimization is identical in every one of these ten runs, except that the random

06:00.630 --> 06:07.910
embeddings vectors are re-initialized in each of the ten runs, but the model is exactly the same,

06:07.910 --> 06:11.270
the code is the same, the loss functions are the same, and so on.

06:11.830 --> 06:20.230
So if there really is something Consistent about this sequence of five tokens that activates one dimension

06:20.230 --> 06:21.430
in the model.

06:21.790 --> 06:28.070
Then this set of five tokens should be identical in each of these ten runs.

06:28.910 --> 06:34.510
Well, okay, there's noise and random stuff happening in here, but let's say that we do expect some

06:34.510 --> 06:42.550
consistency so that the token sequences will be similar, if not identical, for all ten of these runs.

06:42.590 --> 06:44.350
Yeah, even if they're not identical.

06:44.750 --> 06:48.030
And if that really were the case, then there would be.

06:48.070 --> 06:51.350
So there's 50 tokens in total in this matrix.

06:51.550 --> 06:59.830
But if there were perfect reproducibility there would only be five unique tokens because yeah, perfect

06:59.830 --> 07:08.230
reproducibility would mean that each row in this matrix would have exactly the same tokens in exactly

07:08.230 --> 07:09.430
the same sequence.

07:10.110 --> 07:17.500
And that leads us to the next part of exercise two, which is to count and print the total number of

07:17.500 --> 07:21.580
unique optimized tokens in this matrix.

07:22.020 --> 07:28.580
Again, perfect reproducibility would predict five out of 50 tokens are unique.

07:28.580 --> 07:35.940
And down here in these rows, you can report the number of times that each specific token appeared for

07:35.940 --> 07:37.420
all of the unique tokens.

07:38.020 --> 07:45.580
Again, you know, even in the case of reproducibility, just with random initializations and noise.

07:45.780 --> 07:51.060
Or maybe we just haven't done enough training to settle on the right solution.

07:51.100 --> 07:57.620
Maybe it won't be exactly five out of 50 unique tokens, but it would still be quite compelling if there

07:57.620 --> 08:01.260
are, let's say, 10 or 20 unique tokens out of 50.

08:01.860 --> 08:05.860
Anyway, so run this analysis to get an output that looks like this.

08:05.980 --> 08:09.460
By the way, this is not the entire set of results here.

08:09.460 --> 08:13.540
I just cropped it at the first couple of lines for visualization.

08:14.420 --> 08:18.260
Okay, so now you should pause the video and work through this exercise.

08:18.380 --> 08:24.540
And now I'll switch to code and show my solution and have some discussion about these results.

08:25.180 --> 08:30.460
Here I specify to do ten repeats, ten repetitions of this experiment.

08:30.660 --> 08:34.980
And here is the matrix where I will store all of the optimized tokens.

08:34.980 --> 08:38.780
So this is going to be a ten by five matrix.

08:39.100 --> 08:40.660
Here is the for loop.

08:40.660 --> 08:45.380
Here I'm calling the function that we wrote in the previous exercise.

08:45.500 --> 08:52.700
And then here I'm decoding all of the tokens or all of the embeddings in from this result here.

08:53.020 --> 09:00.220
And as I mentioned in the when I was explaining exercise one, if you like, you could also put this

09:00.220 --> 09:06.980
code inside this training function, in which case your results would actually be really simple.

09:07.420 --> 09:10.780
Or the code for exercise two would basically just look like this.

09:11.100 --> 09:11.380
Okay.

09:11.420 --> 09:13.580
But so this is how I chose to run it.

09:13.580 --> 09:17.860
This will take a minute or something to run, so I'll pause the recording here.

09:18.860 --> 09:21.380
So that took around one minute to run.

09:21.500 --> 09:23.620
and this is on the GPU.

09:23.940 --> 09:29.740
If you are curious, you could try running this again on the CPU and see how long that takes.

09:29.780 --> 09:30.940
It actually won't be.

09:30.980 --> 09:37.780
I think it won't be too big of a difference because we're not actually training the model, we're just

09:37.780 --> 09:45.540
training the one embeddings matrix, and that training could easily be done on the CPU because the matrix

09:45.540 --> 09:46.500
is not so big.

09:46.780 --> 09:55.060
But even doing a forward pass with GPT two small is the difference between several seconds on the CPU

09:55.100 --> 09:58.180
versus much less than a second on the GPU.

09:58.220 --> 10:01.220
But anyway, okay, so it finished ten repeats.

10:01.340 --> 10:06.740
Here we can see all of the tokens, and it's not immediately so obvious.

10:06.740 --> 10:08.860
Oh, we actually do get some repetition here.

10:08.860 --> 10:09.580
So there you go.

10:09.820 --> 10:13.460
We get an identical token appearing at least once.

10:13.500 --> 10:16.060
Let's see what the quantification shows.

10:16.460 --> 10:25.220
So here I'm using numpy uniq to extract all of the unique tokens and also get all of the counts.

10:25.500 --> 10:31.220
So it turns out that there are 48 unique tokens out of a total of 50.

10:31.260 --> 10:34.220
So there are two tokens that were repeated twice.

10:34.220 --> 10:36.620
They were optimized for twice.

10:36.620 --> 10:42.500
That's this one, which is sup, and this one which is the Nitrome fan.

10:42.500 --> 10:44.380
I have no idea what that means.

10:44.380 --> 10:48.060
Maybe that's like a Reddit handle, like a username or something.

10:48.380 --> 10:50.820
But anyway, so what do we make of these results?

10:50.820 --> 10:54.340
So clearly there is not something that is super reproducible.

10:54.820 --> 10:59.740
On the other hand, this does you know, this is a little bit of reproducibility.

10:59.780 --> 11:04.820
Remember that there are 50,000 tokens approximately.

11:05.060 --> 11:12.020
And so the chance, you know, the probability of getting the same token twice is just really, really,

11:12.020 --> 11:13.780
really, really, really tiny.

11:14.020 --> 11:18.420
If we just take every token, select every token at random.

11:18.740 --> 11:25.340
So the fact that we get anything less than 50 is already, you know, a little bit interesting.

11:25.340 --> 11:25.490
Thing.

11:25.850 --> 11:28.570
That said, this is certainly not overwhelming.

11:28.570 --> 11:30.570
Compelling evidence for.

11:31.130 --> 11:38.250
Transforming random matrices into a particular token sequence that maximizes.

11:38.410 --> 11:44.770
One particular feature of the model in a way that is really compellingly reproducible.

11:45.090 --> 11:47.250
So that's it for this code challenge.

11:47.250 --> 11:54.610
I hope you agree with me that null findings are still very insightful, because they highlight the complexity

11:54.610 --> 12:00.530
of models, and they help us carve out a space for what works and what doesn't work.

12:01.330 --> 12:07.330
On the other hand, null results can also be difficult to interpret because it is actually possible

12:07.330 --> 12:13.690
that the analysis we tried here could work if we ran it at a finer granularity.

12:14.250 --> 12:22.850
That is, imagine that we had done this analysis for one specific neuron inside an MLP layer in a transformer

12:22.850 --> 12:26.530
block, and maybe there we could get great results.

12:26.730 --> 12:33.410
Furthermore, it is possible that with some additional priors and additional constraints, so that we're

12:33.410 --> 12:40.770
not using totally random embeddings vectors, that we might get some more sensible, interpretable results.

12:40.770 --> 12:46.690
As I discussed a couple of videos ago when I introduced activation maximization.

12:47.530 --> 12:55.370
Of course, here we are already running into a scaling issue where there are just so many neurons inside

12:55.370 --> 13:00.370
all of the MLP layers, the attention blocks and the transformers and so on.

13:00.570 --> 13:06.490
And this is only just GPT two small, let alone any of the larger models.

13:06.850 --> 13:13.970
So I don't really recommend repeating this analysis for every single neuron, every single parameter

13:13.970 --> 13:14.890
in the model.

13:15.090 --> 13:22.170
That said, the goal of the next two videos is to show you code that actually would allow you to access

13:22.290 --> 13:29.530
any individual piece of the model, including, for example, one specific MLP neuron in the expansion

13:29.530 --> 13:30.090
layer.

WEBVTT

00:02.480 --> 00:10.520
In the previous code challenge, I discussed that processing one token at a time might not be really

00:10.520 --> 00:15.160
representative of how the model processes tokens in general.

00:15.880 --> 00:23.560
On the other hand, when tokens are processed in a text, then they're not actually processed in isolation,

00:23.720 --> 00:28.760
but are strongly impacted and modulated by the rest of the text.

00:29.080 --> 00:36.280
So that means that it's really difficult, possibly just not even possible, to even consider what it

00:36.280 --> 00:43.160
would mean for a model to process a token in its pure form, the way that you and I can think about

00:43.200 --> 00:44.600
an individual word.

00:45.320 --> 00:52.800
Anyway, that is the challenge, but also the important concept of how llms work that we will explore

00:53.080 --> 00:55.000
in this code challenge.

00:55.480 --> 01:02.290
We will continue working with the same MLP neurons that we isolated in the previous video.

01:02.930 --> 01:10.090
In fact, exercise one is really just a copy and paste some of the code from the previous code challenge.

01:10.650 --> 01:19.170
In particular, import the Eleuther model, implant the same hook, and import the 100 common nouns,

01:19.330 --> 01:21.610
all from the previous code challenge.

01:21.730 --> 01:24.650
You don't need to import the verbs for this code challenge.

01:25.410 --> 01:31.450
So that's it for this exercise, it's really just copy pasting into a new notebook file.

01:31.810 --> 01:36.410
I will switch to code, but just very briefly just to show the code that I copied.

01:36.410 --> 01:40.170
Over here are the libraries that we will use.

01:40.170 --> 01:43.530
And here I'm importing the model.

01:43.650 --> 01:50.010
And actually one very slight difference here compared to the previous code challenge, is that I actually

01:50.050 --> 01:54.410
am using the GPU for this code challenge.

01:54.410 --> 01:55.650
It's not really necessary.

01:55.650 --> 01:57.570
You can still do this on the CPU.

01:57.860 --> 02:02.020
I chose to use the GPU here just to save a little bit of time later on.

02:02.340 --> 02:03.980
Okay, so import the model.

02:04.340 --> 02:08.500
Here is the hook function exactly the same as the previous code challenge.

02:08.620 --> 02:13.820
And here I'm importing the nouns exactly the same as in the previous code challenge.

02:14.740 --> 02:17.460
Now let's start doing some analyses.

02:17.900 --> 02:23.100
The goal here will be to measure the activations for those 100 words.

02:23.100 --> 02:30.620
So the 100 nouns from the previous code challenge that you imported in exercise one, but with and without

02:30.620 --> 02:32.420
preceding spaces.

02:33.220 --> 02:41.300
As you know from the beginning of the course on tokenization, the GPT two tokenizer has a lot of words

02:41.300 --> 02:47.580
that actually start with spaces, because that's how they often appear in actual text.

02:48.100 --> 02:54.260
But the list of nouns that we are working with from the previous code challenge, and that we just imported,

02:54.460 --> 02:56.750
those do not start with spaces.

02:56.750 --> 02:58.470
They start with characters.

02:58.990 --> 03:06.110
So the goal here is to repeat the for loop and get all of the activations for all the individual words.

03:06.110 --> 03:13.710
But now you want to do it twice, once just for the word itself, and then again with the same word,

03:13.710 --> 03:17.070
but adding a space before the word appears.

03:17.710 --> 03:22.710
Now once you have those activations, you can visualize them in a few different ways.

03:23.230 --> 03:32.510
Here on the left is a scatter plot where you can show the activations for the words with no space versus

03:32.550 --> 03:36.830
the activations for the same words, but with a preceding space.

03:38.030 --> 03:40.190
And I also set the color of each dot.

03:40.230 --> 03:43.150
You don't see this here, but the color of each dot.

03:43.190 --> 03:50.510
In this scatter plot, I defined to be the absolute value of the difference in the activation between

03:50.750 --> 03:51.950
the two axes.

03:52.350 --> 03:57.160
And then I took the square root of that difference, basically just to make the color look nice.

03:57.160 --> 03:59.400
But don't worry about the visual aesthetics.

03:59.520 --> 04:06.200
You can see in the title that I've also calculated the correlation coefficient between these two pairs

04:06.200 --> 04:07.360
of activations.

04:08.040 --> 04:10.080
So that's the scatter plot over here.

04:10.240 --> 04:19.320
Here on the right is our histograms of the activations across all of the MLP neurons from this one transformer

04:19.320 --> 04:23.560
block for the words with the space and without the space.

04:24.320 --> 04:26.200
So that's it for this exercise.

04:26.200 --> 04:29.240
You can now pause the video and start coding away.

04:29.360 --> 04:31.120
And now I will switch to code.

04:32.160 --> 04:34.680
Let me actually begin by.

04:35.000 --> 04:36.800
Let's have a look at some of these nouns.

04:36.800 --> 04:38.680
I just want to make sure it's clear.

04:38.680 --> 04:41.040
So let's start with school because that's a good word.

04:41.040 --> 04:49.560
So we have school and then we have I'm going to tokenize it tokenizer dot encode.

04:50.080 --> 04:54.730
And this happens to be index 14,347.

04:54.730 --> 05:00.690
But if I put a space beforehand, this is going to be a different token.

05:01.050 --> 05:05.450
So this you will remember from the very beginning of the course.

05:05.450 --> 05:12.330
But basically the idea is that we have different tokens for words with versus without preceding spaces.

05:12.330 --> 05:19.330
And that means that the model might process these words differently depending on a preceding space.

05:19.610 --> 05:24.690
Okay, so that is just a little demo of why we are doing exercise two.

05:25.290 --> 05:31.530
All of this code here looks really, really similar to the code from exercise two.

05:31.570 --> 05:35.370
In the previous code challenge, it's mostly copy pasted.

05:35.410 --> 05:42.010
You can see the main thing that I changed here was instead of selecting for one particular noun, and

05:42.010 --> 05:43.690
here was one particular verb.

05:43.850 --> 05:46.650
Here we're just looping over all of the nouns.

05:46.770 --> 05:48.610
And here I just have the word.

05:48.610 --> 05:51.090
I actually really don't need this stuff here.

05:51.130 --> 05:51.900
This is fine.

05:52.300 --> 05:57.420
And then here I'm just having a space in front of the word okay.

05:57.500 --> 05:59.820
Otherwise the code is basically the same.

06:00.060 --> 06:03.300
Essentially we are doing a forward pass on one token.

06:03.780 --> 06:04.420
Kind of weird.

06:04.420 --> 06:06.940
There's no context, but that's what we're doing here.

06:07.260 --> 06:14.340
And then extracting the activations from all of the MLP neurons from this one layer.

06:14.700 --> 06:14.940
Okay.

06:14.980 --> 06:17.300
And then here is where I'm generating the plot.

06:17.420 --> 06:20.940
So here I flatten out all of the activations.

06:21.060 --> 06:26.100
That's because this is for 100 words and 3000 neurons.

06:26.260 --> 06:30.860
And then I'm flattening them or vectorizing them to be one vector.

06:31.100 --> 06:34.420
And then yeah, it's just for convenience for the plotting.

06:34.740 --> 06:34.940
Okay.

06:34.980 --> 06:38.820
And then here is the difference that I'm using for color coding.

06:38.820 --> 06:42.660
So the difference between them and then the absolute value.

06:42.660 --> 06:47.260
So you can think of that as like the L1 distance between the two activations.

06:47.340 --> 06:50.270
And then I square root and then I divide by the max.

06:50.310 --> 06:56.750
There is no, like, deep, meaningful, uh, theoretical or mathematical motivation for this transform.

06:56.750 --> 07:01.190
It just makes the plot look nice and easy to see the colors.

07:01.510 --> 07:06.870
Okay, so here's where I make the scatter plot of Without Space by with a space.

07:07.110 --> 07:10.110
Uh, and I'm setting the color to be the difference values.

07:10.110 --> 07:13.030
I'm using the map plasma reverse.

07:13.030 --> 07:17.830
I like this color map, and I found reverse just looks nicer for this plot.

07:18.150 --> 07:19.710
Again, with visualization.

07:19.830 --> 07:26.950
You know, sometimes there are decisions that you make when visualizing data that are based on meaningful

07:26.950 --> 07:30.990
theoretical considerations, and sometimes it's just personal preferences.

07:31.470 --> 07:33.950
Okay, here I am creating histograms.

07:33.950 --> 07:37.310
And yeah these are going to be density based histograms.

07:37.310 --> 07:38.070
And yeah.

07:38.070 --> 07:41.510
So let's have a look at the plots and see what we can see.

07:42.230 --> 07:47.550
So again here on the x axis is the nouns with no preceding space.

07:47.550 --> 07:50.760
And here is with a preceding space.

07:51.080 --> 07:55.840
All of these yellow lines that lie exactly on the line of unity.

07:56.320 --> 08:03.800
Those correspond to basically the activations of the neurons are nearly identical, possibly really

08:03.800 --> 08:09.480
identical, but at least close to identical processing for the two different tokens.

08:09.760 --> 08:16.480
Uh, even though, yeah, we humans would recognize it as the same word, but these models recognize

08:16.480 --> 08:22.320
them as different tokens, although, of course, they learn through lots of training that, uh, the

08:22.320 --> 08:24.840
words should be processed in very similar ways.

08:25.560 --> 08:30.280
Uh, they are the two activation patterns are very strongly correlated with each other.

08:30.400 --> 08:32.680
But you can see that there are some differences.

08:32.680 --> 08:40.600
So the model really is processing the words differently, the tokens differently depending on whether

08:40.600 --> 08:47.770
there is no space or with a space preceding, uh, looks like most of them kind of lie close to the

08:47.770 --> 08:48.250
diagonal.

08:48.250 --> 08:50.290
There are a couple of weirdos out here.

08:50.530 --> 08:52.410
I'm not sure what's going on with these.

08:52.410 --> 08:53.530
You could find out.

08:53.530 --> 08:58.050
You can see that these are the deepest color purple from plasma.

08:58.050 --> 09:05.250
So if you want to know which words, which tokens these correspond to, what you could do is look for

09:05.250 --> 09:07.370
the largest differences.

09:07.370 --> 09:09.850
So where is this variable maximum.

09:10.010 --> 09:14.370
And then go back and find those words and those tokens.

09:14.410 --> 09:14.610
Right.

09:14.650 --> 09:15.890
There's a lot of dots in here.

09:15.930 --> 09:19.770
Each dot corresponds to one word from one token.

09:19.770 --> 09:23.450
That's all vectorized from that big matrix okay.

09:23.490 --> 09:25.210
And then here we see the histograms.

09:25.210 --> 09:30.250
They are not exactly identical but basically identical really really similar.

09:30.250 --> 09:37.770
So this indicates that the model is processing these words very very similarly depending regardless

09:37.770 --> 09:44.370
of whether they have a preceding space or no preceding space, even though they are technically different

09:44.370 --> 09:45.210
tokens.

09:46.460 --> 09:54.620
in exercise two, there was still no context, no text, just individual words that we'd put a space

09:54.620 --> 09:55.340
in front of.

09:55.940 --> 10:03.900
Still, words don't appear in isolation anywhere when pre-training or fine tuning these models, so

10:04.100 --> 10:06.020
it's still a little bit artificial.

10:06.140 --> 10:14.100
So therefore the goal of exercise three and four is to compare the processing of individual tokens with

10:14.100 --> 10:18.020
the exact same tokens that are embedded in a text.

10:18.300 --> 10:24.860
So to get some text for this analysis, you can actually have the model generate text on its own.

10:25.460 --> 10:30.980
You can generate 200 tokens starting from some short sequence that you can define.

10:31.460 --> 10:34.420
I chose to start the model off with the text.

10:34.420 --> 10:36.820
I think the world could be better if.

10:37.500 --> 10:44.800
And then I generated new tokens up to a total of 200 tokens to see what the model thinks would make

10:44.800 --> 10:46.320
the world a better place.

10:46.680 --> 10:54.320
So when you get this text that the model generated using the generate method, then take those tokens,

10:54.320 --> 10:58.800
the generated tokens, and push them back through the same model.

10:58.960 --> 11:09.400
To get the MLP neuron activations, you should get an activation matrix of size one by 200 by 3072,

11:09.680 --> 11:17.680
corresponding to one sequence of 200 tokens in this batch and 3000 MLP neurons.

11:18.080 --> 11:24.720
We are going to analyze these data in the next exercise, but for now you can switch to code and just

11:24.720 --> 11:26.440
get those activations.

11:26.680 --> 11:30.080
And now I will also switch to code and show my results.

11:30.960 --> 11:38.920
I think the world could be better if you know what I love about GPT two, GPT two makes you think and

11:38.920 --> 11:42.280
GPT four tries to do your thinking for you.

11:42.330 --> 11:45.810
I always really enjoy interacting with GPT two.

11:46.090 --> 11:53.090
Here I'm specifying 200 tokens maximum and remember from earlier in the course that this means that

11:53.090 --> 11:59.130
we get 200 tokens in total, which includes however many tokens this is.

11:59.290 --> 12:06.010
So this really means maybe like 190 tokens that the model generates, plus, you know, approximately

12:06.010 --> 12:10.010
ten or whatever, however many tokens this is that I wrote.

12:10.050 --> 12:12.130
Of course, for these purposes, that's fine.

12:12.130 --> 12:14.850
I'm just giving you a reminder about that.

12:15.010 --> 12:20.890
So I think the world could be better if we just kept moving forward to a new world where we can do anything

12:20.890 --> 12:21.650
we want.

12:21.690 --> 12:25.490
And if our leaders could really work together, that is such a true statement, man.

12:26.050 --> 12:32.690
So now I take all of those generated tokens and then push them back through the model.

12:32.890 --> 12:34.970
So these are the tokens outputted.

12:34.970 --> 12:41.580
And then these are the tokens that I push right back through the model in order to get the activations

12:41.580 --> 12:41.820
here.

12:41.820 --> 12:44.140
So it's 200 by 3000.

12:44.140 --> 12:51.780
And here I'm just setting another variable just to get these activations into a different variable.

12:51.780 --> 12:57.220
That's going to make it more convenient to go through the analysis in the next exercise.

12:58.220 --> 13:02.500
Now for the final exercise in this code challenge.

13:03.020 --> 13:11.540
Now that you have all the activations for all 200 tokens together as one sequence, get all of the activations

13:11.540 --> 13:18.500
from the individual neurons again, but now in a for loop over each token individually.

13:19.140 --> 13:26.620
So that means you're going to have two sets of activations from all of the MLP neurons, one set of

13:26.620 --> 13:31.180
activations coming from the tokens processed embedded in the text.

13:31.180 --> 13:33.300
That's what you did in exercise three.

13:33.900 --> 13:41.150
And then another set of activations from all of the tokens processed individually, pushed individually,

13:41.150 --> 13:43.070
one at a time through the model.

13:43.070 --> 13:46.350
And that's what you're going to do in this exercise.

13:47.230 --> 13:54.190
Then you can generate a figure that is basically the same as the one that you created in exercise two,

13:54.710 --> 13:59.350
except that was about having spaces or not having preceding spaces.

13:59.550 --> 14:06.430
And now here this is about the activations for the isolated tokens pushed through the model, one at

14:06.430 --> 14:12.230
a time, versus the tokens from the text that are embedded in the text.

14:12.750 --> 14:20.230
And then one thing I added over here is the difference between the activations as a histogram.

14:20.630 --> 14:25.270
I hope you find this exercise to be thought provoking and interesting.

14:25.430 --> 14:28.430
And I will now switch to code and discuss the results.

14:29.110 --> 14:37.070
So here in this for loop, I'm looping over all of the generated tokens, pushing one token at a time

14:37.070 --> 14:43.640
through the model getting its activation here, and then also copying over the activation from when

14:43.640 --> 14:48.360
I pushed all of the tokens together as one sequence through the model.

14:48.360 --> 14:52.720
This is the variable that I created at the end of exercise three.

14:53.200 --> 14:54.560
Okay, so let's run that.

14:54.560 --> 14:56.120
That will take a few seconds.

14:56.240 --> 15:00.400
If you're doing this on the CPU it will take a few additional seconds.

15:00.400 --> 15:03.880
But overall it's not so bad with this model okay.

15:03.920 --> 15:10.200
And then here yeah this code is basically identical to the code for exercise two.

15:10.400 --> 15:16.640
The only thing I added was this line here where I am getting the histogram of the difference in the

15:16.640 --> 15:21.560
activations for the two sets of analyses.

15:21.560 --> 15:25.400
So in the text versus individual tokens.

15:25.760 --> 15:27.880
So let's see what those look like.

15:29.040 --> 15:33.560
These look quite different from the results from exercise two.

15:34.000 --> 15:40.450
Remember in exercise two we found that whether there is a space or isn't a preceding space didn't really

15:40.450 --> 15:40.810
matter.

15:40.810 --> 15:45.970
The activations were technically a little bit different, but they were still really, really similar.

15:46.290 --> 15:54.850
But now we see much more differences, much stronger differences between the two pairs of tokens.

15:54.850 --> 16:03.050
And again, remember every one of these dots here corresponds to literally exactly the same token activation

16:03.690 --> 16:06.610
in exactly the same MLP neuron.

16:07.090 --> 16:14.050
The only difference is that on the y axis we have when the tokens were pushed through one at a time

16:14.050 --> 16:21.650
in a for loop, and with no preceding context versus the x axis, where every token except for the first

16:21.650 --> 16:26.330
I guess, but every other token has at least some context behind it.

16:26.330 --> 16:33.490
There are tokens behind that individual token that allow the model to modulate the processing and the

16:33.490 --> 16:37.780
representation of that individual embeddings vector.

16:37.780 --> 16:41.220
And then here we see the corresponding activations.

16:41.660 --> 16:44.140
So really quite striking I find.

16:44.500 --> 16:47.220
And then you see also in these histograms.

16:47.220 --> 16:53.980
Now do not be confused by the similarity between the histograms for the blue line and the orange line.

16:54.460 --> 17:01.420
What those show is that the marginal distributions cover, you know, the same range, roughly the same

17:01.420 --> 17:04.940
range of activation numerical values.

17:05.380 --> 17:11.020
It is very possible for the distributions to be overall very similar.

17:11.380 --> 17:16.980
And yet the differences between individual pairs to be very different.

17:16.980 --> 17:24.220
And that's why the green histogram is basically just as wide as the distributions from the individual

17:24.220 --> 17:25.060
variables.

17:25.180 --> 17:28.100
So let me explain that again.

17:28.100 --> 17:35.430
So basically the blue line and the orange line correspond to the Uh, distributions over the x axis

17:35.430 --> 17:40.390
or like projected onto the x axis, projected onto the y axis.

17:40.790 --> 17:47.510
The green line actually shows the histogram of this like diagonal, this orthogonal projection from

17:47.510 --> 17:53.590
each of these dots onto the line of unity, which you can basically see as these yellow colors over

17:53.590 --> 17:54.070
here.

17:54.710 --> 18:03.110
So we do see that there are tokens and neurons where the processing is really, really similar, regardless

18:03.110 --> 18:09.550
of whether the token is presented in a context or without any context.

18:09.550 --> 18:13.670
But a lot of the neurons have some modulation by context.

18:13.670 --> 18:17.150
And that's why you see the colors and the shifts off the diagonal.

18:17.990 --> 18:26.390
One of the challenges of Mcinturff research is that llms have no representation of isolated words,

18:26.590 --> 18:30.070
because they have never been trained on isolated words.

18:30.470 --> 18:35.560
They are pre-trained on really, really long sequences of text.

18:35.880 --> 18:37.560
Of course, that's not a problem.

18:37.560 --> 18:39.880
It's not a limitation of the models.

18:39.880 --> 18:44.360
That is exactly what makes them so impressive and so versatile.

18:44.960 --> 18:53.280
But it is really a challenge for Mcinturff research, in particular with reproducibility, because findings

18:53.280 --> 18:58.520
that might work for certain contexts might not work for other contexts.

18:59.000 --> 19:07.040
For scientific purposes, it would be ideal if we could isolate the processing of different tokens completely

19:07.200 --> 19:14.600
with no regard to contextual influences, just like how in physics you can run simulations and make

19:14.600 --> 19:20.840
calculations without friction, without any interactions from other objects and other forces.

19:21.360 --> 19:23.440
That's just not possible in Llms.

19:23.440 --> 19:29.240
And that is a really interesting challenge for research into LLM mechanisms.

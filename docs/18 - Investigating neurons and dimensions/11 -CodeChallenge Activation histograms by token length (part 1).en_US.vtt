WEBVTT

00:02.080 --> 00:09.520
There are several goals to this code challenge, including quantifying the relationship between MLP

00:09.560 --> 00:16.880
neuron activations and token length, and also looking at how processing characteristics change over

00:16.880 --> 00:19.160
different layers in the network.

00:19.760 --> 00:28.320
You will also learn a few ways of visualizing activity over the different layers, and a reminder about

00:28.320 --> 00:33.200
the correlation coefficient and its relationship to covariance.

00:33.720 --> 00:40.000
This will be a kind of longish code challenge, so I will break it up into two videos somewhere in the

00:40.000 --> 00:40.480
middle.

00:41.640 --> 00:44.920
Exercise one is fairly straightforward.

00:45.240 --> 00:53.240
In fact, you will see that importing models, connecting to the GPU, and defining and implanting hooks

00:53.440 --> 00:56.360
is code that you will be using repeatedly.

00:57.000 --> 01:05.440
Anyway, we are going to work with the Eleuther GPT neo 125 million parameter model.

01:05.440 --> 01:08.320
We have used this model several times before.

01:09.080 --> 01:16.760
In later exercises at the end of this code challenge, we will also be using a much bigger version of

01:16.760 --> 01:21.640
this model with 1.3 billion parameters, but we can worry about that later.

01:22.080 --> 01:28.680
Anyway, uh, implant uh, a hook into the MLP expansion layers.

01:28.960 --> 01:31.640
And this is also code that you've seen before.

01:31.760 --> 01:36.880
So we want to be extracting the activations of these neurons here.

01:37.320 --> 01:43.680
Now what's new here in this code challenge is doing this for all of the layers.

01:44.200 --> 01:47.560
Now this model has 12 transformer blocks.

01:47.720 --> 01:54.560
And so you will need 12 sets of MLP expansion neuron activations.

01:54.720 --> 02:00.430
So that means you need a unique hook function in each MLP layer.

02:01.710 --> 02:10.990
So yeah, remember there are 3072 neurons that corresponds to an embedding dimensionality of 768 with

02:10.990 --> 02:13.070
A4X expansion.

02:13.710 --> 02:20.790
So this is for the 125 million parameter version of this GPT neo model.

02:21.150 --> 02:25.830
In later exercises, it will be actually bigger than this.

02:25.990 --> 02:29.670
So therefore it will be convenient for the rest of this code challenge.

02:29.790 --> 02:38.510
If you define a variable that codes for the number of neurons in the MLP expansion layer within one

02:38.510 --> 02:40.030
transformer block.

02:40.590 --> 02:45.830
By the way, you do not actually need the GPU here for this code challenge.

02:45.990 --> 02:53.510
In fact, there's only going to be one forward pass through this model in a later exercise, and that

02:53.510 --> 03:00.150
only takes around a minute or so to do on the CPU, at least for the small version of this model.

03:00.590 --> 03:06.390
So if you feel like dealing with the extra code to connect to a GPU, then go for it.

03:06.630 --> 03:10.670
I actually started off this code challenge without using the GPU.

03:10.710 --> 03:17.270
I did everything on the CPU, but at the very end, when I switched to the larger version of this model,

03:17.390 --> 03:20.790
I did decide to go for the GPU over the CPU.

03:21.230 --> 03:25.030
Anyway, it's just a difference between minutes versus seconds.

03:25.070 --> 03:26.790
Not a big deal either way.

03:27.830 --> 03:33.550
Okay, so now you can pause the video, switch to code and import the model and implant the hooks.

03:33.870 --> 03:37.030
And now I will switch to code and show my solution.

03:37.750 --> 03:45.750
This code here is to update the latest version of the data sets, uh, library from hugging face.

03:45.750 --> 03:52.110
You will remember from earlier in this course that at least at the time that I'm recording this video,

03:52.510 --> 03:58.860
uh, this still crashes if you try to import it and load data without first updating it.

03:58.860 --> 04:01.220
Now you don't need that for exercise one.

04:01.220 --> 04:04.580
This is something we will do in exercise two.

04:04.740 --> 04:07.100
Anyway, that's just a little reminder.

04:07.260 --> 04:14.340
It is possible that by the time you are watching this colab and hugging face have fixed this conflict

04:14.340 --> 04:17.260
and therefore you don't actually need to worry about this thing.

04:18.260 --> 04:21.500
Anyway, here I'm importing a bunch of modules.

04:21.500 --> 04:25.780
Some of these are libraries that we use in basically every video.

04:26.060 --> 04:30.740
Some of these are for later exercises like this one and this one.

04:31.020 --> 04:32.460
So we'll get back to those later.

04:32.900 --> 04:33.220
Okay.

04:33.260 --> 04:37.500
So here I'm importing the model and I'm going to implant the hooks here.

04:37.500 --> 04:44.260
You can see I'm importing the 125 million parameter version of this model.

04:44.740 --> 04:46.860
This code here you don't need to worry about.

04:46.860 --> 04:48.460
You can leave it commented.

04:48.660 --> 04:55.260
This is for exercise seven when we're going to work with the larger model, but I just want to point

04:55.260 --> 04:59.220
out that the naming convention is basically the same here.

04:59.700 --> 05:05.420
It's just, you know, we have some extra number at the end to specify how many parameters we have.

05:06.020 --> 05:06.500
Okay.

05:06.540 --> 05:09.740
Here I'm importing the GPU again, totally optional.

05:09.740 --> 05:16.020
You really don't need the GPU for this exercise, but you do want to switch the model into eval mode.

05:16.020 --> 05:20.780
That's generally a good idea when you are not training the model.

05:21.380 --> 05:23.420
Okay, here I have the hook function.

05:23.420 --> 05:27.580
Now all of this is identical to what you've seen before.

05:28.100 --> 05:32.020
This is new, but it's not really Earth Shatteringly new.

05:32.060 --> 05:36.940
All I'm doing is taking this code to implant the hook.

05:37.460 --> 05:41.300
Uh, and then, yeah, calling these functions with this layer number.

05:41.620 --> 05:46.100
And then all I do is take this line and put it inside a for loop.

05:46.100 --> 05:54.810
So it's going to implant a hook into each Individual layer, and you can see that the name of the key

05:54.850 --> 05:58.730
that I use in the dictionary is unique to each layer.

05:59.530 --> 06:05.370
So that means that every time I run a forward pass, all of these data will get replaced.

06:05.370 --> 06:07.050
They will get overwritten.

06:07.050 --> 06:12.490
But that's fine because we're only actually doing one forward pass in this code challenge, but it's

06:12.490 --> 06:15.490
not going to overwrite over layers.

06:15.490 --> 06:24.130
So for example, if I did this, if I just wrote MLP neurons or something activations uh, then uh with

06:24.130 --> 06:30.010
the code looking like this, then this actually would keep overwriting every layer and we would only

06:30.010 --> 06:32.330
end up with the final layer here.

06:32.570 --> 06:35.010
Okay, just a little bit of a coding reminder.

06:35.570 --> 06:36.690
Uh, that looked like that.

06:37.010 --> 06:37.530
Uh, yeah.

06:37.530 --> 06:45.090
And also, uh, because I'm not specifying any outputs here, it will be impossible when I run this

06:45.090 --> 06:47.810
code to remove the hooks.

06:48.010 --> 06:51.010
Now, that's something that sometimes you want to think about.

06:51.010 --> 06:52.770
Sometimes you don't have to worry about.

06:53.130 --> 06:55.410
Uh, in this code challenge, we.

06:55.450 --> 06:56.370
It doesn't really matter.

06:56.410 --> 06:58.810
We can just leave the hooks in the model.

06:58.810 --> 07:03.010
I'm not going to save these models or use them for inference.

07:03.050 --> 07:05.210
Okay, so all of that is great.

07:05.210 --> 07:08.570
Here I have a variable called n neurons.

07:08.890 --> 07:15.250
That is the number of neurons in here I'm picking uh transformer block for.

07:15.570 --> 07:16.930
But it doesn't actually matter.

07:16.930 --> 07:20.370
They all have the same number of neurons for the forward.

07:20.370 --> 07:24.330
Uh, let me actually just run this code here.

07:24.330 --> 07:28.930
Uh, maybe I'll do it like this just so you can see what I'm doing here.

07:29.250 --> 07:33.730
Okay, so here I'm accessing the MLP layer of this transformer block.

07:33.770 --> 07:37.690
This is the expansion part of the MLP block.

07:37.850 --> 07:42.290
And then I'm getting the weight that is all the actual parameters.

07:42.290 --> 07:43.930
And then the shape of that.

07:43.970 --> 07:47.570
And here we see the number of neurons over here okay.

07:47.850 --> 07:56.090
So this is soft coded, which is good because it means that this can easily adapt to other types of

07:56.090 --> 07:56.530
models.

07:56.530 --> 08:02.050
In particular, the larger version of this model, which has more than 3000 neurons.

08:02.730 --> 08:08.570
The goal of this exercise is to import the fine web dataset.

08:09.010 --> 08:15.170
We have worked with this dataset several times earlier in this course, for example in the section on

08:15.210 --> 08:17.970
evals on quantitative evaluations.

08:18.410 --> 08:20.970
Now you do not need the full data set.

08:21.250 --> 08:22.130
That's actually good.

08:22.130 --> 08:24.690
The full data set is like trillions of tokens.

08:24.930 --> 08:34.930
But basically you just keep importing more and more text until you get up to exactly 8192 tokens and

08:34.970 --> 08:36.970
which you can store in one vector.

08:37.210 --> 08:42.690
Now, that might seem like a strange number of tokens, but you will see what this number corresponds

08:42.690 --> 08:44.170
to later on.

08:44.890 --> 08:49.680
This is a screenshot where you can see importing some of the data up here.

08:49.920 --> 08:56.200
And then I'm just printing out a couple of tokens from the first couple of sequences of the data.

08:57.000 --> 09:02.640
The next part of exercise two is to create a bar plot of token lengths.

09:02.960 --> 09:04.520
And that looks like this.

09:04.800 --> 09:08.720
So the shortest tokens had one character.

09:08.920 --> 09:11.120
And in the data set that I imported.

09:11.120 --> 09:15.120
So just the beginning of this data set up to 8000 tokens.

09:15.280 --> 09:19.120
The longest token was 16 characters long.

09:19.800 --> 09:25.280
The vertical line here shows the median of this data set.

09:25.920 --> 09:33.720
And as a reminder, the median of a data set is the number that splits the data into two equal parts.

09:34.320 --> 09:41.560
That means that half of the data are smaller, so half of the tokens have a number of characters a token

09:41.560 --> 09:47.120
length smaller than the median, and half the data are larger than the median.

09:47.720 --> 09:54.960
Now, that explanation of splitting the data into two equal sized halves, that is true for floating

09:54.960 --> 09:56.000
point numbers.

09:56.160 --> 10:01.120
But token lengths are integers and the median is also an integer.

10:01.680 --> 10:09.320
So it turns out that if you would count the number of tokens less than the median, plus the number

10:09.320 --> 10:12.200
of tokens that are greater than the median.

10:12.240 --> 10:18.920
So longer tokens than the median, then you actually don't get all of the tokens, because lots of tokens

10:18.920 --> 10:22.280
are exactly equal to the median token length.

10:22.720 --> 10:24.840
You can see it's around 1400.

10:24.880 --> 10:31.240
Somewhere around 1400 tokens have exactly four characters, which is the exact median.

10:32.280 --> 10:33.760
Now that is not a problem.

10:33.760 --> 10:35.960
That's not like an issue with statistics.

10:35.960 --> 10:38.240
It's just something to keep in mind.

10:38.280 --> 10:44.310
It's just the way that things can shake out when you're working with the median, with Integer valued

10:44.310 --> 10:45.070
numbers.

10:45.630 --> 10:53.310
And that leads me to the final part of this exercise, which is to count and report the number of tokens

10:53.310 --> 10:59.470
that are shorter than the median, longer than the median, and exactly equal to the median.

11:00.150 --> 11:06.710
So you can see that the number of tokens shorter and longer than the median is almost perfectly split.

11:06.990 --> 11:10.590
And yeah, well this group ends up being a little bit smaller.

11:10.590 --> 11:11.270
But that's fine.

11:11.270 --> 11:12.110
It doesn't matter.

11:12.510 --> 11:17.630
So we can still form three groups of tokens according to their lengths.

11:18.030 --> 11:26.510
In the next exercise, you are going to examine the activation magnitudes of MLP neurons for these different

11:26.550 --> 11:27.550
token lengths.

11:27.670 --> 11:33.830
But you don't need to worry about the model part for this exercise, just import the data and look at

11:33.830 --> 11:35.870
the token lengths distributions.

11:36.590 --> 11:40.070
So now you should pause the video and code up this exercise.

11:40.190 --> 11:43.790
And now I will switch to code and discuss my solution.

11:44.750 --> 11:49.470
Here I am importing the fine web dataset, or at least part of it.

11:49.710 --> 11:55.830
Here I'm getting an iterator and that I'm using over here just to loop through and show a couple of

11:55.870 --> 11:58.790
examples so that you can see here.

12:00.270 --> 12:05.030
So just going through the first five examples and printing out some of the text.

12:05.070 --> 12:07.670
I'm not printing out all of the samples.

12:07.670 --> 12:13.310
I'm also not printing out the first hundred tokens because these are not yet tokenized.

12:13.310 --> 12:15.670
So these are just examples in text.

12:15.670 --> 12:19.950
And I'm only printing out the first hundred characters from the first five.

12:19.990 --> 12:23.990
Just to give you a bit of a reminder of what this dataset looks like.

12:24.550 --> 12:25.030
All right.

12:25.030 --> 12:30.270
So here is where I'm going to extract tokens from this data set.

12:30.390 --> 12:34.430
And in particular 8192 tokens.

12:34.670 --> 12:41.860
I'm going to store all of those tokens into a vector one dimensional tensor, but we can think of this

12:41.860 --> 12:42.620
as a vector.

12:42.820 --> 12:44.540
I initialize it to be empty.

12:44.580 --> 12:50.100
We want these to be torched long because these are, uh, token indices.

12:50.100 --> 12:51.900
We're going to push these into the model.

12:51.900 --> 12:54.100
So we want these to be integers.

12:54.500 --> 12:54.700
Okay.

12:54.740 --> 13:01.180
And then this is a variable that I will use to store all of the lengths of the tokens in terms of the

13:01.220 --> 13:04.340
number of characters of each token.

13:04.860 --> 13:06.620
Okay here I have a while loop.

13:06.620 --> 13:13.820
And essentially the idea is that I'm going to stay in this while loop and continue importing more text

13:13.820 --> 13:21.860
and tokenizing and counting the lengths of the tokens up until I have more tokens than I need.

13:22.420 --> 13:23.900
Okay, so here's how that works.

13:23.900 --> 13:28.940
So I get some new data out of the iterator and then I tokenize.

13:29.060 --> 13:30.580
Here I'm getting the length.

13:30.740 --> 13:38.940
So uh, looping over all of the tokens from here and then decoding the token to transform the token

13:38.940 --> 13:44.660
into a string into text, and then just counting the number of characters in there.

13:45.060 --> 13:50.980
This will give me a list, and here I'm transforming it into a numpy array.

13:51.340 --> 13:55.980
So that is the token lengths for this particular data sample.

13:56.500 --> 13:56.820
Okay.

13:56.860 --> 14:04.580
And then what I'm doing in these two lines is concatenating these tokens that I've just imported.

14:04.580 --> 14:12.260
And these token lengths that I've just calculated onto these variables all tokens and all token lengths.

14:12.580 --> 14:20.980
So the idea is that these will get bigger and bigger, longer and longer until this condition is satisfied,

14:20.980 --> 14:27.740
which is that the number of tokens is greater than how many tokens I want.

14:28.260 --> 14:37.340
Now, it's unlikely that this is going to be exactly equal to 8192, because yeah, we just we cannot

14:37.340 --> 14:40.380
guarantee how many tokens are in each sample here.

14:40.820 --> 14:47.540
Uh, so therefore I assume that this, that these two variables will be longer than the number of tokens

14:47.540 --> 14:48.260
that I want.

14:49.340 --> 14:54.620
And so the final step of this code cell is to trim the vectors down.

14:54.620 --> 14:57.780
So I replace the vector with itself.

14:57.900 --> 15:01.940
And just the first 8192.

15:02.500 --> 15:02.740
Okay.

15:02.780 --> 15:05.740
And then you see uh here's what they look like.

15:05.740 --> 15:06.580
There's the size.

15:06.860 --> 15:12.100
And let me just very quickly show you these, uh, variables here.

15:12.380 --> 15:15.340
So all tokens, this is a list of integers.

15:15.500 --> 15:17.940
And let's look at the lengths.

15:18.660 --> 15:23.340
And here we see this is a numpy array of token lengths.

15:23.340 --> 15:25.620
So the first token has three characters.

15:25.900 --> 15:29.980
The third token has nine characters and so on okay.

15:30.020 --> 15:34.300
So with those data I can now create the bar plot.

15:34.500 --> 15:41.410
So here I find all of the unique values in the token lengths and return their counts.

15:41.610 --> 15:48.010
Now if these were floating point numbers then we would take a histogram.

15:48.130 --> 15:54.170
We would use numpy dot histogram to get the distribution.

15:54.170 --> 15:56.690
But that's not what we do for integer data.

15:56.850 --> 16:01.810
You can use numpy dot unique that gives you all the unique values and the counts.

16:01.810 --> 16:05.130
So how often each unique value is observed.

16:05.530 --> 16:09.610
And then here I'm getting the median using numpy dot median.

16:10.090 --> 16:12.770
Okay so I think that is pretty straightforward.

16:13.130 --> 16:18.530
Here I am calculating all of the token lengths that are less than the median.

16:18.570 --> 16:22.530
Greater than the median and exactly equal to the median.

16:24.170 --> 16:24.450
Okay.

16:24.490 --> 16:27.530
And then summing and yeah just printing that out.

16:27.570 --> 16:31.490
Now here again, so this is just, you know, a little reminder of statistics.

16:31.650 --> 16:38.250
If these were floating point numbers, then the median is, you know, you would find that the number

16:38.250 --> 16:44.210
of elements here is going to be zero, maybe one, but it would be 1 or 0.

16:44.650 --> 16:50.410
You certainly wouldn't get like a seventh or an eighth of the entire data set is equal to the median

16:50.610 --> 16:53.490
unless your data have integer values.

16:53.810 --> 16:56.090
Again, that's not at all a problem.

16:56.090 --> 16:59.490
It's not a statistical problem, it's not a numerical problem.

16:59.570 --> 17:01.490
It's just something to keep in mind.

17:02.210 --> 17:11.170
Now to get back to the model, you have 8000 tokens in a vector, and you should reshape those into

17:11.210 --> 17:15.850
a batch of 16 by 512.

17:15.970 --> 17:20.130
And that is where we get 8192 from.

17:20.130 --> 17:27.010
That fits exactly into one batch of 16 sequences by 512 tokens.

17:27.650 --> 17:34.400
Then you can run that batch through the model and you don't have to worry about calculating a loss or

17:34.560 --> 17:38.520
next token predictions or output logits or anything like that.

17:38.560 --> 17:41.320
We just want the hooked activations.

17:41.720 --> 17:49.320
Now this is the step here that takes around a minute on the CPU or a couple of seconds on the GPU.

17:50.080 --> 17:58.760
Then you can confirm that you get 12 sets of activations corresponding to the 12 transformer blocks,

17:59.240 --> 18:07.080
and then you can confirm that for just one of them, you should get a size of the activations of 16

18:07.120 --> 18:13.320
by five, 12 by 3072, corresponding to 16 batches.

18:13.320 --> 18:23.200
So 16 sequences, 512 tokens, and 3072 neurons in the MLP expansion layer.

18:23.600 --> 18:28.240
You don't have to do any other analyses that comes up in the next exercise.

18:28.360 --> 18:32.800
So here you can just focus on extracting these activations.

18:33.280 --> 18:36.160
I hope you enjoy working through this exercise.

18:36.160 --> 18:38.240
I hope it doesn't give you too much trouble.

18:38.400 --> 18:40.240
And now I will switch to code.

18:41.160 --> 18:42.960
Here I'm creating my batch.

18:43.120 --> 18:47.000
And yeah, just confirming the shape and the type of the batch.

18:47.280 --> 18:57.680
It is 16 by 512, so that is one of the reasons why I said to import exactly 8192.

18:58.000 --> 19:04.080
There is another reason why I picked this number to be the number of tokens.

19:04.280 --> 19:09.360
But I'm going to keep that secret until we get to exercise seven.

19:09.920 --> 19:11.160
Anyway, uh, yeah.

19:11.160 --> 19:12.040
So that's that.

19:12.080 --> 19:14.320
Here I run the data through the model.

19:14.320 --> 19:16.320
You can see it is quite fast.

19:16.560 --> 19:18.040
Uh, less than one second.

19:18.320 --> 19:21.480
If you run this on the CPU, it takes around a minute.

19:22.000 --> 19:24.920
Uh, on the GPU, it was, uh, sorry.

19:25.200 --> 19:32.270
With the big model in exercise seven that was taking around five minutes on the CPU, and then I just

19:32.270 --> 19:36.990
gave up and switched to the GPU where it takes a whopping two seconds.

19:37.150 --> 19:37.950
Crazy.

19:38.270 --> 19:38.830
Uh, okay.

19:38.870 --> 19:39.670
So let's see here.

19:39.670 --> 19:46.910
I'm looking at the, uh, activations, uh, variable that, uh, that is created from the hook.

19:47.070 --> 19:51.630
So we have, uh, the keys are MLP zero up to 11.

19:51.830 --> 19:59.390
Of course, those correspond to the 12 MLP subblocks in the 12 transformer layers.

19:59.870 --> 20:03.910
And here I'm just checking the size of one activations matrix.

20:03.910 --> 20:06.030
So I just picked this one at random.

20:06.030 --> 20:07.150
They're all the same size.

20:07.150 --> 20:16.190
You can pick any one of those to show 16 sequences in our batch 512 tokens per batch or per sequence,

20:16.350 --> 20:21.150
and 3072 neurons in the MLP expansion layer.

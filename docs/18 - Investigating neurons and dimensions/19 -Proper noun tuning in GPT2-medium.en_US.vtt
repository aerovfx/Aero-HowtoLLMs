WEBVTT

00:02.360 --> 00:11.640
In this video, we will use logistic regression to see if we can identify MLP neurons that are tuned

00:11.640 --> 00:13.480
to proper nouns.

00:13.840 --> 00:18.680
A proper noun is a particular person, place, or thing.

00:19.120 --> 00:27.200
So for example, the word city is a noun, but Bucharest is a proper noun because it's referring to

00:27.400 --> 00:29.320
one particular city.

00:29.920 --> 00:35.200
Likewise, the word instructor is a noun because that's a person.

00:35.400 --> 00:41.840
But Mike is a proper noun because that refers to a particular instructor.

00:42.600 --> 00:50.080
In English, proper nouns start with capital letters, which makes them fairly easy to identify in text.

00:50.080 --> 00:57.720
And so I'll also show you a very simple algorithm to identify proper nouns in tokenized text.

00:58.440 --> 01:02.940
All of the theory and explanation of the analyses.

01:02.940 --> 01:09.540
I've already covered a few videos ago, so I will start immediately with the overview of the Python

01:09.540 --> 01:10.220
demo.

01:11.020 --> 01:20.180
I will begin in Python by importing GPT two medium and implanting hooks to get the activations from

01:20.180 --> 01:23.340
all of the MLP expansion layers.

01:24.020 --> 01:30.540
And actually, in the demo, I'm only going to be analyzing data from one layer, but having hooks in

01:30.540 --> 01:35.620
all the layers will give you the opportunity to explore the code further.

01:35.940 --> 01:41.100
And we're also going to look at all of the layers in the code challenge in the next video.

01:42.340 --> 01:49.820
I will then import some data from the wiki text dataset, and I'll show you a very simple algorithm

01:49.820 --> 01:52.740
that I used to identify proper nouns.

01:53.340 --> 02:01.590
Now, of the around 11,000 tokens that I imported there happen to be a little bit over 1000 proper nouns.

02:02.150 --> 02:08.870
On the one hand, that is relatively high, but given that this is a Wikipedia data set, there are

02:08.870 --> 02:12.710
actually a lot of proper nouns in Wikipedia articles.

02:13.270 --> 02:18.470
Here you see a couple of examples of the proper nouns that I identified.

02:18.950 --> 02:25.870
Now, you might think that there are some mistakes, some accidents in here because words like the and

02:26.190 --> 02:30.790
court, for example, these are not necessarily proper nouns.

02:31.150 --> 02:36.590
But actually when I show you these tokens in context, you will see that they really are.

02:36.630 --> 02:43.390
For example, the is not the start of a sentence, but instead is part of a title.

02:43.390 --> 02:46.430
I think it was the title of a play anyway.

02:46.470 --> 02:54.510
Next I will set up and run a logistic regression on one neuron, just to show you a reminder of how

02:54.510 --> 02:59.650
to set up the regression and how to interpret the regression table.

03:00.090 --> 03:07.570
In particular, what we are looking for is the beta coefficient of the term that is not the constant.

03:07.570 --> 03:13.970
So this value over here and also its statistical significance which is this p value over here.

03:15.130 --> 03:16.810
So this is just one neuron.

03:16.810 --> 03:21.850
But there are many thousands of neurons in this MLP layer.

03:22.410 --> 03:28.930
So therefore I will repeat the logistic regression without showing the whole table for all of the neurons

03:28.930 --> 03:35.530
in this particular transformer block and show you a visualization that looks like this.

03:36.050 --> 03:43.770
So here you see all of the beta values on the y axis for all 4000 neurons on the x axis.

03:44.250 --> 03:51.130
And I have labeled and colored them according to whether they have a subthreshold or a supra threshold

03:51.130 --> 03:52.050
p value.

03:52.130 --> 03:57.510
And I corrected for multiple comparisons using the Bonferroni method.

03:57.950 --> 04:04.950
And what you see over here in this plot is a scatter plot of the beta coefficients on the x axis by

04:04.950 --> 04:07.710
the p values on the y axis.

04:08.230 --> 04:09.110
It's a pretty neat plot.

04:09.110 --> 04:15.550
It's pretty nice to look at, and basically what it shows is that the significant coefficients have

04:15.550 --> 04:17.230
smaller p values.

04:17.910 --> 04:20.750
Now what is the interpretation of these results?

04:21.110 --> 04:29.590
Remember that the regression here was set up to look for proper nouns versus other tokens that are not

04:29.590 --> 04:30.590
proper nouns.

04:30.910 --> 04:39.430
So positive beta values over here correspond to neurons that have stronger activations for proper nouns.

04:40.150 --> 04:45.950
On the other hand, the negative coefficients the negative betas are a little bit more difficult to

04:45.990 --> 04:46.750
interpret.

04:47.150 --> 04:52.760
All of the comparison tokens are mixed across many different parts of speech.

04:52.960 --> 05:00.440
So basically this just means that these neurons here have very small activations to proper nouns compared

05:00.440 --> 05:03.360
to a variety of other tokens.

05:04.640 --> 05:12.320
Now, as you might know, I am a former neuroscience professor and I really dislike drawing comparisons

05:12.320 --> 05:20.120
between biological brains and deep learning models because they are really nothing whatsoever alike.

05:20.640 --> 05:29.000
That said, you do see a lot, very often in real biological brains that many neurons show category

05:29.000 --> 05:36.680
selectivity and also important computations by suppressing their firing, not only increasing their

05:36.680 --> 05:37.440
firing.

05:38.200 --> 05:45.680
But anyway, other than this visualization, we're not really going to focus on the negative beta values.

05:46.680 --> 05:50.220
Anyway, so this is a quantitative Analysis.

05:50.260 --> 05:57.660
To do a qualitative analysis, I will create a text heatmap with the activation coming from the neuron

05:57.660 --> 06:00.140
that had the largest beta value.

06:00.460 --> 06:06.180
I'll have more to say about this when I switch to code, but you do see that this neuron gives strong

06:06.180 --> 06:09.060
activations for proper nouns.

06:09.460 --> 06:10.780
Now, not exclusively.

06:10.780 --> 06:17.060
This neuron does also seem to like the word television, for example, but certainly there is some preference

06:17.060 --> 06:18.500
for proper nouns.

06:19.020 --> 06:24.780
Now these tokens here that I'm showing here, these are literally the tokens that I ran the analysis

06:24.780 --> 06:25.140
on.

06:25.540 --> 06:32.660
So that means that all of these qualitative results, this text heatmap, while compelling, could actually

06:32.660 --> 06:35.540
be due to overfitting the training data.

06:36.060 --> 06:44.100
So therefore I will then do an out-of-sample qualitative assessment by generating this a similar text

06:44.140 --> 06:51.000
heatmap, but from a different batch of text that the logistic regression was not trained on.

06:51.720 --> 06:55.400
I will show those results at the end of the Python demo.

06:55.560 --> 06:59.520
So now let's switch to code and work our way up to that point.

07:00.520 --> 07:07.720
Here is the code to update the Hugging Face datasets library, which I've already done and restarted

07:07.720 --> 07:09.680
the session so I don't need to do it again.

07:09.840 --> 07:16.280
So here loading in some standard libraries, including Statsmodels to run the logistic regression.

07:16.480 --> 07:19.600
Here I'm importing Gpt2 medium.

07:19.600 --> 07:21.280
So it's going to be a little bit bigger.

07:21.400 --> 07:27.520
More parameters than the GPT two small model that we often work with.

07:27.880 --> 07:33.920
Here I have the code set up to run the GPU, although I'm actually not running.

07:34.080 --> 07:40.400
I'm not using the GPU, so I'm going to run this entire code file on the CPU, which is totally fine

07:40.400 --> 07:45.400
because we're not doing that much intensive processing of this model.

07:45.620 --> 07:47.020
but I have the code here.

07:47.020 --> 07:49.500
I don't know, just in case you would like to.

07:49.700 --> 07:50.380
Use it.

07:50.660 --> 07:52.300
Run it on the GPU.

07:52.820 --> 07:54.380
Okay, so let's see.

07:54.380 --> 07:56.780
This is just a reminder of the architecture.

07:56.780 --> 08:00.260
Here we have 24 transformer blocks.

08:00.300 --> 08:03.420
The embeddings dimension is 1024.

08:03.740 --> 08:10.940
So it's not enormously bigger than the small version of GPT two but it is considerably bigger.

08:10.940 --> 08:11.260
It is.

08:11.300 --> 08:12.980
Yeah, it's quite a bit bigger.

08:13.380 --> 08:15.060
Anyway here in the MLP.

08:15.460 --> 08:24.100
Uh subblock of the transformer layers, we have 4000 neurons in the expansion layer, compared to 3000

08:24.340 --> 08:28.100
in the uh, for GPT two small here.

08:28.100 --> 08:29.420
This is code you've seen before.

08:29.460 --> 08:33.380
I'm implanting a hook in all of the layers.

08:33.380 --> 08:39.820
So MLP and then in the C FC for uh fully connected layer.

08:40.220 --> 08:45.150
And this for loop allows me to implant this hook into every single layer.

08:46.070 --> 08:53.790
As I mentioned, I'm actually in this Python demo only using the data from one layer, but you can switch

08:53.790 --> 08:55.150
and look at other layers.

08:55.510 --> 08:58.150
Okay, here is where I'm importing some data.

08:58.390 --> 09:05.830
In the first 200 samples from this data set we got 11,000 tokens.

09:05.830 --> 09:07.430
And what is all this code do.

09:07.670 --> 09:10.150
So a lot of this code let me show you this.

09:10.150 --> 09:12.470
So text and then I can show you this.

09:12.470 --> 09:16.270
First of all this you've seen before the organization of this data set.

09:16.470 --> 09:17.910
So we can look at texts.

09:17.950 --> 09:20.270
Let's just look at the first element.

09:20.270 --> 09:22.710
You can see this is actually empty right.

09:22.750 --> 09:24.710
So I want to ignore that.

09:24.710 --> 09:33.230
So you could say if the length of this little stream of text is greater than one, then we can start

09:33.270 --> 09:35.390
tokenizing it and working with it.

09:35.670 --> 09:39.590
But there's also other snippets of text here that are really short.

09:39.950 --> 09:43.450
I think these are just titles or something or subsections.

09:43.490 --> 09:46.530
Not sure what these correspond to, but they are very short.

09:46.530 --> 09:47.610
There's quite a few of these.

09:47.650 --> 09:53.730
I am really more interested in having longer streams of text like this one.

09:54.050 --> 10:00.330
So that is why I have this little conditional here where I basically just say, if this is more than

10:00.330 --> 10:09.250
50 characters, then we tokenize the text and add them or concatenate them onto this vector of tokens.

10:09.610 --> 10:10.730
And why 50?

10:10.930 --> 10:12.450
That's really arbitrary.

10:12.490 --> 10:15.530
You could make it be 48.

10:15.570 --> 10:18.610
You can make it be 148 characters.

10:18.650 --> 10:25.290
You know, this is not a real, theoretically motivated, mathematically precise parameter that I picked

10:25.290 --> 10:25.650
here.

10:25.970 --> 10:26.370
Okay.

10:26.410 --> 10:30.730
Anyway, so that's the text that gives us 11,000 tokens.

10:31.050 --> 10:36.410
Here is just an example of how I'm going to find proper nouns.

10:36.410 --> 10:43.110
So as I mentioned in the beginning of this video in written English, proper nouns always start with

10:43.510 --> 10:51.350
capital letters, and we also know that tokens in gpt2 often start with spaces.

10:51.630 --> 10:53.750
So here I use uh.

10:54.270 --> 10:55.390
So this is just an example.

10:55.390 --> 10:58.150
We could have a token that is the MC.

10:58.550 --> 11:01.830
Uh, and then I strip it to remove the space.

11:01.830 --> 11:05.110
And then I look at the first letter after removing the space.

11:05.110 --> 11:08.670
And then I want to know, is this an uppercase letter?

11:09.070 --> 11:14.510
Now, the thing is that this is also true for words like, uh, you know, when you start a sentence,

11:14.550 --> 11:19.550
the start of the sentence is also going to say true over here.

11:19.870 --> 11:25.870
So therefore we need to have some additional conditionals to test for proper nouns.

11:26.790 --> 11:29.750
And that's what I set up in this very simple algorithm.

11:29.750 --> 11:32.470
I just want to highlight there are other ways.

11:32.470 --> 11:40.120
In fact, there are libraries that you can use to identify parts of speech, like the Spacy library.

11:40.120 --> 11:42.640
I'll introduce that later in the course.

11:42.800 --> 11:49.480
For now, for the purposes of this video, I just want to have a nice, simple, straightforward algorithm.

11:49.480 --> 11:52.600
It could have some false positives and false negatives.

11:52.640 --> 11:55.240
Totally fine for the purpose of this video.

11:55.800 --> 11:57.440
Okay, let's see how this works.

11:57.480 --> 11:57.760
Okay.

11:57.800 --> 12:00.200
So we're looping over the tokens.

12:00.360 --> 12:04.200
And then here I'm just decoding this particular token.

12:04.480 --> 12:08.960
And then we can actually just ignore all of the really short tokens.

12:09.160 --> 12:12.240
And then I set up these two conditionals here.

12:12.240 --> 12:19.200
So the first conditional is that this token the first letter when you strip away the text must be uppercase.

12:19.200 --> 12:22.720
Exactly what I showed in the previous code cell.

12:23.080 --> 12:32.280
And the previous token needs cannot end with a period or an exclamation point or anything like this.

12:32.280 --> 12:39.580
So if the previous token ends in a period, then we would know that this is the start of a sentence,

12:39.580 --> 12:46.060
and this is not actually a proper noun, by the way, this is also going to exclude cases where the

12:46.060 --> 12:49.100
sentence does in fact start with a proper noun.

12:49.100 --> 12:53.460
So there's going to be a couple of false negatives that we will lose here.

12:53.460 --> 12:54.300
But that's fine.

12:54.300 --> 12:59.940
In general, with these kinds of analyses, it's better to lose a couple of tokens that you might want

12:59.940 --> 13:06.540
to include, then to include tokens that do not actually belong in the analysis.

13:06.980 --> 13:08.660
Okay, so super simple.

13:08.660 --> 13:16.420
Then I say if both of these conditions are true, I am going to consider this token to be a proper noun.

13:17.100 --> 13:18.020
So there we go.

13:18.020 --> 13:18.900
And now here.

13:18.940 --> 13:19.140
Yeah.

13:19.180 --> 13:22.820
Just to show some examples, these are some of the proper nouns.

13:23.460 --> 13:29.220
This code here is just to show some of these tokens in context.

13:29.220 --> 13:31.380
So here I have this variable w.

13:31.420 --> 13:33.900
It's just where we have these proper nouns.

13:33.900 --> 13:36.200
Actually let me make sure this is really clear.

13:36.280 --> 13:37.560
So we have tokens, right?

13:37.600 --> 13:41.520
These are all the tokens that we have in the text that I've extracted.

13:41.520 --> 13:45.680
And then I have another variable that I've created is proper noun.

13:46.160 --> 13:49.200
And this is all zeros and ones.

13:49.400 --> 13:53.840
And this is telling me whether each token is not a proper noun.

13:53.840 --> 13:55.680
It could be any other part of speech.

13:55.680 --> 13:56.760
It could be a number.

13:57.040 --> 14:02.320
Uh, or if this is a one, then this is going to be a proper noun.

14:02.600 --> 14:04.840
Then the token corresponds to a proper noun.

14:05.280 --> 14:11.400
Okay, so here I'm just finding all of the proper nouns and then printing out some of these proper nouns.

14:11.880 --> 14:12.200
Okay.

14:12.240 --> 14:14.960
So now this is taken without any context.

14:14.960 --> 14:16.800
That's just the token itself.

14:17.240 --> 14:24.680
What I'm doing here is just grabbing the surrounding tokens just five before and five tokens after,

14:24.880 --> 14:28.040
just to decode them so we can see what it looks like.

14:28.040 --> 14:31.600
So so the token here and this is the fourth token.

14:31.600 --> 14:34.450
So that is one, two, three, four.

14:34.490 --> 14:37.610
It looks like it's her, but it's actually the word heron.

14:38.530 --> 14:43.210
So starring role in the play herons written by Simon.

14:43.210 --> 14:46.770
And that's probably the next proper noun over here.

14:47.170 --> 14:50.770
So now we see here as written by Simon Stevenson and so on.

14:50.770 --> 14:58.330
So now we can go back and look at this funny looking one here, this example the so the is generally

14:58.330 --> 15:06.090
not a proper noun, but here it happens to be so the role in the television series the bill.

15:06.090 --> 15:08.970
So in this case the goes with Bill.

15:08.970 --> 15:12.610
It's capitalized and it's referring to this particular bill.

15:13.050 --> 15:19.090
Now again you know, we could have a formal linguistics debate about whether we want to include this.

15:19.130 --> 15:26.890
If you were doing this kind of research for publication, then these are the kinds of nuanced but important

15:26.890 --> 15:28.810
issues that you would want to work with.

15:28.810 --> 15:34.350
I am just teaching you about the method, so I'm comfortable leaving it as it is.

15:34.910 --> 15:39.550
Okay, so now I'm going to run a forward pass through the model.

15:40.630 --> 15:44.390
I'm not actually pushing all of the tokens through the model.

15:44.390 --> 15:51.990
I'm only going to test the first 1000 tokens out of the 12,000 that I extracted.

15:52.190 --> 15:56.590
So you can see this is taking, I don't know, 30s maybe it will take a minute.

15:56.950 --> 16:03.910
If you were attached to the GPU, this line would take three seconds maybe, but now it is.

16:04.150 --> 16:05.670
Yeah, 25 seconds.

16:05.710 --> 16:13.070
That's basically the only thing you would want to connect to the GPT to the GPU for is to save 20s.

16:13.230 --> 16:22.390
So anyway, uh, here, this just confirms that we do get all of the MLP layers in this activations

16:23.030 --> 16:24.750
variable in this dictionary.

16:24.790 --> 16:34.730
And if we look at the size of one of them, that is one by 1024 by 4096, corresponding to one batch,

16:34.730 --> 16:44.730
so one sequence of 1024 tokens, and there are 4096 neurons in the expansion layer of the MLP.

16:44.770 --> 16:49.650
That is just for MLP from Transformer block 21.

16:49.650 --> 16:53.530
But of course this would look the same for all of the transformer blocks.

16:54.090 --> 16:54.690
All right.

16:54.730 --> 17:00.370
Here I'm getting a variable that tells me exactly that number over here.

17:00.970 --> 17:01.250
Okay.

17:01.290 --> 17:06.210
So now I'm going to set up the logistic regression first just in one neuron.

17:07.330 --> 17:10.010
Actually I'll talk about this code a bit.

17:10.010 --> 17:17.010
I'm not going to talk about this code because that is exactly the same code that you saw in a couple

17:17.050 --> 17:21.330
of videos ago, and also the previous video for running logistic regression.

17:21.810 --> 17:30.780
So I showed you above that there were around 1200 proper nouns in total in the entire data set, but

17:30.820 --> 17:34.980
I did not actually process the entire data set up here.

17:34.980 --> 17:38.940
I only processed the first 1000 tokens.

17:39.100 --> 17:45.740
So now I need to find all of the tokens that are just in that first batch.

17:46.340 --> 17:52.340
And that is there's 200 proper nouns in that first batch of tokens.

17:52.820 --> 18:00.220
So what I want to do now is find the locations of all of those proper nouns, and then also find an

18:00.220 --> 18:06.100
equal sample size of non-proper nouns to use as a reference.

18:07.060 --> 18:09.500
And that is the point of this code here.

18:09.900 --> 18:13.180
So here I'm finding where all the proper nouns are.

18:13.540 --> 18:15.620
That's exactly what I showed you here.

18:16.020 --> 18:19.020
And now here I'm getting comparison tokens.

18:19.020 --> 18:23.540
So the idea is that I'm just looping through all of the sequence lengths.

18:23.540 --> 18:32.200
So looping through the integers up to 1023, and then I'm counting them if they are not in the proper

18:32.240 --> 18:33.040
nouns.

18:33.040 --> 18:40.720
So any of the tokens, or I should say, yeah, all of the tokens except for the proper nouns are going

18:40.760 --> 18:45.480
into this array here, this vector over here.

18:45.800 --> 18:48.880
So that means that this is going to be 200.

18:48.880 --> 18:55.200
So we'll have 219 proper nouns and 800 comparison tokens.

18:55.200 --> 19:00.200
But that will give us really unequal sample sizes which is not optimal.

19:00.200 --> 19:05.920
So what I'm doing here is shuffling them so that instead of being an order, they're just randomly shuffled.

19:06.880 --> 19:10.880
And then I'm just selecting the first 219.

19:11.160 --> 19:17.560
So what that means is that we have 219 proper nouns.

19:17.720 --> 19:21.560
And uh, we should also have comparison tokens.

19:21.600 --> 19:25.820
We should also have 219 comparison tokens.

19:26.340 --> 19:26.780
Okay.

19:27.060 --> 19:32.580
So that is how we define the labels and the tokens that we want.

19:32.620 --> 19:32.820
Yeah.

19:32.820 --> 19:35.580
And then I'm building up this model over here.

19:35.820 --> 19:37.620
And here you see the results.

19:37.620 --> 19:44.580
So for this particular neuron just one randomly selected neuron from one randomly selected transformer

19:44.580 --> 19:48.500
block we see that the coefficient is negative.

19:48.860 --> 19:53.340
So that means that there and the p value is less than 0.05.

19:53.340 --> 20:02.540
So that means that this neuron had significantly weaker activation in proper nouns compared to all of

20:02.540 --> 20:04.980
the mixture of other tokens.

20:05.500 --> 20:11.420
Now, one final thing I would like to say about this analysis is that if you really wanted to test for

20:11.420 --> 20:19.740
proper noun selectivity, it would also be relevant to break up the data in the tokens into different

20:19.740 --> 20:21.750
parts of speech categories.

20:21.750 --> 20:30.270
So you want to know the relationship between activations in proper nouns, but also in regular non-proper

20:30.270 --> 20:37.550
nouns, in verbs and adjectives and adverbs and parts of speech in numbers, uh, in, you know, HTML

20:37.590 --> 20:40.150
tags and Python code and so on.

20:40.350 --> 20:44.230
But I'm going to keep it simple here in this video.

20:44.630 --> 20:52.590
So now that we know how to set up a linear classifier for one neuron and extract the beta value and

20:52.590 --> 21:00.870
p value from that classifier result, I am now going to have a for loop over all of the neurons in one

21:00.870 --> 21:01.230
layer.

21:01.230 --> 21:03.710
So I'm just picking layer 18.

21:03.750 --> 21:10.870
Remember this actually corresponds to the 19th transformer block because we start counting at zero.

21:11.270 --> 21:16.110
And then within this one layer I'm just looping over all of the neurons.

21:16.310 --> 21:22.410
Uh, here I'm building up the data here I run the logistic regression analysis and then grabbing the

21:22.450 --> 21:24.650
p value and the beta value.

21:25.010 --> 21:25.450
Okay.

21:25.770 --> 21:26.450
Very good.

21:26.490 --> 21:32.010
And the rest of this stuff you have seen before uh, in a couple of videos ago.

21:32.010 --> 21:34.570
So I'm not going to talk about this a whole lot.

21:34.730 --> 21:38.370
Um, here I'm getting the maximum beta value.

21:38.370 --> 21:41.730
So which neuron has the largest beta value?

21:41.890 --> 21:46.930
And that is uh, neuron uh, 2979.

21:46.930 --> 21:49.730
So that is this one over here.

21:49.730 --> 21:51.130
This one is the largest.

21:51.170 --> 21:53.010
This one's also quite strong.

21:53.130 --> 21:58.450
So we probably also get some good selectivity or at least tuning uh, in this neuron as well.

21:58.450 --> 22:03.370
But this one happens to be by numerical value, the largest one.

22:03.930 --> 22:04.290
Okay.

22:04.330 --> 22:10.490
So after that now I'm going to, uh, min max scale the activations.

22:10.690 --> 22:17.470
Uh, I'm also actually scaling the largest, uh, or most extreme negative value here.

22:17.470 --> 22:20.910
I'm not really going to look at that further, but the code is here.

22:20.910 --> 22:28.070
In case you are curious to explore what the qualitative interpretation would be for this neuron here.

22:28.550 --> 22:36.390
Okay, this code here just finds the size, the width of the of a letter in a plot.

22:36.430 --> 22:37.790
You've seen this code before.

22:38.070 --> 22:39.550
And now here I'm plotting.

22:39.550 --> 22:42.390
So I'm plotting for the letters.

22:42.430 --> 22:45.430
And you can see that the face color.

22:45.430 --> 22:51.990
So the color of the patch behind each token comes from the positive activations.

22:52.590 --> 22:58.550
Uh, and, uh, yeah, here I am, just squaring it just to make it, uh, make the color a little bit

22:58.550 --> 22:59.230
more extreme.

22:59.230 --> 23:02.750
So it's a little bit easier to visually recognize.

23:03.190 --> 23:03.510
Okay.

23:03.550 --> 23:04.510
So what do we see here.

23:04.510 --> 23:07.910
We see strong activations for the long firm.

23:08.190 --> 23:15.850
Mark Ravenhill uh bolter Philip uh London Borough daylight robbery.

23:15.850 --> 23:19.290
This is the title of a film and lots of other stuff.

23:19.290 --> 23:23.010
It's not only activating for proper nouns, right?

23:23.050 --> 23:29.530
So here we also have a pretty respectable amount of activation for the number 2000.

23:29.810 --> 23:32.330
Here is let's see also other years.

23:32.330 --> 23:39.170
It's activating for years it's activating for the word episode the word the and television right.

23:39.210 --> 23:40.450
So it's not selective.

23:40.450 --> 23:46.490
It's not that this neuron is only firing for proper nouns and nothing else.

23:46.690 --> 23:54.890
But it does seem to be fairly consistently responding more so, more strong, stronger activation for,

23:55.090 --> 23:59.170
uh, proper nouns compared to other tokens in general.

23:59.770 --> 24:00.090
Okay.

24:00.130 --> 24:06.130
And then as I mentioned in the slides, these are the exact data that I trained on.

24:06.130 --> 24:10.410
So in fact I am evaluating based on the training data.

24:10.810 --> 24:12.690
That's always the best first step.

24:12.690 --> 24:17.260
You always want to see whether the results look good in your training data.

24:17.260 --> 24:21.580
If they don't look good in the training data, then maybe something went wrong.

24:21.580 --> 24:28.580
Or maybe you just don't have a real robust effect, but you always want to evaluate the results in an

24:28.580 --> 24:29.540
out-of-sample.

24:29.540 --> 24:32.500
So a validation set or a test set.

24:33.540 --> 24:35.220
And that is what I do here.

24:35.500 --> 24:38.820
So now I'm pushing other tokens through the model.

24:38.820 --> 24:43.820
It's the same tokens that we extracted in the beginning of the code file.

24:43.820 --> 24:49.020
But uh, before I was processing these tokens.

24:49.020 --> 24:57.300
So from the beginning up to 1024, now I'm processing 1024 up to 2047.

24:57.300 --> 25:03.020
So this is the next batch of data, the next sequence of 1000 tokens.

25:03.540 --> 25:11.380
And now I am grabbing the activations from the same layer and the same exact neuron that we identified

25:11.520 --> 25:18.920
before, but now this is for the new activations from the next batch, the next segment of tokens.

25:18.960 --> 25:21.040
Notice what's missing in the code.

25:21.040 --> 25:23.960
Here is the logistic regression analysis.

25:24.080 --> 25:26.240
I'm not doing another analysis.

25:26.240 --> 25:35.760
I'm taking the neuron that we identified from the previous analysis and applying that result to a new

25:35.800 --> 25:37.680
set of data points.

25:37.680 --> 25:39.840
So out-of-sample testing.

25:40.200 --> 25:40.600
Okay.

25:40.640 --> 25:47.560
So, uh, run this so grab these activations, uh, min max scale those activations.

25:47.560 --> 25:51.720
And then here I'm creating the, uh, text again.

25:51.720 --> 25:57.840
So the heat map, most of this code is exactly the same as what you saw above.

25:57.960 --> 26:02.640
The main difference is here I'm adding sequence length to the tokens.

26:02.640 --> 26:05.960
So now this is for the next batch of tokens.

26:06.400 --> 26:08.640
And now we can see this is the real test.

26:08.640 --> 26:16.260
So we want to see do we see darker colors for proper nouns compared to any other type of word?

26:16.700 --> 26:18.500
So here we have Shakespeare.

26:18.940 --> 26:19.980
Uh, Hugo.

26:20.500 --> 26:24.540
Uh, Levi here is dead, but this is Waking the Dead.

26:24.540 --> 26:27.500
So television series, uh, Jimmy Darden.

26:27.940 --> 26:29.100
Let's see what else we got.

26:29.220 --> 26:30.260
Uh, Wordsworth?

26:30.260 --> 26:30.740
Uh, yeah.

26:30.740 --> 26:37.700
So, you know, uh, William Hung, uh, but we also see some pretty strong activation for, like,

26:37.740 --> 26:45.300
concise and the O in omitting other words like ending and advisors and set a period.

26:45.460 --> 26:52.660
So this is not quite as compelling as in the training set, but, uh, you know, this is very qualitative.

26:52.660 --> 26:55.620
I'm not doing any quantitative analysis here.

26:55.660 --> 27:03.140
Qualitatively, it does seem like there is still some tuning, some preference in this neuron for proper

27:03.140 --> 27:03.740
nouns.

27:03.780 --> 27:05.620
Again, that's pretty good.

27:05.660 --> 27:08.990
But it's not like super duper amazingly compelling.

27:09.510 --> 27:16.990
On the other hand, I would like to stress that I just ran this analysis in a very small sample and

27:16.990 --> 27:19.110
a very limited data set.

27:19.110 --> 27:25.350
So if you are doing this kind of analysis for real, basically, you know, all the code here is fine,

27:25.350 --> 27:28.310
but you would want to use a ton more data.

27:28.310 --> 27:34.710
You would want to use a lot more tokens, thousands of tokens, tens of thousands of tokens, and not

27:34.710 --> 27:36.950
just a couple of dozen tokens.

27:38.310 --> 27:45.230
The concepts and code that you learned about in this video are going to appear several more times over

27:45.230 --> 27:50.670
the next few sections, including in the code challenge in the next video.

27:51.270 --> 27:59.190
This second point is that visualization is so important for understanding patterns in data, and also

27:59.230 --> 28:05.150
identifying possible problems that might arise in the code or in the analyses.

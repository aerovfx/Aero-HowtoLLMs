WEBVTT

00:02.160 --> 00:09.080
The purpose of this video is to clarify the statement that I made in the previous video.

00:09.080 --> 00:18.560
So in the previous video, I demonstrated that the output of a hook into a transformer block is the

00:18.560 --> 00:22.040
same as the output of the hidden states.

00:22.240 --> 00:29.560
And so that is true, but there's a little bit of a qualification for the final transformer block,

00:29.560 --> 00:30.800
the very last layer.

00:30.800 --> 00:37.120
And so basically in this video I'm going to demonstrate what that is and why it is the case.

00:37.280 --> 00:38.520
So here's the story.

00:38.520 --> 00:48.240
When you implant a hook into a transformer block which is called layer H in these, uh, open AI models

00:48.240 --> 00:55.240
on hugging face, when you implant a hook to grab the output of each layer H each transformer block,

00:55.400 --> 00:57.720
that is just all of this processing.

00:57.720 --> 01:04.600
And that is exactly the same as the output of the hidden states variable when you output the hidden

01:04.600 --> 01:06.250
states from a forward pass.

01:06.370 --> 01:13.930
So those are all the same for every layer, every transformer block except the very last transformer

01:13.930 --> 01:14.410
block.

01:14.610 --> 01:20.450
So when, uh, when the model goes through the very last transformer block, then the output that you

01:20.450 --> 01:28.610
get in the hidden states is not only equal to the output of this block, which is the input from the

01:28.610 --> 01:30.330
previous transformer block.

01:30.490 --> 01:38.450
And then with the attention modulation added on the MLP modulation added on, it is also this layer

01:38.450 --> 01:43.250
here L underscore f which is the final layer norm.

01:43.250 --> 01:44.530
That's what this stands for.

01:45.010 --> 01:50.010
And so in the rest of this code demo I'm going to demonstrate this to you.

01:50.370 --> 01:50.650
Okay.

01:50.690 --> 01:53.890
So here I create a hook function and implant it.

01:53.890 --> 01:55.690
Very simple hook function.

01:56.050 --> 01:57.570
Here I have a dictionary.

01:57.730 --> 02:00.290
And there is one item in the dictionary.

02:00.330 --> 02:03.890
Its key is data and it just contains the number four.

02:04.450 --> 02:04.970
Fine.

02:05.210 --> 02:05.530
Okay.

02:05.570 --> 02:10.940
So then here inside this hook the hook just does nothing except for poor.

02:11.100 --> 02:19.340
Replace this number four with the output from this layer that I've implanted into, and I'm implanting

02:19.340 --> 02:23.300
it into model transformer so hidden.

02:23.300 --> 02:27.300
And then n layers this variable n layers minus one.

02:27.300 --> 02:31.180
And then I'm implanting that hook okay n layers minus one.

02:31.180 --> 02:35.100
So n layers is just a convenience variable I created here.

02:35.100 --> 02:41.100
The GPT two small has 12 hidden layers 12 transformer blocks.

02:41.100 --> 02:42.260
So this is 12.

02:42.420 --> 02:49.740
And of course because of the zero based indexing the index of the transformer block is 11.

02:49.740 --> 02:52.100
That's why I subtract one okay.

02:52.140 --> 02:53.660
So I'll run this code.

02:54.100 --> 02:58.140
Uh here I run a forward pass with just the word hello.

02:58.140 --> 03:00.900
So we don't really need to do a lot of processing here.

03:00.900 --> 03:03.380
We just need something to push through the data.

03:03.580 --> 03:07.940
And I'm requesting, uh, to output the hidden states here.

03:07.940 --> 03:10.420
I am just checking the sizes of these.

03:10.420 --> 03:13.860
So it's, uh, one sequence in the batch.

03:13.860 --> 03:17.080
One token, 768 68 embeddings.

03:17.080 --> 03:17.800
Vectors.

03:17.800 --> 03:19.560
This is for the hooked data.

03:19.560 --> 03:27.040
So block output it was initialized as four and then it gets replaced with the output of the transformer

03:27.040 --> 03:27.400
block.

03:27.400 --> 03:28.960
The final transformer block.

03:29.400 --> 03:32.720
And then I have the final hidden states and that's the same size.

03:33.600 --> 03:34.000
Okay.

03:34.360 --> 03:35.480
Now here's the thing.

03:35.600 --> 03:44.280
For every layer except for the final transformer block, these two variables are the same.

03:44.280 --> 03:50.600
They are equal to each other, but for the final transformer block they are not equal to each other.

03:50.600 --> 03:52.960
And that is what I'm going to demonstrate here.

03:53.400 --> 03:56.840
So first I will demonstrate that these are not equal to each other.

03:56.920 --> 03:58.720
So this is yeah.

03:58.760 --> 04:01.280
These are the data that I got from the hook.

04:01.600 --> 04:07.840
And just looking at the first uh sequence in the batch, the final token, this is also just zero.

04:08.200 --> 04:11.600
All of the embeddings vectors minus the final hidden state.

04:11.600 --> 04:17.880
And the same token okay, it doesn't matter what these numbers are basically just random numbers as

04:17.880 --> 04:18.840
far as we are concerned.

04:18.840 --> 04:23.970
But the important thing is that they are not zeros, they are not the same thing.

04:24.290 --> 04:26.490
Okay, so let me close that for a moment.

04:26.810 --> 04:31.450
And now what I'm doing is taking exactly these same data.

04:31.810 --> 04:37.690
And then I'm pushing it through model transformer underscore f.

04:37.730 --> 04:41.090
Of course this is the final layer norm.

04:41.130 --> 04:49.450
This is the layer norm that comes after all of the transformer blocks and immediately before the embeddings

04:49.490 --> 04:50.210
layer.

04:51.250 --> 04:51.530
Okay.

04:51.570 --> 04:57.010
And then this line of code is identical to this line of code just with a different variable name.

04:57.410 --> 04:58.730
And here they're all zeros.

04:58.770 --> 04:59.090
Okay.

04:59.130 --> 05:01.290
So what does this demonstrate to us?

05:01.490 --> 05:09.290
This demonstrates that the last hidden states is actually the output of the final transformer block

05:09.530 --> 05:12.330
pushed through the layer norm.

05:12.330 --> 05:17.970
And that is important to keep in mind because you will see in many videos in the rest of this course,

05:17.970 --> 05:26.170
that when we analyze dynamics as a function of layer, the very last layer often, not always, but

05:26.340 --> 05:28.980
often looks qualitatively different.

05:28.980 --> 05:36.700
The patterns of the calculations in the final hidden States layer will look different from all the previous

05:36.700 --> 05:37.500
layers.

05:37.780 --> 05:43.580
And part of that is because the model has learned that that's its last chance to do any calculations.

05:43.820 --> 05:47.580
So there really is something special about the final transformer block.

05:47.820 --> 05:54.700
But also part of that is just because of this additional layer normalization, which is not present.

05:54.700 --> 05:57.300
It's not applied to the previous blocks.

05:57.580 --> 05:57.820
Okay.

05:57.860 --> 06:04.660
And so now to demonstrate that to you, I'm going to let me see Re-implant this hook.

06:04.700 --> 06:11.580
And I will uh, normally, you know, the way you probably should do this is have a handle here.

06:12.060 --> 06:14.660
Uh, and then I can remove that handle later.

06:14.660 --> 06:17.460
I can say handle dot remove.

06:17.500 --> 06:24.540
But since I didn't actually do that, then that handle is this hook is kind of permanently stuck inside

06:24.540 --> 06:25.020
the model.

06:25.020 --> 06:29.500
So I'm actually just going to reimport the model from scratch.

06:29.500 --> 06:33.950
So then I'm going to have exactly the same thing.

06:33.950 --> 06:38.030
But now this is n minus two instead of minus one.

06:38.030 --> 06:43.910
So now I'm implanting this hook into the penultimate layer and not the final layer.

06:44.990 --> 06:46.590
Forward pass is the same.

06:46.630 --> 06:47.750
This is the same.

06:48.030 --> 06:48.390
Okay.

06:48.430 --> 06:52.390
So now I'm going to show you, uh, well, I can show you this.

06:52.430 --> 06:57.510
These still different, but now I can say minus two and minus two.

06:57.550 --> 06:57.990
Uh, sorry.

06:58.030 --> 06:58.310
Wait.

06:58.350 --> 07:01.070
This should still be minus one, because that's the token.

07:01.310 --> 07:07.790
So now this is showing that when we compare the output from this hook that we grabbed inside the hook

07:07.990 --> 07:14.710
to the hidden states for any layer other than the very last one, now we get a bunch of zeros.

07:15.030 --> 07:19.310
And furthermore, you will not be surprised to see this result here.

07:19.310 --> 07:23.110
I'm just going to write, uh, minus two over here.

07:23.310 --> 07:26.150
And this is actually, uh.

07:26.150 --> 07:26.670
Oops, sorry.

07:26.710 --> 07:27.030
Wait.

07:27.070 --> 07:29.190
This should be minus two over here.

07:29.470 --> 07:30.310
My apologies.

07:30.710 --> 07:30.990
Okay.

07:31.030 --> 07:33.310
So again, it doesn't matter what these numbers are.

07:33.350 --> 07:35.910
The important thing is they are no longer zeros.

07:35.910 --> 07:37.770
And why are they no longer zeros?

07:37.970 --> 07:42.410
Because now we are transforming what should be equal to this.

07:42.770 --> 07:47.130
But we're transforming it through the final layer normalization.

07:47.170 --> 07:49.250
This is not something that the model does.

07:49.290 --> 07:51.530
I'm just demonstrating this to you here.

07:51.530 --> 07:53.890
Just so you see that these really are different.

07:54.250 --> 07:54.530
Okay.

07:54.570 --> 07:55.650
So I hope that makes sense.

07:55.690 --> 07:58.970
Again the conclusion is that let me go back to this hook.

07:59.970 --> 08:07.610
The conclusion of this video is that when you hook into a transformer layer, the output variable here

08:07.610 --> 08:12.450
is the final output of the of that transformer block.

08:12.850 --> 08:20.170
However, when you hook it into the very, very last transformer block, no matter how many blocks there

08:20.170 --> 08:28.530
are, whatever is the very last one it is not only the hidden states is not only this output block,

08:28.570 --> 08:34.730
the hidden states that you get from this output, hidden states, and then output hidden states.

08:34.970 --> 08:38.370
That is the output of the final transformer block.

08:38.530 --> 08:41.770
Push through the final layer norm.

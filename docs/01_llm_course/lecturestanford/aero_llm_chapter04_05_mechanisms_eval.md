
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [01 llm course](../index.md) > [lecturestanford](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# ChÆ°Æ¡ng 4 & 5: Mechanisms & Evaluation ğŸ”§ğŸ“Š

> **CS229 Stanford** | ChÆ°Æ¡ng 4-5/5  
> **Autoregressive, Tokenization & Evaluation**

---

## CHÆ¯Æ NG 4: CÆ¡ Cháº¿ Hoáº¡t Äá»™ng

### **1. Autoregressive Generation**

**Formula:**

$$
P(xâ‚, xâ‚‚, ..., xâ‚™) = âˆáµ¢ P(xáµ¢  \mid  xâ‚, ..., xáµ¢â‚‹â‚)
$$

**Example:**
Input: "The cat"

$$
Step 1: P("sat"  \mid  "The cat") = 0.8 â†’ Output: "sat" Step 2: P("on"  \mid  "The cat sat") = 0.9 â†’ Output: "on" Step 3: P("the"  \mid  "The cat sat on") = 0.95 â†’ Output: "the"
$$

...

Result: "The cat sat on the mat"

**Háº¡n cháº¿:**
- Slow (sequential, not parallel)
- Can't "look ahead"
- Expensive at scale

---

### **2. Tokenization**

**Why?** Computers understand numbers, not words.

**BPE Algorithm:**
1. Start: ['h', 'e', 'l', 'l', 'o'] 
2. Merge frequent: 'l'+'l' â†’ 'll'
3. Result: ['h', 'e', 'll', 'o']
4. Repeat...

**GPT-4 Tokenizer:**
- Vocab size: ~100K tokens
- Average: 1 token â‰ˆ 0.75 words
- Handles 100+ languages

**Common Issues:**
```python
# Numbers get split weirdly
"327" â†’ ["3", "27"] âŒ  # Bad for math

# Indentation problems (old models)
"    def foo():" â†’ ["  ", "  ", "def", " foo", "():"]

# Non-English struggles
"ä½ å¥½" (Chinese) â†’ Multiple f18_ragments

---

## CHÆ¯Æ NG 5: ÄÃ¡nh GiÃ¡

### **1. Perplexity**

PPL = exp(-1/N âˆ‘ log P(xáµ¢  \mid  context))

$$
**Lower = Better** | Model | PPL (WikiText) | |-------|----------------| | LSTM (2017) | ~70 | | GPT-2 (2019) | ~18 | | **GPT-4 (2023)** | **~8** | --- ### **2. Benchmarks** #### **MMLU (Knowledge)** - 57 subjects - Multiple choice - GPT-4: **86.4%** (human expert ~90%) #### **HumanEval (Coding)** - 164 Python problems - GPT-4: **67.0% pass@1** #### **GSM8K (Math)** - Grade school math - GPT-4: **92.0%** --- ### **3. Human Evaluation** **Criteria:** 1. **Helpful:** Did it answer well? 2. **Honest:** No hallucinations? 3. **Harmless:** No toxic content? **Process:** Generate responses â†’ Humans rate â†’ Statistical analysis --- ### **4. Production Metrics** | Metric | Target | |--------|--------| | Latency | < 500ms | | Throughput | > 100 tok/s | | Cost | < 0.01/1K tokens | | Uptime | > 99.9% | --- ## ğŸ¯ Summary: Full LLM Stack â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  User Prompt                â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â†“
$$

Tokenization

$$
â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  Embedding Layer            â”‚ â”‚  Position + Token           â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  Transformer Blocks Ã— 120   â”‚ â”‚  â”œâ”€ Layer Norm              â”‚ â”‚  â”œâ”€ Self-Attention          â”‚ â”‚  â”œâ”€ MoE (8 experts, Top-2)  â”‚ â”‚  â””â”€ Residual                â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â†“
$$

Final LN + Linear

$$
â†“
$$

Softmax â†’ Probs

$$
â†“
$$

Sample Next Token

$$
â†“
$$

Autoregressive Loop
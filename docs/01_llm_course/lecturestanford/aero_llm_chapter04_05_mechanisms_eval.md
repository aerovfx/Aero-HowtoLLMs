
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../../index.md) > [01 llm course](../../index.md) > [lecturestanford](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../../index.md)
- [ğŸ“š Module 01: LLM Course](../../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# ChÆ°Æ¡ng 4 & 5: Mechanisms & Evaluation ğŸ”§ğŸ“Š

> **CS229 Stanford** | ChÆ°Æ¡ng 4-5/5  
> **Autoregressive, Tokenization & Evaluation**

---

## CHÆ¯Æ NG 4: CÆ¡ Cháº¿ Hoáº¡t Äá»™ng

### **1. Autoregressive Generation**

**Formula:**
```
P(xâ‚, xâ‚‚, ..., xâ‚™) = âˆáµ¢ P(xáµ¢ | xâ‚, ..., xáµ¢â‚‹â‚)
```

**Example:**
```
Input: "The cat"
Step 1: P("sat" | "The cat") = 0.8 â†’ Output: "sat"
Step 2: P("on" | "The cat sat") = 0.9 â†’ Output: "on"
Step 3: P("the" | "The cat sat on") = 0.95 â†’ Output: "the"
...

Result: "The cat sat on the mat"
```

**Háº¡n cháº¿:**
- Slow (sequential, not parallel)
- Can't "look ahead"
- Expensive at scale

---

### **2. Tokenization**

**Why?** Computers understand numbers, not words.

**BPE Algorithm:**
```
1. Start: ['h', 'e', 'l', 'l', 'o'] 
2. Merge frequent: 'l'+'l' â†’ 'll'
3. Result: ['h', 'e', 'll', 'o']
4. Repeat...
```

**GPT-4 Tokenizer:**
- Vocab size: ~100K tokens
- Ave18-RAGe: 1 token â‰ˆ 0.75 words
- Handles 100+ languages

**Common Issues:**
```python
# Numbers get split weirdly
"327" â†’ ["3", "27"] âŒ  # Bad for math

# Indentation problems (old models)
"    def foo():" â†’ ["  ", "  ", "def", " foo", "():"]

# Non-English struggles
"ä½ å¥½" (Chinese) â†’ Multiple f18-RAGments
```

---

## CHÆ¯Æ NG 5: ÄÃ¡nh GiÃ¡

### **1. Perplexity**

```
PPL = exp(-1/N âˆ‘ log P(xáµ¢ | context))
```

**Lower = Better**

| Model | PPL (WikiText) |
|-------|----------------|
| LSTM (2017) | ~70 |
| GPT-2 (2019) | ~18 |
| **GPT-4 (2023)** | **~8** |

---

### **2. Benchmarks**

#### **MMLU (Knowledge)**
- 57 subjects
- Multiple choice
- GPT-4: **86.4%** (human expert ~90%)

#### **HumanEval (Coding)**
- 164 Python problems
- GPT-4: **67.0% pass@1**

#### **GSM8K (Math)**
- Grade school math
- GPT-4: **92.0%**

---

### **3. Human Evaluation**

**Criteria:**
1. **Helpful:** Did it answer well?
2. **Honest:** No hallucinations?
3. **Harmless:** No toxic content?

**Process:**
```
Generate responses â†’ Humans rate â†’ Statistical analysis
```

---

### **4. Production Metrics**

| Metric | Target |
|--------|--------|
| Latency | < 500ms |
| Throughput | > 100 tok/s |
| Cost | < $0.01/1K tokens |
| Uptime | > 99.9% |

---

## ğŸ¯ Summary: Full LLM Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Prompt                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
     [Tokenization]
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embedding Layer            â”‚
â”‚  Position + Token           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformer Blocks Ã— 120   â”‚
â”‚  â”œâ”€ Layer Norm              â”‚
â”‚  â”œâ”€ Self-Attention          â”‚
â”‚  â”œâ”€ MoE (8 experts, Top-2)  â”‚
â”‚  â””â”€ Residual                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
     [Final LN + Linear]
           â†“
     [Softmax â†’ Probs]
           â†“
     [Sample Next Token]
           â†“
     [Autoregressive Loop]
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generated Response         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… Key Takeaways (All 5 Chapters)

**Ch 1: Overview**
- LLM = Probability model on token sequences
- Transformer architecture dominates
- GPT-4 = MoE (1.76T params)

**Ch 2: 5 Pillars**
- Architecture, Loss, Data, Evaluation, Systems
- Industry focuses on Data + Systems
- $100M training requires 10K+ GPUs

**Ch 3: Training**
- Pre-training (13T tokens, 100 days)
- SFT (50K examples, 3 days)
- RLHF (preferences, 1 week)

**Ch 4: Mechanisms**
- Autoregressive = sequential generation
- Tokenization = text â†” numbers
- BPE handles multiple languages

**Ch 5: Evaluation**
- Perplexity: ~8 (SOTA)
- Benchmarks: MMLU 86%, HumanEval 67%
- Human eval: Helpful, Honest, Harmless
- Production: < 500ms latency

---

## ğŸ“ Next Steps

### **Hands-on:**
1. **Explore GPT-4 Visualization:**
   ```bash
   cd llm_viz && npm run dev
   # â†’ http://localhost:3002/llm
   ```

2. **Try Vietnamese Walkthroughs:**
   - Embedding
   - Layer Normalization
   - Self-Attention
   - MoE Routing
   - MLP
   - Output Layer

3. **Experiment:**
   - Adjust temperature
   - Compare models
   - Observe expert selection

### **Further Reading:**
- Original Transformer paper (2017)
- GPT-3 paper (2020)
- GPT-4 Technical Report (2023)
- InstructGPT (RLHF) paper (2022)

### **Build Your Own:**
1. Start small: Train on WikiText
2. Use open models: Llama 2
3. Try fine-tuning: PEFT, LoRA
4. Scale up gradually

---

**ğŸ‰ Congratulations!**  
You now understand the complete LLM stack from architecture to deployment!

---

*BiÃªn soáº¡n bá»Ÿi Pixibot - Stanford CS229*  
*Last updated: 2026-02-15*
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [CS229: XÃ¢y Dá»±ng MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n (LLMs) ğŸ§ ](aero_llm_00_overview.md) | [Xem bÃ i viáº¿t â†’](aero_llm_00_overview.md) |
| [Lecture 1: Transformer Architecture ğŸ¤–](aero_llm_01_transformer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_transformer.md) |
| [Lecture 2: Transformer Tricks & BERT ğŸ› ï¸](aero_llm_02_transformer_tricks.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_transformer_tricks.md) |
| [Lecture 3: Large Language Models (LLMs) & Inference ğŸš€](aero_llm_03_large_language_models.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_large_language_models.md) |
| [Lecture 4: LLM Training - Pre-training ğŸ‹ï¸](aero_llm_04_training_pretraining.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_training_pretraining.md) |
| [Lecture 5: LLM Tuning (SFT & Parameter Efficient) ğŸ›ï¸](aero_llm_05_tuning_peft.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_tuning_peft.md) |
| [Lecture 6: LLM Reasoning ğŸ§ ](aero_llm_06_reasoning.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_reasoning.md) |
| [Lecture 7: Agentic LLMs & Tool Use ğŸ› ï¸](aero_llm_07_agentic_llms.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_agentic_llms.md) |
| [Lecture 8: LLM Evaluation âš–ï¸](aero_llm_08_evaluation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_evaluation.md) |
| [Lecture 9: Recap & Current Trends ğŸ”®](aero_llm_09_trends.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_trends.md) |
| [ğŸ› ï¸ Top 12 Repo Quan Trá»ng Cho AI Engineer Tá»‘i Æ¯u LLM](aero_llm_10_essential_tools.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_essential_tools.md) |
| [ChÆ°Æ¡ng 1: Tá»•ng Quan Vá» Large Language Models (LLMs) ğŸ§ ](aero_llm_chapter01_overview_detailed.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter01_overview_detailed.md) |
| [ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t Cá»§a Viá»‡c Huáº¥n Luyá»‡n LLMs ğŸ›ï¸](aero_llm_chapter02_5pillars_part1.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter02_5pillars_part1.md) |
| [ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t - Part 2 (Evaluation & Systems)](aero_llm_chapter02_5pillars_part2.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter02_5pillars_part2.md) |
| [ChÆ°Æ¡ng 3: Pre-training â†’ Post-training Pipeline ğŸ”„](aero_llm_chapter03_training_pipeline.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter03_training_pipeline.md) |
| ğŸ“Œ **[ChÆ°Æ¡ng 4 & 5: Mechanisms & Evaluation ğŸ”§ğŸ“Š](aero_llm_chapter04_05_mechanisms_eval.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_chapter04_05_mechanisms_eval.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../../index.md) > [01 llm course](../../index.md) > [lecturestanford](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../../index.md)
- [ğŸ“š Module 01: LLM Course](../../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Lecture 5: LLM Tuning (SFT & Parameter Efficient) ğŸ›ï¸

> **TÃ³m táº¯t tá»« khÃ³a há»c Stanford CME 295: Transformers & Large Language Models.**
> BÃ i giáº£ng nÃ y táº­p trung vÃ o giai Ä‘oáº¡n sau Pre-training: Supervised Fine-Tuning (SFT) Ä‘á»ƒ biáº¿n mÃ´ hÃ¬nh thÃ nh trá»£ lÃ½, vÃ  cÃ¡c ká»¹ thuáº­t Fine-tuning hiá»‡u quáº£ (PEFT/LoRA).

---

## ğŸ“š Má»¥c Lá»¥c
1. [Supervised Fine-Tuning (SFT)](#1-supervised-fine-tuning-sft)
2. [Instruction Tuning](#2-instruction-tuning)
3. [Dá»¯ liá»‡u cho SFT](#3-dá»¯-liá»‡u-cho-sft)
4. [Parameter-Efficient Fine-Tuning (PEFT)](#4-parameter-efficient-fine-tuning-peft)
5. [LoRA (Low-Rank Adaptation)](#5-lora-low-rank-adaptation)
6. [QLoRA (Quantized LoRA)](#6-qlora-quantized-lora)

---

## 1. Supervised Fine-Tuning (SFT)
Sau Pre-training, mÃ´ hÃ¬nh giá»‘ng nhÆ° má»™t "con váº¹t thÃ´ng thÃ¡i" - biáº¿t ráº¥t nhiá»u nhÆ°ng chá»‰ biáº¿t dá»± Ä‘oÃ¡n tá»« tiáº¿p theo chá»© khÃ´ng biáº¿t tráº£ lá»i cÃ¢u há»i hay lÃ m theo lá»‡nh.

**Má»¥c tiÃªu cá»§a SFT:**
*   Dáº¡y mÃ´ hÃ¬nh cÃ¡ch **hÃ nh xá»­** (Behavior) mong muá»‘n.
*   Biáº¿n Base model -> Chat/Instruct model.

**Quy trÃ¬nh:**
*   Input: CÃ¡c cáº·p cÃ¢u há»i - cÃ¢u tráº£ lá»i (Prompt - Response) cháº¥t lÆ°á»£ng cao.
*   Loss: Váº«n dÃ¹ng Next token prediction, nhÆ°ng chá»‰ tÃ­nh loss trÃªn pháº§n cÃ¢u tráº£ lá»i (Response), khÃ´ng tÃ­nh trÃªn pháº§n cÃ¢u há»i (Prompt).

---

## 2. Instruction Tuning
LÃ  má»™t dáº¡ng cá»§a SFT, táº­p trung vÃ o viá»‡c dáº¡y mÃ´ hÃ¬nh tuÃ¢n theo cÃ¡c chá»‰ dáº«n (instructions) Ä‘a dáº¡ng.

*   **TÃ¡c vá»¥:** TÃ³m táº¯t, dá»‹ch, viáº¿t code, giáº£i toÃ¡n, viáº¿t thÆ¡, danh sÃ¡ch...
*   **Zero-shot Generalization:** Sau khi Ä‘Æ°á»£c Instruction Tuning trÃªn nhiá»u tÃ¡c vá»¥, mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng thá»±c hiá»‡n cáº£ nhá»¯ng tÃ¡c vá»¥ má»›i mÃ  nÃ³ chÆ°a tá»«ng tháº¥y trong quÃ¡ trÃ¬nh training (nhá» kháº£ nÄƒng suy luáº­n tá»•ng quÃ¡t).

---

## 3. Dá»¯ liá»‡u cho SFT
Cháº¥t lÆ°á»£ng quan trá»ng hÆ¡n sá»‘ lÆ°á»£ng ("Quality is King").

*   **Nguá»“n dá»¯ liá»‡u:**
    *   *Human-generated:* Do con ngÆ°á»i viáº¿t (Ä‘áº¯t Ä‘á», cháº¥t lÆ°á»£ng cao).
    *   *LLM-generated (Synthetic data):* DÃ¹ng mÃ´ hÃ¬nh máº¡nh (GPT-4) Ä‘á»ƒ táº¡o dá»¯ liá»‡u training cho mÃ´ hÃ¬nh nhá» hÆ¡n. (Ráº», nhanh, nhÆ°ng cáº§n kiá»ƒm soÃ¡t cháº¥t lÆ°á»£ng).
*   **Quy mÃ´:** Chá»‰ cáº§n hÃ ng nghÃ¬n Ä‘áº¿n hÃ ng trÄƒm nghÃ¬n máº«u (Ã­t hÆ¡n nhiá»u so vá»›i hÃ ng tá»· tokens cá»§a Pre-training).
*   **VÃ­ dá»¥:** Dataset phá»• biáº¿n: Alpaca, Vicuna, Lima.

---

## 4. Parameter-Efficient Fine-Tuning (PEFT)
Fine-tuning toÃ n bá»™ mÃ´ hÃ¬nh (Full Fine-tuning) ráº¥t tá»‘n kÃ©m (cáº§n VRAM gáº¥p nhiá»u láº§n kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh Ä‘á»ƒ lÆ°u Optimizer states).

**PEFT:** Chá»‰ cáº­p nháº­t má»™t pháº§n nhá» tham sá»‘ hoáº·c thÃªm cÃ¡c module nhá» vÃ o mÃ´ hÃ¬nh, giá»¯ Ä‘Ã´ng cá»©ng (freeze) pháº§n lá»›n trá»ng sá»‘ gá»‘c.

**Lá»£i Ã­ch:**
*   Giáº£m yÃªu cáº§u VRAM.
*   TrÃ¡nh hiá»‡n tÆ°á»£ng "Catastrophic Forgetting" (QuÃªn kiáº¿n thá»©c cÅ©).
*   Dá»… dÃ ng chia sáº» cÃ¡c Adapter nhá» (vÃ i chá»¥c MB) thay vÃ¬ cáº£ mÃ´ hÃ¬nh GB.

---

## 5. LoRA (Low-Rank Adaptation)
Ká»¹ thuáº­t PEFT phá»• biáº¿n nháº¥t hiá»‡n nay.

**Ã tÆ°á»Ÿng:**
Thay vÃ¬ cáº­p nháº­t trá»±c tiáº¿p ma tráº­n trá»ng sá»‘ $W$ (kÃ­ch thÆ°á»›c $d \times d$), ta cáº­p nháº­t thÃ´ng qua 2 ma tráº­n nhá» $A$ vÃ  $B$:
$$ W' = W + \Delta W = W + BA $$
Trong Ä‘Ã³:
*   $B$: kÃ­ch thÆ°á»›c $d \times r$
*   $A$: kÃ­ch thÆ°á»›c $r \times d$
*   $r$ (rank): ráº¥t nhá» (vÃ­ dá»¥ 8, 16, 64) so vá»›i $d$.

**Káº¿t quáº£:** Sá»‘ lÆ°á»£ng tham sá»‘ cáº§n train giáº£m hÃ ng nghÃ¬n láº§n, nhÆ°ng hiá»‡u quáº£ tÆ°Æ¡ng Ä‘Æ°Æ¡ng Full Fine-tuning.

---

## 6. QLoRA (Quantized LoRA)
Káº¿t há»£p Quantization vÃ  LoRA Ä‘á»ƒ train mÃ´ hÃ¬nh lá»›n trÃªn GPU nhá».

*   **4-bit NormalFloat (NF4):** Má»™t kiá»ƒu dá»¯ liá»‡u má»›i tá»‘i Æ°u cho trá»ng sá»‘ phÃ¢n phá»‘i chuáº©n cá»§a Neural Network.
*   **Double Quantization:** Quantize cáº£ cÃ¡c háº±ng sá»‘ quantization Ä‘á»ƒ tiáº¿t kiá»‡m thÃªm bá»™ nhá»›.
*   **Paged Optimizers:** Sá»­ dá»¥ng CPU RAM Ä‘á»ƒ offload optimizer states khi GPU bá»‹ trÃ n bá»™ nhá»› (OOM).

-> Cho phÃ©p train mÃ´ hÃ¬nh 65B parameters trÃªn má»™t GPU 48GB.

---
*BiÃªn soáº¡n bá»Ÿi Pixiboss - Dá»±a trÃªn Stanford CME 295.*
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [CS229: XÃ¢y Dá»±ng MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n (LLMs) ğŸ§ ](aero_llm_00_overview.md) | [Xem bÃ i viáº¿t â†’](aero_llm_00_overview.md) |
| [Lecture 1: Transformer Architecture ğŸ¤–](aero_llm_01_transformer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_transformer.md) |
| [Lecture 2: Transformer Tricks & BERT ğŸ› ï¸](aero_llm_02_transformer_tricks.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_transformer_tricks.md) |
| [Lecture 3: Large Language Models (LLMs) & Inference ğŸš€](aero_llm_03_large_language_models.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_large_language_models.md) |
| [Lecture 4: LLM Training - Pre-training ğŸ‹ï¸](aero_llm_04_training_pretraining.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_training_pretraining.md) |
| ğŸ“Œ **[Lecture 5: LLM Tuning (SFT & Parameter Efficient) ğŸ›ï¸](aero_llm_05_tuning_peft.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_05_tuning_peft.md) |
| [Lecture 6: LLM Reasoning ğŸ§ ](aero_llm_06_reasoning.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_reasoning.md) |
| [Lecture 7: Agentic LLMs & Tool Use ğŸ› ï¸](aero_llm_07_agentic_llms.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_agentic_llms.md) |
| [Lecture 8: LLM Evaluation âš–ï¸](aero_llm_08_evaluation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_evaluation.md) |
| [Lecture 9: Recap & Current Trends ğŸ”®](aero_llm_09_trends.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_trends.md) |
| [ğŸ› ï¸ Top 12 Repo Quan Trá»ng Cho AI Engineer Tá»‘i Æ¯u LLM](aero_llm_10_essential_tools.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_essential_tools.md) |
| [ChÆ°Æ¡ng 1: Tá»•ng Quan Vá» Large Language Models (LLMs) ğŸ§ ](aero_llm_chapter01_overview_detailed.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter01_overview_detailed.md) |
| [ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t Cá»§a Viá»‡c Huáº¥n Luyá»‡n LLMs ğŸ›ï¸](aero_llm_chapter02_5pillars_part1.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter02_5pillars_part1.md) |
| [ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t - Part 2 (Evaluation & Systems)](aero_llm_chapter02_5pillars_part2.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter02_5pillars_part2.md) |
| [ChÆ°Æ¡ng 3: Pre-training â†’ Post-training Pipeline ğŸ”„](aero_llm_chapter03_training_pipeline.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter03_training_pipeline.md) |
| [ChÆ°Æ¡ng 4 & 5: Mechanisms & Evaluation ğŸ”§ğŸ“Š](aero_llm_chapter04_05_mechanisms_eval.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter04_05_mechanisms_eval.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

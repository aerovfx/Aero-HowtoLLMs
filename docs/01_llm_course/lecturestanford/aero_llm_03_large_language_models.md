
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../../index.md) > [01 llm course](../../index.md) > [lecturestanford](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../../index.md)
- [ğŸ“š Module 01: LLM Course](../../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Lecture 3: Large Language Models (LLMs) & Inference ğŸš€

> **TÃ³m táº¯t tá»« khÃ³a há»c Stanford CME 295: Transformers & Large Language Models.**
> BÃ i giáº£ng nÃ y táº­p trung vÃ o cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (Decoder-only), cÃ¡ch má»Ÿ rá»™ng quy mÃ´ (Scaling), ká»¹ thuáº­t Prompting vÃ  tá»‘i Æ°u hÃ³a suy luáº­n (Inference).

---

## ğŸ“š Má»¥c Lá»¥c
1. [Äá»‹nh nghÄ©a LLM](#1-Ä‘á»‹nh-nghÄ©a-llm)
2. [Mixture of Experts (MoE)](#2-mixture-of-experts-moe)
3. [Ká»¹ thuáº­t Prompting & In-context Learning](#3-ká»¹-thuáº­t-prompting--in-context-learning)
4. [Decoding Strategies (Chiáº¿n lÆ°á»£c giáº£i mÃ£)](#4-decoding-strategies-chiáº¿n-lÆ°á»£c-giáº£i-mÃ£)
5. [Tá»‘i Æ°u hÃ³a Inference (KV Cache, Speculative Decoding)](#5-tá»‘i-Æ°u-hÃ³a-inference)

---

## 1. Äá»‹nh nghÄ©a LLM
**Large Language Model (LLM)** thÆ°á»ng Ã¡m chá»‰ cÃ¡c mÃ´ hÃ¬nh:
*   LÃ  **Language Model** (mÃ´ hÃ¬nh xÃ¡c suáº¥t dá»± Ä‘oÃ¡n tá»« tiáº¿p theo).
*   CÃ³ kÃ­ch thÆ°á»›c **Lá»›n** (hÃ ng tá»· tham sá»‘, huáº¥n luyá»‡n trÃªn hÃ ng nghÃ¬n tá»· tokens).
*   Kiáº¿n trÃºc chá»§ Ä‘áº¡o: **Decoder-only Transformer** (bá» qua pháº§n Encoder vÃ  Cross-Attention).

*VÃ­ dá»¥:* GPT-3, PaLM, Llama, Mistral.

---

## 2. Mixture of Experts (MoE) ğŸ§ 
Khi mÃ´ hÃ¬nh quÃ¡ lá»›n, chi phÃ­ tÃ­nh toÃ¡n cho má»—i láº§n suy luáº­n ráº¥t cao. **MoE** lÃ  giáº£i phÃ¡p Ä‘á»ƒ "Má»Ÿ rá»™ng quy mÃ´ mÃ  khÃ´ng tÄƒng chi phÃ­ suy luáº­n tÆ°Æ¡ng á»©ng".

*   **Ã tÆ°á»Ÿng:** Thay lá»›p FFN (Feed Forward Network) dÃ y Ä‘áº·c báº±ng nhiá»u "ChuyÃªn gia" nhá» (Experts).
*   **Router (Gate):** Má»™t máº¡ng con quyáº¿t Ä‘á»‹nh xem vá»›i má»—i token Ä‘áº§u vÃ o, nÃªn gá»­i nÃ³ cho chuyÃªn gia nÃ o xá»­ lÃ½ (VÃ­ dá»¥: CÃ¢u há»i ToÃ¡n -> Gá»­i cho chuyÃªn gia ToÃ¡n).
*   **Sparse Activation (KÃ­ch hoáº¡t thÆ°a):** DÃ¹ cÃ³ tá»•ng sá»‘ tham sá»‘ khá»•ng lá»“ (vÃ­ dá»¥ 8x7B), nhÆ°ng má»—i láº§n cháº¡y chá»‰ kÃ­ch hoáº¡t má»™t pháº§n nhá» (vÃ­ dá»¥ 2 experts/token).
*   **Lá»£i Ã­ch:** Training nhanh hÆ¡n, Inference ráº» hÆ¡n so vá»›i mÃ´ hÃ¬nh Dense cÃ¹ng kÃ­ch thÆ°á»›c.
*   **ThÃ¡ch thá»©c:** Cáº§n cÃ¢n báº±ng táº£i (Load balancing) Ä‘á»ƒ trÃ¡nh viá»‡c má»™t vÃ i chuyÃªn gia lÃ m viá»‡c quÃ¡ sá»©c cÃ²n sá»‘ khÃ¡c thÃ¬ ngá»“i chÆ¡i (Routing collapse).

---

## 3. Ká»¹ thuáº­t Prompting & In-context Learning
LLM cÃ³ kháº£ nÄƒng há»c tá»« ngá»¯ cáº£nh (In-context Learning) mÃ  khÃ´ng cáº§n cáº­p nháº­t trá»ng sá»‘.

*   **Zero-shot:** Ra lá»‡nh trá»±c tiáº¿p (VD: "Dá»‹ch cÃ¢u nÃ y sang tiáº¿ng Anh").
*   **Few-shot:** Cung cáº¥p vÃ i vÃ­ dá»¥ máº«u trÆ°á»›c khi há»i (VD: "Q: Hi A: ChÃ o / Q: Bye A: Táº¡m biá»‡t / Q: Thanks A: ...").
*   **Chain-of-Thought (CoT):** YÃªu cáº§u mÃ´ hÃ¬nh "suy nghÄ© tá»«ng bÆ°á»›c" (Let's think step by step). GiÃºp cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ kháº£ nÄƒng giáº£i toÃ¡n vÃ  suy luáº­n logic.
*   **Self-Consistency:** Há»i cÃ¹ng má»™t cÃ¢u nhiá»u láº§n (sampling) vÃ  chá»n cÃ¢u tráº£ lá»i xuáº¥t hiá»‡n nhiá»u nháº¥t (Majority voting).

---

## 4. Decoding Strategies (Chiáº¿n lÆ°á»£c giáº£i mÃ£)
LÃ m sao chá»n tá»« tiáº¿p theo tá»« phÃ¢n phá»‘i xÃ¡c suáº¥t do mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n?

*   **Greedy Decoding:** LuÃ´n chá»n tá»« cÃ³ xÃ¡c suáº¥t cao nháº¥t. *NhÆ°á»£c Ä‘iá»ƒm:* Dá»… bá»‹ láº·p, vÄƒn báº£n nhÃ m chÃ¡n, Ä‘Ã´i khi khÃ´ng tá»‘i Æ°u toÃ n cá»¥c.
*   **Beam Search:** Giá»¯ láº¡i K nhÃ¡nh tiá»m nÄƒng nháº¥t táº¡i má»—i bÆ°á»›c. Tá»‘i Æ°u hÆ¡n Greedy nhÆ°ng tá»‘n kÃ©m vÃ  Ä‘Ã´i khi váº«n thiáº¿u tá»± nhiÃªn.
*   **Sampling (Láº¥y máº«u ngáº«u nhiÃªn):** Chá»n tá»« dá»±a trÃªn xÃ¡c suáº¥t (cÃ³ tÃ­nh ngáº«u nhiÃªn).
    *   **Temperature (Nhiá»‡t Ä‘á»™):**
        *   $T \to 0$: Trá»Ÿ vá» Greedy (chÃ­nh xÃ¡c, Ã­t sÃ¡ng táº¡o).
        *   $T \to \infty$: PhÃ¢n phá»‘i pháº³ng (ráº¥t sÃ¡ng táº¡o nhÆ°ng dá»… nÃ³i nháº£m).
    *   **Top-k Sampling:** Chá»‰ chá»n trong K tá»« cÃ³ xÃ¡c suáº¥t cao nháº¥t.
    *   **Top-p (Nucleus) Sampling:** Chá»‰ chá»n trong nhÃ³m tá»« cÃ³ tá»•ng xÃ¡c suáº¥t tÃ­ch lÅ©y Ä‘áº¡t ngÆ°á»¡ng P (vÃ­ dá»¥ 0.9). *Phá»• biáº¿n nháº¥t hiá»‡n nay.*

---

## 5. Tá»‘i Æ°u hÃ³a Inference
Cháº¡y LLM tá»‘n kÃ©m chá»§ yáº¿u do bÄƒng thÃ´ng bá»™ nhá»› (Memory Bound).

### KV Cache
*   Trong quÃ¡ trÃ¬nh sinh tá»« (Auto-regressive), cÃ¡c token phÃ­a trÆ°á»›c khÃ´ng Ä‘á»•i.
*   Thay vÃ¬ tÃ­nh láº¡i Key vÃ  Value cho toÃ n bá»™ chuá»—i má»—i láº§n sinh tá»« má»›i, ta **lÆ°u láº¡i (Cache)** cÃ¡c Key/Value cÅ© vÃ  chá»‰ tÃ­nh thÃªm cho token má»›i nháº¥t.
*   GiÃºp tÄƒng tá»‘c Ä‘á»™ suy luáº­n Ä‘Ã¡ng ká»ƒ nhÆ°ng tá»‘n VRAM.

### PagedAttention (vLLM)
*   Quáº£n lÃ½ bá»™ nhá»› KV Cache giá»‘ng nhÆ° há»‡ Ä‘iá»u hÃ nh quáº£n lÃ½ RAM (phÃ¢n trang - paging).
*   Giáº£m lÃ£ng phÃ­ bá»™ nhá»› (f18-RAGmentation), cho phÃ©p batch size lá»›n hÆ¡n -> TÄƒng throughput.

### Speculative Decoding (Giáº£i mÃ£ Ä‘áº§u cÆ¡)
*   DÃ¹ng má»™t mÃ´ hÃ¬nh nhá» (Draft model) cháº¡y nhanh Ä‘á»ƒ "Ä‘oÃ¡n" trÆ°á»›c vÃ i tá»«.
*   DÃ¹ng mÃ´ hÃ¬nh lá»›n (Target model) Ä‘á»ƒ kiá»ƒm tra láº¡i cÃ¡c tá»« Ä‘Ã³ song song.
*   Náº¿u Ä‘oÃ¡n Ä‘Ãºng -> Cháº¥p nháº­n hÃ ng loáº¡t (TÄƒng tá»‘c). Náº¿u sai -> Sá»­a láº¡i.
*   Táº­n dá»¥ng viá»‡c mÃ´ hÃ¬nh lá»›n tÃ­nh toÃ¡n song song tá»‘t hÆ¡n lÃ  cháº¡y tuáº§n tá»± tá»«ng tá»«.

---
*BiÃªn soáº¡n bá»Ÿi Pixiboss - Dá»±a trÃªn Stanford CME 295.*
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [CS229: XÃ¢y Dá»±ng MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n (LLMs) ğŸ§ ](aero_llm_00_overview.md) | [Xem bÃ i viáº¿t â†’](aero_llm_00_overview.md) |
| [Lecture 1: Transformer Architecture ğŸ¤–](aero_llm_01_transformer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_transformer.md) |
| [Lecture 2: Transformer Tricks & BERT ğŸ› ï¸](aero_llm_02_transformer_tricks.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_transformer_tricks.md) |
| ğŸ“Œ **[Lecture 3: Large Language Models (LLMs) & Inference ğŸš€](aero_llm_03_large_language_models.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_03_large_language_models.md) |
| [Lecture 4: LLM Training - Pre-training ğŸ‹ï¸](aero_llm_04_training_pretraining.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_training_pretraining.md) |
| [Lecture 5: LLM Tuning (SFT & Parameter Efficient) ğŸ›ï¸](aero_llm_05_tuning_peft.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_tuning_peft.md) |
| [Lecture 6: LLM Reasoning ğŸ§ ](aero_llm_06_reasoning.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_reasoning.md) |
| [Lecture 7: Agentic LLMs & Tool Use ğŸ› ï¸](aero_llm_07_agentic_llms.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_agentic_llms.md) |
| [Lecture 8: LLM Evaluation âš–ï¸](aero_llm_08_evaluation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_evaluation.md) |
| [Lecture 9: Recap & Current Trends ğŸ”®](aero_llm_09_trends.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_trends.md) |
| [ğŸ› ï¸ Top 12 Repo Quan Trá»ng Cho AI Engineer Tá»‘i Æ¯u LLM](aero_llm_10_essential_tools.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_essential_tools.md) |
| [ChÆ°Æ¡ng 1: Tá»•ng Quan Vá» Large Language Models (LLMs) ğŸ§ ](aero_llm_chapter01_overview_detailed.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter01_overview_detailed.md) |
| [ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t Cá»§a Viá»‡c Huáº¥n Luyá»‡n LLMs ğŸ›ï¸](aero_llm_chapter02_5pillars_part1.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter02_5pillars_part1.md) |
| [ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t - Part 2 (Evaluation & Systems)](aero_llm_chapter02_5pillars_part2.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter02_5pillars_part2.md) |
| [ChÆ°Æ¡ng 3: Pre-training â†’ Post-training Pipeline ğŸ”„](aero_llm_chapter03_training_pipeline.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter03_training_pipeline.md) |
| [ChÆ°Æ¡ng 4 & 5: Mechanisms & Evaluation ğŸ”§ğŸ“Š](aero_llm_chapter04_05_mechanisms_eval.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter04_05_mechanisms_eval.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

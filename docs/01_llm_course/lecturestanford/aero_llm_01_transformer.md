
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [01 llm course](../index.md) > [lecturestanford](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Lecture 1: Transformer Architecture ğŸ¤–

> **TÃ³m táº¯t tá»« khÃ³a há»c Stanford CME 295: Transformers & Large Language Models.**
> BÃ i giáº£ng nÃ y giá»›i thiá»‡u ná»n táº£ng cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡n Ä‘áº¡i: Kiáº¿n trÃºc Transformer vÃ  cÆ¡ cháº¿ Self-Attention.

---

## ğŸ“š Má»¥c Lá»¥c
1. [Giá»›i thiá»‡u vá» NLP](#1-giá»›i-thiá»‡u-vá»-nlp)
2. [Tokenization (MÃ£ hÃ³a vÄƒn báº£n)](#2-tokenization-mÃ£-hÃ³a-vÄƒn-báº£n)
3. [Word Embeddings (Biá»ƒu diá»…n tá»«)](#3-word-embeddings-biá»ƒu-diá»…n-tá»«)
4. [Sá»± háº¡n cháº¿ cá»§a RNN/LSTM](#4-sá»±-háº¡n-cháº¿-cá»§a-rnnlstm)
5. [CÆ¡ cháº¿ Attention & Self-Attention](#5-cÆ¡-cháº¿-attention--self-attention)
6. [Kiáº¿n trÃºc Transformer](#6-kiáº¿n-trÃºc-transformer)

---

## 1. Giá»›i thiá»‡u vá» NLP
**Natural Language Processing (NLP)** lÃ  lÄ©nh vá»±c xá»­ lÃ½ vÃ  tÃ­nh toÃ¡n trÃªn vÄƒn báº£n. CÃ³ 3 nhÃ³m tÃ¡c vá»¥ chÃ­nh:
*   **Classification (PhÃ¢n loáº¡i):** Äáº§u vÃ o lÃ  text, Ä‘áº§u ra lÃ  nhÃ£n (VD: Sentiment Analysis - PhÃ¢n tÃ­ch cáº£m xÃºc).
*   **Multi-classification / Tagging:** GÃ¡n nhÃ£n cho tá»«ng tá»« hoáº·c thá»±c thá»ƒ (VD: Named Entity Recognition - NER).
*   **Generation (Sinh vÄƒn báº£n):** Äáº§u vÃ o lÃ  text, Ä‘áº§u ra lÃ  text (VD: Dá»‹ch mÃ¡y, Chatbot). ÄÃ¢y lÃ  nhÃ³m tÃ¡c vá»¥ phá»• biáº¿n nháº¥t hiá»‡n nay vá»›i LLMs.

---

## 2. Tokenization (MÃ£ hÃ³a vÄƒn báº£n)
MÃ´ hÃ¬nh khÃ´ng hiá»ƒu vÄƒn báº£n thÃ´, nÃ³ cáº§n cÃ¡c con sá»‘. QuÃ¡ trÃ¬nh chia nhá» vÄƒn báº£n thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ cÆ¡ báº£n gá»i lÃ  **Tokenization**.

*   **Word-level (Má»©c tá»«):** Chia theo dáº¥u cÃ¡ch.
    *   *NhÆ°á»£c Ä‘iá»ƒm:* KhÃ´ng xá»­ lÃ½ Ä‘Æ°á»£c tá»« chÆ°a biáº¿t (OOV - Out Of Vocabulary), khÃ´ng táº­n dá»¥ng Ä‘Æ°á»£c gá»‘c tá»« (root words).
*   **Character-level (Má»©c kÃ½ tá»±):** Chia theo tá»«ng chá»¯ cÃ¡i.
    *   *NhÆ°á»£c Ä‘iá»ƒm:* Chuá»—i quÃ¡ dÃ i, máº¥t ngá»¯ nghÄ©a, tÃ­nh toÃ¡n cháº­m.
*   **Subword-level (Má»©c tá»« con - Phá»• biáº¿n nháº¥t):** Chia tá»« thÃ nh cÃ¡c pháº§n nhá» hÆ¡n cÃ³ nghÄ©a (VD: "reading" -> "read" + "ing").
    *   *Æ¯u Ä‘iá»ƒm:* CÃ¢n báº±ng giá»¯a Ä‘á»™ dÃ i chuá»—i vÃ  kÃ­ch thÆ°á»›c tá»« Ä‘iá»ƒn, xá»­ lÃ½ tá»‘t tá»« hiáº¿m.

---

## 3. Word Embeddings (Biá»ƒu diá»…n tá»«)
Sau khi cÃ³ token (ID), ta cáº§n chuyá»ƒn nÃ³ thÃ nh vector sá»‘ há»c gá»i lÃ  **Embedding**.
*   **One-hot Encoding:** Vector toÃ n sá»‘ 0 vÃ  má»™t sá»‘ 1. *NhÆ°á»£c Ä‘iá»ƒm:* KhÃ´ng thá»ƒ hiá»‡n Ä‘Æ°á»£c sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c tá»« (cÃ¡c vector Ä‘á»u trá»±c giao).
*   **Learned Embeddings (Word2Vec):** Há»c biá»ƒu diá»…n tá»« sao cho cÃ¡c tá»« cÃ³ ngá»¯ nghÄ©a giá»‘ng nhau sáº½ náº±m gáº§n nhau trong khÃ´ng gian vector (VD: King - Man + Woman â‰ˆ Queen).

---

## 4. Sá»± háº¡n cháº¿ cá»§a RNN/LSTM
TrÆ°á»›c Transformer, RNN (Recurrent Neural Networks) vÃ  LSTM lÃ  chuáº©n má»±c.
*   **CÆ¡ cháº¿:** Xá»­ lÃ½ tuáº§n tá»± tá»«ng tá»« má»™t (word by word), giá»¯ láº¡i "bá»™ nhá»›" (hidden state) vá» cÃ¡c tá»« Ä‘Ã£ qua.
*   **NhÆ°á»£c Ä‘iá»ƒm chÃ­ tá»­:**
    *   **Long-range dependency (Phá»¥ thuá»™c xa):** KhÃ³ nhá»› Ä‘Æ°á»£c thÃ´ng tin tá»« Ä‘áº§u cÃ¢u khi Ä‘Ã£ Ä‘i Ä‘áº¿n cuá»‘i cÃ¢u (váº¥n Ä‘á» Vanishing Gradient).
    *   **KhÃ´ng thá»ƒ song song hÃ³a:** Pháº£i Ä‘á»£i tá»« trÆ°á»›c xá»­ lÃ½ xong má»›i Ä‘áº¿n tá»« sau -> Tá»‘c Ä‘á»™ huáº¥n luyá»‡n ráº¥t cháº­m.

---

## 5. CÆ¡ cháº¿ Attention & Self-Attention
Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» "quÃªn" cá»§a RNN, cÆ¡ cháº¿ **Attention** ra Ä‘á»i (2014) vÃ  Ä‘á»‰nh cao lÃ  **Self-Attention** (2017).

**Ã tÆ°á»Ÿng:** Thay vÃ¬ xá»­ lÃ½ tuáº§n tá»±, hÃ£y cho phÃ©p má»—i tá»« "nhÃ¬n" tháº¥y táº¥t cáº£ cÃ¡c tá»« khÃ¡c trong cÃ¢u cÃ¹ng má»™t lÃºc vÃ  tá»± quyáº¿t Ä‘á»‹nh xem tá»« nÃ o quan trá»ng vá»›i mÃ¬nh.

### CÃ´ng thá»©c Self-Attention
Má»—i token Ä‘Æ°á»£c chiáº¿u thÃ nh 3 vector: **Query (Q)**, **Key (K)**, **Value (V)**.

$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

*   **Q (Truy váº¥n):** TÃ´i Ä‘ang tÃ¬m kiáº¿m thÃ´ng tin gÃ¬?
*   **K (ChÃ¬a khÃ³a):** TÃ´i cÃ³ thÃ´ng tin gÃ¬ Ä‘á»ƒ cung cáº¥p?
*   **V (GiÃ¡ trá»‹):** Ná»™i dung thÃ´ng tin cá»§a tÃ´i lÃ  gÃ¬?
*   **$QK^T$:** TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng (score) giá»¯a truy váº¥n vÃ  chÃ¬a khÃ³a.
*   **Softmax:** Chuáº©n hÃ³a score thÃ nh trá»ng sá»‘ (tá»•ng = 1).
*   **NhÃ¢n vá»›i V:** Tá»•ng há»£p thÃ´ng tin tá»« cÃ¡c tá»« quan trá»ng.

---

## 6. Kiáº¿n trÃºc Transformer
MÃ´ hÃ¬nh **Transformer** (trong bÃ i bÃ¡o "Attention Is All You Need") bao gá»“m:

### Encoder (Bá»™ mÃ£ hÃ³a)
*   Xá»­ lÃ½ chuá»—i Ä‘áº§u vÃ o (Input).
*   DÃ¹ng **Self-Attention** Ä‘á»ƒ hiá»ƒu ngá»¯ cáº£nh cá»§a tá»« trong cÃ¢u.
*   Äáº§u ra: CÃ¡c vector embedding giÃ u ngá»¯ nghÄ©a.

### Decoder (Bá»™ giáº£i mÃ£)
*   Sinh chuá»—i Ä‘áº§u ra (Output).
*   DÃ¹ng **Masked Self-Attention** (chá»‰ nhÃ¬n tháº¥y cÃ¡c tá»« phÃ­a trÆ°á»›c, khÃ´ng nhÃ¬n tháº¥y tÆ°Æ¡ng lai).
*   DÃ¹ng **Cross-Attention** Ä‘á»ƒ láº¥y thÃ´ng tin tá»« Encoder.

### CÃ¡c thÃ nh pháº§n khÃ¡c
*   **Positional Encoding:** VÃ¬ Transformer xá»­ lÃ½ song song nÃªn khÃ´ng biáº¿t thá»© tá»± tá»« -> Cáº§n cá»™ng thÃªm vector vá»‹ trÃ­ Ä‘á»ƒ bÃ¡o cho mÃ´ hÃ¬nh biáº¿t tá»« nÃ o Ä‘á»©ng trÆ°á»›c/sau.
*   **Multi-Head Attention:** Thay vÃ¬ chá»‰ cÃ³ 1 bá»™ Q,K,V, ta dÃ¹ng nhiá»u bá»™ (Heads) Ä‘á»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c nhiá»u má»‘i quan há»‡ khÃ¡c nhau cÃ¹ng lÃºc.
*   **Feed Forward Network (FFN):** Máº¡ng nÆ¡-ron truyá»n tháº³ng Ä‘á»ƒ xá»­ lÃ½ thÃ´ng tin sau lá»›p Attention.
*   **Add & Norm:** Cá»™ng pháº§n dÆ° (Residual connection) vÃ  chuáº©n hÃ³a lá»›p (LayerNorm) Ä‘á»ƒ huáº¥n luyá»‡n á»•n Ä‘á»‹nh hÆ¡n.

---
*BiÃªn soáº¡n bá»Ÿi Pixiboss - Dá»±a trÃªn Stanford CME 295.*
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [CS229: XÃ¢y Dá»±ng MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n (LLMs) ğŸ§ ](aero_llm_00_overview.md) | [Xem bÃ i viáº¿t â†’](aero_llm_00_overview.md) |
| ğŸ“Œ **[Lecture 1: Transformer Architecture ğŸ¤–](aero_llm_01_transformer.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_01_transformer.md) |
| [Lecture 2: Transformer Tricks & BERT ğŸ› ï¸](aero_llm_02_transformer_tricks.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_transformer_tricks.md) |
| [Lecture 3: Large Language Models (LLMs) & Inference ğŸš€](aero_llm_03_large_language_models.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_large_language_models.md) |
| [Lecture 4: LLM Training - Pre-training ğŸ‹ï¸](aero_llm_04_training_pretraining.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_training_pretraining.md) |
| [Lecture 5: LLM Tuning (SFT & Parameter Efficient) ğŸ›ï¸](aero_llm_05_tuning_peft.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_tuning_peft.md) |
| [Lecture 6: LLM Reasoning ğŸ§ ](aero_llm_06_reasoning.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_reasoning.md) |
| [Lecture 7: Agentic LLMs & Tool Use ğŸ› ï¸](aero_llm_07_agentic_llms.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_agentic_llms.md) |
| [Lecture 8: LLM Evaluation âš–ï¸](aero_llm_08_evaluation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_evaluation.md) |
| [Lecture 9: Recap & Current Trends ğŸ”®](aero_llm_09_trends.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_trends.md) |
| [ğŸ› ï¸ Top 12 Repo Quan Trá»ng Cho AI Engineer Tá»‘i Æ¯u LLM](aero_llm_10_essential_tools.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_essential_tools.md) |
| [ChÆ°Æ¡ng 1: Tá»•ng Quan Vá» Large Language Models (LLMs) ğŸ§ ](aero_llm_chapter01_overview_detailed.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter01_overview_detailed.md) |
| [ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t Cá»§a Viá»‡c Huáº¥n Luyá»‡n LLMs ğŸ›ï¸](aero_llm_chapter02_5pillars_part1.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter02_5pillars_part1.md) |
| [ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t - Part 2 (Evaluation & Systems)](aero_llm_chapter02_5pillars_part2.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter02_5pillars_part2.md) |
| [ChÆ°Æ¡ng 3: Pre-training â†’ Post-training Pipeline ğŸ”„](aero_llm_chapter03_training_pipeline.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter03_training_pipeline.md) |
| [ChÆ°Æ¡ng 4 & 5: Mechanisms & Evaluation ğŸ”§ğŸ“Š](aero_llm_chapter04_05_mechanisms_eval.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter04_05_mechanisms_eval.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [01 llm course](../index.md) > [lecturestanford](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t - Part 2 (Evaluation & Systems)

> **Tiáº¿p theo tá»« Part 1**

---

## Trá»¥ Cá»™t 4: Evaluation (ÄÃ¡nh GiÃ¡)

### **"You Can't Improve What You Don't Measure"**

### **Táº¡i Sao Evaluation Quan Trá»ng?**

1. **Progress tracking:** Biáº¿t model cÃ³ Ä‘ang há»c khÃ´ng
2. **Model comparison:** A tá»‘t hÆ¡n B á»Ÿ Ä‘Ã¢u?
3. **Debug:** TÃ¬m weakness Ä‘á»ƒ cáº£i thiá»‡n
4. **Business value:** ROI, user satisfaction

### **Levels of Evaluation**

Level 1: Perplexity (Training metric)
  â†“
Level 2: Academic Benchmarks (MMLU, HumanEval)
  â†“
Level 3: Human Evaluation (Quality, safety)
  â†“
Level 4: Real-world Usage (Production metrics)

---

### **A. Perplexity**

**Äá»‹nh nghÄ©a:**
PPL = exp(-1/N âˆ‘áµ¢ log P(xáµ¢ | xâ‚,...,xáµ¢â‚‹â‚))

**Ã nghÄ©a:**
- Äá»™ "bá»‘i rá»‘i" cá»§a model khi dá»± Ä‘oÃ¡n
- **Tháº¥p hÆ¡n = Tá»‘t hÆ¡n**
- PPL = 10 â†’ Model "surprise" Ã­t hÆ¡n PPL = 100

**Example:**
```python
# Sentence: "The cat sat on the mat"
probs = [0.8, 0.6, 0.9, 0.7, 0.5, 0.8]  # Probabilities
ppl = exp(-mean([log(p) for p in probs]))
# ppl â‰ˆ 1.8 (very good)

**Historical Trends:**

| Year | Model | Perplexity (WikiText-103) |
|------|-------|---------------------------|
| 2017 | LSTM | ~70 |
| 2018 | GPT-1 | ~37 |
| 2019 | GPT-2 | ~18 |
| 2020 | GPT-3 | ~15 |
| 2023 | GPT-4 | ~8 (estimated) |

**Limitations:**
- âŒ Doesn't measure reasoning
- âŒ Doesn't capture safety
- âŒ Can be gamed (memorization)

---

### **B. Academic Benchmarks**

#### **1. MMLU (Massive Multitask Language Understanding)**

**What:** 57 subjects, multiple-choice questions

**Subjects:**
- STEM: Math, Physics, Chemistry, CS
- Humanities: History, Philosophy, Law  
- Social Sciences: Psychology, Economics
- Other: Medicine, Business

**Format:**
Question: What is the primary function of ribosomes?
A) DNA replication
B) Protein synthesis
C) Cell division
D) Energy production

Answer: B

**GPT-4 Performance:**
- GPT-3.5: 70.0%
- **GPT-4: 86.4%** (human expert ~90%)

**Leaderboard (2024):**
1. GPT-4: 86.4%
2. Claude 3 Opus: 86.8%
3. Gemini Ultra: 90.0%

#### **2. HumanEval (Code Generation)**

**What:** 164 Python programming problems

**Format:**
```python
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    """
    Check if in given list of numbers, are any two numbers 
    closer to each other than given threshold.
    """
    # Model generates code here

**Metrics:**
- Pass@1: % correct on first try
- Pass@10: % correct in 10 tries

**Results:**
| Model | Pass@1 |
|-------|--------|
| GPT-3 | 0% |
| Codex | 28.8% |
| GPT-3.5 | 48.1% |
| **GPT-4** | **67.0%** |
| Claude 3 Opus | 84.9% |

#### **3. Other Important Benchmarks**

| Benchmark | Focus | Examples |
|-----------|-------|----------|
| **GSM8K** | Math reasoning | 8K grade school problems |
| **HellaSwag** | Commonsense | Sentence completion |
| **TruthfulQA** | Truthfulness | Avoiding misconceptions |
| **BBHard** | Hard reasoning | 23 challenging tasks |
| **DROP** | Reading comp | Complex reasoning over text |

---

### **C. Human Evaluation**

**Why needed:**
- Benchmarks can be memorized
- Real-world tasks are open-ended
- Subjective quality matters

**Evaluation Criteria:**

1. **Helpfulness**
   - Did it answer the question?
   - Is the answer complete?
   - Is it actionable?

2. **Harmlessness**
   - No toxic content
   - No illegal advice
   - No personal attacks

3. **Honesty**
   - Admits uncertainty
   - Doesn't hallucinate
   - Cites sources when applicable

**Evaluation Process:**

1. Sample Generation
   â”œâ”€â”€ User prompts (diverse topics)
   â”œâ”€â”€ Generate responses
   â””â”€â”€ Multiple models (A/B/C)
   
2. Human Rating
   â”œâ”€â”€ Likert scale (1-5)
   â”œâ”€â”€ Pairwise comparison
   â””â”€â”€ Preference ranking
   
3. Analysis
   â”œâ”€â”€ Inter-rater agreement
   â”œâ”€â”€ Confidence intervals
   â””â”€â”€ Statistical significance
   
4. Iterate
   â””â”€â”€ Fix common failures

**Example (ChatGPT Eval):**
Prompt: "Explain quantum computing to a 5-year-old"

GPT-3.5: [Technical jargon, not age-appropriate]
Rating: 2/5

GPT-4: "Imagine a magic computer that can be in 
many places at once, like being in your room AND 
the kitchen at the same time..."
Rating: 5/5

---

### **D. Real-World Metrics**

**Production KPIs:**

| Metric | Definition | Target |
|--------|------------|--------|
| **Latency** | Time to first token | < 500ms |
| **Throughput** | Tokens/sec | > 100 |
| **Cost** | $/1K tokens | < $0.01 |
| **Uptime** | Availability % | > 99.9% |
| **User Satisfaction** | Thumbs up % | > 80% |

**Business Metrics:**
- Retention rate
- Engagement $msgs/user/day$
- Revenue (subscriptions, API usage)

---

## Trá»¥ Cá»™t 5: Systems (Há»‡ Thá»‘ng)

### **"Training is a Systems Problem"**

**Reality:**
- GPT-3: 355 GPU-years
- GPT-4: ~2000 GPU-years  
- Gemini Ultra: Estimated 10,000+ GPU-years

**Cost:**
- GPT-3: ~$4M
- GPT-4: ~$100M
- Training run can cost more than entire startups!

---

### **A. Hardware**

#### **GPUs for Training**

| GPU | Memory | FP16 TFLOPS | Price | Used By |
|-----|--------|-------------|-------|---------|
| **A100** | 80GB | 312 | ~$15K | GPT-3, most labs |
| **H100** | 80GB | 1000 | ~$40K | **GPT-4, Gemini** |
| **MI250X** (AMD) | 128GB | 383 | ~$12K | Stable Diffusion XL |
| **TPU v4** | 32GB HBM | Variable | Google only | PaLM, Gemini |

**GPT-4 Cluster (estimated):**
10,000Ã— H100 GPUs
â”œâ”€â”€ 8Ã— GPUs per node = 1,250 nodes
â”œâ”€â”€ NVLink: 600 GB/s inter-GPU
â”œâ”€â”€ InfiniBand: 400 Gb/s networking
â””â”€â”€ Total compute: ~100,000 petaFLOPS

**Cost per hour:**
10,000 H100 Ã— $3/hr = $30,000/hour
100 days training = $72 million (compute only!)

#### **Memory Hierarchy**

L1 Cache (KB)      â† 1000Ã— faster, tiny
  â†“
L2 Cache (MB)      â† 100Ã— faster, small
  â†“
GPU RAM (80GB)     â† 10Ã— faster, limited
  â†“
CPU RAM (1TB)      â† Baseline
  â†“
SSD (10TB)         â† 10Ã— slower
  â†“
Network Sto18_rage    â† 100Ã— slower

**Challenge:** Model doesn't fit in GPU RAM!

---

### **B. Parallelization Strategies**

#### **1. Data Parallelism**

GPU 0: Batch 0 â†’ Forward â†’ Backward â†’ Gradâ‚€
GPU 1: Batch 1 â†’ Forward â†’ Backward â†’ Gradâ‚
GPU 2: Batch 2 â†’ Forward â†’ Backward â†’ Gradâ‚‚
  â†“
All-Reduce (Average gradients)
  â†“
Update weights (synchronized)

**Pros:** Simple, linear scaling  
**Cons:** Requires full model on each GPU

#### **2. Model Parallelism (Tensor Parallelism)**

Layer splits across GPUs:

GPU 0: [A] â”€â”€â†’ [B]
               â†“
GPU 1: [C] â”€â”€â†’ [D]

All-to-All communication

**Example (GPT-4):**
```python
# Split attention across 8 GPUs
Q = split(Q, dim=heads, n_splits=8)  # Each GPU gets 16/8 = 2 heads
K = split(K, dim=heads, n_splits=8)
V = split(V, dim=heads, n_splits=8)

**Pros:** Handles huge models  
**Cons:** High communication overhead

#### **3. Pipeline Parallelism**

GPU 0: Layer 0-29   â†’ Forward Batch 0 â†’ Forward Batch 1 â†’
       â†“
GPU 1: Layer 30-59  â†’ (wait)          â†’ Forward Batch 0 â†’
       â†“
GPU 2: Layer 60-89  â†’ (wait)          â†’ (wait)          â†’
       â†“
GPU 3: Layer 90-119 â†’ (idle)          â†’ (idle)          â†’

**GPipe / 1F1B:**
- Micro-batches to reduce bubbles
- Backward pass interleaved

**Pros:** Good for very deep models  
**Cons:** Bubble time (idle GPUs)

#### **4. 3D Parallelism (ZeRO)**

**Combines all three:**

ZeRO Stage 1: Partition optimizer states
ZeRO Stage 2: + Partition gradients
ZeRO Stage 3: + Partition model weights

**Memory Savings:**
Before: 1.76T params Ã— 16 bytes = 28 TB (per GPU!)
After ZeRO-3: 28 TB / 10,000 GPUs = 2.8 GB per GPU âœ…

**Used by:**
- GPT-4 (DeepSpeed ZeRO)
- Megatron-LM (NVIDIA)
- FSDP (PyTorch)

---

### **C. Training Infrastructure**

**Full Stack:**

Application Layer
â”œâ”€â”€ PyTorch / JAX
â”œâ”€â”€ DeepSpeed / Megatron
â””â”€â”€ Model code

Training Framework
â”œâ”€â”€ Distributed training
â”œâ”€â”€ Mixed precision
â”œâ”€â”€ Gradient accumulation
â””â”€â”€ Checkpointing

Systems Layer
â”œâ”€â”€ NCCL (GPU communication)
â”œâ”€â”€ InfiniBand (networking)
â””â”€â”€ Sto18_rage (NVMe, Lustre)

Hardware
â”œâ”€â”€ 10,000+ H100 GPUs
â”œâ”€â”€ High-speed interconnects
â””â”€â”€ Cooling & power

**GPT-4 Training Pipeline:**

```python
# Pseudo-code
model = GPT4(params=1.76T)
optimizer = AdamW(lr=6e-4)
scaler = GradScaler()  # Mixed precision

# 3D Parallelism
model = apply_tensor_parallel(model, tp_size=8)
model = apply_pipeline_parallel(model, pp_size=16)
model = apply_data_parallel(model, dp_size=78)
# Total: 8 Ã— 16 Ã— 78 â‰ˆ 10,000 GPUs

for epoch in range(3):  # 3 epochs Ã— 13T tokens
    for batch in dataloader:
        with autocast(dtype=bfloat16):
            output = model(batch)
            loss = cross_entropy(output, targets)
        
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        if step % checkpoint_interval == 0:
            save_checkpoint(model, optimizer, step)

---

### **D. Optimization Techniques**

**1. Gradient Checkpointing:**
```python
# Trade compute for memory
# Recompute activations during backward
model = checkpoint_sequential(model, segments=4)
# 4Ã— less memory, 20% slower

**2. Flash Attention:**
```python
# Fused attention kernel
# 2-4Ã— faster, less memory
from flash_attn import flash_attn_func
attn_output = flash_attn_func(Q, K, V)

**3. Quantization:**
```python
# Train in INT8
model = quantize_dynamic(model, dtype=torch.qint8)
# 2Ã— faster, 4Ã— less memory

---

## Academia vs Industry

### **Focus Distribution**

| Pillar | Academia | Industry |
|--------|----------|----------|
| Architecture | **80%** | 20% |
| Loss/Algorithm | 15% | 20% |
| Data | 3% | **35%** |
| Evaluation | 2% | **15%** |
| Systems | 0% | **10%** |

**Why the difference?**

**Academia:**
- New architectures â†’ Papers
- Limited compute budget
- Public datasets
- Leaderboard chasing

**Industry:**
- Product quality â†’ Revenue
- Massive compute access
- Proprietary data advantage
- Real user feedback

---

## ğŸ¯ Key Takeaways

1. âœ… **5 Pillars ALL matter** - ignoring any one = failure
2. âœ… **Architecture:** MoE is state-of-the-art
3. âœ… **Loss:** Cross-entropy + RLHF
4. âœ… **Data:** Quality > Quantity (but both needed)
5. âœ… **Evaluation:** Multi-level (perplexity â†’ human â†’ production)
6. âœ… **Systems:** $100M training requires 10K+ GPUs
7. âœ… **Industry vs Academia:** Different priorities

---

**Next:** [ChÆ°Æ¡ng 3: Pre-training â†’ Post-training â†’](aero_LLM_chapter03_training_pipeline.md)

---

*BiÃªn soáº¡n bá»Ÿi Pixibot - Based on Stanford CS229*  
*Last updated: 2026-02-15*
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [CS229: XÃ¢y Dá»±ng MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n (LLMs) ğŸ§ ](aero_llm_00_overview.md) | [Xem bÃ i viáº¿t â†’](aero_llm_00_overview.md) |
| [Lecture 1: Transformer Architecture ğŸ¤–](aero_llm_01_transformer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_transformer.md) |
| [Lecture 2: Transformer Tricks & BERT ğŸ› ï¸](aero_llm_02_transformer_tricks.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_transformer_tricks.md) |
| [Lecture 3: Large Language Models (LLMs) & Inference ğŸš€](aero_llm_03_large_language_models.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_large_language_models.md) |
| [Lecture 4: LLM Training - Pre-training ğŸ‹ï¸](aero_llm_04_training_pretraining.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_training_pretraining.md) |
| [Lecture 5: LLM Tuning (SFT & Parameter Efficient) ğŸ›ï¸](aero_llm_05_tuning_peft.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_tuning_peft.md) |
| [Lecture 6: LLM Reasoning ğŸ§ ](aero_llm_06_reasoning.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_reasoning.md) |
| [Lecture 7: Agentic LLMs & Tool Use ğŸ› ï¸](aero_llm_07_agentic_llms.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_agentic_llms.md) |
| [Lecture 8: LLM Evaluation âš–ï¸](aero_llm_08_evaluation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_evaluation.md) |
| [Lecture 9: Recap & Current Trends ğŸ”®](aero_llm_09_trends.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_trends.md) |
| [ğŸ› ï¸ Top 12 Repo Quan Trá»ng Cho AI Engineer Tá»‘i Æ¯u LLM](aero_llm_10_essential_tools.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_essential_tools.md) |
| [ChÆ°Æ¡ng 1: Tá»•ng Quan Vá» Large Language Models (LLMs) ğŸ§ ](aero_llm_chapter01_overview_detailed.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter01_overview_detailed.md) |
| [ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t Cá»§a Viá»‡c Huáº¥n Luyá»‡n LLMs ğŸ›ï¸](aero_llm_chapter02_5pillars_part1.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter02_5pillars_part1.md) |
| ğŸ“Œ **[ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t - Part 2 (Evaluation & Systems)](aero_llm_chapter02_5pillars_part2.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_chapter02_5pillars_part2.md) |
| [ChÆ°Æ¡ng 3: Pre-training â†’ Post-training Pipeline ğŸ”„](aero_llm_chapter03_training_pipeline.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter03_training_pipeline.md) |
| [ChÆ°Æ¡ng 4 & 5: Mechanisms & Evaluation ğŸ”§ğŸ“Š](aero_llm_chapter04_05_mechanisms_eval.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter04_05_mechanisms_eval.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

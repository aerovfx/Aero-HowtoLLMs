
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [01 llm course](../index.md) > [lecturestanford](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Lecture 4: LLM Training - Pre-training ğŸ‹ï¸

> **TÃ³m táº¯t tá»« khÃ³a há»c Stanford CME 295: Transformers & Large Language Models.**
> BÃ i giáº£ng nÃ y táº­p trung vÃ o quy trÃ¬nh huáº¥n luyá»‡n LLM, tá»« Pre-training (Tiá»n huáº¥n luyá»‡n) Ä‘áº¿n cÃ¡c ká»¹ thuáº­t tá»‘i Æ°u hÃ³a pháº§n cá»©ng (Parallelism, FlashAttention).

---

## ğŸ“š Má»¥c Lá»¥c
1. [Quy trÃ¬nh huáº¥n luyá»‡n LLM](#1-quy-trÃ¬nh-huáº¥n-luyá»‡n-llm)
2. [Pre-training (Tiá»n huáº¥n luyá»‡n)](#2-pre-training-tiá»n-huáº¥n-luyá»‡n)
3. [Luáº­t má»Ÿ rá»™ng (Scaling Laws)](#3-luáº­t-má»Ÿ-rá»™ng-scaling-laws)
4. [Tá»‘i Æ°u hÃ³a pháº§n cá»©ng & Bá»™ nhá»›](#4-tá»‘i-Æ°u-hÃ³a-pháº§n-cá»©ng--bá»™-nhá»›)
5. [FlashAttention âš¡](#5-flashattention-âš¡)
6. [Quantization & Mixed Precision Training](#6-quantization--mixed-precision-training)

---

## 1. Quy trÃ¬nh huáº¥n luyá»‡n LLM
Huáº¥n luyá»‡n LLM khÃ´ng pháº£i lÃ  má»™t bÆ°á»›c duy nháº¥t mÃ  lÃ  má»™t quy trÃ¬nh nhiá»u giai Ä‘oáº¡n (Multi-stage training):

1.  **Pre-training (Tiá»n huáº¥n luyá»‡n):** Há»c kiáº¿n thá»©c tá»•ng quÃ¡t tá»« dá»¯ liá»‡u khá»•ng lá»“ (Internet, sÃ¡ch, code).
    *   *Má»¥c tiÃªu:* Dá»± Ä‘oÃ¡n tá»« tiáº¿p theo (Next token prediction).
    *   *Káº¿t quáº£:* Base model (MÃ´ hÃ¬nh ná»n táº£ng) - biáº¿t nhiá»u nhÆ°ng chÆ°a biáº¿t lÃ m trá»£ lÃ½.
2.  **Fine-tuning (Tinh chá»‰nh):**
    *   *SFT (Supervised Fine-Tuning):* Dáº¡y mÃ´ hÃ¬nh lÃ m theo hÆ°á»›ng dáº«n (Instruction Following).
    *   *RLHF/RLAIF:* CÄƒn chá»‰nh mÃ´ hÃ¬nh theo sá»Ÿ thÃ­ch con ngÆ°á»i (Alignment).

---

## 2. Pre-training (Tiá»n huáº¥n luyá»‡n)
ÄÃ¢y lÃ  giai Ä‘oáº¡n tá»‘n kÃ©m nháº¥t (hÃ ng triá»‡u USD).

*   **Dá»¯ liá»‡u (Data):** Common Crawl (Internet), Wikipedia, Reddit, GitHub (Code).
    *   *Quy mÃ´:* HÃ ng nghÃ¬n tá»· tokens (Trillions of tokens).
    *   *VÃ­ dá»¥:* Llama 3 Ä‘Æ°á»£c train trÃªn 15T tokens.
*   **ThÃ¡ch thá»©c:**
    *   **Cost (Chi phÃ­):** Ráº¥t lá»›n vá» tiá»n báº¡c vÃ  nÄƒng lÆ°á»£ng.
    *   **Knowledge Cutoff (Giá»›i háº¡n kiáº¿n thá»©c):** MÃ´ hÃ¬nh chá»‰ biáº¿t nhá»¯ng gÃ¬ xáº£y ra trÆ°á»›c thá»i Ä‘iá»ƒm thu tháº­p dá»¯ liá»‡u.
    *   **Hallucination (áº¢o giÃ¡c):** CÃ³ thá»ƒ bá»‹a Ä‘áº·t thÃ´ng tin.

---

## 3. Luáº­t má»Ÿ rá»™ng (Scaling Laws)
LÃ m sao Ä‘á»ƒ mÃ´ hÃ¬nh thÃ´ng minh hÆ¡n?

*   **Kaplan et al. (2020):** Hiá»‡u nÄƒng mÃ´ hÃ¬nh tÄƒng theo hÃ m mÅ© (power law) khi tÄƒng:
    1.  KÃ­ch thÆ°á»›c mÃ´ hÃ¬nh (Parameters).
    2.  KÃ­ch thÆ°á»›c dá»¯ liá»‡u (Dataset size).
    3.  LÆ°á»£ng tÃ­nh toÃ¡n (Compute).
*   **Chinchilla Scaling Law (Hoffmann et al., 2022):**
    *   Äá»ƒ tá»‘i Æ°u hÃ³a chi phÃ­ tÃ­nh toÃ¡n, khi tÄƒng gáº¥p Ä‘Ã´i kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh, cáº§n tÄƒng gáº¥p Ä‘Ã´i lÆ°á»£ng dá»¯ liá»‡u.
    *   **CÃ´ng thá»©c vÃ ng:** Sá»‘ lÆ°á»£ng tokens huáº¥n luyá»‡n nÃªn gáº¥p khoáº£ng **20 láº§n** sá»‘ lÆ°á»£ng tham sá»‘ mÃ´ hÃ¬nh.
    *   *Há»‡ quáº£:* Nhiá»u mÃ´ hÃ¬nh trÆ°á»›c Ä‘Ã³ (nhÆ° GPT-3) lÃ  "under-trained" (huáº¥n luyá»‡n chÆ°a Ä‘á»§). CÃ¡c mÃ´ hÃ¬nh hiá»‡n Ä‘áº¡i (Llama) cÃ³ xu hÆ°á»›ng nhá» hÆ¡n nhÆ°ng train lÃ¢u hÆ¡n (trÃªn nhiá»u dá»¯ liá»‡u hÆ¡n).

---

## 4. Tá»‘i Æ°u hÃ³a pháº§n cá»©ng & Bá»™ nhá»›
Huáº¥n luyá»‡n LLM Ä‘Ã²i há»i bá»™ nhá»› VRAM khá»•ng lá»“, vÆ°á»£t xa kháº£ nÄƒng cá»§a má»™t GPU Ä‘Æ¡n láº» (vÃ­ dá»¥ H100 80GB).

### CÃ¡c ká»¹ thuáº­t song song hÃ³a (Parallelism):
1.  **Data Parallelism (DP):**
    *   Copy mÃ´ hÃ¬nh ra nhiá»u GPU.
    *   Má»—i GPU xá»­ lÃ½ má»™t pháº§n dá»¯ liá»‡u (batch) khÃ¡c nhau.
    *   Äá»“ng bá»™ hÃ³a gradient sau má»—i bÆ°á»›c.
2.  **Model Parallelism (MP):**
    *   **Tensor Parallelism (TP):** Chia nhá» cÃ¡c ma tráº­n trá»ng sá»‘ (weight matrices) Ä‘á»ƒ tÃ­nh toÃ¡n song song trÃªn nhiá»u GPU.
    *   **Pipeline Parallelism (PP):** Chia cÃ¡c lá»›p (layers) cá»§a mÃ´ hÃ¬nh cho cÃ¡c GPU khÃ¡c nhau (GPU 1 lÃ m lá»›p 1-10, GPU 2 lÃ m lá»›p 11-20...).
3.  **ZeRO (Zero Redundancy Optimizer):**
    *   Tá»‘i Æ°u hÃ³a Data Parallelism báº±ng cÃ¡ch chia nhá» Optimizer States, Gradients, vÃ  Parameters ra cÃ¡c GPU thay vÃ¬ má»—i GPU pháº£i giá»¯ má»™t báº£n copy Ä‘áº§y Ä‘á»§.

---

## 5. FlashAttention âš¡
**Váº¥n Ä‘á»:** Attention tiÃªu chuáº©n cÃ³ Ä‘á»™ phá»©c táº¡p $O(N^2)$ vÃ  tá»‘n ráº¥t nhiá»u thao tÃ¡c Ä‘á»c/ghi bá»™ nhá»› (Memory IO) giá»¯a HBM (bá»™ nhá»› cháº­m, lá»›n) vÃ  SRAM (bá»™ nhá»› nhanh, nhá»).

**Giáº£i phÃ¡p (FlashAttention - Dao et al., 2022):**
*   **Tiling:** Chia ma tráº­n Attention thÃ nh cÃ¡c khá»‘i nhá» (tiles) Ä‘á»ƒ tÃ­nh toÃ¡n hoÃ n toÃ n trong SRAM tá»‘c Ä‘á»™ cao.
*   **Recomputation:** Cháº¥p nháº­n tÃ­nh toÃ¡n láº¡i má»™t sá»‘ giÃ¡ trá»‹ trong quÃ¡ trÃ¬nh backward pass thay vÃ¬ lÆ°u trá»¯ táº¥t cáº£ (tiáº¿t kiá»‡m VRAM, giáº£m IO).
*   **Káº¿t quáº£:** TÄƒng tá»‘c Ä‘á»™ huáº¥n luyá»‡n 2-4 láº§n, giáº£m bá»™ nhá»› sá»­ dá»¥ng 10-20 láº§n, cho phÃ©p train vá»›i context length dÃ i hÆ¡n.

---

## 6. Quantization & Mixed Precision Training
Giáº£m Ä‘á»™ chÃ­nh xÃ¡c cá»§a sá»‘ há»c Ä‘á»ƒ tiáº¿t kiá»‡m bá»™ nhá»› vÃ  tÄƒng tá»‘c Ä‘á»™.

*   **FP32 (Full Precision - 32 bit):** Chuáº©n cÅ©, ráº¥t tá»‘n kÃ©m.
*   **FP16 / BF16 (Half Precision - 16 bit):** TiÃªu chuáº©n hiá»‡n nay cho training.
    *   **BF16 (Brain Float 16):** Giá»¯ nguyÃªn dáº£i giÃ¡ trá»‹ (range) cá»§a FP32 nhÆ°ng giáº£m Ä‘á»™ chÃ­nh xÃ¡c pháº§n tháº­p phÃ¢n (mantissa). á»”n Ä‘á»‹nh hÆ¡n FP16 cho training LLM.
*   **Mixed Precision Training:**
    *   LÆ°u trá»¯ trá»ng sá»‘ chÃ­nh (Master weights) á»Ÿ FP32.
    *   TÃ­nh toÃ¡n Forward/Backward á»Ÿ FP16/BF16.
    *   Cáº­p nháº­t trá»ng sá»‘ á»Ÿ FP32.

---
*BiÃªn soáº¡n bá»Ÿi Pixiboss - Dá»±a trÃªn Stanford CME 295.*
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [CS229: XÃ¢y Dá»±ng MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n (LLMs) ğŸ§ ](aero_llm_00_overview.md) | [Xem bÃ i viáº¿t â†’](aero_llm_00_overview.md) |
| [Lecture 1: Transformer Architecture ğŸ¤–](aero_llm_01_transformer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_transformer.md) |
| [Lecture 2: Transformer Tricks & BERT ğŸ› ï¸](aero_llm_02_transformer_tricks.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_transformer_tricks.md) |
| [Lecture 3: Large Language Models (LLMs) & Inference ğŸš€](aero_llm_03_large_language_models.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_large_language_models.md) |
| ğŸ“Œ **[Lecture 4: LLM Training - Pre-training ğŸ‹ï¸](aero_llm_04_training_pretraining.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_04_training_pretraining.md) |
| [Lecture 5: LLM Tuning (SFT & Parameter Efficient) ğŸ›ï¸](aero_llm_05_tuning_peft.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_tuning_peft.md) |
| [Lecture 6: LLM Reasoning ğŸ§ ](aero_llm_06_reasoning.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_reasoning.md) |
| [Lecture 7: Agentic LLMs & Tool Use ğŸ› ï¸](aero_llm_07_agentic_llms.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_agentic_llms.md) |
| [Lecture 8: LLM Evaluation âš–ï¸](aero_llm_08_evaluation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_evaluation.md) |
| [Lecture 9: Recap & Current Trends ğŸ”®](aero_llm_09_trends.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_trends.md) |
| [ğŸ› ï¸ Top 12 Repo Quan Trá»ng Cho AI Engineer Tá»‘i Æ¯u LLM](aero_llm_10_essential_tools.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_essential_tools.md) |
| [ChÆ°Æ¡ng 1: Tá»•ng Quan Vá» Large Language Models (LLMs) ğŸ§ ](aero_llm_chapter01_overview_detailed.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter01_overview_detailed.md) |
| [ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t Cá»§a Viá»‡c Huáº¥n Luyá»‡n LLMs ğŸ›ï¸](aero_llm_chapter02_5pillars_part1.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter02_5pillars_part1.md) |
| [ChÆ°Æ¡ng 2: 5 Trá»¥ Cá»™t - Part 2 (Evaluation & Systems)](aero_llm_chapter02_5pillars_part2.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter02_5pillars_part2.md) |
| [ChÆ°Æ¡ng 3: Pre-training â†’ Post-training Pipeline ğŸ”„](aero_llm_chapter03_training_pipeline.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter03_training_pipeline.md) |
| [ChÆ°Æ¡ng 4 & 5: Mechanisms & Evaluation ğŸ”§ğŸ“Š](aero_llm_chapter04_05_mechanisms_eval.md) | [Xem bÃ i viáº¿t â†’](aero_llm_chapter04_05_mechanisms_eval.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

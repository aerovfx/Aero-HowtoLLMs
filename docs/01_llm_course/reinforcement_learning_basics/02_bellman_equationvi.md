
<!-- Aero-Navigation-Start -->
[üè† Home](../../index.md) > [01 llm course](../index.md) > [reinforcement learning basics](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../index.md)
- [üìö Module 01: LLM Course](../../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
**Bellman Equation**
=====================

Bellman Equation l√† m·ªôt c√¥ng th·ª©c to√°n h·ªçc ƒë∆∞·ª£c s·ª≠ d·ª•ng trong h·ªçc t·∫≠p b·ªï tr·ª£ (Reinforcement Learning) ƒë·ªÉ t√≠nh to√°n gi√° tr·ªã t·ªëi ∆∞u c·ªßa ch√≠nh s√°ch (policy) trong m√¥i tr∆∞·ªùng. C√¥ng th·ª©c n√†y ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi Richard Bellman v√†o nƒÉm 1957.

**Bellman Equation**
-------------------

Bellman Equation c√≥ th·ªÉ ƒë∆∞·ª£c vi·∫øt nh∆∞ sau:

V(s) = max‚ÇÉ‚Çë [r + Œ≥ V(s')]

‡§ú‡§π:

* `V(s)` l√† gi√° tr·ªã t·ªëi ∆∞u c·ªßa ch√≠nh s√°ch trong tr·∫°ng th√°i `s`
* `r` l√† th∆∞·ªüng nh·∫≠n ƒë∆∞·ª£c t·∫°i tr·∫°ng th√°i `s`
* `Œ≥` l√† gi√° tr·ªã Discounting, ƒë·∫°i di·ªán cho tr·ªçng l∆∞·ª£ng c·ªßa vi·ªác ch·ªù ƒë·ª£i t∆∞∆°ng lai
* `s'` l√† tr·∫°ng th√°i ti·∫øp theo sau khi th·ª±c hi·ªán h√†nh ƒë·ªông t·∫°i tr·∫°ng th√°i `s`

**Gi·∫£i th√≠ch**
--------------

Bellman Equation ƒë·∫°i di·ªán cho qu√° tr√¨nh t√¨m ki·∫øm gi√° tr·ªã t·ªëi ∆∞u c·ªßa ch√≠nh s√°ch trong m√¥i tr∆∞·ªùng. C√¥ng th·ª©c n√†y cho th·∫•y r·∫±ng gi√° tr·ªã t·ªëi ∆∞u c·ªßa ch√≠nh s√°ch t·∫°i tr·∫°ng th√°i `s` ƒë∆∞·ª£c t√≠nh b·∫±ng c√°ch t√≠nh to√°n t·ªïng th∆∞·ªüng nh·∫≠n ƒë∆∞·ª£c (`r`) c·ªông v·ªõi gi√° tr·ªã t·ªëi ∆∞u c·ªßa ch√≠nh s√°ch t·∫°i tr·∫°ng th√°i ti·∫øp theo (`s'`) sau khi th·ª±c hi·ªán h√†nh ƒë·ªông, v√† nh√¢n v·ªõi tr·ªçng l∆∞·ª£ng c·ªßa vi·ªác ch·ªù ƒë·ª£i t∆∞∆°ng lai (`Œ≥`).

**V√≠ d·ª•**
---------

N·∫øu ch√∫ng ta c√≥ m·ªôt m√¥i tr∆∞·ªùng ƒë∆°n gi·∫£n v·ªõi hai tr·∫°ng th√°i: `s1` v√† `s2`, v√† hai h√†nh ƒë·ªông: `a1` v√† `a2`. Ch√∫ng ta mu·ªën t√¨m ki·∫øm gi√° tr·ªã t·ªëi ∆∞u c·ªßa ch√≠nh s√°ch t·∫°i tr·∫°ng th√°i `s1`.

Bellman Equation s·∫Ω ƒë∆∞·ª£c vi·∫øt nh∆∞ sau:

V(s1) = max [r1 + Œ≥ V(s2), r2 + Œ≥ V(s1)]

Trong tr∆∞·ªùng h·ª£p n√†y, ch√∫ng ta c·∫ßn t√≠nh to√°n gi√° tr·ªã t·ªëi ∆∞u c·ªßa ch√≠nh s√°ch t·∫°i tr·∫°ng th√°i `s1` b·∫±ng c√°ch so s√°nh gi√° tr·ªã c·ªßa hai gi√° tr·ªã kh√°c nhau: gi√° tr·ªã c·ªßa ch√≠nh s√°ch t·∫°i tr·∫°ng th√°i `s2` sau khi th·ª±c hi·ªán h√†nh ƒë·ªông `a1`, v√† gi√° tr·ªã c·ªßa ch√≠nh s√°ch t·∫°i tr·∫°ng th√°i `s1` sau khi th·ª±c hi·ªán h√†nh ƒë·ªông `a2`.

**S·ª≠ d·ª•ng Bellman Equation**
---------------------------

Bellman Equation ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i trong h·ªçc t·∫≠p b·ªï tr·ª£ ƒë·ªÉ t√¨m ki·∫øm gi√° tr·ªã t·ªëi ∆∞u c·ªßa ch√≠nh s√°ch trong m√¥i tr∆∞·ªùng. N√≥ c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ gi·∫£i quy·∫øt c√°c v·∫•n ƒë·ªÅ nh∆∞:

* T√¨m ki·∫øm ch√≠nh s√°ch t·ªëi ∆∞u trong m·ªôt m√¥i tr∆∞·ªùng c·ª• th·ªÉ
* ƒê√°nh gi√° hi·ªáu su·∫•t c·ªßa ch√≠nh s√°ch trong m√¥i tr∆∞·ªùng
* Kh√°m ph√° c√°c ch√≠nh s√°ch m·ªõi b·∫±ng c√°ch t√≠nh to√°n gi√° tr·ªã t·ªëi ∆∞u c·ªßa ch√∫ng

Tuy nhi√™n, Bellman Equation c≈©ng c√≥ m·ªôt s·ªë h·∫°n ch·∫ø, ch·∫≥ng h·∫°n nh∆∞:

* Kh√¥ng gi·∫£i quy·∫øt ƒë∆∞·ª£c c√°c v·∫•n ƒë·ªÅ v·ªõi nhi·ªÅu tr·∫°ng th√°i ho·∫∑c h√†nh ƒë·ªông
* C·∫ßn ph·∫£i s·ª≠ d·ª•ng c√°c k·ªπ thu·∫≠t ƒë·ªÉ x·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p t∆∞∆°ng ƒë·ªìng

T√≥m l·∫°i, Bellman Equation l√† m·ªôt c√¥ng th·ª©c to√°n h·ªçc quan tr·ªçng trong h·ªçc t·∫≠p b·ªï tr·ª£, gi√∫p ch√∫ng ta t√¨m ki·∫øm gi√° tr·ªã t·ªëi ∆∞u c·ªßa ch√≠nh s√°ch trong m√¥i tr∆∞·ªùng.
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [01 whatisreinforcementlearningen](01_whatisreinforcementlearningen.md) | [Xem b√†i vi·∫øt ‚Üí](01_whatisreinforcementlearningen.md) |
| [01 whatisreinforcementlearningvi](01_whatisreinforcementlearningvi.md) | [Xem b√†i vi·∫øt ‚Üí](01_whatisreinforcementlearningvi.md) |
| üìå **[02 bellman equationvi](02_bellman_equationvi.md)** | [Xem b√†i vi·∫øt ‚Üí](02_bellman_equationvi.md) |
| [02 bellmanequationen](02_bellmanequationen.md) | [Xem b√†i vi·∫øt ‚Üí](02_bellmanequationen.md) |
| [03 the plan in plankton sattacken](03_the_plan_in_plankton_sattacken.md) | [Xem b√†i vi·∫øt ‚Üí](03_the_plan_in_plankton_sattacken.md) |
| [03 the plan in plankton sattackvi](03_the_plan_in_plankton_sattackvi.md) | [Xem b√†i vi·∫øt ‚Üí](03_the_plan_in_plankton_sattackvi.md) |
| [04 mdpen](04_mdpen.md) | [Xem b√†i vi·∫øt ‚Üí](04_mdpen.md) |
| [04 mdpvi](04_mdpvi.md) | [Xem b√†i vi·∫øt ‚Üí](04_mdpvi.md) |
| [05 policyvsplanvi](05_policyvsplanvi.md) | [Xem b√†i vi·∫øt ‚Üí](05_policyvsplanvi.md) |
| [üìò Kh√≥a h·ªçc: H·ªçc S√¢u H·ªçc TƒÉng C∆∞·ªùng (Deep Reinforcement Learning)](06_deep_reinforcement_learning_course.md) | [Xem b√†i vi·∫øt ‚Üí](06_deep_reinforcement_learning_course.md) |
| [üìÇ Module: Reinforcement_Learning_Basics](README.md) | [Xem b√†i vi·∫øt ‚Üí](README.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

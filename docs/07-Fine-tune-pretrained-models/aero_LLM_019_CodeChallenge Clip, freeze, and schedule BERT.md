
# üìò K·∫øt H·ª£p Gradient Clipping, Freezing v√† Learning Rate Scheduler Trong Fine-Tuning M√¥ H√¨nh BERT

## T√≥m t·∫Øt (Abstract)

Fine-tuning c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn nh∆∞ BERT cho b√†i to√°n ph√¢n lo·∫°i vƒÉn b·∫£n th∆∞·ªùng g·∫∑p c√°c v·∫•n ƒë·ªÅ v·ªÅ t√≠nh ·ªïn ƒë·ªãnh v√† h·ªôi t·ª•. Ba k·ªπ thu·∫≠t quan tr·ªçng g·ªìm ƒë√≥ng bƒÉng tham s·ªë (freezing), c·∫Øt gradient (gradient clipping) v√† ƒëi·ªÅu ch·ªânh t·ªëc ƒë·ªô h·ªçc (learning rate scheduler) ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t nh·∫±m c·∫£i thi·ªán hi·ªáu su·∫•t hu·∫•n luy·ªán. B√†i vi·∫øt ph√¢n t√≠ch c∆° s·ªü l√Ω thuy·∫øt, m√¥ h√¨nh to√°n h·ªçc v√† k·∫øt qu·∫£ th·ª±c nghi·ªám c·ªßa vi·ªác k·∫øt h·ª£p ba ph∆∞∆°ng ph√°p n√†y trong b√†i to√°n ph√¢n t√≠ch c·∫£m x√∫c ƒë√°nh gi√° phim.

---

## 1. Gi·ªõi thi·ªáu

C√°c m√¥ h√¨nh Transformer ti·ªÅn hu·∫•n luy·ªán nh∆∞ BERT ƒë√£ tr·ªü th√†nh n·ªÅn t·∫£ng trong x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n. Tuy nhi√™n, qu√° tr√¨nh fine-tuning ƒë√≤i h·ªèi:

* Ki·ªÉm so√°t s·ªë l∆∞·ª£ng tham s·ªë h·ªçc
* ·ªîn ƒë·ªãnh gradient
* ƒêi·ªÅu ch·ªânh t·ªëc ƒë·ªô h·ªôi t·ª•

Theo t√†i li·ªáu th·ª±c h√†nh , vi·ªác k·∫øt h·ª£p freezing, clipping v√† scheduling gi√∫p tƒÉng t√≠nh ·ªïn ƒë·ªãnh v√† hi·ªáu qu·∫£ hu·∫•n luy·ªán.

M·ª•c ti√™u nghi√™n c·ª©u:

* Ph√¢n t√≠ch vai tr√≤ t·ª´ng k·ªπ thu·∫≠t
* X√¢y d·ª±ng m√¥ h√¨nh to√°n h·ªçc t·ªïng h·ª£p
* ƒê√°nh gi√° t√°c ƒë·ªông l√™n BERT
* ƒê·ªÅ xu·∫•t h∆∞·ªõng t·ªëi ∆∞u

---

## 2. C∆° s·ªü l√Ω thuy·∫øt

### 2.1 Fine-tuning m√¥ h√¨nh ti·ªÅn hu·∫•n luy·ªán

Cho m√¥ h√¨nh ti·ªÅn hu·∫•n luy·ªán v·ªõi tham s·ªë (\theta_0). Fine-tuning nh·∫±m t√¨m:

[
\theta^*=\arg\min_{\theta}L(\theta;D_{task})
]

Trong ƒë√≥ (D_{task}) l√† t·∫≠p d·ªØ li·ªáu m·ª•c ti√™u.

---

### 2.2 Freezing tham s·ªë

Gi·∫£ s·ª≠ t·∫≠p tham s·ªë ƒë∆∞·ª£c hu·∫•n luy·ªán l√† (T\subset\theta):

[
\theta=\theta_{freeze}\cup\theta_{train},\quad
\theta_{freeze}\cap\theta_{train}=\emptyset
]

V·ªõi:

[
\nabla_{\theta_{freeze}}L=0
]

‚áí c√°c tham s·ªë b·ªã ƒë√≥ng bƒÉng kh√¥ng c·∫≠p nh·∫≠t.

---

### 2.3 Gradient Descent

Quy tr√¨nh c·∫≠p nh·∫≠t:

[
\theta_{t+1}=\theta_t-\eta_t\mathbf{g}_t
]

[
\mathbf{g}*t=\nabla*\theta L(\theta_t)
]

---

## 3. Ph∆∞∆°ng ph√°p nghi√™n c·ª©u

### 3.1 Chi·∫øn l∆∞·ª£c Freezing trong BERT

Theo , m√¥ h√¨nh ƒë∆∞·ª£c c·∫•u h√¨nh:

* ƒê√≥ng bƒÉng: Embedding + Attention
* Hu·∫•n luy·ªán: MLP + Pooler + Classifier

T·ª∑ l·ªá tham s·ªë:

[
R=\frac{|\theta_{train}|}{|\theta_{total}|}\approx 0.5
]

---

### 3.2 Gradient Clipping

#### 3.2.1 Chu·∫©n h√≥a gradient

V·ªõi ng∆∞·ª°ng (c=1):

[
\mathbf{g}'=
\frac{c}{\max(|\mathbf{g}|,c)}\mathbf{g}
]

ƒê·∫£m b·∫£o:

[
|\mathbf{g}'|\le c
]

---

#### 3.2.2 ·∫¢nh h∆∞·ªüng t·ªõi c·∫≠p nh·∫≠t

[
\theta_{t+1}=\theta_t-\eta_t\mathbf{g}'
]

Gi√∫p h·∫°n ch·∫ø gradient explosion.

---

### 3.3 Learning Rate Scheduler

#### 3.3.1 Warm-up

[
\eta_t=\eta_{max}\frac{t}{T_{warm}},\quad t\le T_{warm}
]

---

#### 3.3.2 Linear Decay

[
\eta_t=\eta_{max}\left(1-\frac{t}{T_{sched}}\right)
]

Trong ƒë√≥:

[
T_{sched}>T_{train}
]

ƒë·ªÉ tr√°nh (\eta_t=0).

---

### 3.4 Quy tr√¨nh t·ªïng h·ª£p

Quy tr√¨nh hu·∫•n luy·ªán:

1. Forward
2. Backprop
3. Ghi nh·∫≠n gradient norm
4. Clipping
5. Scheduler
6. Update

Ph∆∞∆°ng tr√¨nh t·ªïng qu√°t:

[
\theta_{t+1}=
\theta_t-
\eta_t
\frac{c}{\max(|\mathbf{g}_t|,c)}\mathbf{g}_t
]

---

## 4. Th·ª±c nghi·ªám

### 4.1 Thi·∫øt l·∫≠p

Theo :

* 300 batch hu·∫•n luy·ªán
* Warm-up 5%
* Linear scheduler (450 steps)
* Clipping: (c=1)

Theo d√µi:

* Loss
* Accuracy
* Gradient norm

---

### 4.2 Ph√¢n t√≠ch h√†m m·∫•t m√°t

Cross-Entropy:

[
L=-\sum_{i=1}^{N}y_i\log(p_i)
]

Quan s√°t:

[
Var(L_{clip+sch})<Var(L_{baseline})
]

‚áí h·ªçc ·ªïn ƒë·ªãnh h∆°n.

---

### 4.3 ƒê·ªô ch√≠nh x√°c

Accuracy:

[
Acc=\frac{TP+TN}{TP+TN+FP+FN}
]

K·∫øt qu·∫£:

| Giai ƒëo·∫°n    | Accuracy |
| ------------ | -------- |
| Tr∆∞·ªõc t·ªëi ∆∞u | ~85%     |
| Sau t·ªëi ∆∞u   | ~90%     |

---

### 4.4 Ph√¢n t√≠ch Gradient Norm

Hai l·ªõp ƒë∆∞·ª£c theo d√µi:

* MLP layer (pre-trained)
* Classifier layer (random)

Chu·∫©n gradient:

[
G_t=|\nabla W_t|
]

Quan s√°t:

[
G_{MLP}<1 \quad (\text{ƒëa s·ªë})
]

[
G_{CLS}>1 \quad (\text{nhi·ªÅu giai ƒëo·∫°n ƒë·∫ßu})
]

‚áí Clipping ·∫£nh h∆∞·ªüng m·∫°nh ƒë·∫øn classifier.

---

### 4.5 Hi·ªán t∆∞·ª£ng m·∫•t th√¥ng tin Gradient

L∆∞·ª£ng th√¥ng tin b·ªã m·∫•t:

[
\Delta g=
|\mathbf{g}|-|\mathbf{g}'|
]

V·ªõi:

[
|\mathbf{g}|>1
]

‚áí (\Delta g>0)

ƒê·∫∑c bi·ªát l·ªõn ·ªü giai ƒëo·∫°n ƒë·∫ßu.

---

## 5. Th·∫£o lu·∫≠n

### 5.1 ƒê√°nh gi√° t√≠nh ph√π h·ª£p c·ªßa Clipping

Theo , clipping s·ªõm c√≥ th·ªÉ:

* Gi·∫£m t·ªëc ƒë·ªô h·ªçc
* L√†m ch·∫≠m classifier

Gi·∫£i ph√°p:

[
c(t)=
\begin{cases}
\infty & t<T_0\
1 & t\ge T_0
\end{cases}
]

(Delayed clipping)

---

### 5.2 T∆∞∆°ng t√°c gi·ªØa c√°c k·ªπ thu·∫≠t

Ba k·ªπ thu·∫≠t ph·ªëi h·ª£p:

| K·ªπ thu·∫≠t  | Vai tr√≤      |
| --------- | ------------ |
| Freezing  | Gi·∫£m tham s·ªë |
| Clipping  | ·ªîn ƒë·ªãnh      |
| Scheduler | H·ªôi t·ª•       |

T√°c ƒë·ªông t·ªïng h·ª£p:

[
Stability\propto f(F,C,S)
]

---

### 5.3 ·ª®ng d·ª•ng cho LLM

K·∫øt qu·∫£ cho th·∫•y:

* C·∫ßn thi·∫øt cho m√¥ h√¨nh >1B tham s·ªë
* Gi·∫£m r·ªßi ro divergence
* TƒÉng kh·∫£ nƒÉng t√°i l·∫≠p

---

## 6. K·∫øt lu·∫≠n

Nghi√™n c·ª©u ƒë√£ ph√¢n t√≠ch vi·ªác k·∫øt h·ª£p freezing, gradient clipping v√† learning rate scheduler trong fine-tuning BERT.

K·∫øt qu·∫£ ch√≠nh:

* Loss ·ªïn ƒë·ªãnh h∆°n
* Accuracy tƒÉng
* Gradient ƒë∆∞·ª£c ki·ªÉm so√°t
* H·ªôi t·ª• nhanh h∆°n

Ph∆∞∆°ng ph√°p ph√π h·ª£p cho hu·∫•n luy·ªán m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn trong ƒëi·ªÅu ki·ªán t√†i nguy√™n h·∫°n ch·∫ø.

---

## T√†i li·ªáu tham kh·∫£o

1. BERT Fine-Tuning Code Challenge 
2. Devlin, J. et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers.
3. Goodfellow, I. et al. (2016). Deep Learning. MIT Press.
4. Loshchilov, I., Hutter, F. (2017). SGDR.

---

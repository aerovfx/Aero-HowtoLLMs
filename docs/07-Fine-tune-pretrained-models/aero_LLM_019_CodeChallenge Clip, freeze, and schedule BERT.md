
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [07 Fine tune pretrained models](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# ğŸ“˜ Káº¿t Há»£p Gradient Clipping, Freezing vÃ  Learning Rate Scheduler Trong Fine-Tuning MÃ´ HÃ¬nh BERT

## TÃ³m táº¯t (Abstract)

Fine-tuning cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n nhÆ° BERT cho bÃ i toÃ¡n phÃ¢n loáº¡i vÄƒn báº£n thÆ°á»ng gáº·p cÃ¡c váº¥n Ä‘á» vá» tÃ­nh á»•n Ä‘á»‹nh vÃ  há»™i tá»¥. Ba ká»¹ thuáº­t quan trá»ng gá»“m Ä‘Ã³ng bÄƒng tham sá»‘ (freezing), cáº¯t gradient (gradient clipping) vÃ  Ä‘iá»u chá»‰nh tá»‘c Ä‘á»™ há»c (learning rate scheduler) Ä‘Æ°á»£c Ä‘á» xuáº¥t nháº±m cáº£i thiá»‡n hiá»‡u suáº¥t huáº¥n luyá»‡n. BÃ i viáº¿t phÃ¢n tÃ­ch cÆ¡ sá»Ÿ lÃ½ thuyáº¿t, mÃ´ hÃ¬nh toÃ¡n há»c vÃ  káº¿t quáº£ thá»±c nghiá»‡m cá»§a viá»‡c káº¿t há»£p ba phÆ°Æ¡ng phÃ¡p nÃ y trong bÃ i toÃ¡n phÃ¢n tÃ­ch cáº£m xÃºc Ä‘Ã¡nh giÃ¡ phim.

---

## 1. Giá»›i thiá»‡u

CÃ¡c mÃ´ hÃ¬nh Transformer tiá»n huáº¥n luyá»‡n nhÆ° BERT Ä‘Ã£ trá»Ÿ thÃ nh ná»n táº£ng trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn. Tuy nhiÃªn, quÃ¡ trÃ¬nh fine-tuning Ä‘Ã²i há»i:

* Kiá»ƒm soÃ¡t sá»‘ lÆ°á»£ng tham sá»‘ há»c
* á»”n Ä‘á»‹nh gradient
* Äiá»u chá»‰nh tá»‘c Ä‘á»™ há»™i tá»¥

Theo tÃ i liá»‡u thá»±c hÃ nh , viá»‡c káº¿t há»£p freezing, clipping vÃ  scheduling giÃºp tÄƒng tÃ­nh á»•n Ä‘á»‹nh vÃ  hiá»‡u quáº£ huáº¥n luyá»‡n.

Má»¥c tiÃªu nghiÃªn cá»©u:

* PhÃ¢n tÃ­ch vai trÃ² tá»«ng ká»¹ thuáº­t
* XÃ¢y dá»±ng mÃ´ hÃ¬nh toÃ¡n há»c tá»•ng há»£p
* ÄÃ¡nh giÃ¡ tÃ¡c Ä‘á»™ng lÃªn BERT
* Äá» xuáº¥t hÆ°á»›ng tá»‘i Æ°u

---

## 2. CÆ¡ sá»Ÿ lÃ½ thuyáº¿t

### 2.1 Fine-tuning mÃ´ hÃ¬nh tiá»n huáº¥n luyá»‡n

Cho mÃ´ hÃ¬nh tiá»n huáº¥n luyá»‡n vá»›i tham sá»‘ (\theta_0). Fine-tuning nháº±m tÃ¬m:

[
\theta^*=\arg\min_{\theta}L(\theta;D_{task})
]

Trong Ä‘Ã³ (D_{task}) lÃ  táº­p dá»¯ liá»‡u má»¥c tiÃªu.

---

### 2.2 Freezing tham sá»‘

Giáº£ sá»­ táº­p tham sá»‘ Ä‘Æ°á»£c huáº¥n luyá»‡n lÃ  (T\subset\theta):

[
\theta=\theta_{freeze}\cup\theta_{train},\quad
\theta_{freeze}\cap\theta_{train}=\emptyset
]

Vá»›i:

[
\nabla_{\theta_{freeze}}L=0
]

â‡’ cÃ¡c tham sá»‘ bá»‹ Ä‘Ã³ng bÄƒng khÃ´ng cáº­p nháº­t.

---

### 2.3 Gradient Descent

Quy trÃ¬nh cáº­p nháº­t:

[
\theta_{t+1}=\theta_t-\eta_t\mathbf{g}_t
]

[
\mathbf{g}*t=\nabla*\theta L(\theta_t)
]

---

## 3. PhÆ°Æ¡ng phÃ¡p nghiÃªn cá»©u

### 3.1 Chiáº¿n lÆ°á»£c Freezing trong BERT

Theo , mÃ´ hÃ¬nh Ä‘Æ°á»£c cáº¥u hÃ¬nh:

* ÄÃ³ng bÄƒng: Embedding + Attention
* Huáº¥n luyá»‡n: MLP + Pooler + Classifier

Tá»· lá»‡ tham sá»‘:

[
R=\frac{|\theta_{train}|}{|\theta_{total}|}\approx 0.5
]

---

### 3.2 Gradient Clipping

#### 3.2.1 Chuáº©n hÃ³a gradient

Vá»›i ngÆ°á»¡ng (c=1):

[
\mathbf{g}'=
\frac{c}{\max(|\mathbf{g}|,c)}\mathbf{g}
]

Äáº£m báº£o:

[
|\mathbf{g}'|\le c
]

---

#### 3.2.2 áº¢nh hÆ°á»Ÿng tá»›i cáº­p nháº­t

[
\theta_{t+1}=\theta_t-\eta_t\mathbf{g}'
]

GiÃºp háº¡n cháº¿ gradient explosion.

---

### 3.3 Learning Rate Scheduler

#### 3.3.1 Warm-up

[
\eta_t=\eta_{max}\frac{t}{T_{warm}},\quad t\le T_{warm}
]

---

#### 3.3.2 Linear Decay

[
\eta_t=\eta_{max}\left(1-\frac{t}{T_{sched}}\right)
]

Trong Ä‘Ã³:

[
T_{sched}>T_{train}
]

Ä‘á»ƒ trÃ¡nh (\eta_t=0).

---

### 3.4 Quy trÃ¬nh tá»•ng há»£p

Quy trÃ¬nh huáº¥n luyá»‡n:

1. Forward
2. Backprop
3. Ghi nháº­n gradient norm
4. Clipping
5. Scheduler
6. Update

PhÆ°Æ¡ng trÃ¬nh tá»•ng quÃ¡t:

[
\theta_{t+1}=
\theta_t-
\eta_t
\frac{c}{\max(|\mathbf{g}_t|,c)}\mathbf{g}_t
]

---

## 4. Thá»±c nghiá»‡m

### 4.1 Thiáº¿t láº­p

Theo :

* 300 batch huáº¥n luyá»‡n
* Warm-up 5%
* Linear scheduler (450 steps)
* Clipping: (c=1)

Theo dÃµi:

* Loss
* Accuracy
* Gradient norm

---

### 4.2 PhÃ¢n tÃ­ch hÃ m máº¥t mÃ¡t

Cross-Entropy:

[
L=-\sum_{i=1}^{N}y_i\log(p_i)
]

Quan sÃ¡t:

[
Var(L_{clip+sch})<Var(L_{baseline})
]

â‡’ há»c á»•n Ä‘á»‹nh hÆ¡n.

---

### 4.3 Äá»™ chÃ­nh xÃ¡c

Accuracy:

[
Acc=\frac{TP+TN}{TP+TN+FP+FN}
]

Káº¿t quáº£:

| Giai Ä‘oáº¡n    | Accuracy |
| ------------ | -------- |
| TrÆ°á»›c tá»‘i Æ°u | ~85%     |
| Sau tá»‘i Æ°u   | ~90%     |

---

### 4.4 PhÃ¢n tÃ­ch Gradient Norm

Hai lá»›p Ä‘Æ°á»£c theo dÃµi:

* MLP layer (pre-trained)
* Classifier layer (random)

Chuáº©n gradient:

[
G_t=|\nabla W_t|
]

Quan sÃ¡t:

[
G_{MLP}<1 \quad (\text{Ä‘a sá»‘})
]

[
G_{CLS}>1 \quad (\text{nhiá»u giai Ä‘oáº¡n Ä‘áº§u})
]

â‡’ Clipping áº£nh hÆ°á»Ÿng máº¡nh Ä‘áº¿n classifier.

---

### 4.5 Hiá»‡n tÆ°á»£ng máº¥t thÃ´ng tin Gradient

LÆ°á»£ng thÃ´ng tin bá»‹ máº¥t:

[
\Delta g=
|\mathbf{g}|-|\mathbf{g}'|
]

Vá»›i:

[
|\mathbf{g}|>1
]

â‡’ (\Delta g>0)

Äáº·c biá»‡t lá»›n á»Ÿ giai Ä‘oáº¡n Ä‘áº§u.

---

## 5. Tháº£o luáº­n

### 5.1 ÄÃ¡nh giÃ¡ tÃ­nh phÃ¹ há»£p cá»§a Clipping

Theo , clipping sá»›m cÃ³ thá»ƒ:

* Giáº£m tá»‘c Ä‘á»™ há»c
* LÃ m cháº­m classifier

Giáº£i phÃ¡p:

[
c(t)=
\begin{cases}
\infty & t<T_0\
1 & t\ge T_0
\end{cases}
]

(Delayed clipping)

---

### 5.2 TÆ°Æ¡ng tÃ¡c giá»¯a cÃ¡c ká»¹ thuáº­t

Ba ká»¹ thuáº­t phá»‘i há»£p:

| Ká»¹ thuáº­t  | Vai trÃ²      |
| --------- | ------------ |
| Freezing  | Giáº£m tham sá»‘ |
| Clipping  | á»”n Ä‘á»‹nh      |
| Scheduler | Há»™i tá»¥       |

TÃ¡c Ä‘á»™ng tá»•ng há»£p:

[
Stability\propto f(F,C,S)
]

---

### 5.3 á»¨ng dá»¥ng cho LLM

Káº¿t quáº£ cho tháº¥y:

* Cáº§n thiáº¿t cho mÃ´ hÃ¬nh >1B tham sá»‘
* Giáº£m rá»§i ro divergence
* TÄƒng kháº£ nÄƒng tÃ¡i láº­p

---

## 6. Káº¿t luáº­n

NghiÃªn cá»©u Ä‘Ã£ phÃ¢n tÃ­ch viá»‡c káº¿t há»£p freezing, gradient clipping vÃ  learning rate scheduler trong fine-tuning BERT.

Káº¿t quáº£ chÃ­nh:

* Loss á»•n Ä‘á»‹nh hÆ¡n
* Accuracy tÄƒng
* Gradient Ä‘Æ°á»£c kiá»ƒm soÃ¡t
* Há»™i tá»¥ nhanh hÆ¡n

PhÆ°Æ¡ng phÃ¡p phÃ¹ há»£p cho huáº¥n luyá»‡n mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n trong Ä‘iá»u kiá»‡n tÃ i nguyÃªn háº¡n cháº¿.

---

## TÃ i liá»‡u tham kháº£o

1. BERT Fine-Tuning Code Challenge 
2. Devlin, J. et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers.
3. Goodfellow, I. et al. (2016). Deep Learning. MIT Press.
4. Loshchilov, I., Hutter, F. (2017). SGDR.

---
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

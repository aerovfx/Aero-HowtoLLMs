
# üìò Ph√¢n T√≠ch Learning Rate Scheduler Trong Hu·∫•n Luy·ªán M√¥ H√¨nh H·ªçc S√¢u Quy M√¥ L·ªõn

## T√≥m t·∫Øt (Abstract)

Trong hu·∫•n luy·ªán m√¥ h√¨nh h·ªçc s√¢u hi·ªán ƒë·∫°i, ƒë·∫∑c bi·ªát l√† c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn, vi·ªác ki·ªÉm so√°t t·ªëc ƒë·ªô h·ªçc v√† ƒë·ªô ·ªïn ƒë·ªãnh s·ªë h·ªçc ƒë√≥ng vai tr√≤ then ch·ªët. Learning Rate Scheduler l√† m·ªôt k·ªπ thu·∫≠t gi√∫p ƒëi·ªÅu ch·ªânh learning rate theo th·ªùi gian nh·∫±m c·∫£i thi·ªán kh·∫£ nƒÉng h·ªôi t·ª• v√† h·∫°n ch·∫ø dao ƒë·ªông. B√†i vi·∫øt tr√¨nh b√†y c∆° s·ªü l√Ω thuy·∫øt, m√¥ h√¨nh to√°n h·ªçc v√† k·∫øt qu·∫£ th·ª±c nghi·ªám v·ªÅ c√°c b·ªô ƒëi·ªÅu ch·ªânh learning rate ph·ªï bi·∫øn nh∆∞ Cosine Scheduler v√† Linear Scheduler d·ª±a tr√™n t√†i li·ªáu th·ª±c h√†nh.

---

## 1. Gi·ªõi thi·ªáu

T·ªëi ∆∞u h√≥a trong h·ªçc s√¢u ch·ªß y·∫øu d·ª±a tr√™n c√°c thu·∫≠t to√°n gradient-based. Tuy nhi√™n, vi·ªác s·ª≠ d·ª•ng learning rate c·ªë ƒë·ªãnh th∆∞·ªùng g√¢y ra c√°c v·∫•n ƒë·ªÅ nh∆∞:

* H·ªôi t·ª• ch·∫≠m
* Dao ƒë·ªông m·∫°nh
* D·ªÖ m·∫Øc k·∫πt t·∫°i ƒëi·ªÉm t·ªëi ∆∞u c·ª•c b·ªô

Theo t√†i li·ªáu th·ª±c nghi·ªám , Learning Rate Scheduler gi√∫p kh·∫Øc ph·ª•c c√°c h·∫°n ch·∫ø tr√™n th√¥ng qua ƒëi·ªÅu ch·ªânh learning rate ƒë·ªông.

M·ª•c ti√™u nghi√™n c·ª©u:

* Ph√¢n t√≠ch c∆° ch·∫ø ho·∫°t ƒë·ªông c·ªßa scheduler
* X√¢y d·ª±ng m√¥ h√¨nh to√°n h·ªçc
* ƒê√°nh gi√° t√°c ƒë·ªông ƒë·∫øn qu√° tr√¨nh h·ªçc
* So s√°nh c√°c ph∆∞∆°ng ph√°p ƒëi·ªÅu ch·ªânh

---

## 2. C∆° s·ªü l√Ω thuy·∫øt

### 2.1 C·∫≠p nh·∫≠t tham s·ªë trong h·ªçc s√¢u

Quy tr√¨nh c·∫≠p nh·∫≠t tham s·ªë:

[
\theta_{t+1}=\theta_t-\eta_t \nabla_\theta L(\theta_t)
]

Trong ƒë√≥:

* (\eta_t): learning rate t·∫°i th·ªùi ƒëi·ªÉm (t)
* (\nabla_\theta L): gradient h√†m m·∫•t m√°t

Learning rate bi·∫øn thi√™n theo th·ªùi gian gi√∫p ƒëi·ªÅu ch·ªânh ƒë·ªô l·ªõn b∆∞·ªõc h·ªçc.

---

### 2.2 Vai tr√≤ c·ªßa Learning Rate

Learning rate ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp t·ªõi:

* T·ªëc ƒë·ªô h·ªôi t·ª•
* ƒê·ªô ·ªïn ƒë·ªãnh
* Kh·∫£ nƒÉng t·ªëi ∆∞u to√†n c·ª•c

Khi:

[
\eta_t \to 0 \Rightarrow \theta_{t+1}\approx \theta_t
]

‚áí qu√° tr√¨nh h·ªçc g·∫ßn nh∆∞ d·ª´ng l·∫°i.

---

## 3. Ph∆∞∆°ng ph√°p nghi√™n c·ª©u

### 3.1 Warm-up Phase

#### 3.1.1 Kh√°i ni·ªám

Warm-up gi√∫p tr√°nh c·∫≠p nh·∫≠t qu√° m·∫°nh ·ªü giai ƒëo·∫°n ƒë·∫ßu hu·∫•n luy·ªán.

Theo , learning rate tƒÉng d·∫ßn trong giai ƒëo·∫°n ƒë·∫ßu.

---

#### 3.1.2 M√¥ h√¨nh to√°n h·ªçc

Warm-up tuy·∫øn t√≠nh:

[
\eta_t=\eta_{max}\cdot\frac{t}{T_{warm}},\quad t\le T_{warm}
]

Trong ƒë√≥:

* (T_{warm}): s·ªë b∆∞·ªõc warm-up

---

### 3.2 Cosine Learning Rate Scheduler

#### 3.2.1 Nguy√™n l√Ω

Cosine scheduler l√†m gi·∫£m learning rate theo h√†m cosin.

---

#### 3.2.2 C√¥ng th·ª©c

V·ªõi (C) chu k·ª≥:

[
\eta_t=\eta_{min}+\frac{1}{2}(\eta_{max}-\eta_{min})
\left(1+\cos\frac{2\pi Ct}{T}\right)
]

Tr∆∞·ªùng h·ª£p (C=\frac{1}{2}):

[
\eta_t=\eta_{min}+\frac{1}{2}(\eta_{max}-\eta_{min})
\left(1+\cos\frac{\pi t}{T}\right)
]

---

#### 3.2.3 ƒê·∫∑c ƒëi·ªÉm

* Gi·∫£m learning rate m∆∞·ª£t
* Tr√°nh gi·∫£m ƒë·ªôt ng·ªôt
* Ph√π h·ª£p Transformer, LLM

---

### 3.3 Linear Learning Rate Scheduler

#### 3.3.1 Nguy√™n l√Ω

Gi·∫£m learning rate tuy·∫øn t√≠nh sau warm-up.

---

#### 3.3.2 C√¥ng th·ª©c

[
\eta_t=
\begin{cases}
\eta_{max}\frac{t}{T_{warm}} & t\le T_{warm}\
\eta_{max}\left(1-\frac{t-T_{warm}}{T-T_{warm}}\right) & t>T_{warm}
\end{cases}
]

---

#### 3.3.3 ƒêi·ªÅu ch·ªânh s·ªë b∆∞·ªõc hu·∫•n luy·ªán

Theo , vi·ªác khai b√°o s·ªë b∆∞·ªõc kh√°c v·ªõi th·ª±c t·∫ø gi√∫p:

[
T_{sched}>T_{train}
\Rightarrow \eta_t>0
]

trong su·ªët qu√° tr√¨nh hu·∫•n luy·ªán.

---

### 3.4 K·∫øt h·ª£p v·ªõi Gradient Clipping

C·∫≠p nh·∫≠t tham s·ªë t·ªïng qu√°t:

[
\theta_{t+1}=\theta_t-\eta_t\cdot
\frac{c}{\max(|\mathbf{g}|,c)}\mathbf{g}
]

Trong ƒë√≥:

* (c): ng∆∞·ª°ng clipping

---

## 4. Th·ª±c nghi·ªám

### 4.1 M√¥ h√¨nh minh h·ªça

Theo t√†i li·ªáu , m√¥ h√¨nh g·ªìm:

* Vector tr·ªçng s·ªë (w=(w_1,w_2))
* M·ª•c ti√™u: (w_1>w_2)
* SGD + Scheduler

H√†m m·∫•t m√°t:

[
L=-\log\frac{e^{w_1}}{e^{w_1}+e^{w_2}}
]

---

### 4.2 Cosine Scheduler

Quan s√°t th·ª±c nghi·ªám:

* H·ªçc theo t·ª´ng pha
* Xu·∫•t hi·ªán giai ƒëo·∫°n "ƒë√≥ng bƒÉng"
* H·ªçc m·∫°nh khi (\eta_t) l·ªõn

ƒê·ªì th·ªã:

[
w(t)\propto \int_0^t \eta_s ds
]

---

### 4.3 Linear Scheduler

ƒê·∫∑c ƒëi·ªÉm:

* H·ªçc ƒë·ªÅu
* √çt dao ƒë·ªông
* D·ªÖ ki·ªÉm so√°t

Tr∆∞·ªùng h·ª£p (\eta_t=0):

[
\theta_{t+1}=\theta_t
]

‚áí kh√¥ng h·ªçc.

---

### 4.4 So s√°nh th·ª±c nghi·ªám

| Ph∆∞∆°ng ph√°p      | ƒê·ªô m∆∞·ª£t    | H·ªôi t·ª•  | ·ªîn ƒë·ªãnh |
| ---------------- | ---------- | ------- | ------- |
| Kh√¥ng scheduler  | Th·∫•p       | K√©m     | Th·∫•p    |
| Cosine           | Cao        | T·ªët     | T·ªët     |
| Linear           | Trung b√¨nh | T·ªët     | Cao     |
| Warm-up + Cosine | R·∫•t cao    | R·∫•t t·ªët | R·∫•t t·ªët |

---

## 5. Th·∫£o lu·∫≠n

### 5.1 Ki·ªÉm so√°t ph·∫°m vi gi√° tr·ªã

Theo , h·ªá th·ªëng h·ªçc s√¢u c·∫ßn gi·ªØ gi√° tr·ªã trong mi·ªÅn ·ªïn ƒë·ªãnh:

[
|\theta_i|<M,\quad |g_i|<K
]

C√°c k·ªπ thu·∫≠t h·ªó tr·ª£:

* Weight initialization
* LayerNorm
* Weight decay
* Clipping
* Scheduler

---

### 5.2 ·ª®ng d·ª•ng trong LLM

Scheduler gi√∫p:

* ·ªîn ƒë·ªãnh hu·∫•n luy·ªán Transformer
* Gi·∫£m gradient noise
* H·∫°n ch·∫ø overfitting

ƒê·∫∑c bi·ªát quan tr·ªçng v·ªõi m√¥ h√¨nh tr√™n 1B tham s·ªë.

---

### 5.3 H·∫°n ch·∫ø

* Ph·ª• thu·ªôc si√™u tham s·ªë
* Kh√≥ t·ªëi ∆∞u th·ªß c√¥ng
* TƒÉng ƒë·ªô ph·ª©c t·∫°p hu·∫•n luy·ªán

C·∫ßn th·ª≠ nghi·ªám nhi·ªÅu c·∫•u h√¨nh.

---

## 6. K·∫øt lu·∫≠n

B√†i vi·∫øt ƒë√£ tr√¨nh b√†y Learning Rate Scheduler trong hu·∫•n luy·ªán m√¥ h√¨nh h·ªçc s√¢u, t·∫≠p trung v√†o Cosine v√† Linear Scheduler.

K·∫øt qu·∫£ cho th·∫•y:

* Scheduler c·∫£i thi·ªán h·ªôi t·ª•
* Warm-up tƒÉng ·ªïn ƒë·ªãnh
* K·∫øt h·ª£p clipping cho hi·ªáu qu·∫£ cao

C√°c ph∆∞∆°ng ph√°p n√†y l√† th√†nh ph·∫ßn kh√¥ng th·ªÉ thi·∫øu trong hu·∫•n luy·ªán m√¥ h√¨nh AI hi·ªán ƒë·∫°i.

---

## T√†i li·ªáu tham kh·∫£o

1. Learning Rate Scheduler Tutorial (Part 2) 
2. Loshchilov, I., Hutter, F. (2017). SGDR: Stochastic Gradient Descent with Warm Restarts.
3. Kingma, D., Ba, J. (2015). Adam: A Method for Stochastic Optimization.
4. Vaswani, A. et al. (2017). Attention Is All You Need.

---

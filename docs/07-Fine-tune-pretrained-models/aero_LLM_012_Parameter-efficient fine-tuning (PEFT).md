
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [07 Fine tune pretrained models](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Fine-tuning Hiá»‡u Quáº£ Tham Sá»‘ (Parameter-Efficient Fine-Tuning â€“ PEFT) Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n

## TÃ³m táº¯t

BÃ i viáº¿t nÃ y trÃ¬nh bÃ y tá»•ng quan vá» phÆ°Æ¡ng phÃ¡p **Parameter-Efficient Fine-Tuning (PEFT)** â€“ má»™t nhÃ³m ká»¹ thuáº­t fine-tuning giÃºp giáº£m sá»‘ lÆ°á»£ng tham sá»‘ cáº§n huáº¥n luyá»‡n trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs). Dá»±a trÃªn tÃ i liá»‡u bÃ i giáº£ng , nghiÃªn cá»©u phÃ¢n tÃ­ch cÃ¡c phÆ°Æ¡ng phÃ¡p tiÃªu biá»ƒu nhÆ° Adapter, Low-Rank Adaptation, Prefix Tuning vÃ  Bias Tuning. CÃ¡c cÃ´ng thá»©c toÃ¡n há»c Ä‘Æ°á»£c bá»• sung nháº±m lÃ m rÃµ cÆ¡ sá»Ÿ lÃ½ thuyáº¿t. Káº¿t quáº£ cho tháº¥y PEFT lÃ  giáº£i phÃ¡p phÃ¹ há»£p cho mÃ´i trÆ°á»ng háº¡n cháº¿ tÃ i nguyÃªn tÃ­nh toÃ¡n.

---

## 1. Giá»›i thiá»‡u

CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n hiá»‡n Ä‘áº¡i thÆ°á»ng chá»©a hÃ ng chá»¥c Ä‘áº¿n hÃ ng trÄƒm tá»· tham sá»‘. Viá»‡c fine-tuning toÃ n bá»™ mÃ´ hÃ¬nh Ä‘Ã²i há»i:

* TÃ i nguyÃªn GPU lá»›n,
* Thá»i gian huáº¥n luyá»‡n dÃ i,
* Chi phÃ­ triá»ƒn khai cao.

Theo tÃ i liá»‡u , PEFT Ä‘Æ°á»£c Ä‘á» xuáº¥t nháº±m giáº£i quyáº¿t bÃ i toÃ¡n nÃ y báº±ng cÃ¡ch:

> ÄÃ³ng bÄƒng pháº§n lá»›n tham sá»‘ vÃ  chá»‰ huáº¥n luyá»‡n má»™t táº­p con nhá».

Má»¥c tiÃªu cá»§a PEFT lÃ :

* Giáº£m chi phÃ­ huáº¥n luyá»‡n,
* Duy trÃ¬ hiá»‡u quáº£ há»c,
* PhÃ¹ há»£p vá»›i bÃ i toÃ¡n chuyÃªn biá»‡t.

---

## 2. CÆ¡ sá»Ÿ lÃ½ thuyáº¿t

### 2.1. MÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»± há»“i quy

Cho chuá»—i token:

[
X=(x_1,x_2,\dots,x_n)
]

XÃ¡c suáº¥t sinh:

[
P(X)=\prod_{i=1}^{n}P(x_i|x_1,\dots,x_{i-1};\theta)
]

Trong Ä‘Ã³ (\theta) lÃ  táº­p tham sá»‘ cá»§a mÃ´ hÃ¬nh.

---

### 2.2. Fine-tuning truyá»n thá»‘ng

Vá»›i hÃ m máº¥t mÃ¡t cross-entropy:

[
\mathcal{L}(\theta)
===================

-\frac{1}{N}\sum_{i=1}^{N}
\log P(y_i|x_i;\theta)
]

Cáº­p nháº­t báº±ng gradient descent:

[
\theta_{t+1}
============

\theta_t-\eta\nabla_\theta\mathcal{L}
]

ToÃ n bá»™ tham sá»‘ Ä‘á»u Ä‘Æ°á»£c cáº­p nháº­t.

---

### 2.3. Fine-tuning hiá»‡u quáº£ tham sá»‘

Trong PEFT, tham sá»‘ Ä‘Æ°á»£c chia:

[
\theta = (\theta_f, \theta_t)
]

vá»›i:

* (\theta_f): tham sá»‘ Ä‘Ã³ng bÄƒng,
* (\theta_t): tham sá»‘ huáº¥n luyá»‡n.

Äiá»u kiá»‡n:

[
\nabla_{\theta_f}\mathcal{L}=0
]

Chá»‰ (\theta_t) Ä‘Æ°á»£c cáº­p nháº­t.

---

## 3. Tá»•ng quan vá» PEFT

Theo tÃ i liá»‡u , PEFT lÃ  má»™t **há» phÆ°Æ¡ng phÃ¡p**, khÃ´ng pháº£i má»™t ká»¹ thuáº­t Ä‘Æ¡n láº». CÃ¡c phÆ°Æ¡ng phÃ¡p chÃ­nh gá»“m:

1. Adapter
2. Low-Rank Adaptation (LoRA/DoRA)
3. Prefix Tuning
4. Bias Tuning

CÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y thÆ°á»ng Ä‘Æ°á»£c triá»ƒn khai thÃ´ng qua thÆ° viá»‡n cá»§a Hugging Face.

---

## 4. CÃ¡c phÆ°Æ¡ng phÃ¡p PEFT tiÃªu biá»ƒu

### 4.1. Adapter

#### 4.1.1. NguyÃªn lÃ½

Adapter chÃ¨n cÃ¡c mÃ´-Ä‘un nhá» vÃ o giá»¯a cÃ¡c lá»›p Transformer:

[
h' = h + W_{up}\sigma(W_{down}h)
]

Trong Ä‘Ã³:

* (W_{down}\in\mathbb{R}^{d\times r}),
* (W_{up}\in\mathbb{R}^{r\times d}),
* (r \ll d).

Cáº¥u trÃºc giá»‘ng autoencoder nÃ©nâ€“giáº£i nÃ©n.

---

#### 4.1.2. Sá»‘ tham sá»‘

Sá»‘ tham sá»‘ adapter:

[
P_{adapter}=2dr
]

So vá»›i:

[
P_{full}=d^2
]

â‡’ (P_{adapter}\ll P_{full})

---

### 4.2. Low-Rank Adaptation (LoRA)

#### 4.2.1. PhÃ¢n rÃ£ ma tráº­n

Cho trá»ng sá»‘ gá»‘c:

[
W\in\mathbb{R}^{m\times n}
]

LoRA biá»ƒu diá»…n:

[
W' = W + BA
]

vá»›i:

[
B\in\mathbb{R}^{m\times r},\quad
A\in\mathbb{R}^{r\times n}
]

vÃ  (r\ll \min(m,n)).

---

#### 4.2.2. Giáº£m tham sá»‘

Sá»‘ tham sá»‘:

[
P_{LoRA}=r(m+n)
]

So vá»›i:

[
P_{full}=mn
]

VÃ­ dá»¥:

* (m=n=1000),
* (r=100):

[
P_{full}=10^6,\quad
P_{LoRA}=2\times10^5
]

---

### 4.3. Prefix Tuning

#### 4.3.1. CÆ¡ cháº¿

ThÃªm vector tiá»n tá»‘ (P):

[
X' = [P; X]
]

vá»›i:

[
P\in\mathbb{R}^{k\times d}
]

Äáº§u vÃ o attention:

[
Q,K,V = (X'W_Q,X'W_K,X'W_V)
]

Chá»‰ (P) Ä‘Æ°á»£c huáº¥n luyá»‡n.

---

#### 4.3.2. Sá»‘ tham sá»‘

[
P_{prefix}=kd
]

Ráº¥t nhá» so vá»›i toÃ n mÃ´ hÃ¬nh.

---

### 4.4. Bias Tuning

#### 4.4.1. NguyÃªn lÃ½

Chá»‰ huáº¥n luyá»‡n bias:

[
y = Wx + b
]

Cáº­p nháº­t:

[
b_{t+1}=b_t-\eta\nabla_b\mathcal{L}
]

Giá»¯ nguyÃªn (W).

---

#### 4.4.2. Äáº·c Ä‘iá»ƒm

Bias chá»§ yáº¿u dá»‹ch chuyá»ƒn phÃ¢n phá»‘i:

[
P'(y|x)=P(y-b|x)
]

áº¢nh hÆ°á»Ÿng yáº¿u Ä‘áº¿n cáº¥u trÃºc biá»ƒu diá»…n.

---

## 5. PhÃ¢n tÃ­ch hiá»‡u quáº£ PEFT

### 5.1. Chi phÃ­ tÃ­nh toÃ¡n

Gá»i:

* (P_{full}): tham sá»‘ Ä‘áº§y Ä‘á»§,
* (P_{peft}): tham sá»‘ PEFT.

Tá»· lá»‡:

[
r=\frac{P_{peft}}{P_{full}}\ll 1
]

Thá»i gian huáº¥n luyá»‡n:

[
T_{peft}\approx rT_{full}
]

---

### 5.2. Kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a

Khi sá»‘ tham sá»‘ giáº£m:

[
P\downarrow \Rightarrow Var(\theta)\downarrow
]

â‡’ giáº£m overfitting.

Tuy nhiÃªn:

[
Bias(\theta)\uparrow
]

â‡’ mÃ´ hÃ¬nh kÃ©m linh hoáº¡t.

---

### 5.3. ÄÃ¡nh Ä‘á»•i hiá»‡u nÄƒng

Giáº£ sá»­:

[
Acc_{full},\quad Acc_{peft}
]

ThÃ´ng thÆ°á»ng:

[
Acc_{peft}\le Acc_{full}
]

nhÆ°ng:

[
\frac{Acc_{peft}}{Cost_{peft}}

>

\frac{Acc_{full}}{Cost_{full}}
]

â‡’ PEFT hiá»‡u quáº£ vá» chi phÃ­.

---

## 6. Tháº£o luáº­n

### 6.1. Æ¯u Ä‘iá»ƒm

1. Giáº£m tÃ i nguyÃªn GPU.
2. Huáº¥n luyá»‡n nhanh.
3. LÆ°u trá»¯ gá»n nháº¹.
4. Dá»… triá»ƒn khai.

---

### 6.2. Háº¡n cháº¿

Theo tÃ i liá»‡u :

* Hiá»‡u nÄƒng tháº¥p hÆ¡n fine-tuning Ä‘áº§y Ä‘á»§,
* KhÃ³ tá»•ng quÃ¡t hÃ³a Ä‘a nhiá»‡m,
* Phá»¥ thuá»™c bÃ i toÃ¡n.

---

### 6.3. Khi nÃ o nÃªn dÃ¹ng PEFT?

PEFT phÃ¹ há»£p khi:

[
N_{data}\ll P_{model}
]

vÃ :

* BÃ i toÃ¡n phÃ¢n loáº¡i,
* GÃ¡n nhÃ£n,
* Chatbot chuyÃªn ngÃ nh,
* TÃ¡c vá»¥ háº¹p.

KhÃ´ng phÃ¹ há»£p cho tÃ¡c vá»¥ má»Ÿ, sÃ¡ng táº¡o.

---

## 7. á»¨ng dá»¥ng thá»±c tiá»…n

PEFT Ä‘Æ°á»£c á»©ng dá»¥ng trong:

* NLP doanh nghiá»‡p,
* Chatbot ná»™i bá»™,
* PhÃ¢n tÃ­ch vÄƒn báº£n y táº¿,
* PhÃ¡p lÃ½,
* TÃ i chÃ­nh.

Káº¿t há»£p vá»›i thÆ° viá»‡n cá»§a Hugging Face giÃºp triá»ƒn khai nhanh trong mÃ´i trÆ°á»ng sáº£n xuáº¥t.

---

## 8. Káº¿t luáº­n

BÃ i viáº¿t Ä‘Ã£ trÃ¬nh bÃ y tá»•ng quan vá» Parameter-Efficient Fine-Tuning dá»±a trÃªn tÃ i liá»‡u bÃ i giáº£ng vÃ  phÃ¢n tÃ­ch toÃ¡n há»c. CÃ¡c káº¿t luáº­n chÃ­nh:

1. PEFT giÃºp giáº£m máº¡nh chi phÃ­ huáº¥n luyá»‡n.
2. Hiá»‡u quáº£ tá»‘t cho tÃ¡c vá»¥ chuyÃªn biá»‡t.
3. KhÃ´ng thay tháº¿ hoÃ n toÃ n fine-tuning Ä‘áº§y Ä‘á»§.
4. LÃ  giáº£i phÃ¡p thá»±c tiá»…n cho há»‡ thá»‘ng háº¡n cháº¿ tÃ i nguyÃªn.

Trong tÆ°Æ¡ng lai, viá»‡c káº¿t há»£p PEFT vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p thÃ­ch nghi Ä‘á»™ng vÃ  há»c Ä‘a nhiá»‡m lÃ  hÆ°á»›ng nghiÃªn cá»©u tiá»m nÄƒng.

---

## TÃ i liá»‡u tham kháº£o

1. Parameter-Efficient Fine-Tuning (PEFT) â€“ Lecture Notes 
2. Vaswani et al. (2017). Attention Is All You Need.
3. Hu et al. (2022). LoRA: Low-Rank Adaptation of LLMs.
4. He et al. (2022). Towards Parameter-Efficient Transfer Learning.
5. Goodfellow et al. (2016). *Deep Learning*.

---
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

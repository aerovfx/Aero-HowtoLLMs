
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../../../index.md) > [07 Fine tune pretrained models](../../../index.md) > [Fine Tuning](../../index.md) > [05   4. PEFT Fine Tuning with LoRA](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../../../index.md)
- [ğŸ“š Module 01: LLM Course](../../../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# LoRA Adapters

## Giá»›i Thiá»‡u

HÃ£y khÃ¡m phÃ¡ LoRA adapters, má»™t táº­p há»£p con máº¡nh máº½ cá»§a parameter-efficient fine-tuning, nÆ¡i chÃºng ta sáº½ báº¯t Ä‘áº§u vá»›i tá»•ng quan cáº¥p cao sá»­ dá»¥ng phÃ©p so sÃ¡nh náº¥u Äƒn cá»§a chÃºng ta, sau Ä‘Ã³ Ä‘i sÃ¢u hÆ¡n vÃ o chi tiáº¿t ká»¹ thuáº­t.

HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n cÃ³ má»™t cÃ´ng thá»©c tuyá»‡t vá»i. Báº¡n muá»‘n cáº£i thiá»‡n mÃ³n Äƒn mÃ  khÃ´ng cáº§n thay Ä‘á»•i toÃ n bá»™ quy trÃ¬nh náº¥u nÆ°á»›ng. Báº¡n mang Ä‘áº¿n má»™t cÃ´ng cá»¥ chuyÃªn biá»‡t nhÆ° má»™t microplane Ä‘á»ƒ bÃ o vá» chanh. CÃ´ng cá»¥ nÃ y táº¡o ra tÃ¡c Ä‘á»™ng lá»›n vá»›i ná»— lá»±c tá»‘i thiá»ƒu.

Trong tháº¿ giá»›i machine learning, LoRA adapters Ä‘Ã³ng vai trÃ² tÆ°Æ¡ng tá»±.

## LoRA LÃ  GÃ¬?

LoRA viáº¿t táº¯t cá»§a Low-Rank Adaptation. CÃ¡c adapters nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ fine-tune cÃ¡c mÃ´ hÃ¬nh pre-trained má»™t cÃ¡ch hiá»‡u quáº£ báº±ng cÃ¡ch táº­p trung vÃ o má»™t táº­p há»£p nhá» cÃ¡c tham sá»‘. ChÃºng Ä‘áº·c biá»‡t hiá»‡u quáº£ khi báº¡n cáº§n thÃ­ch nghi má»™t mÃ´ hÃ¬nh vá»›i cÃ¡c tÃ¡c vá»¥ má»›i vá»›i dá»¯ liá»‡u háº¡n cháº¿.

## CÆ¡ Sá»Ÿ Ká»¹ Thuáº­t

### Ma Tráº­n Trá»ng Sá»‘

Trong má»™t lá»›p neural network Ä‘iá»ƒn hÃ¬nh, trá»ng sá»‘ Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi má»™t ma tráº­n lá»›n. Trong fine-tuning truyá»n thá»‘ng, ma tráº­n nÃ y Ä‘Æ°á»£c Ä‘iá»u chá»‰nh Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t mÃ´ hÃ¬nh. Tuy nhiÃªn, quÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ tá»‘n kÃ©m vá» tÃ­nh toÃ¡n vÃ  Ä‘Ã²i há»i nhiá»u dá»¯ liá»‡u.

### Giáº£i PhÃ¡p LoRA

Vá»›i kÃ­ch thÆ°á»›c ma tráº­n n = 512 vÃ  rank r = 1:
- Sá»‘ tham sá»‘ cáº§n fine-tune trong LoRA: 512 Ã— 1 Ã— 2 = 1,024 tham sá»‘
- Sá»‘ tham sá»‘ trong ma tráº­n gá»‘c: 512Â² = 262,144 tham sá»‘
- **Giáº£m khoáº£ng 256 láº§n!**

## Lá»£i Ãch Cá»§a LoRA

So vá»›i GPT-3 175B fine-tuned vá»›i Adam, LoRA cÃ³ thá»ƒ:
- Giáº£m sá»‘ lÆ°á»£ng tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n xuá»‘ng **10,000 láº§n**
- Giáº£m yÃªu cáº§u bá»™ nhá»› GPU xuá»‘ng **3 láº§n**

LoRA thá»±c hiá»‡n tÆ°Æ¡ng Ä‘Æ°Æ¡ng hoáº·c tá»‘t hÆ¡n so vá»›i fine-tuning vá» cháº¥t lÆ°á»£ng mÃ´ hÃ¬nh trÃªn RoBERTa, DeBERTa, GPT-2, vÃ  GPT-3.

## CÃ´ng Thá»©c LoRA

LoRA Ä‘á» xuáº¥t sá»­ dá»¥ng phÃ¢n rÃ£ háº¡ng tháº¥p:

$$W' = W + \Delta W = W + BA$$

Trong Ä‘Ã³:
- $W$: Ma tráº­n trá»ng sá»‘ pre-trained (Ä‘Ã´ng cá»©ng)
- $B \in \mathbb{R}^{d \times r}$: Ma tráº­n háº¡ng tháº¥p thá»© nháº¥t
- $A \in \mathbb{R}^{r \times d}$: Ma tráº­n háº¡ng tháº¥p thá»© hai
- $r \ll d$: Rank cá»§a ma tráº­n thÃ­ch nghi

## Káº¿t Luáº­n

TÃ³m láº¡i, LoRA adapters lÃ  má»™t táº­p há»£p con cá»§a PEFT sá»­ dá»¥ng cÃ¡c ma tráº­n háº¡ng tháº¥p Ä‘á»ƒ fine-tune cÃ¡c mÃ´ hÃ¬nh má»™t cÃ¡ch hiá»‡u quáº£. Báº±ng cÃ¡ch cáº­p nháº­t chá»‰ má»™t sá»‘ nhá» cÃ¡c tham sá»‘, chÃºng cung cáº¥p cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ vá»›i chi phÃ­ tÃ­nh toÃ¡n tá»‘i thiá»ƒu. Äiá»u nÃ y lÃ m cho chÃºng trá»Ÿ thÃ nh má»™t cÃ´ng cá»¥ vÃ´ giÃ¡ Ä‘á»ƒ thÃ­ch nghi cÃ¡c mÃ´ hÃ¬nh pre-trained cho cÃ¡c tÃ¡c vá»¥ má»›i.

---

*Nguá»“n: File subtitle 02 - LoRA adapters.vtt*
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Giá»›i Thiá»‡u Vá» PEFT](01 - Introduction to PEFT.md) | [Xem bÃ i viáº¿t â†’](01 - Introduction to PEFT.md) |
| ğŸ“Œ **[LoRA Adapters](02 - LoRA adapters.md)** | [Xem bÃ i viáº¿t â†’](02 - LoRA adapters.md) |
| [LoRA: PhÃ¢n TÃ­ch Ká»¹ Thuáº­t SÃ¢u](03 - LoRA in depth Technical analysis.md) | [Xem bÃ i viáº¿t â†’](03 - LoRA in depth Technical analysis.md) |
| [Demo LoRA Fine-tuning TrÃªn FLAN-T5](04 - Demo LoRA fine-tuning on FLAN-T5.md) | [Xem bÃ i viáº¿t â†’](04 - Demo LoRA fine-tuning on FLAN-T5.md) |
| [Triá»ƒn Khai LoRA trong Large Language Models](05 - Implementing LoRA in LLMs.md) | [Xem bÃ i viáº¿t â†’](05 - Implementing LoRA in LLMs.md) |
| [Demo Thá»­ Nghiá»‡m Tham Sá»‘ LoRA](06 - Demo Challenges in LoRA.md) | [Xem bÃ i viáº¿t â†’](06 - Demo Challenges in LoRA.md) |
| [Giáº£i PhÃ¡p Fine-tuning FLAN-T5 cho Dá»‹ch Thuáº­t vá»›i LoRA](07 - Solution Fine-tuning FLAN-T5 for translation.md) | [Xem bÃ i viáº¿t â†’](07 - Solution Fine-tuning FLAN-T5 for translation.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

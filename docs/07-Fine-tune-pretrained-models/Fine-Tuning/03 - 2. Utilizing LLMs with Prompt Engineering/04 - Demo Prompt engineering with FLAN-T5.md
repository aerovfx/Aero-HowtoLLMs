
<!-- Aero-Navigation-Start -->
[üè† Home](../../../../index.md) > [07 Fine tune pretrained models](../../../index.md) > [Fine Tuning](../../index.md) > [03   2. Utilizing LLMs with Prompt Engineering](../index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../../../index.md)
- [üìö Module 01: LLM Course](../../../../01-LLM_Course/index.md)
- [üî¢ Module 02: Tokenization](../../../../02-Words-to-tokens-to-numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../../../04-buildGPT/index.md)
- [üéØ Module 07: Fine-tuning](../../../../07-Fine-tune-pretrained-models/index.md)
- [üîç Module 19: AI Safety](../../../../19-AI-safety/index.md)
- [üêç Module 20: Python for AI](../../../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Demo Prompt Engineering V·ªõi FLAN-T5

## Gi·ªõi Thi·ªáu

Ch√†o m·ª´ng m·ªçi ng∆∞·ªùi ƒë·∫øn v·ªõi demo ƒë·∫ßu ti√™n c·ªßa kh√≥a h·ªçc n√†y. T·∫•t c·∫£ c√°c demo trong kh√≥a h·ªçc n√†y s·∫Ω s·ª≠ d·ª•ng Google Colaboratory.

Google Colab l√† m·ªôt n·ªÅn t·∫£ng cho ph√©p ch√∫ng ta l∆∞u tr·ªØ c√°c file notebook v√† k·∫øt n·ªëi mi·ªÖn ph√≠ ƒë·∫øn m·ªôt instance tr√™n Google Cloud Platform n∆°i ch√∫ng ta c≈©ng c√≥ th·ªÉ k·∫øt n·ªëi GPU. ƒêi·ªÅu n√†y r·∫•t h·ªØu √≠ch, ƒë·∫∑c bi·ªát cho vi·ªác prototype c√°c √Ω t∆∞·ªüng.

Truy c·∫≠p: colab.research.google.com

## Thi·∫øt L·∫≠p M√¥i Tr∆∞·ªùng

### K·∫øt N·ªëi Google Colab

1. Truy c·∫≠p trang web Colab
2. Upload notebook t·ª´ Exercise Files
3. Click "Connect" ƒë·ªÉ k·∫øt n·ªëi v·ªõi GPU mi·ªÖn ph√≠

**L∆∞u √Ω:** Lo·∫°i GPU ph·ª• thu·ªôc v√†o:
- Kh·∫£ nƒÉng s·∫µn c√≥ theo m√∫i gi·ªù
- T·∫ßn su·∫•t s·ª≠ d·ª•ng GPU g·∫ßn ƒë√¢y
- V√¨ l√† mi·ªÖn ph√≠ n√™n kh√¥ng ƒë·∫£m b·∫£o ƒë∆∞·ª£c lo·∫°i GPU c·ª• th·ªÉ

## C√†i ƒê·∫∑t Th∆∞ Vi·ªán

```python
# C√†i ƒë·∫∑t Transformers v√† TensorFlow
!pip install transformers tensorflow
```

## T·∫£i M√¥ H√¨nh FLAN-T5

```python
from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM

# T·∫£i tokenizer v√† model
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-large")
model = TFAutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-large")
```

**L∆∞u √Ω v·ªÅ warnings:**
- Warning v·ªÅ x√°c th·ª±c HuggingFace l√† b√¨nh th∆∞·ªùng
- Warning v·ªÅ vi·ªác model ƒë∆∞·ª£c train b·∫±ng PyTorch r·ªìi convert sang TensorFlow - ƒë·ªô ch√≠nh x√°c 99.9% t∆∞∆°ng ƒë∆∞∆°ng

## Quy Tr√¨nh Prompt V·ªõi FLAN-T5

Vi·ªác prompt m·ªôt LLM lu√¥n g·ªìm 4 b∆∞·ªõc:
1. ƒê·ªãnh nghƒ©a prompt
2. Tokenize (chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh tokens)
3. Model.generate() (t·∫°o output)
4. Tokenizer.decode() (chuy·ªÉn ƒë·ªïi IDs v·ªÅ vƒÉn b·∫£n)

### 1. T√≥m T·∫Øt VƒÉn B·∫£n (Summarization)

```python
# ƒê·ªãnh nghƒ©a prompt
prompt = "summarize: Studies show that eating carrots help improve vision..."

# Tokenize
inputs = tokenizer(prompt, return_tensors="tf", max_length=512, 
                    truncation=True, padding=True)

# Generate
outputs = model.generate(inputs.input_ids, max_length=50)

# Decode
summary = tokenizer.decode(outputs[0])
print(summary)
```

**K·∫øt qu·∫£:** "eat carrots" - m·ªôt b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn

### 2. D·ªãch Thu·∫≠t (Translation)

```python
# Prompt d·ªãch ti·∫øng Anh sang ti·∫øng T√¢y Ban Nha
prompt = "translate English to Spanish: cheese is delicious"

# Tokenize
inputs = tokenizer(prompt, return_tensors="tf", max_length=512, 
                    truncation=True, padding=True)

# Generate
outputs = model.generate(inputs.input_ids, max_length=40)

# Decode
translation = tokenizer.decode(outputs[0])
print(translation)
```

### 3. Tr·∫£ L·ªùi C√¢u H·ªèi (Question Answering)

```python
# Context v√† c√¢u h·ªèi
context = "The Great Wall of China is over 13,000 miles long."
question = "question: How long is the Great Wall of China?"

prompt = context + " " + question

# Tokenize
inputs = tokenizer(prompt, return_tensors="tf", max_length=512, 
                    truncation=True, padding=True)

# Generate
outputs = model.generate(inputs.input_ids, max_length=50)

# Decode
answer = tokenizer.decode(outputs[0])
print(answer)
```

**K·∫øt qu·∫£:** "It's over 13,000 miles long."

## T·ªïng K·∫øt

B·∫°n ƒë√£ th·∫•y:
- To√†n b·ªô workflow ho·∫°t ƒë·ªông v·ªõi ch·ªâ 4 b∆∞·ªõc ƒë∆°n gi·∫£n
- FLAN-T5 c√≥ th·ªÉ th·ª±c hi·ªán nhi·ªÅu t√°c v·ª•: t√≥m t·∫Øt, d·ªãch thu·∫≠t, tr·∫£ l·ªùi c√¢u h·ªèi
- Kh√¥ng c·∫ßn training - ch·ªâ c·∫ßn prompt l√† c√≥ k·∫øt qu·∫£

V·ªõi ki·∫øn th·ª©c n√†y, b·∫°n c√≥ th·ªÉ t√≠ch h·ª£p LLMs v√†o b·∫•t k·ª≥ chatbot n√†o m√† kh√¥ng c·∫ßn l√†m th√™m nhi·ªÅu c√¥ng vi·ªác ph·ª©c t·∫°p.

## T√†i li·ªáu tham kh·∫£o

1. **Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., & Polosukhin, I. (2017).** *Attention Is All You Need.* Advances in Neural Information Processing Systems, 30, 5998-6008. https://arxiv.org/abs/1706.03762

2. **Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018).** *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.* arXiv preprint arXiv:1810.04805. https://arxiv.org/abs/1810.04805

3. **Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2019).** *Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.* arXiv preprint arXiv:1910.10683. https://arxiv.org/abs/1910.10683

4. **Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021).** *LoRA: Low-Rank Adaptation of Large Language Models.* arXiv preprint arXiv:2106.09685. https://arxiv.org/abs/2106.09685

5. **Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., ... & Wei, J. (2022).** *Scaling Instruction-Finetuned Language Models.* arXiv preprint arXiv:2210.11416. https://arxiv.org/abs/2210.11416

6. **Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J., & Roberts, A. (2023).** *The Flan Collection: Designing Data and Methods for Effective Instruction Tuning.* arXiv preprint arXiv:2301.13688. https://arxiv.org/abs/2301.13688

7. **Han, Z., Gao, C., Liu, J., Zhang, J., & Zhang, S. Q. (2024).** *Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey.* arXiv preprint arXiv:2403.14608. https://arxiv.org/abs/2403.14608

8. **Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P. S., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., Kenton, Z., Brown, S., Hawkins, W., Stepleton, T., Biles, C., Birhane, A., Haas, J., Rimell, L., Hendricks, L. A., ... & Gabriel, I. (2021).** *Ethical and Social Risks of Harm from Language Models.* DeepMind. https://storage.googleapis.com/deepmind-media/research/language-research/Ethical%20and%20social%20risks.pdf

9. **Bengio, Y., Mindermann, S., Privitera, D., Besiroglu, T., Bommasani, R., Casper, S., Choi, Y., Goldfarb, D., Heidari, H., Khalatbari, L., Longpre, S., Mavroudis, V., Mazeika, M., Ng, K. Y., Okolo, C. T., Raji, D., Skeadas, T., Tram√®r, F., Adekanmbi, B., ... & Zhou, D. (2024).** *International Scientific Report on the Safety of Advanced AI (Interim Report).* arXiv preprint arXiv:2412.05282. https://arxiv.org/abs/2412.05282

10. **Amodei, D., Ananthanarayanan, S., Bapna, R., Chen, Z., Du, E., Goodfellow, I., ... & Sutskever, I. (2016).** *Concrete Problems in AI Safety.* arXiv preprint arXiv:1606.06565. https://arxiv.org/abs/1606.06565

11. **Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2024).** *QLoRA: Efficient Finetuning of Quantized LLMs.* arXiv preprint arXiv:2305.14314. https://arxiv.org/abs/2305.14314

12. **Zhang, Y., Yang, X., Cai, Y., & Giannakis, G. B. (2025).** *ScaLoRA: Optimally Scaled Low-Rank Adaptation for Efficient High-Rank Fine-Tuning.* arXiv preprint arXiv:2510.23818. https://arxiv.org/abs/2510.23818

13. **Wang, R., Dvijotham, K. D., & Manchester, I. R. (2025).** *Norm-Bounded Low-Rank Adaptation.* arXiv preprint arXiv:2501.19050. https://arxiv.org/abs/2501.19050

14. **Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T. A., Bernhard, M., ... & Houlsby, N. (2022).** *LoRA+: Efficient Low Rank Adaptation of Large Models.* arXiv preprint arXiv:2402.12354. https://arxiv.org/abs/2402.12354

15. **Wang, L., Lyu, C., Ji, T., Chen, M., Yu, Z., Shi, A., ... & Yu, P. S. (2023).** *A Survey on Parameter-Efficient Fine-Tuning for Foundation Models.* arXiv preprint arXiv:2504.21099. https://arxiv.org/abs/2504.21099

16. **Laakso, A., Kemell, K. K., & Nurminen, J. K. (2024).** *Ethical Issues in Large Language Models: A Systematic Literature Review.* CEUR Workshop Proceedings, 3901. https://ceur-ws.org/Vol-3901/paper_4.pdf

17. **Bosma, M., & Wei, J. (2021).** *Introducing FLAN: More Generalizable Language Models with Instruction Fine-Tuning.* Google AI Blog. https://research.google/blog/introducing-flan-more-generalizable-language-models-with-instruction-fine-tuning/

18. **Roberts, A., & Raffel, C. (2020).** *Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer.* Google AI Blog. https://research.google/blog/exploring-transfer-learning-with-t5-the-text-to-text-transfer-transformer/

19. **Lester, B., Al-Rfou, R., & Wang, L. (2021).** *The Power of Scale for Parameter-Efficient Prompt Tuning.* Proceedings of EMNLP 2021. https://arxiv.org/abs/2104.08691

20. **Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020).** *Language Models are Few-Shot Learners.* Advances in Neural Information Processing Systems, 33, 1877-1901. https://arxiv.org/abs/2005.14165
<!-- Aero-Footer-Start -->
---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

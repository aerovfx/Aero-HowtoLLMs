
<!-- Aero-Navigation-Start -->
[üè† Home](../../../../index.md) > [07 Fine tune pretrained models](../../../index.md) > [Fine Tuning](../../index.md) > [03   2. Utilizing LLMs with Prompt Engineering](../index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../../../index.md)
- [üìö Module 01: LLM Course](../../../../01-LLM_Course/index.md)
- [üî¢ Module 02: Tokenization](../../../../02-Words-to-tokens-to-numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../../../04-buildGPT/index.md)
- [üéØ Module 07: Fine-tuning](../../../../07-Fine-tune-pretrained-models/index.md)
- [üîç Module 19: AI Safety](../../../../19-AI-safety/index.md)
- [üêç Module 20: Python for AI](../../../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Gi·∫£i Ph√°p Thi·∫øt K·∫ø Prompt D·ªãch Thu·∫≠t

## T·ªïng Quan

Trong b√†i h·ªçc n√†y, ch√∫ng ta s·∫Ω xem x√©t gi·∫£i ph√°p cho b√†i t·∫≠p thi·∫øt k·∫ø prompt d·ªãch thu·∫≠t. B√†i t·∫≠p n√†y y√™u c·∫ßu ch√∫ng ta s·ª≠ d·ª•ng k·ªπ thu·∫≠t few-shot learning ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh FLAN-T5 th·ª±c hi·ªán d·ªãch thu·∫≠t t·ª´ ti·∫øng Anh sang ti·∫øng T√¢y Ban Nha, s·ª≠ d·ª•ng t·∫≠p d·ªØ li·ªáu CNN DailyMail.

## 1. B·ªëi C·∫£nh B√†i To√°n

### 1.1 M·ª•c Ti√™u

- S·ª≠ d·ª•ng k·ªπ thu·∫≠t few-shot learning
- Fine-tune FLAN-T5 cho t√°c v·ª• d·ªãch thu·∫≠t
- S·ª≠ d·ª•ng t·∫≠p d·ªØ li·ªáu CNN DailyMail
- D·ªãch t·ª´ ti·∫øng Anh sang ti·∫øng T√¢y Ban Nha

### 1.2 Th√°ch Th·ª©c

T·∫≠p d·ªØ li·ªáu CNN DailyMail ban ƒë·∫ßu ƒë∆∞·ª£c thi·∫øt k·∫ø cho t√°c v·ª• t√≥m t·∫Øt, kh√¥ng ph·∫£i d·ªãch thu·∫≠t. ƒêi·ªÅu n√†y ƒë√≤i h·ªèi ch√∫ng ta ph·∫£i:

- Chuy·ªÉn ƒë·ªïi ƒë·ªãnh d·∫°ng d·ªØ li·ªáu
- T·∫°o c√°c c·∫∑p v√≠ d·ª• few-shot
- √Åp d·ª•ng k·ªπ thu·∫≠t h·ªçc ƒëa ph∆∞∆°ng th·ª©c (multimodal learning)

## 2. Tri·ªÉn Khai Gi·∫£i Ph√°p

### 2.1 B∆∞·ªõc 1: T·∫£i D·ªØ Li·ªáu

```python
from datasets import load_dataset

# T·∫£i t·∫≠p d·ªØ li·ªáu CNN DailyMail
dataset = load_dataset("cnn_dailymail", "3.0.0", split="train[:3%]")
```

### 2.2 B∆∞·ªõc 2: Ti·ªÅn X·ª≠ L√Ω D·ªØ Li·ªáu

Do gi·ªõi h·∫°n v·ªÅ b·ªô nh·ªõ GPU, ch√∫ng ta th·ª±c hi·ªán:

1. **T√≥m t·∫Øt b√†i vi·∫øt** tr∆∞·ªõc khi d·ªãch
2. **T·∫°o c·∫∑p v√≠ d·ª•** cho few-shot learning

```python
# T·∫°o prompt cho few-shot learning
def create_translation_prompt(article, translation, task="translate English to Spanish"):
    return f"{task}: {article}\n{translation}"
```

### 2.3 B∆∞·ªõc 3: X√¢y D·ª±ng Prompt Few-Shot

```python
# V√≠ d·ª• few-shot
few_shot_examples = """
translate English to Spanish: The quick brown fox jumps over the lazy dog.
El r√°pido zorro marr√≥n salta sobre el perro perezoso.

translate English to Spanish: The weather is beautiful today.
El clima est√° hermoso hoy.

translate English to Spanish: I love learning new languages.
Me encanta aprender nuevos idiomas.

translate English to Spanish:
"""
```

### 2.4 B∆∞·ªõc 4: Hu·∫•n Luy·ªán v√† D·ªãch Thu·∫≠t

```python
# T·∫°o prompt ho√†n ch·ªânh
full_prompt = few_shot_examples + test_article

# Tokenize v√† t·∫°o ƒë·∫ßu ra
inputs = tokenizer(full_prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=500)
translation = tokenizer.decode(outputs[0])
```

## 3. K·∫øt Qu·∫£

### 3.1 ƒê√°nh Gi√°

| Ch·ªâ s·ªë | Gi√° tr·ªã |
|---------|---------|
| ƒê·ªô ch√≠nh x√°c | Cao |
| T√≠nh m·∫°ch l·∫°c | T·ªët |
| Ng·ªØ ph√°p | Ch√≠nh x√°c |

### 3.2 Ph√¢n T√≠ch

K·∫øt qu·∫£ cho th·∫•y k·ªπ thu·∫≠t few-shot learning c√≥ hi·ªáu qu·∫£ cao trong vi·ªác:

- **Chuy·ªÉn giao ki·∫øn th·ª©c**: M√¥ h√¨nh h·ªçc ƒë∆∞·ª£c c·∫•u tr√∫c d·ªãch thu·∫≠t t·ª´ c√°c v√≠ d·ª•
- **T·ªïng qu√°t h√≥a**: √Åp d·ª•ng cho vƒÉn b·∫£n m·ªõi ch∆∞a t·ª´ng th·∫•y
- **Ti·∫øt ki·ªám t√†i nguy√™n**: Kh√¥ng c·∫ßn fine-tuning full model

## 4. H·ªçc ƒêa Ph∆∞∆°ng Th·ª©c (Multimodal Learning)

### 4.1 Kh√°i Ni·ªám

H·ªçc ƒëa ph∆∞∆°ng th·ª©c l√† qu√° tr√¨nh k·∫øt h·ª£p nhi·ªÅu lo·∫°i d·ªØ li·ªáu kh√°c nhau (vƒÉn b·∫£n, h√¨nh ·∫£nh, √¢m thanh) ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh. Trong b√†i t·∫≠p n√†y, ch√∫ng ta:

1. S·ª≠ d·ª•ng vƒÉn b·∫£n g·ªëc (input)
2. T·∫°o t√≥m t·∫Øt (intermediate representation)
3. D·ªãch sang ng√¥n ng·ªØ m·ªõi (output)

### 4.2 M√¥ H√¨nh To√°n H·ªçc

$$\text{Translation} = f_{\theta}( \text{FewShotExamples} \oplus \text{NewInput} )$$

Trong ƒë√≥:
- $f_{\theta}$ l√† m√¥ h√¨nh ng√¥n ng·ªØ v·ªõi tham s·ªë $\theta$
- $\oplus$ l√† ph√©p n·ªëi chu·ªói

## 5. B√†i H·ªçc R√∫t Ra

### 5.1 ƒêi·ªÉm Quan Tr·ªçng

1. **T√≠nh linh ho·∫°t c·ªßa Few-shot**: C√≥ th·ªÉ √°p d·ª•ng cho nhi·ªÅu t√°c v·ª• kh√°c nhau
2. **Ch·∫•t l∆∞·ª£ng v√≠ d·ª•**: V√≠ d·ª• c√†ng li√™n quan, k·∫øt qu·∫£ c√†ng t·ªët
3. **X·ª≠ l√Ω d·ªØ li·ªáu**: C·∫ßn ti·ªÅn x·ª≠ l√Ω ph√π h·ª£p v·ªõi ƒë·ªãnh d·∫°ng m·ªõi

### 5.2 Khuy·∫øn Ngh·ªã

- S·ª≠ d·ª•ng 2-3 v√≠ d·ª• cho few-shot
- ƒê·∫£m b·∫£o ƒë·ªãnh d·∫°ng nh·∫•t qu√°n
- Th·ª≠ nghi·ªám v·ªõi c√°c bi·∫øn th·ªÉ kh√°c nhau

## 6. K·∫øt Lu·∫≠n

B√†i t·∫≠p n√†y ƒë√£ ch·ª©ng minh kh·∫£ nƒÉng c·ªßa k·ªπ thu·∫≠t few-shot learning trong vi·ªác m·ªü r·ªông kh·∫£ nƒÉng c·ªßa m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn cho c√°c t√°c v·ª• m·ªõi m√† kh√¥ng c·∫ßn fine-tuning full model. ƒê√¢y l√† m·ªôt k·ªπ thu·∫≠t quan tr·ªçng trong vi·ªác ·ª©ng d·ª•ng LLMs v√†o c√°c b√†i to√°n th·ª±c t·∫ø.

## T√†i Li·ªáu Tham Kh·∫£o

1. Raffel, C., et al. (2020). "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer." *Journal of Machine Learning Research*, 21(140), 1-67.

2. Wei, J., et al. (2022). "Fine-tuned Language Models are Zero-Shot Learners." *arXiv:2109.01652*.

3. Sanh, V., et al. (2022). "Multitask Prompted Training Enables Zero-Shot Task Generalization." *arXiv:2110.08207*.
<!-- Aero-Footer-Start -->
---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

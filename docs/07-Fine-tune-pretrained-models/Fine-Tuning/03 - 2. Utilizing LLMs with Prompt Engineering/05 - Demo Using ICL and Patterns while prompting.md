
<!-- Aero-Navigation-Start -->
[üè† Home](../../../../index.md) > [07 Fine tune pretrained models](../../../index.md) > [Fine Tuning](../../index.md) > [03   2. Utilizing LLMs with Prompt Engineering](../index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../../../index.md)
- [üìö Module 01: LLM Course](../../../../01-LLM_Course/index.md)
- [üî¢ Module 02: Tokenization](../../../../02-Words-to-tokens-to-numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../../../04-buildGPT/index.md)
- [üéØ Module 07: Fine-tuning](../../../../07-Fine-tune-pretrained-models/index.md)
- [üîç Module 19: AI Safety](../../../../19-AI-safety/index.md)
- [üêç Module 20: Python for AI](../../../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# H·ªçc T·∫≠p Trong Ng·ªØ C·∫£nh (In-Context Learning) v√† M·∫´u Few-Shot v·ªõi FLAN-T5

## T·ªïng Quan

Trong b√†i h∆∞·ªõng d·∫´n th·ª±c h√†nh n√†y, ch√∫ng ta s·∫Ω kh√°m ph√° c√°ch c·∫£i thi·ªán c√°c prompt cho m√¥ h√¨nh FLAN-T5 th√¥ng qua k·ªπ thu·∫≠t **h·ªçc t·∫≠p trong ng·ªØ c·∫£nh (In-Context Learning - ICL)** v√† m·∫´u **few-shot learning**. ƒê√¢y l√† nh·ªØng k·ªπ thu·∫≠t quan tr·ªçng trong k·ªπ thu·∫≠t prompt hi·ªán ƒë·∫°i, cho ph√©p m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM) th·ª±c hi·ªán c√°c t√°c v·ª• m·ªõi m√† kh√¥ng c·∫ßn hu·∫•n luy·ªán l·∫°i.

## 1. Gi·ªõi Thi·ªáu v·ªÅ Few-Shot Learning

### 1.1 Kh√°i Ni·ªám C∆° B·∫£n

Few-shot learning l√† m·ªôt k·ªπ thu·∫≠t trong ƒë√≥ ch√∫ng ta cung c·∫•p cho m√¥ h√¨nh m·ªôt s·ªë v√≠ d·ª• minh h·ªça v·ªÅ t√°c v·ª• c·∫ßn th·ª±c hi·ªán. C√°c v√≠ d·ª• n√†y ƒë∆∞·ª£c ƒë∆∞a v√†o prompt ƒë·ªÉ m√¥ h√¨nh hi·ªÉu:

- Lo·∫°i t√°c v·ª• c·∫ßn th·ª±c hi·ªán (v√≠ d·ª•: t√≥m t·∫Øt, d·ªãch thu·∫≠t)
- ƒê·ªãnh d·∫°ng ƒë·∫ßu ra mong mu·ªën
- ƒê·ªô d√†i v√† phong c√°ch c·ªßa k·∫øt qu·∫£

### 1.2 C∆° Ch·∫ø Ho·∫°t ƒê·ªông

Khi cung c·∫•p c√°c v√≠ d·ª• few-shot, m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c:

$$\text{K·∫øt qu·∫£} = f(\text{v√≠ d·ª•}_1, \text{v√≠ d·ª•}_2, ..., \text{v√≠ d·ª•}_n, \text{ƒë·∫ßu v√†o m·ªõi})$$

Trong ƒë√≥:
- $f$ l√† m√¥ h√¨nh ng√¥n ng·ªØ
- C√°c v√≠ d·ª• cung c·∫•p "ng·ªØ c·∫£nh" ƒë·ªÉ m√¥ h√¨nh suy lu·∫≠n
- M√¥ h√¨nh t·ªïng qu√°t h√≥a t·ª´ c√°c v√≠ d·ª• ƒë·ªÉ x·ª≠ l√Ω ƒë·∫ßu v√†o m·ªõi

## 2. Tri·ªÉn Khai V·ªõi FLAN-T5

### 2.1 T√≥m T·∫Øt VƒÉn B·∫£n (Summarization)

```python
# V√≠ d·ª• few-shot cho t√°c v·ª• t√≥m t·∫Øt
few_shot_examples = """
summarize: The quick brown fox jumps over the lazy dog. The dog was not amused by the fox's antics.
The fox jumped over the dog who was not happy.

summarize: Rain in Spain falls mainly on the plain. The weather has been unusual this year.
Weather patterns in Spain are interesting.

summarize: Carrots are rich in vitamin A and are excellent for eye health. They also contain fiber.
"""
```

K·∫øt qu·∫£ thu ƒë∆∞·ª£c:
- **Kh√¥ng c√≥ few-shot**: "eat carrots"
- **C√≥ few-shot**: "Carrots are a great source of vitamin A, which is crucial for maintaining healthy eyesight"

### 2.2 D·ªãch Thu·∫≠t (Translation)

```python
# V√≠ d·ª• few-shot cho t√°c v·ª• d·ªãch Anh - T√¢y Ban Nha
translation_examples = """
translate English to Spanish: Hello, how are you?
Hola, ¬øc√≥mo est√°s?

translate English to Spanish: Good morning
Buenos d√≠as

translate English to Spanish: Cheese is delicious
"""
```

## 3. So S√°nh Hi·ªáu Qu·∫£

### 3.1 ƒê√°nh Gi√° ƒê·ªãnh T√≠nh

| Ph∆∞∆°ng ph√°p | ƒê·∫ßu ra | Ch·∫•t l∆∞·ª£ng |
|-------------|---------|-------------|
| Zero-shot | "eat carrots" | C∆° b·∫£n |
| Few-shot | "Carrots are a great source of vitamin A..." | T·ªët |

### 3.2 Ph√¢n T√≠ch To√°n H·ªçc

Hi·ªáu qu·∫£ c·ªßa few-shot learning c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn:

$$P(y|x, \text{v√≠ d·ª•}) = \frac{1}{Z} \sum_{i=1}^{n} w_i \cdot \text{sim}(x, x_i) \cdot P(y|x_i)$$

Trong ƒë√≥:
- $w_i$ l√† tr·ªçng s·ªë c·ªßa v√≠ d·ª• th·ª© $i$
- $\text{sim}$ l√† h√†m ƒëo ƒë·ªô t∆∞∆°ng ƒë·ªìng
- $Z$ l√† h·∫±ng s·ªë chu·∫©n h√≥a

## 4. ·ª®ng D·ª•ng Th·ª±c T·∫ø

### 4.1 Trong C√°c Lƒ©nh V·ª±c Kh√°c Nhau

K·ªπ thu·∫≠t few-shot learning c√≥ th·ªÉ √°p d·ª•ng cho:

1. **Ph√¢n T√≠ch C·∫£m X√∫c (Sentiment Analysis)**
2. **Tr·∫£ L·ªùi C√¢u H·ªèi (Question Answering)**
3. **D·ªãch Thu·∫≠t (Translation)**
4. **T√≥m T·∫Øt (Summarization)**
5. **Chain-of-Thought Reasoning**

### 4.2 V√≠ D·ª• Tr·∫£ L·ªùi C√¢u H·ªèi

```python
# Few-shot cho QA
qa_examples = """
Context: The Great Wall of China is over 13,000 miles long.
Question: How long is the Great Wall of China?
Answer: Over 13,000 miles.

Context: Mount Everest is the highest mountain in the world.
Question: What is the highest mountain in the world?
Answer:
"""
```

## 5. K·∫øt Lu·∫≠n

K·ªπ thu·∫≠t h·ªçc t·∫≠p trong ng·ªØ c·∫£nh v√† few-shot learning l√† nh·ªØng c√¥ng c·ª• m·∫°nh m·∫Ω ƒë·ªÉ c·∫£i thi·ªán ƒë√°ng k·ªÉ hi·ªáu su·∫•t c·ªßa c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn nh∆∞ FLAN-T5. B·∫±ng c√°ch cung c·∫•p c√°c v√≠ d·ª• minh h·ªça trong prompt, ch√∫ng ta c√≥ th·ªÉ:

- C·∫£i thi·ªán ch·∫•t l∆∞·ª£ng ƒë·∫ßu ra
- ƒê·ªãnh d·∫°ng k·∫øt qu·∫£ theo y√™u c·∫ßu
- Gi·∫£m thi·ªÉu nhu c·∫ßu fine-tuning cho c√°c t√°c v·ª• m·ªõi

## T√†i Li·ªáu Tham Kh·∫£o

1. Brown, T., et al. (2020). "Language Models are Few-Shot Learners." *Advances in Neural Information Processing Systems*, 33, 1877-1901.

2. Wei, J., et al. (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models." *arXiv:2201.11903*.

3. Dong, Q., et al. (2022). "A Survey on In-context Learning." *arXiv:2301.00234*.
<!-- Aero-Footer-Start -->
---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

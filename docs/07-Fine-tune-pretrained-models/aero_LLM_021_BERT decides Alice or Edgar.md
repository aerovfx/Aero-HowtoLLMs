
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [07 Fine tune pretrained models](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# á»¨ng Dá»¥ng MÃ´ HÃ¬nh BERT Trong PhÃ¢n Loáº¡i VÄƒn Báº£n VÄƒn Há»c: TrÆ°á»ng Há»£p Alice vÃ  Edgar

## TÃ³m táº¯t

BÃ i viáº¿t nÃ y trÃ¬nh bÃ y quÃ¡ trÃ¬nh xÃ¢y dá»±ng vÃ  fine-tuning mÃ´ hÃ¬nh BERT nháº±m phÃ¢n loáº¡i cÃ¡c Ä‘oáº¡n vÄƒn báº£n trÃ­ch tá»« hai nguá»“n vÄƒn há»c: *Alice Through the Looking-Glass* cá»§a *Lewis Carroll* vÃ  tuyá»ƒn táº­p thÆ¡ â€“ truyá»‡n cá»§a *Edgar Allan Poe*. Dá»±a trÃªn tÃ i liá»‡u thá»±c nghiá»‡m , nghiÃªn cá»©u phÃ¢n tÃ­ch kiáº¿n trÃºc mÃ´ hÃ¬nh, quy trÃ¬nh huáº¥n luyá»‡n, phÆ°Æ¡ng phÃ¡p lÃ m mÆ°á»£t dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ phÃ¢n loáº¡i. Káº¿t quáº£ cho tháº¥y BERT cÃ³ kháº£ nÄƒng phÃ¢n biá»‡t phong cÃ¡ch vÄƒn há»c vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao trong Ä‘iá»u kiá»‡n dá»¯ liá»‡u huáº¥n luyá»‡n háº¡n cháº¿.

---

## 1. Giá»›i thiá»‡u

Trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, bÃ i toÃ¡n phÃ¢n loáº¡i vÄƒn báº£n theo tÃ¡c giáº£ hoáº·c phong cÃ¡ch (author attribution) cÃ³ Ã½ nghÄ©a quan trá»ng trong nghiÃªn cá»©u vÄƒn há»c sá»‘ vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh sinh vÄƒn báº£n.

Theo tÃ i liá»‡u , tÃ¡c giáº£ Ä‘Ã£ xÃ¢y dá»±ng má»™t bá»™ phÃ¢n loáº¡i dá»±a trÃªn BERT Ä‘á»ƒ xÃ¡c Ä‘á»‹nh xem má»™t Ä‘oáº¡n vÄƒn báº£n thuá»™c vá» â€œAliceâ€ hay â€œEdgarâ€. MÃ´ hÃ¬nh nÃ y cÃ²n Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m cÃ´ng cá»¥ Ä‘Ã¡nh giÃ¡ giÃ¡n tiáº¿p cho cÃ¡c mÃ´ hÃ¬nh sinh vÄƒn báº£n.

BERT do nhÃ³m nghiÃªn cá»©u táº¡i **Google** phÃ¡t triá»ƒn, lÃ  ná»n táº£ng cho nhiá»u há»‡ thá»‘ng NLP hiá»‡n Ä‘áº¡i.

Má»¥c tiÃªu nghiÃªn cá»©u:

* PhÃ¢n tÃ­ch mÃ´ hÃ¬nh BERT cho phÃ¢n loáº¡i vÄƒn há»c,
* MÃ´ táº£ quy trÃ¬nh fine-tuning,
* TrÃ¬nh bÃ y phÆ°Æ¡ng phÃ¡p lÃ m mÆ°á»£t (smoothing) dá»¯ liá»‡u huáº¥n luyá»‡n,
* Äá» xuáº¥t hÆ°á»›ng á»©ng dá»¥ng trong Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh sinh.

---

## 2. CÆ¡ sá»Ÿ lÃ½ thuyáº¿t

### 2.1. MÃ´ hÃ¬nh ngÃ´n ngá»¯ hai chiá»u

BERT há»c biá»ƒu diá»…n ngá»¯ cáº£nh hai chiá»u cho chuá»—i token:

[
X=(x_1,x_2,\dots,x_n)
]

Biá»ƒu diá»…n áº©n táº¡i vá»‹ trÃ­ (i):

[
h_i = f(x_1,\dots,x_n;\theta)
]

Trong Ä‘Ã³ (\theta) lÃ  táº­p tham sá»‘ mÃ´ hÃ¬nh.

---

### 2.2. Biá»ƒu diá»…n [CLS] vÃ  phÃ¢n loáº¡i

Vá»›i token Ä‘áº·c biá»‡t [CLS], vector biá»ƒu diá»…n:

[
h_{CLS}\in\mathbb{R}^d
]

Ä‘Æ°á»£c dÃ¹ng cho phÃ¢n loáº¡i:

[
z = W h_{CLS} + b
]

[
\hat{y}=\text{softmax}(z)
]

Trong Ä‘Ã³ (W,b) lÃ  tham sá»‘ cá»§a táº§ng phÃ¢n loáº¡i.

---

### 2.3. HÃ m máº¥t mÃ¡t Cross-Entropy

Vá»›i nhÃ£n tháº­t (y\in{0,1}):

[
\mathcal{L}
===========

-\frac{1}{N}\sum_{i=1}^{N}
\sum_{c=1}^{2}
y_{ic}\log(\hat{y}_{ic})
]

Má»¥c tiÃªu:

[
\theta^*=\arg\min_\theta \mathcal{L}(\theta)
]

---

## 3. PhÆ°Æ¡ng phÃ¡p nghiÃªn cá»©u

### 3.1. Dá»¯ liá»‡u huáº¥n luyá»‡n

Dá»¯ liá»‡u gá»“m cÃ¡c Ä‘oáº¡n vÄƒn báº£n ngáº¯n trÃ­ch tá»« hai nguá»“n vÄƒn há»c khÃ¡c nhau .

Táº­p dá»¯ liá»‡u:

[
\mathcal{D}={(x_i,y_i)}_{i=1}^{N}
]

Trong Ä‘Ã³:

* (x_i): chuá»—i token,
* (y_i\in{0,1}): nhÃ£n Alice hoáº·c Edgar.

---

### 3.2. Thiáº¿t láº­p huáº¥n luyá»‡n

Theo tÃ i liá»‡u :

* Batch size: 64,
* Äá»™ dÃ i chuá»—i: 256 token,
* Sá»‘ epoch: 150,
* Learning rate: ráº¥t nhá»,
* Huáº¥n luyá»‡n trÃªn GPU.

Tá»•ng sá»‘ token:

[
M = N\times L
]

vá»›i (L=256).

---

### 3.3. Quy trÃ¬nh fine-tuning

Quy trÃ¬nh gá»“m:

1. Token hÃ³a dá»¯ liá»‡u,
2. Náº¡p mÃ´ hÃ¬nh BERT tiá»n huáº¥n luyá»‡n,
3. ThÃªm táº§ng phÃ¢n loáº¡i,
4. Huáº¥n luyá»‡n báº±ng backpropagation,
5. LÆ°u mÃ´ hÃ¬nh sau huáº¥n luyá»‡n.

Tham sá»‘ Ä‘Æ°á»£c cáº­p nháº­t theo:

[
\theta_{t+1}
============

\theta_t-\eta\nabla_\theta\mathcal{L}_t
]

---

### 3.4. Sinh batch vÃ  gÃ¡n nhÃ£n

Theo , má»—i batch gá»“m:

* 32 máº«u tá»« Alice,
* 32 máº«u tá»« Edgar.

Vector nhÃ£n:

[
y=(\underbrace{0,\dots,0}*{32},
\underbrace{1,\dots,1}*{32})
]

---

## 4. PhÆ°Æ¡ng phÃ¡p lÃ m mÆ°á»£t trung bÃ¬nh (Mean Smoothing)

### 4.1. Äá»‹nh nghÄ©a

Giáº£ sá»­ chuá»—i loss:

[
x=(x_1,x_2,\dots,x_n)
]

Vá»›i cá»­a sá»• kÃ­ch thÆ°á»›c (k), giÃ¡ trá»‹ lÃ m mÆ°á»£t:

[
y_i
===

\frac{1}{k}
\sum_{j=i-w}^{i+w} x_j
]

vá»›i:

[
w=\frac{k-1}{2}
]

---

### 4.2. Ã nghÄ©a

* Giáº£m nhiá»…u,
* LÃ m ná»•i báº­t xu hÆ°á»›ng há»™i tá»¥,
* Há»— trá»£ trá»±c quan hÃ³a.

Theo , giÃ¡ trá»‹ (k=3) cho káº¿t quáº£ cÃ¢n báº±ng giá»¯a mÆ°á»£t vÃ  trung thá»±c.

---

### 4.3. Hiá»‡u á»©ng biÃªn

Táº¡i biÃªn chuá»—i:

[
i<w \quad \text{hoáº·c} \quad i>n-w
]

sáº½ xuáº¥t hiá»‡n sai lá»‡ch:

[
y_i \approx \frac{1}{m}\sum x_j,\quad m<k
]

GÃ¢y ra hiá»‡n tÆ°á»£ng â€œedge effectâ€.

---

## 5. PhÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡

### 5.1. Äá»™ chÃ­nh xÃ¡c (Accuracy)

[
\text{Acc}
==========

\frac{1}{N}
\sum_{i=1}^{N}\mathbf{1}(\hat{y}_i=y_i)
]

Theo tÃ i liá»‡u , Ä‘á»™ chÃ­nh xÃ¡c Ä‘áº¡t má»©c cao chá»‰ sau vÃ i chá»¥c epoch.

---

### 5.2. HÃ m máº¥t mÃ¡t

QuÃ¡ trÃ¬nh huáº¥n luyá»‡n cho tháº¥y:

[
\mathcal{L}_{initial}

>

\mathcal{L}_{final}
]

â‡’ mÃ´ hÃ¬nh há»™i tá»¥.

---

### 5.3. ÄÃ¡nh giÃ¡ Ä‘á»‹nh tÃ­nh

NgoÃ i chá»‰ sá»‘ Ä‘á»‹nh lÆ°á»£ng, tÃ¡c giáº£ cÃ²n quan sÃ¡t:

* Kháº£ nÄƒng nháº­n diá»‡n phong cÃ¡ch,
* Äá»™ á»•n Ä‘á»‹nh dá»± Ä‘oÃ¡n,
* Sá»± nháº¡y cáº£m vá»›i prompt.

---

## 6. Káº¿t quáº£ thá»±c nghiá»‡m

Theo káº¿t quáº£ trong :

* Accuracy tÄƒng nhanh theo epoch,
* Loss giáº£m Ä‘á»u,
* MÃ´ hÃ¬nh phÃ¢n biá»‡t tá»‘t hai phong cÃ¡ch vÄƒn há»c.

Quan há»‡ giá»¯a loss vÃ  epoch:

[
\frac{d\mathcal{L}}{dt}<0
]

Cho tháº¥y xu hÆ°á»›ng há»c á»•n Ä‘á»‹nh.

Biá»ƒu Ä‘á»“ hai trá»¥c (lossâ€“accuracy) giÃºp trá»±c quan hÃ³a quÃ¡ trÃ¬nh há»™i tá»¥.

---

## 7. Tháº£o luáº­n

### 7.1. Hiá»‡u quáº£ cá»§a learning rate nhá»

Vá»›i (\eta) nhá»:

[
|\theta_{t+1}-\theta_t|\ll1
]

â‡’ háº¡n cháº¿ phÃ¡ vá»¡ tri thá»©c tiá»n huáº¥n luyá»‡n.

---

### 7.2. Vai trÃ² trong Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh sinh

MÃ´ hÃ¬nh phÃ¢n loáº¡i cÃ³ thá»ƒ dÃ¹ng Ä‘á»ƒ Ä‘o:

[
S = P(\text{Alice}|\text{text})
]

hoáº·c

[
P(\text{Edgar}|\text{text})
]

Tá»« Ä‘Ã³ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh sinh vÄƒn báº£n theo phong cÃ¡ch.

---

### 7.3. Háº¡n cháº¿

* Dá»¯ liá»‡u háº¡n cháº¿,
* Chá»‰ phÃ¢n loáº¡i nhá»‹ phÃ¢n,
* Phá»¥ thuá»™c cháº¥t lÆ°á»£ng trÃ­ch Ä‘oáº¡n.

---

## 8. á»¨ng dá»¥ng thá»±c tiá»…n

PhÆ°Æ¡ng phÃ¡p cÃ³ thá»ƒ Ã¡p dá»¥ng trong:

* PhÃ¢n tÃ­ch phong cÃ¡ch tÃ¡c giáº£,
* PhÃ¡t hiá»‡n Ä‘áº¡o vÄƒn,
* ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh sinh vÄƒn há»c,
* Há»‡ thá»‘ng há»c táº­p vÄƒn chÆ°Æ¡ng sá»‘.

Trong nghiÃªn cá»©u AI sÃ¡ng táº¡o, bá»™ phÃ¢n loáº¡i Ä‘Ã³ng vai trÃ² â€œbá»™ giÃ¡m kháº£o tá»± Ä‘á»™ngâ€.

---

## 9. Káº¿t luáº­n

BÃ i viáº¿t Ä‘Ã£ trÃ¬nh bÃ y quÃ¡ trÃ¬nh fine-tuning BERT cho bÃ i toÃ¡n phÃ¢n loáº¡i vÄƒn báº£n Alice â€“ Edgar. CÃ¡c káº¿t quáº£ chÃ­nh:

1. BERT há»c nhanh phong cÃ¡ch vÄƒn há»c,
2. Learning rate nhá» giÃºp á»•n Ä‘á»‹nh mÃ´ hÃ¬nh,
3. Mean smoothing há»— trá»£ phÃ¢n tÃ­ch huáº¥n luyá»‡n,
4. MÃ´ hÃ¬nh cÃ³ tiá»m nÄƒng lÃ m cÃ´ng cá»¥ Ä‘Ã¡nh giÃ¡ sinh vÄƒn báº£n.

Trong tÆ°Æ¡ng lai, cÃ³ thá»ƒ má»Ÿ rá»™ng sang phÃ¢n loáº¡i Ä‘a tÃ¡c giáº£ vÃ  káº¿t há»£p vá»›i PEFT hoáº·c instruction tuning.

---

## TÃ i liá»‡u tham kháº£o

1. BERT decides Alice or Edgar â€“ Code Challenge 
2. Devlin et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers.
3. Jurafsky & Martin (2023). Speech and Language Processing.
4. Goodfellow et al. (2016). Deep Learning.

---
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

WEBVTT

00:02.040 --> 00:11.000
Now that you've seen some instruction and code demos on how to create, train and interpret sparse autoencoders

00:11.000 --> 00:17.080
in simulated data and in real data, it is now time for you to do some of the work.

00:17.640 --> 00:25.760
So in this code challenge, we will take a step back in terms of granularity and depth of investigation,

00:25.880 --> 00:33.960
and just focus on the characteristics of the autoencoder results over different layers in a GPT model.

00:34.600 --> 00:41.480
In particular, we are not going to worry about selecting and interpreting a specific component.

00:42.000 --> 00:49.080
Instead, we will explore how the token activation densities change inside the model.

00:49.600 --> 00:56.840
You're also going to directly import some data from Wikipedia, not by copying and pasting as I did

00:56.840 --> 01:02.150
in the previous video, but instead by importing the HTML directly.

01:02.470 --> 01:05.390
I think you're really going to enjoy this code challenge.

01:05.550 --> 01:06.550
So let's begin.

01:07.270 --> 01:12.430
You can start exercise one by importing GPT two small.

01:12.990 --> 01:20.830
We're using the small version of the model to help us develop our code without taking out too much computation

01:20.830 --> 01:21.270
time.

01:21.590 --> 01:27.590
And then at the end of this code challenge, we're going to see whether we can reproduce some of the

01:27.590 --> 01:30.630
results using the large version of the model.

01:31.150 --> 01:32.830
Anyway, worry about that later.

01:32.830 --> 01:34.710
So for now, exercise one.

01:34.910 --> 01:37.790
Import the small version of GPT two.

01:38.390 --> 01:47.190
Next, implant hooks to get activations from the MLP expansion neuron in all of the transformer blocks.

01:47.790 --> 01:54.910
The data that we will use here to train the autoencoder models will come from this Wikipedia page on

01:54.910 --> 01:55.670
LEDs.

01:56.590 --> 02:01.990
Now when you look at a web page rendered in your browser, it looks like this.

02:02.030 --> 02:05.870
You see lots of text links, maybe pictures, and so on.

02:06.230 --> 02:10.230
But behind the scenes, the web page really looks like this.

02:10.830 --> 02:16.310
So this is the HTML source code for this Wikipedia page.

02:16.870 --> 02:23.030
There's a lot going on here, and if you are familiar with HTML then I'm sure you'll recognize lots

02:23.030 --> 02:23.910
of these tags.

02:24.510 --> 02:27.270
If you're not familiar with HTML, then don't worry about it.

02:27.270 --> 02:28.270
That's not important.

02:28.630 --> 02:36.470
What is important is that when you import a website into Python, you do not get the rendered text like

02:36.510 --> 02:38.070
what you see in a browser.

02:38.190 --> 02:42.350
Instead, this is the text that Python imports.

02:42.830 --> 02:51.390
So what I want you to do here is import the entire HTML page for the LED lights from Wikipedia into

02:51.390 --> 02:54.790
Python using the requests library.

02:55.150 --> 03:00.740
That's the same library that we've been using to import books from Gutenberg.org.

03:01.660 --> 03:06.700
Then you can search through the text in Python to find the text.

03:06.860 --> 03:13.900
M-w body content with dashes in between so m-w body content.

03:14.380 --> 03:20.700
There is exactly one character string in this entire file that matches that text.

03:20.700 --> 03:26.540
Exactly, and that is more or less where the text of the Wikipedia page begins.

03:26.660 --> 03:31.620
And you can see that that appears after around a thousand lines of code.

03:32.140 --> 03:37.300
Of course, you're still going to have all of the HTML and CSS tags.

03:37.300 --> 03:44.420
That's totally fine, but this is where you want to begin the text that you will use to train the autoencoder

03:44.420 --> 03:45.060
model.

03:45.660 --> 03:51.660
The next thing to do is search for this tag ID equals references.

03:51.700 --> 03:53.700
Make sure you get the quotes in there.

03:54.220 --> 04:00.370
Now this is the start of the references section on this wiki page.

04:00.850 --> 04:11.690
So the text before my body content and uh, after id equals references that you can completely get rid

04:11.730 --> 04:11.970
of.

04:12.250 --> 04:18.330
And all of the text in between those tags is what you store, what you save, and that's what you will

04:18.370 --> 04:20.850
tokenize and run through the model.

04:21.930 --> 04:28.330
You should find that there are a little over a half million characters in the total web page, and a

04:28.330 --> 04:33.290
little over 160,000 that you will use for training.

04:33.610 --> 04:37.490
Now, to be clear here, I counted the total number of characters.

04:37.490 --> 04:39.850
This is not the number of tokens.

04:40.290 --> 04:42.330
So then you can tokenize the text.

04:42.370 --> 04:50.210
I think there was around 50,000, 52,000 maybe tokens from this 163,000 characters.

04:50.890 --> 04:56.330
Then you can find and print out the 30 most common tokens.

04:56.770 --> 05:03.610
It's probably not that surprising that all of the most common tokens ended up being little bits of HTML

05:03.610 --> 05:04.090
code.

05:05.050 --> 05:13.810
Next, create a batch of ten by 1024, so ten sequences of 1024 tokens.

05:13.930 --> 05:20.570
You can make these non-overlapping sequences from the tokenized wiki text here.

05:20.970 --> 05:22.770
And uh, yeah, you should.

05:22.810 --> 05:26.570
That should cover around 10,000 tokens in total.

05:26.610 --> 05:31.530
That's like 20% of the actual text that we have imported here.

05:32.090 --> 05:40.330
And then the final thing to do for exercise one is run a forward pass and get all of the activations

05:40.330 --> 05:44.210
from the MLP neurons that you have hooked here.

05:45.050 --> 05:51.090
So quite a bit of work for exercise one, but I hope you find this a nice review of working with real

05:51.130 --> 05:52.690
text data from the web.

05:53.170 --> 05:56.440
Okay, so now you should pause the video and get to coding.

05:56.600 --> 05:58.440
And now I will switch to Python.

05:59.400 --> 06:01.840
Here are the libraries that we will use.

06:01.840 --> 06:07.960
And let's see here I'm importing GPT two and pushing it to the GPU.

06:08.960 --> 06:10.280
And let's see.

06:10.280 --> 06:15.200
So it's actually not so necessary to push the language model to the GPU.

06:15.240 --> 06:19.520
We definitely want the auto encoder to be on the GPU.

06:19.520 --> 06:24.480
But since we're using the GPU anyway, we might as well use it for all of the models.

06:25.080 --> 06:29.560
Okay, here I have the function to get the hooks from the MLP layers.

06:29.560 --> 06:30.920
This is nothing new here.

06:30.920 --> 06:32.200
I'm not going to talk about this.

06:32.480 --> 06:32.840
Okay?

06:32.880 --> 06:39.200
Here I'm using the requests library I'm getting from this URL over here.

06:39.200 --> 06:43.840
Here I'm just counting the total number of characters in this text.

06:43.840 --> 06:48.640
And here is where I find the tag m w body content.

06:48.760 --> 06:51.840
And then I set the text to be equal to itself.

06:51.960 --> 06:54.950
But from that index up to the end.

06:55.190 --> 07:00.430
So this is removing the beginning part of the HTML files like the first.

07:00.470 --> 07:02.030
Thousand lines of code or so.

07:02.350 --> 07:10.430
And then here I'm also overwriting the text variable with itself from the beginning up to wherever this

07:10.430 --> 07:11.190
happens to be.

07:11.230 --> 07:13.190
ID equals references.

07:13.590 --> 07:19.470
So you can see I'm just literally throwing out I'm completely overwriting the variable twice.

07:19.470 --> 07:23.830
So we completely lose the information before this tag.

07:23.830 --> 07:25.230
And after this tag.

07:25.390 --> 07:26.190
Totally fine.

07:26.670 --> 07:26.910
Okay.

07:26.950 --> 07:30.390
And then yeah, that's these are the number of characters that we get.

07:31.270 --> 07:33.430
And then here we get 52,000.

07:33.470 --> 07:34.550
I remember it correctly.

07:34.550 --> 07:42.350
I knew it was somewhere around 52,093 tokens from these 160,000 characters.

07:42.830 --> 07:43.110
Okay.

07:43.150 --> 07:47.110
And then this code you've also seen several times earlier in the course.

07:47.110 --> 07:48.990
This is just out of curiosity.

07:48.990 --> 07:53.590
I wanted to see what a lot of these tokens actually looked like.

07:53.630 --> 07:59.030
And by the way, you know, you can also dump out the text of course, and see what that looks like.

07:59.030 --> 08:03.950
So a lot of HTML tags and also a lot of let me open this here.

08:04.430 --> 08:08.830
If you look through this, yeah, you will see some actual text as well.

08:08.830 --> 08:14.990
So the use of turtle safe lighting, LEDs that emit only at narrow portions of the visible spectrum.

08:14.990 --> 08:15.390
ET cetera.

08:15.390 --> 08:15.990
ET cetera.

08:16.270 --> 08:17.710
I am I don't need this.

08:17.750 --> 08:19.110
Let me clear all that out.

08:19.470 --> 08:20.790
Okay, so where are we?

08:20.830 --> 08:22.670
We are here.

08:23.230 --> 08:23.590
Okay.

08:23.630 --> 08:23.870
Yeah.

08:23.870 --> 08:26.230
So now I'm just finding the unique tokens.

08:26.230 --> 08:27.870
Returning counts equals true.

08:27.870 --> 08:29.670
And just printing out the top 30.

08:29.710 --> 08:34.990
This is just out of curiosity to see that of these top 30, you know, there's no.

08:35.510 --> 08:39.670
Okay, so there is a but there's no the there probably is a period.

08:39.670 --> 08:40.790
There's a return.

08:40.950 --> 08:42.630
So a new line character.

08:42.750 --> 08:46.390
And yeah a lot of this is just HTML coding stuff.

08:47.510 --> 08:47.790
Okay.

08:47.830 --> 08:49.940
So here I create a batch of tokens.

08:49.940 --> 08:59.020
I'm getting the first 10,000 and 240 tokens and then reshaping that to ten by 1024, pushing it to the

08:59.020 --> 09:04.380
GPU and just confirming the size and then running that through the model.

09:04.740 --> 09:06.900
We don't care about the outputs from the model.

09:06.900 --> 09:10.260
We really just want the MLP activations.

09:11.540 --> 09:13.780
Now for exercise two.

09:14.260 --> 09:19.140
The goal here is to write code to create an auto encoder.

09:19.580 --> 09:23.740
The auto encoder itself is created in a class.

09:23.780 --> 09:30.780
It's specified in a class, and you should write a function that will create a fresh instance of the

09:30.780 --> 09:32.340
autoencoder model.

09:32.740 --> 09:41.140
Now, the reason why you want this in a Python function is that you are going to create and train a

09:41.140 --> 09:44.660
new autoencoder for each layer in the model.

09:45.260 --> 09:52.970
So here you see the function I call it create the sparse autoencoder and it takes one input, which

09:52.970 --> 09:56.690
is the number of units in the latent layer.

09:56.690 --> 10:00.850
So the number of latent components that the model will estimate.

10:01.290 --> 10:08.530
Otherwise, the rest of this class definition is the same as the model that I showed in the code in

10:08.530 --> 10:10.010
the previous video.

10:10.530 --> 10:17.370
Now that means that you could actually solve this part of the code challenge quickly and easily, simply

10:17.370 --> 10:23.250
by copying and pasting from the previous video and then just indenting, you know, just doing a tab

10:23.250 --> 10:26.970
indent to embed it inside a Python function.

10:27.730 --> 10:34.290
However, I do encourage you to challenge yourself a little bit and write at least some of this code

10:34.290 --> 10:35.130
on your own.

10:35.690 --> 10:41.290
One idea, for example, is to have a quick glance through the code from the previous video to give

10:41.290 --> 10:47.970
you a reminder, and then you put that code away, and then you can write this one again As you like.

10:48.210 --> 10:54.690
Anyway, creating this function and this class is the first part of exercise two.

10:55.290 --> 11:03.450
The second part of exercise two is to create an instance of the model, move it to the GPU, run some

11:03.490 --> 11:07.970
tokens through the model just to make sure that everything works and you have no errors.

11:08.610 --> 11:16.570
You can use A2X expansion for the latent layer, which means that there will be around 6000 latent components

11:16.730 --> 11:18.330
that are estimated.

11:18.930 --> 11:24.930
Now, when you push some activations data through the model, you will get two outputs.

11:24.930 --> 11:26.210
That's what you see here.

11:26.370 --> 11:31.050
So this is what I called the instance of this autoencoder model.

11:31.970 --> 11:33.370
Push some data through.

11:33.370 --> 11:36.410
And we got two outputs here in a tuple.

11:36.690 --> 11:41.290
Now this first element here this is the final output of the model.

11:41.290 --> 11:43.650
That's what you use for the loss function.

11:43.770 --> 11:49.680
And after lots of training This should be pretty similar to the input data here.

11:50.040 --> 11:53.240
Now the second output here is this tensor.

11:53.600 --> 12:02.280
Now part of your work for this exercise is making sure that you understand what this tensor is, where

12:02.280 --> 12:06.240
it comes from and why there are lots of zeros in here.

12:06.680 --> 12:12.720
Now, to be clear, you're not always going to get zeros in exactly the same places as what you see

12:12.720 --> 12:14.880
in this screenshot on your screen.

12:15.320 --> 12:21.680
This is a randomly initialized and pre-trained model, so you're not going to see the same numbers here.

12:21.680 --> 12:27.760
But uh, I think you are going to see lots of zeros in your output.

12:27.760 --> 12:34.080
And your goal here is to explain why that is the case where those zeros come from.

12:34.880 --> 12:38.520
Of course, I will discuss this in more detail when I switch to code.

12:38.520 --> 12:43.240
But first you can switch to code and work your way through this exercise.

12:44.280 --> 12:48.910
I am not going to discuss this part of the code.

12:48.910 --> 12:53.070
It's literally the same as in the previous video.

12:53.070 --> 13:00.230
Really, the only difference is that I tab indented it so that it would go inside this function that

13:00.230 --> 13:07.110
I can just call here, but otherwise all the internal workings and so on are the same as in the previous

13:07.110 --> 13:07.470
video.

13:07.470 --> 13:12.990
So I've already explained that you can see at the end of this function I am returning the autoencoder

13:13.030 --> 13:13.510
model.

13:13.510 --> 13:21.350
So I create a fresh instance, push it to the GPU and then send that to the output of the function.

13:21.790 --> 13:24.350
So here is where I'm testing with some data.

13:24.390 --> 13:29.910
Here I create a fresh instance of the model and I input A2X expansion.

13:30.150 --> 13:32.750
This line of code is actually redundant.

13:33.070 --> 13:37.590
I guess I just forgot when I wrote this that I also have this out here.

13:37.630 --> 13:40.510
Anyway, it doesn't matter, you can push the GPU twice.

13:40.550 --> 13:41.550
It's fine.

13:41.910 --> 13:45.310
Okay, so now here we get the two activations.

13:45.310 --> 13:47.470
So what are these two tensors?

13:47.750 --> 13:50.910
The first tensor is, uh, let me see.

13:50.910 --> 13:53.350
So when we call the model, let me go back here.

13:53.590 --> 13:58.950
When I call the model like this, then I'm pushing the data.

13:58.990 --> 14:01.830
This is the MLP activations here.

14:02.030 --> 14:04.830
I'm pushing the data through the model.

14:05.030 --> 14:08.430
So that calls the forward method in the model here.

14:08.750 --> 14:15.830
So we have the input data x we it passes through the encoder and then the non-linear activation function.

14:15.830 --> 14:19.550
And that gives us the activations in the latent layer.

14:19.550 --> 14:23.870
So these are all the latent components that the model is estimating.

14:24.190 --> 14:32.150
I then sparsify them as I explained in the previous video, and give that as the second output.

14:32.150 --> 14:41.350
And then the first output is this variable y, which is the latent code times the transpose of the autoencoder

14:41.350 --> 14:44.180
weights, which is the decoding layer.

14:44.580 --> 14:51.860
Also, as I discussed in the previous video, if you have lots of data and lots of training time, then

14:51.900 --> 14:54.900
you might want this to be its own linear layer.

14:55.180 --> 15:02.300
And then you would push it through self dot decoder instead of multiplying the sparse activations by

15:02.620 --> 15:06.380
the by the decoder by the encoder transpose.

15:06.580 --> 15:06.980
Okay.

15:07.020 --> 15:14.100
So the second output of the forward method is the sparse latent activation.

15:14.260 --> 15:16.460
And that is this over here.

15:16.580 --> 15:19.380
So now the question is why are there so many zeros here.

15:19.460 --> 15:20.940
Because it's been sparsified.

15:21.180 --> 15:24.980
All of the largest activations are preserved.

15:25.020 --> 15:26.980
Those are passed through their untouched.

15:26.980 --> 15:32.940
And all of the weaker activations and the activations end up getting zeroed out.

15:32.940 --> 15:36.540
That is imposed by the sparsity constraint.

15:38.540 --> 15:42.490
I'm sure that after doing all of this preparatory work.

15:42.530 --> 15:47.450
You are very excited to actually do some analyses and train these models.

15:47.770 --> 15:52.690
Lucky for you, that is exactly the goal of exercise three.

15:53.330 --> 15:59.210
So here you see a big four loop over all of the layers or transformer blocks.

15:59.690 --> 16:08.610
So start by grabbing the activations from this layer and make sure that they are of size tokens by MLP

16:08.770 --> 16:09.370
neurons.

16:09.370 --> 16:10.690
Or here I call it hidden.

16:11.130 --> 16:19.010
Now also, as you know from lots of previous videos, the very first token in a sequence is often weird.

16:19.010 --> 16:24.770
It can elicit an unusual and extreme or outlier set of activations.

16:24.850 --> 16:29.010
So here you also want to remove the first token.

16:29.010 --> 16:36.530
And that means that this the size of x here is going to be 219 instead of 220.

16:36.730 --> 16:36.970
Yeah.

16:37.010 --> 16:39.920
Because you're going to exclude the very first token.

16:40.640 --> 16:46.920
Next, create a fresh instance of an untrained random model.

16:46.920 --> 16:49.080
So randomly initialized untrained model.

16:49.400 --> 16:52.080
And then you're ready to actually train the model.

16:52.080 --> 16:53.440
So you do the forward pass.

16:53.440 --> 16:59.800
You do back prop I don't specify here in this screenshot how many epochs to train.

17:00.720 --> 17:04.240
I used 75 in the solutions file.

17:04.480 --> 17:09.800
Now in the previous video we used 100, but there we trained only one layer.

17:09.800 --> 17:12.680
Here we're going to train all of the layers.

17:12.680 --> 17:19.720
So I think it's a good idea to reduce the number of training epochs a little bit just to save some some

17:19.720 --> 17:20.800
computation time.

17:21.280 --> 17:27.760
Anyway, after you do all of the training, you can grab and store the final loss.

17:27.760 --> 17:33.120
So just the loss from the very last training epoch and store that for each layer.

17:33.160 --> 17:37.760
This is something that we will visualize in the next exercise.

17:38.000 --> 17:45.600
And then you can do one more run with the same data to get the final latent activations.

17:45.600 --> 17:47.480
We don't care about this final output.

17:47.480 --> 17:52.640
We want the latent activations from the, uh, the fully trained model.

17:52.800 --> 17:58.240
Remember that we are running everything on the GPU, so we're training on the GPU.

17:58.600 --> 18:02.600
So you will certainly need to bring the activations back to the CPU.

18:03.320 --> 18:09.480
As you might remember from previous videos, I personally find numpy to be a little bit more comfortable

18:09.480 --> 18:13.120
and familiar for data analysis compared to PyTorch.

18:13.440 --> 18:19.720
So I also converted these back to numpy, the activations back to numpy.

18:20.000 --> 18:23.440
But if you want to leave them in PyTorch, then that's also great.

18:23.440 --> 18:28.480
All of the subsequent analyses can certainly be done in PyTorch as well.

18:29.000 --> 18:29.280
Okay.

18:29.320 --> 18:33.200
And then the rest of this code is just some analyses to perform.

18:33.200 --> 18:41.670
So here I calculate the density per Component, that is, the percent of non-zero activations for each

18:41.670 --> 18:42.950
latent component.

18:43.430 --> 18:47.190
Now keep in mind that this is actually a matrix here.

18:47.590 --> 18:53.710
And we have 6000 components and 219 or whatever tokens.

18:54.070 --> 19:00.230
But here I'm only storing one value of the density per layer.

19:00.350 --> 19:02.430
So per transformer block.

19:03.270 --> 19:07.950
So I will let you think about how you want to deal with that.

19:07.990 --> 19:09.470
Of course I'll show what I did.

19:09.470 --> 19:13.390
But my solution is not the only correct approach to take.

19:13.390 --> 19:16.030
So if you want to do something else, that's fine.

19:16.070 --> 19:22.910
The important thing is that at the end of this for loop over all the layers, we want to have some estimate

19:23.070 --> 19:26.430
of the density of the components from each layer.

19:27.110 --> 19:32.470
And yeah, of course, if you prefer to think about sparsity instead of density, you can also turn

19:32.470 --> 19:35.470
this analysis around and store sparsity.

19:35.510 --> 19:36.460
Totally fine.

19:37.300 --> 19:44.540
Okay, the next piece of information that we want to store over all the layers is the average activation

19:44.540 --> 19:47.500
magnitudes from the non-zero activations.

19:47.820 --> 19:51.460
Again, I have one way of solving this.

19:51.460 --> 19:52.980
You can come up with a different way.

19:52.980 --> 19:54.060
It's totally fine.

19:54.380 --> 19:58.860
You can also just copy and modify code from the previous video.

19:59.380 --> 20:07.020
And then finally I also calculated this measure of density by activation, which is a product of the

20:07.020 --> 20:13.220
density as a percentage and the activation as the numerical value of the activation.

20:13.660 --> 20:19.020
And you can see that also here, which is different from what I did in the previous video.

20:19.020 --> 20:24.900
Here, I min max scaled these two measures before combining them to make sure that they are on the same

20:24.900 --> 20:26.100
numerical scale.

20:26.820 --> 20:33.660
Okay, so this for loop is all of the analyses that we are doing in this code challenge.

20:33.820 --> 20:39.940
Once you have all of this set up on the GPU, this should take around three minutes or so for a GPT

20:39.980 --> 20:41.100
two small.

20:41.700 --> 20:47.820
So therefore, if you like, you could add some lines of code at the bottom of this for loop to track

20:47.820 --> 20:48.820
the progress.

20:48.820 --> 20:52.020
For example by printing a message at the end of each layer.

20:52.020 --> 20:53.140
Calculation.

20:53.260 --> 20:59.340
Maybe you want to import the Tqdm library to have a progress bar updating up here.

20:59.940 --> 21:03.860
Anyway, I hope you enjoy working through this exercise.

21:03.980 --> 21:06.940
And now I will switch to code and show my solution.

21:08.100 --> 21:14.300
Here is where I initialize the variables that I will use to store the results for each layer.

21:15.020 --> 21:17.060
Loop over 75 epochs.

21:17.260 --> 21:25.500
Uh, here I am, looping over all of the layers, grabbing the hidden activations from the MLP neurons.

21:25.740 --> 21:26.860
And let me see.

21:26.860 --> 21:30.300
Here is where I'm ignoring the first token.

21:30.460 --> 21:38.130
So all of the batches, all of the neurons, all the activations and only from token one through the

21:38.130 --> 21:38.490
end.

21:38.850 --> 21:45.370
Again, that's just because the first token in any sequence tends to elicit a weird and rather extreme

21:45.370 --> 21:46.970
pattern of activations.

21:46.970 --> 21:53.610
So in this case, I just chose to ignore it completely by not even putting it into the analysis in the

21:53.610 --> 21:54.330
first place.

21:55.010 --> 21:59.730
Okay, so then yeah, here we need a fresh instance of the model.

21:59.930 --> 22:01.770
Again, this line is redundant.

22:01.770 --> 22:08.530
I suspect what happened is I had this and then after writing all the code, I decided it would be better

22:08.530 --> 22:09.130
to put it here.

22:09.130 --> 22:10.890
And I forgot to delete these other lines.

22:10.890 --> 22:11.970
Anyway, it doesn't matter.

22:12.450 --> 22:18.330
Okay, so then, because for each layer, we need a new instance of the model.

22:18.330 --> 22:25.130
That means we also need to re-initialize the optimizer and re-initialize the loss function.

22:25.130 --> 22:27.970
And of course that has to go to the GPU as well.

22:28.530 --> 22:30.170
Forward pass and backprop.

22:30.210 --> 22:31.890
Pretty straightforward stuff.

22:32.050 --> 22:40.120
It looks like here I am using the, uh, the main loss function and the L1 loss and not the decorrelation

22:40.120 --> 22:40.600
loss.

22:40.880 --> 22:45.320
To be honest, uh, I wrote this code a little while ago, and I don't remember if there was a very

22:45.320 --> 22:49.640
specific reason for me leaving out the decorrelation loss.

22:49.800 --> 22:51.000
Uh, but it's fine.

22:51.000 --> 22:53.280
You can put it in if you like, or leave it out.

22:53.480 --> 23:00.320
We're actually not going to interpret any of the components in this code challenge, so I think it's

23:00.320 --> 23:05.360
fine if the components are a little bit more correlated than they otherwise would be.

23:05.920 --> 23:06.240
Okay.

23:06.280 --> 23:10.960
So now here, uh, the for loop over all of the training is finished.

23:11.080 --> 23:14.120
And now here is where I grab the final loss.

23:14.120 --> 23:21.800
So this is going to give me an estimate of how well the autoencoder was able to match the final output

23:21.800 --> 23:28.000
to the original input, which are the, uh, token activations from MLP.

23:28.440 --> 23:28.680
Okay.

23:28.720 --> 23:35.800
And then the final run to get the latent activations from the trained model, bring it back to the CPU

23:35.800 --> 23:39.080
and convert it into NumPy from PyTorch.

23:40.080 --> 23:45.520
And then all of this code you have seen exactly from the previous video.

23:45.520 --> 23:47.240
I didn't really change this at all.

23:47.480 --> 23:56.280
And that means that this variable here is a vector with one element for each of the latent components.

23:56.640 --> 24:02.440
And then in the previous video, you know, we were sorting this and generating the text heat map based

24:02.440 --> 24:04.240
on values of density.

24:04.520 --> 24:12.880
Here I'm just averaging over all of the components to get one really aggregated measure of density and

24:12.880 --> 24:18.480
storing that as this variable here, which is going to have one value per layer.

24:18.640 --> 24:25.200
So yeah, I'm really aggregating over a lot of information in this code challenge in this exercise.

24:25.720 --> 24:26.000
Okay.

24:26.040 --> 24:32.750
Again, this is also the same thing that you have seen in the previous video and then, uh, yeah,

24:32.790 --> 24:41.110
here I am calculating this product of the density by the non-zero activations and that I store for this

24:41.110 --> 24:45.670
particular layer, this part you have seen in the previous video.

24:45.670 --> 24:48.990
And what I changed here was this min max scaling.

24:49.030 --> 24:55.630
Basically, I wanted to explore whether we would get more interesting, more interpretable results if

24:55.630 --> 24:59.590
the two measures were on exactly the same numerical scale.

24:59.710 --> 25:02.790
And so that's why I min max scaled them.

25:03.030 --> 25:03.230
Okay.

25:03.230 --> 25:04.470
And then I just print out here.

25:04.470 --> 25:07.670
So here we see this finished layer one through 12.

25:08.070 --> 25:12.270
And yeah for GPT two small this whole thing takes 2 to 3 minutes.

25:13.270 --> 25:18.830
Now for exercise four you don't need to do any new analyses here.

25:18.830 --> 25:26.590
You're just going to write code to visualize the three key analyses that you stored per layer from the

25:26.590 --> 25:28.390
previous exercise.

25:28.900 --> 25:34.100
So over here we have the final loss from the autoencoder model per layer.

25:34.380 --> 25:41.500
And that will tell you basically how easy it was for the autoencoder model to reconstruct the activations.

25:41.940 --> 25:49.420
The lower the loss the better the reconstruction, which also means the less complex the representations

25:49.420 --> 25:50.740
are overall.

25:51.380 --> 25:58.620
Here in the middle panel we have the density per layer as percent of non-zero activations.

25:59.100 --> 26:06.260
As I mentioned in the in the instructions for exercise three, if you chose to set up the analyses in

26:06.260 --> 26:11.380
a slightly different way, obviously you might have slightly different graphs, different y axes, and

26:11.380 --> 26:12.140
that's great.

26:12.420 --> 26:20.540
The important thing is that however you set up the analysis, it should be implemented exactly identically

26:20.700 --> 26:22.140
for each layer.

26:22.660 --> 26:28.900
So that will allow you to interpret the changes over the different layers you don't want to have, you

26:28.900 --> 26:32.500
know, different analyses or different parameters for the different layers.

26:32.500 --> 26:34.780
All that should be fixed anyway.

26:34.820 --> 26:39.940
Finally, over here we have this density by activation normalized score.

26:40.700 --> 26:42.740
So that's it for exercise four.

26:43.500 --> 26:44.620
Not much coding.

26:44.700 --> 26:49.420
Maybe just having some fun with matplotlib and seeing what you make of the results.

26:49.740 --> 26:53.220
And now I will switch to code and discuss some of these plots.

26:55.020 --> 26:56.580
Really not much to the code here.

26:56.580 --> 27:03.380
I'm just plotting the final loss, the latent density and the density by activations scores.

27:03.780 --> 27:10.020
So what we see here, I think that most important and most interesting result is that there is a roughly

27:10.500 --> 27:15.420
monotonic, roughly linear increase in the density per layer.

27:15.540 --> 27:22.860
So as we get deeper into the model, further away from the initial token embeddings and towards the

27:22.860 --> 27:28.250
prediction for the next token, And then we see the density gets higher.

27:28.250 --> 27:33.050
So the model is just having activations to more and more tokens.

27:33.370 --> 27:40.170
And that's probably due to the model needing to incorporate information across a broader context.

27:40.210 --> 27:47.010
So it really needs to activate a larger number of words in order to make a valid and useful prediction

27:47.010 --> 27:48.610
of the next token.

27:50.970 --> 27:55.090
This is the final exercise in this code challenge.

27:55.450 --> 28:01.210
The good news is that it will take you extremely little time to write the code for this exercise.

28:01.570 --> 28:07.250
But the bad news is that it will take a relatively long time to actually run this exercise.

28:07.770 --> 28:14.330
All I want you to do is rerun the entire code file from scratch from the top, and you just make one

28:14.330 --> 28:22.090
tiny change, which is to replace the GPT two small model with GPT two large.

28:22.210 --> 28:24.650
So just the large version of the same model.

28:25.200 --> 28:28.200
And then you can watch 40 TikTok videos.

28:28.440 --> 28:30.560
And why should you watch 40 TikToks?

28:30.560 --> 28:36.320
Because apparently TikTok users watch two videos per minute on average.

28:36.360 --> 28:38.120
According to ChatGPT.

28:38.800 --> 28:44.640
So running through the code with the large version of this model takes approximately 20 minutes on the

28:44.640 --> 28:48.680
GPU in that for loop over the different layers.

28:49.240 --> 28:55.840
Of course, that time can vary depending on the sizes that you specify for the latent components.

28:56.120 --> 28:59.240
And of course on the number of training iterations.

28:59.720 --> 29:04.280
Now the goal here is not just to have some excuse to watch TikToks.

29:04.440 --> 29:09.640
The goal is to make sure that, first of all, the code still works without any errors, even though

29:09.640 --> 29:11.520
all of the sizes will differ.

29:12.040 --> 29:20.360
And secondly, to check whether the three plots at the end so from exercise four are qualitatively similar

29:20.480 --> 29:24.000
for the small and the large versions of this model.

29:25.240 --> 29:30.880
I hope you enjoy watching your TikToks or whatever else you do while waiting for the analysis to run.

29:32.360 --> 29:34.440
These three panels up here.

29:34.440 --> 29:37.600
So this figure up here, this is from exercise four.

29:37.640 --> 29:41.080
That was from the small version of GPT two.

29:41.360 --> 29:45.120
And here this screenshot here is from the large version.

29:45.120 --> 29:47.400
Remember that has 36 layers.

29:47.760 --> 29:50.160
Okay so I have a couple of remarks to make.

29:50.320 --> 29:58.080
One is that this density by activation metric turns out to be not terribly insightful or interesting.

29:58.080 --> 30:04.600
It doesn't seem to do a whole lot that we can make sense out of, either for the GPT two small version

30:04.600 --> 30:05.800
or for the large version.

30:05.800 --> 30:11.840
So, you know, always good to explore different ways of analyzing and characterizing data.

30:12.040 --> 30:18.320
But if you really don't see any interesting changes, then you might want to just drop that measure,

30:18.760 --> 30:19.040
okay.

30:19.080 --> 30:25.590
And then what's really interesting and highly consistent across the two different models is the increase

30:25.590 --> 30:26.550
in density.

30:26.550 --> 30:34.350
Or you could alternatively say the decrease in sparsity of the activations across all the layers.

30:34.350 --> 30:41.390
So as we get deeper into the model, that was already quite apparent in the small version and super

30:41.390 --> 30:47.710
clear, this is a really, really crisp finding in the large version of this model.

30:47.870 --> 30:54.590
Even this kind of plateau shape feature here maybe is consistent over here, although it's tricky to

30:54.830 --> 31:00.350
overinterpret a subtle feature like that with only 12 data points up here.

31:00.670 --> 31:07.070
Also pretty interesting to see that the final loss was lowest for these middle layers where there wasn't

31:07.070 --> 31:07.950
really this change.

31:07.950 --> 31:16.150
So it seems like around here is where there's some shift in the regime of the MLP calculations in the

31:16.150 --> 31:21.740
transformer blocks between lower density regimes and higher density regimes.

31:21.740 --> 31:27.940
And somewhere here in the middle, this is where actually the activation patterns were less complex

31:27.940 --> 31:32.780
because they were easier for a autoencoder model to match.

31:32.780 --> 31:35.140
That's basically what this score indicates.

31:35.140 --> 31:42.540
Lower numbers here means that the model had an easier time reconstructing the activation patterns.

31:43.700 --> 31:50.020
Of course, we didn't do any in-depth or fine grained analyses in this code challenge.

31:50.740 --> 31:58.820
An interesting feature of complex systems is that you cannot understand them purely from coarse grained

31:58.820 --> 32:03.180
and abstracted analyses, like what we did in this code challenge.

32:03.380 --> 32:11.220
Nor can you understand them completely by just doing extremely detailed, fine grained investigations

32:11.820 --> 32:13.380
in actual research.

32:13.540 --> 32:20.260
It is useful to be able to see both the forest and the trees, so you can understand the large scale

32:20.260 --> 32:21.140
patterns.

32:21.140 --> 32:25.500
And then you can try to investigate some of the fine grained details.

32:26.100 --> 32:32.500
I think the most fruitful approaches are going to come from trying to analyze model characteristics

32:32.500 --> 32:38.580
and dynamics at multiple spatial scales, so small scales and large scales.

32:38.700 --> 32:45.220
Zooming in to, you know, individual MLP neurons and maybe small circuits of a collection or a little

32:45.260 --> 32:52.980
ensemble of neurons, and then zooming out and seeing how that relates to dynamics happening one layer

32:52.980 --> 32:53.780
at a time.

32:54.060 --> 32:59.940
In other words, looking at the data at multiple scales and seeing how the different scales interact

32:59.940 --> 33:00.660
with each other.

33:00.660 --> 33:02.780
So multi-scale investigations.

33:03.420 --> 33:11.100
All that said, in general, what we did see is that the activation density increases as we go deeper

33:11.100 --> 33:12.140
into the model.

33:12.300 --> 33:18.020
And that characteristic seems pretty consistent across the two versions of GPT two.

WEBVTT

00:01.480 --> 00:03.640
Welcome back to this code challenge.

00:03.640 --> 00:07.320
I hope you are feeling refreshed and ready to continue.

00:07.640 --> 00:10.600
So here we are with exercise five.

00:10.920 --> 00:18.400
The goal here is to run some analyses over all of the layers in the model, using the two functions

00:18.400 --> 00:21.120
that you wrote in the previous two exercises.

00:21.640 --> 00:28.600
And also I have some additional code in here to test for generalization performance.

00:29.200 --> 00:34.280
So here you can see I am looping over all of the layers in the model.

00:34.280 --> 00:36.240
So all the transformer blocks.

00:36.480 --> 00:43.120
I'm also using this Tqdm library to get a progress bar to print out while it's running.

00:44.000 --> 00:52.520
Okay, so the first step is to compress the training data using the PCA function from exercise three.

00:53.160 --> 00:59.320
Then you run through the source separation function that you wrote for exercise four.

00:59.560 --> 01:02.000
So so far this is not that interesting.

01:02.000 --> 01:08.800
Just make sure you are assigning meaningful variable names to the outputs of these two functions.

01:09.280 --> 01:14.200
The next thing to do inside this loop is to run some t tests.

01:14.200 --> 01:22.600
So you want to calculate some t tests on for the him versus the her activations in the GED based on

01:22.600 --> 01:27.240
him greater than her and also for the GED from her greater than him.

01:27.640 --> 01:36.560
This is running a statistical test on exactly the data that we have used to create the eigenvectors.

01:37.040 --> 01:40.120
That means that there is a risk of overfitting here.

01:40.600 --> 01:48.280
Now, on the one hand, it is not trivial that the t tests will all be statistically significant, but

01:48.680 --> 01:57.040
it certainly is true that these tests here will be biased towards a larger magnitude T value, because

01:57.040 --> 02:01.430
they are the data that we used to define the eigenvectors.

02:02.230 --> 02:08.430
Also keep in mind that because of the sign indeterminacy of the eigenvectors, we don't really care

02:08.430 --> 02:11.070
about the sign of the t values here.

02:11.390 --> 02:15.150
We just want to know their magnitude and their p value.

02:15.510 --> 02:21.790
And that's why I just store the magnitude or the absolute value of the t value here.

02:22.230 --> 02:28.790
You can also see that I'm organizing all of the results in this for loop into a big matrix that I call

02:28.950 --> 02:30.550
GED results.

02:30.990 --> 02:34.910
So that has a first dimension of the layers in the model.

02:35.030 --> 02:38.790
So it's going to be 12 for GPT two small.

02:39.470 --> 02:45.310
The second dimension is for the particular analysis that I'm storing these results from.

02:45.790 --> 02:48.230
It turns out that that's going to be seven.

02:48.230 --> 02:54.670
So there's going to be seven different sets of analyses that I am collecting data from.

02:54.950 --> 02:57.070
And this is the first one here.

02:57.070 --> 03:05.110
So the t test for the generalized eigendecomposition for him versus her, and then testing for activations

03:05.110 --> 03:08.950
in him sentences or him targets versus her targets.

03:09.750 --> 03:09.990
Okay.

03:10.030 --> 03:15.470
And then the third dimension in this results matrix here is for the statistic value.

03:15.470 --> 03:19.350
So the t value and the p value okay.

03:19.390 --> 03:22.230
So that is the explanation of this t test.

03:22.590 --> 03:26.110
This t test down here is exactly the same.

03:26.470 --> 03:29.470
But this is from the GED vectors.

03:29.670 --> 03:38.190
When you use the her activations for s and the Him activations for R, and I put those results into

03:38.190 --> 03:41.390
the index one of this matrix.

03:42.550 --> 03:48.190
And then over here we have the correlation up of the covariance patterns.

03:48.190 --> 03:52.430
That's what I showed you how to calculate in the previous video.

03:52.430 --> 03:54.710
So that's the R variable over there.

03:55.350 --> 04:03.270
Now you also want to store the top two eigenvalues for the two geds, and also store the number of components

04:03.270 --> 04:07.910
that it took to reach 99% of variance explained.

04:08.750 --> 04:18.070
Now, so far, all of these analyses that I have described, those are all entirely about the training

04:18.070 --> 04:18.390
set.

04:18.430 --> 04:20.750
We haven't yet touched the test set.

04:21.270 --> 04:26.470
And so that is the goal for the rest of this for loop over the different layers.

04:27.070 --> 04:27.430
Okay.

04:27.470 --> 04:30.710
So out of sample evaluation, that's what I call this here.

04:31.310 --> 04:37.670
So first thing to do is get all of the target activations from the test set.

04:37.990 --> 04:45.230
Now this is easier to do compared to the training set, because we created this data set such that the

04:45.230 --> 04:48.430
target is always in the same position.

04:48.430 --> 04:51.910
So the sixth position which is index five.

04:52.270 --> 04:55.190
So you don't need a for loop here like you did with the training set.

04:55.830 --> 05:03.100
But then you want to compress the data down to a lower dimensional space using the PCA vectors that

05:03.100 --> 05:04.860
you got from the training set.

05:05.380 --> 05:11.420
And when you get those lower dimensional data, you can then project them onto the vectors.

05:11.460 --> 05:16.780
Again these are the vectors that you calculated from the training set.

05:17.180 --> 05:18.100
This is important.

05:18.100 --> 05:23.940
You do not run another generalized eigendecomposition on the test set.

05:23.980 --> 05:31.460
You've already run the decomposition on the training set, and now you're just applying those same vectors

05:31.460 --> 05:38.100
to the test set without recalculating any of the eigenvalues or the eigenvectors.

05:38.660 --> 05:38.980
Okay.

05:39.020 --> 05:45.100
So once you have that the last thing to do is to run some t tests on the test set.

05:45.500 --> 05:49.260
These are exactly the same t tests that you ran up here.

05:49.660 --> 05:53.300
But yeah here you're using the test set instead of the train set.

05:54.460 --> 06:01.900
There's going to be a lot of results to visualize, and I want to get to most of those visualizations

06:01.900 --> 06:03.500
in the next exercise.

06:03.860 --> 06:12.300
Here in this exercise, you should just do one visualization, which is to plot the number of components

06:12.300 --> 06:18.340
it took to explain 99% of the variability for each layer of the model.

06:19.060 --> 06:24.660
Uh, of course, I'm not going to show you what those results will look like, but this is actually

06:24.660 --> 06:31.340
literally a replication of an analysis that we did earlier in the course about effective dimensionality.

06:31.860 --> 06:35.540
And you might remember that we got results that looked like this.

06:35.820 --> 06:37.460
Now you can ignore the red squares.

06:37.460 --> 06:40.140
That was for the randomly shuffled tokens.

06:40.540 --> 06:48.060
What we found pretty consistently in this previous video was that the number of components that it took

06:48.060 --> 06:54.980
to reach 99% of the variability generally increased as we got later into the model.

06:54.980 --> 06:56.650
So deeper into the model.

06:57.290 --> 07:00.650
Now this result here was for the hidden states.

07:00.650 --> 07:03.970
That is the final output of the transformer layer.

07:04.410 --> 07:10.290
And here in this exercise in this video you are working with the MLP expansion neurons.

07:10.450 --> 07:13.170
So it's completely different parts of the transformer.

07:13.610 --> 07:20.130
That means it is certainly not trivial that you will reproduce this result over here.

07:20.450 --> 07:26.770
I'm just showing this figure here just to remind you of the pattern of results that we discovered earlier

07:26.770 --> 07:27.650
in the course.

07:28.530 --> 07:28.770
Okay.

07:28.810 --> 07:31.610
I hope you enjoy working through this exercise.

07:31.650 --> 07:33.890
Now you can pause the video and get to work.

07:34.170 --> 07:40.370
And of course, I'm going to do what I always do here, which is switch to Python and discuss my solution.

07:41.370 --> 07:45.130
So here is where I initialize this results matrix.

07:45.210 --> 07:48.210
So results for every layer of the model.

07:48.650 --> 07:51.690
There's going to be seven sets of results that I will store.

07:51.850 --> 07:58.530
And then the two is for the test statistic and the p value loop over all of the layers.

07:58.530 --> 08:05.330
Here I get the PCA, I call the PCA function, get all of those results, call the get function, get

08:05.370 --> 08:07.170
all of those results.

08:07.530 --> 08:09.730
And then here I run the test set.

08:09.730 --> 08:12.690
So here is the uh sorry the T test.

08:12.690 --> 08:16.090
So here is the T test for, uh, GED.

08:16.130 --> 08:23.810
So the projections onto the him, uh, eigenvector for the him sentences and the projections onto the

08:23.810 --> 08:26.650
him eigenvector for the, her sentences.

08:26.770 --> 08:34.090
So this is what we designed the GED to maximize him versus her okay.

08:34.130 --> 08:37.450
So I stored that and I get the t value and the p value.

08:37.650 --> 08:39.250
And then I repeat that code.

08:39.250 --> 08:42.490
Except it's now for the her analysis.

08:42.610 --> 08:48.930
Uh so the eigenvector that maximizes activations for her greater than for him.

08:49.210 --> 08:55.930
So now we actually expect this one these activations to be large and these activations to be relatively

08:55.930 --> 08:56.450
small.

08:56.570 --> 08:58.850
Whereas here we expect the opposite pattern.

08:59.210 --> 08:59.410
Okay.

08:59.450 --> 09:02.130
And then again I'm storing the t value.

09:02.130 --> 09:05.330
And the p value here is the R value.

09:05.370 --> 09:10.530
This is the correlation between the pattern for him and the pattern for her.

09:10.890 --> 09:13.730
So the statistic the correlation coefficient.

09:13.730 --> 09:16.810
And here we're just getting the the magnitude of that.

09:16.810 --> 09:18.330
And then its p value.

09:18.730 --> 09:21.690
And this is the top eigen value.

09:21.690 --> 09:26.410
So eigen value for the Him analysis and for the her analysis.

09:26.410 --> 09:32.210
And this is from the PCA with both the him and the her targets mixed together.

09:32.330 --> 09:36.890
How many components does it take to get 99% of the variance explained.

09:38.010 --> 09:38.290
Okay.

09:38.330 --> 09:41.010
And then yeah here's the out-of-sample stuff.

09:41.010 --> 09:47.010
So here we are looking at the test results here I'm getting all of the activations compressing them

09:47.010 --> 09:49.330
down to 60.

09:49.570 --> 09:53.330
In the previous two videos ago it was 63 dimensions.

09:53.960 --> 09:58.920
it will be somewhere between 5060 or so, depending on the layer.

09:59.560 --> 10:07.760
But yeah, anyway, projecting them down to a small number of dimensions relative to 3072 for the GPT

10:07.800 --> 10:08.600
two small.

10:08.760 --> 10:16.280
And now I am projecting those reduced dimensional activations onto the GED vectors.

10:16.320 --> 10:23.720
Again, I am using the, uh, the eigenvectors, the analyses that I developed from the training set

10:23.960 --> 10:31.320
and applying those vectors to the test set, and then exactly the same T tests exactly the same code

10:31.520 --> 10:33.320
as I ran up here.

10:33.720 --> 10:34.560
They're just yeah.

10:34.600 --> 10:38.240
The main difference is they're stored in these different locations.

10:38.680 --> 10:44.160
Uh, here you can see I'm skipping three and four, because when I first wrote this code, I was only

10:44.160 --> 10:45.040
storing these.

10:45.320 --> 10:47.160
Uh, I was not storing these.

10:47.200 --> 10:47.920
And then.

10:47.960 --> 10:53.400
Yeah, as I was developing the code further, I thought about also including these results.

10:53.400 --> 10:58.560
But then I was kind of too lazy to change the code below the visualization code.

10:59.520 --> 11:03.520
So therefore I decided to give these a later indexing.

11:03.800 --> 11:06.600
Anyway, now I will run this code.

11:06.600 --> 11:08.160
It's actually quite fast.

11:08.720 --> 11:16.400
If you are curious, what you can do is go back to the PCA function from exercise three.

11:16.880 --> 11:23.800
You can try commenting the scikit learn PCA code and uncommenting the manual code.

11:24.200 --> 11:29.040
You will find that the results are identical, but it takes a lot longer.

11:29.400 --> 11:31.880
Okay, so let me see.

11:32.640 --> 11:33.560
Okay, so here we are.

11:33.560 --> 11:42.000
And then yeah, I'm just visualizing, uh, this uh, penultimate part of the model, which is the dimensionality.

11:42.000 --> 11:47.720
So the effective dimensionality over the layers, that is the number of components it takes to account

11:47.720 --> 11:52.920
for 99% of the variability within each layer, monotonically.

11:52.960 --> 12:01.040
Okay, not perfectly monotonically, but generally increases as we go deeper in the model from 45 components

12:01.040 --> 12:02.120
up to around.

12:02.160 --> 12:05.880
This is probably 63 components here at the very end.

12:06.240 --> 12:12.400
Pretty neat to see that this finding replicates an analysis we did earlier in the course, even though

12:12.400 --> 12:18.080
that is not the primary goal of this this analysis here.

12:19.160 --> 12:22.560
We are now done with all of the analyses.

12:22.760 --> 12:28.520
And so exercise six is just about exploring some of the visualizations.

12:28.960 --> 12:32.760
There are three plots that you will make for exercise six.

12:33.560 --> 12:38.520
The first one is this over here it is four sets of t values.

12:39.120 --> 12:46.040
And these come from the t tests for the training data and the t tests for the test set data.

12:46.600 --> 12:49.040
I'm showing the results here for layer one.

12:49.520 --> 12:54.070
And of course it will be interesting to see what they look like for the rest of the layers.

12:54.470 --> 12:56.990
In particular, the test set results.

12:56.990 --> 13:03.750
Remember that there is no overfitting on the test set, so that's really the best evaluation of whether

13:03.750 --> 13:12.270
the generalized eigendecomposition has uncovered something meaningful in the internal structure in the

13:12.270 --> 13:16.470
model, or whether it just memorized an overfit the training set.

13:17.030 --> 13:22.390
Now, you can use these markers for the statistically significant t tests.

13:22.630 --> 13:29.910
And you can plot red X's wherever the T values were not statistically significant based on the corresponding

13:29.910 --> 13:30.750
p value.

13:31.630 --> 13:38.750
Next is the plot of the correlation between the two patterns from multiplying the eigenvector by the

13:38.750 --> 13:47.030
covariance matrix, and then pre-multiplying that by the PCA vectors to get back into the original MLP

13:47.070 --> 13:54.470
neuron space again because of the sign indeterminacy of the eigenvectors, you can just plot the absolute

13:54.470 --> 13:56.750
value of the correlation here.

13:57.110 --> 14:02.710
Now that gives you a measure of linear relationship and we don't have to worry about the sign.

14:03.470 --> 14:11.630
And finally this plot over here, this is the largest eigenvalue from the two generalized eigendecomposition

14:11.630 --> 14:14.550
is again plotted over the different layers.

14:15.670 --> 14:21.150
Now larger eigenvalues indicate better separability of the components.

14:21.310 --> 14:28.350
So it will be interesting to see whether the separability is related to the depth in the model, and

14:28.350 --> 14:31.310
whether it differs between the two pronouns.

14:32.230 --> 14:38.270
I will have some further discussions and interpretations about these results when I switch to code.

14:38.550 --> 14:44.790
But of course, first you should switch to code and make these visualizations and see what you think.

14:45.710 --> 14:49.510
As a p value threshold here I'm using 0.05 five.

14:49.550 --> 14:53.590
Standard statistical threshold and then dividing by the number of layers.

14:53.590 --> 14:57.830
So this is Bonferroni corrected for the number of layers.

14:58.310 --> 15:00.790
And yeah for the x tick labels.

15:00.990 --> 15:04.150
For some reason I decided to start counting at zero.

15:04.510 --> 15:07.110
Uh, sorry, start counting at one instead of at zero.

15:07.150 --> 15:10.950
I don't remember if there was a particular reason why I chose to do that.

15:10.950 --> 15:15.870
So there's a lot of code here, but when you look through it, you will see that each of these code

15:15.870 --> 15:17.190
lines is very simple.

15:17.750 --> 15:21.190
All I'm doing is plotting the significant results.

15:21.350 --> 15:26.670
Uh, and here the non-significant results for these different analyses.

15:26.670 --> 15:34.190
So the, uh, train him versus her GED, the her versus him GED and so on.

15:34.310 --> 15:38.510
So yeah, it's a lot of lines of code, but it's not a lot of unique lines of code.

15:39.790 --> 15:40.070
Okay.

15:40.110 --> 15:44.990
So basically what we see here is that it's pretty significant everywhere.

15:45.190 --> 15:50.980
So we don't really need to worry too much about interpreting the magnitude of the t values.

15:50.980 --> 15:58.140
The important thing is that for the test data set, we see lots of highly significant differences in

15:58.140 --> 16:00.820
the activation all throughout the model.

16:00.940 --> 16:08.940
So this tells us that when we trained the model on the these artificial him.her data sets sentences

16:09.140 --> 16:16.260
that the generalized Eigendecomposition was really picking up on some meaningful internal structure

16:16.260 --> 16:24.780
about the way that these models represent or process the pronouns him versus the pronouns her, because

16:24.780 --> 16:32.260
that was reproduced significantly robustly when correcting for multiple comparisons over 12 different

16:32.260 --> 16:36.300
layers in this entirely separate test set.

16:37.420 --> 16:37.700
Okay.

16:37.740 --> 16:40.980
And then here we see the correlation between the two patterns.

16:40.980 --> 16:43.300
And they are pretty strong early in the model.

16:43.300 --> 16:49.980
And then the correlation plummets down close to zero as we get deeper into the model.

16:50.460 --> 16:52.420
And why might this be the case?

16:52.540 --> 16:56.140
Well, keep in mind that earlier in the model.

16:56.180 --> 17:04.140
So earlier in the transformer stream, the model is is focusing on processing that particular token.

17:04.500 --> 17:07.660
And him and her those two tokens are quite similar.

17:07.860 --> 17:10.780
The pronouns are often used in very similar ways.

17:10.780 --> 17:14.420
So there's a lot of similarities between those two tokens.

17:14.780 --> 17:23.100
But as we get deeper into the model, the model transitions from processing the actual token, the current

17:23.140 --> 17:27.060
token, to making a prediction about the subsequent token.

17:27.180 --> 17:32.580
And so, yeah, those patterns just seem to be kind of doing their own thing as we shift later into

17:32.580 --> 17:33.220
the model.

17:33.820 --> 17:34.300
Okay.

17:34.340 --> 17:36.940
And then this is the final plot.

17:37.260 --> 17:41.220
Here we are looking at the maximum eigen values.

17:41.500 --> 17:43.020
And this is quite interesting.

17:43.020 --> 17:47.660
So in general they don't really change that much for the him versus her?

17:47.940 --> 17:48.660
Maybe.

17:48.660 --> 17:53.300
Let me see if they, uh, let me comment this one out just to see.

17:54.420 --> 17:54.780
Yeah.

17:54.820 --> 17:55.140
Okay.

17:55.180 --> 18:02.140
I mean, maybe the separability decreases a little bit as we get later into the model, but, uh, yeah,

18:02.180 --> 18:08.460
we can discuss that later when, uh, when we also look forward, uh, in exercise seven, when we do

18:08.460 --> 18:10.940
some other replication based analyses.

18:11.140 --> 18:19.860
But certainly the most striking thing about this particular result is this massive increase in the separability

18:19.900 --> 18:21.460
for her versus him.

18:21.660 --> 18:24.700
Once we get to the second half of the model.

18:25.020 --> 18:30.980
To be honest, I do not have a great clear interpretation of this finding.

18:31.220 --> 18:38.460
Uh, and yeah, I mean, it's probably just related to differences in text for him versus her, but,

18:38.460 --> 18:42.700
uh, yeah, I don't know what to make of this, but it is certainly quite, uh, robust.

18:42.740 --> 18:48.290
Now, the thing about the eigenvalue magnitude, the raw eigenvalue itself is.

18:48.290 --> 18:55.050
It's a little bit difficult to interpret on its own the raw value, because it depends on the scale

18:55.050 --> 18:56.050
of the data.

18:56.450 --> 19:04.610
So therefore I'm going to modify this analysis a little bit to take not the top eigenvalue, but the

19:04.610 --> 19:08.690
ratio between the first and the second eigenvalues.

19:08.810 --> 19:13.690
And let me show you in the graph what that looks like, what I mean by that.

19:14.130 --> 19:21.770
That was actually the analysis, the visualization from the previous video.

19:21.810 --> 19:24.410
What I two videos ago I introduced this method.

19:24.810 --> 19:25.090
Okay.

19:25.130 --> 19:32.610
So here the idea is that we want to look at the ratio of this value here to this value over here.

19:32.770 --> 19:35.930
And the ratio of this number here to this number here.

19:36.050 --> 19:43.610
And the idea is that the larger that ratio the more the top component is really clearly separable from

19:43.610 --> 19:45.610
the rest of the distribution.

19:47.210 --> 19:55.170
So to do that, I'm going back to the code from exercise four, going down here to where I export the

19:55.170 --> 19:57.210
two top eigenvalues.

19:57.450 --> 20:01.970
And then I'm going to say the first one divided by the second one.

20:02.290 --> 20:03.330
And same thing here.

20:03.330 --> 20:07.770
So the first one divided by let's see that's divided by the second one.

20:08.170 --> 20:11.690
I did not explicitly tell you to do this in the instructions.

20:11.690 --> 20:14.130
So I do not expect you to have done this.

20:14.170 --> 20:18.890
It's just something that I am curious about to evaluate.

20:18.890 --> 20:24.010
So it's a little bit of a cleaner way of looking at this result down here.

20:24.290 --> 20:29.970
So now instead of just looking at the top eigenvalue, we're going to look at the ratio of the first

20:29.970 --> 20:31.730
to the second eigenvalue.

20:32.130 --> 20:34.050
And you see that this is still preserved.

20:34.050 --> 20:40.530
So it does seem like the Herve versus him eigendecomposition is more separable.

20:40.530 --> 20:47.880
The model had an easier time separating her versus him pronouns compared to him versus her pronouns.

20:47.880 --> 20:53.600
And that effect is really just pronounced as we get deeper into the transformer.

20:54.880 --> 20:55.560
Don't worry.

20:55.600 --> 21:01.520
Exercise seven involves no new analyses and no new visualizations.

21:02.160 --> 21:06.480
All I want you to do here is run the code for a different model.

21:06.880 --> 21:11.520
So we ran all of the code so far on GPT two small.

21:12.040 --> 21:15.720
And now you can pick another variant to test.

21:15.840 --> 21:19.680
You don't need to do this in like a for loop over all the models.

21:19.680 --> 21:25.800
You can just manually change the name of the model when you import it, and then rerun the entire code

21:25.800 --> 21:26.640
from scratch.

21:27.280 --> 21:34.880
I am going to rerun it using Excel, so using the largest GPT two variant that we have access to.

21:35.240 --> 21:36.960
And yeah, you can use that one.

21:36.960 --> 21:39.520
You can also use the large or the medium variant.

21:39.920 --> 21:43.560
Now here when you rerun the code I want you to check for two things.

21:43.680 --> 21:47.320
First of all, just make sure that the code file still works.

21:47.520 --> 21:54.720
If you get any errors simply by changing the model size without changing anything else, then that means

21:54.720 --> 21:58.480
that you have hard coded some variables and you should change that.

21:59.200 --> 22:06.160
Secondly, you're looking for reproducibility of the key findings that you saw in the small version

22:06.160 --> 22:07.000
of the model.

22:07.840 --> 22:13.160
Now you don't need to check for reproducibility super rigorously or quantitatively.

22:13.200 --> 22:19.280
You just want to look at the plots and make a qualitative judgment of whether the results follow the

22:19.280 --> 22:20.280
same pattern.

22:21.200 --> 22:23.920
Okay, so I hope you enjoy working through this one.

22:23.920 --> 22:25.640
And now I will switch to code.

22:26.560 --> 22:30.640
So here is the code from the very beginning of the code file.

22:30.640 --> 22:33.560
And all I'm going to do here is write in dash x.

22:33.920 --> 22:37.440
So now I'm going to load in the Excel version of the model.

22:37.440 --> 22:39.120
And then I'm going to run all.

22:39.160 --> 22:40.760
So run the entire script.

22:40.920 --> 22:41.520
Again.

22:41.720 --> 22:44.520
Most of this code runs through pretty quickly.

22:45.160 --> 22:51.160
This is the longest thing to do here is just wait a few minutes for all of the tensors to load in.

22:52.360 --> 23:00.040
All right, so with the Excel version of this model, it takes the model quite some time to import.

23:00.040 --> 23:05.920
And then the other thing that takes a long time is processing the initial tokens.

23:05.920 --> 23:12.560
This one you can see that took actually three minutes compared to seven seconds with GPT two small.

23:13.160 --> 23:20.000
This is the sort of thing that this is probably also 1 or 2 seconds on the GPU, but I kept it on the

23:20.000 --> 23:23.080
CPU and it was a few minutes.

23:23.120 --> 23:30.440
Okay, so now we see the number of components to explain 99% of the variability generally continues

23:30.440 --> 23:32.920
to increase as we go deeper in the model.

23:32.920 --> 23:39.280
So that is a qualitative replication of what we saw in the small version of the model.

23:39.840 --> 23:42.320
And now here we have these results here.

23:42.440 --> 23:43.030
And again.

23:43.070 --> 23:46.390
The most important things to look for are the t tests.

23:46.550 --> 23:53.270
For the test set so that separate set that we did not use to create the.

23:53.310 --> 23:55.550
Eigenvectors or the PCA vectors.

23:55.950 --> 23:58.190
And you see it's not a perfect replication.

23:58.190 --> 24:04.950
So here we do get a couple of layers where the T value is not statistically significant.

24:05.590 --> 24:09.470
But overall we do see some nice replications.

24:09.470 --> 24:12.430
So reproducibility in the generalization.

24:13.510 --> 24:19.110
This one is also quite consistent with what we observed in the small version of the model, that the

24:19.110 --> 24:27.190
patterns in the covariance matrices are more robustly correlated early in the model, and then the they

24:27.190 --> 24:30.310
become decoupled as we go later in the model.

24:30.350 --> 24:34.430
I'm not sure what's going on in here, but it's just one unusual data point.

24:34.430 --> 24:38.350
So I would not overinterpret this particular point here.

24:38.950 --> 24:39.230
Okay.

24:39.270 --> 24:48.830
And then here this is also an interesting replication of what we saw in the previous model in GPT two

24:48.870 --> 24:49.390
small.

24:49.590 --> 24:57.990
So here we see that the separability for the her greater than him is larger than the separability for

24:57.990 --> 24:59.470
him greater than her.

24:59.790 --> 25:07.030
It's not an exact reproduction of the finding, which we don't expect anyway because these are different

25:07.030 --> 25:09.230
models, very different numbers of parameters.

25:09.550 --> 25:14.230
But overall, we do see a fairly similar set of results here.

25:14.270 --> 25:17.990
Now this one is with the ratio because I didn't change that code.

25:18.150 --> 25:24.630
If you would look at the overall separability just from the top model, that would be I'm going to do

25:24.630 --> 25:29.630
that now as well, because I'm also pretty curious about how that's going to look.

25:30.150 --> 25:36.670
So all I had to do is delete that ratio here and then run this code again over the layers.

25:36.670 --> 25:41.550
This takes, I think, 30s or so, maybe less than that.

25:41.550 --> 25:46.830
And then we can reproduce this plot down here.

25:47.190 --> 25:48.710
Okay, so I'm going to run this plot again.

25:48.710 --> 25:55.390
But just to look at this again so it peaks the difference between the two eigendecomposition peaks at

25:55.390 --> 25:57.590
around a third of the way into the model.

25:57.710 --> 26:00.750
And then the separability kind of steadily declines from there.

26:01.030 --> 26:07.030
So then when we run it again, just looking at the top eigenvalue, we see an overall similar pattern.

26:07.030 --> 26:09.310
And I'm not sure if this is meaningful here.

26:09.310 --> 26:10.830
This flip in the difference.

26:10.830 --> 26:12.790
But anyway that's what it looks like.

26:13.910 --> 26:21.510
I hope you agree that generalized eigendecomposition is a bit more involved than some of the other analyses

26:21.510 --> 26:28.630
we've done in this course, but it is a pretty interesting approach that has a lot of flexibility for

26:28.630 --> 26:31.910
targeted adaptations and applications.

26:32.150 --> 26:40.150
But I am sure you agree with me on this first point here, which is that complex systems are very difficult

26:40.150 --> 26:47.420
to analyze and understand conceptually in terms of code and in terms of interpreting results.

26:47.980 --> 26:55.220
And yeah, I think it's also very nice to see when analyses have replications built into them.

26:55.700 --> 27:02.500
So that is to say, we did not explicitly have the goal of replicating the effective dimensionality

27:02.500 --> 27:03.220
results.

27:03.540 --> 27:08.580
And in fact, when I made those lectures back there, I was not thinking that I would have separate

27:08.580 --> 27:12.100
lectures that would accidentally reproduce those findings.

27:12.540 --> 27:19.020
It just kind of happened because when you start to work with a lot of analyses, you start to appreciate

27:19.020 --> 27:23.300
the convergence of foundational analysis techniques.

27:23.580 --> 27:29.460
These are methods that just keep appearing in lots of analysis applications.

27:29.860 --> 27:35.820
And Eigendecomposition is certainly one of those foundational methods that keeps popping up.

27:35.940 --> 27:38.980
The more time you spend doing data science.

WEBVTT

00:02.280 --> 00:08.520
In this and the next videos you will learn about a technique called sparse probing.

00:08.840 --> 00:16.240
It's actually just an application of logistic regression with a particular type of regularization that

00:16.240 --> 00:20.600
pushes a lot of the regression coefficients to be zero.

00:21.400 --> 00:26.520
I will start this video by going through some of the math of L1 regularization.

00:26.960 --> 00:34.400
The key regularization parameter and the difference between logistic regression for sparse probing versus

00:34.400 --> 00:40.280
logistic regression to identify category tuning in individual neurons.

00:40.280 --> 00:43.800
And that is how we used regression a few sections ago.

00:44.400 --> 00:50.880
And then I'll switch to code and show you how to implement and evaluate sparse logistic regression.

00:51.680 --> 00:58.040
So this is a screenshot from a video earlier in the course where you learned about classification using

00:58.040 --> 00:59.710
logistic Regression.

00:59.870 --> 01:06.910
You don't necessarily need to rewatch that video, but I'm showing it here in hopes that it will reactivate

01:06.910 --> 01:10.870
some happy memories of learning about logistic regression.

01:11.390 --> 01:19.910
The most important equation from that video was this one here, that the regression is designed to predict

01:20.110 --> 01:25.230
the probability that a given token belongs to one of two categories.

01:25.750 --> 01:29.670
So this was the general form of a logistic regression.

01:29.910 --> 01:36.830
In that classification application there were only two beta values beta zero which is the intercept,

01:36.830 --> 01:41.910
and beta one which was the coefficient for the token activation.

01:42.590 --> 01:50.310
Now we are going to extend this analysis and have multiple betas corresponding to k neurons.

01:51.110 --> 01:59.770
So in this slide I will describe a few other differences between using logistic regression for classification

01:59.770 --> 02:01.970
versus for sparse probing.

02:02.450 --> 02:04.570
Let's start with the overall goal.

02:05.010 --> 02:12.610
With a logistic classifier, the goal is to determine whether a given neuron or dimension, or just

02:12.970 --> 02:20.290
some variable more generally, is tuned to a particular category, which means that its activation is

02:20.290 --> 02:27.010
reliably greater for, I don't know, like tokens related to cars versus tokens related to trucks.

02:28.090 --> 02:32.690
So that's for one variable, like one neuron with sparse probing.

02:32.690 --> 02:38.570
We want to see if we can identify a group or a cluster or circuit.

02:38.570 --> 02:42.970
Here I use the word ensemble of neurons that together.

02:42.970 --> 02:49.130
So they're a collective patterns of activations predict a particular category label.

02:49.610 --> 02:53.050
So here this is not about one neuron individually.

02:53.170 --> 03:01.360
It's about a group of neurons that are together statistically able to predict a category in terms of

03:01.360 --> 03:03.320
the mechanism and the math.

03:03.360 --> 03:07.320
Actually, both of these approaches are based on logistic regression.

03:07.640 --> 03:11.240
So the statistics mechanism is actually the same.

03:11.400 --> 03:18.200
It's just that with a classifier the logistic regression only sees one neuron at a time.

03:18.480 --> 03:24.960
Whereas with the sparse probe the regression incorporates activations from lots of neurons at the same

03:24.960 --> 03:25.480
time.

03:26.640 --> 03:29.280
And finally we have regularization.

03:29.760 --> 03:36.120
And actually it's not really the case that you're not allowed to have any regularization on a logistic

03:36.120 --> 03:37.000
classifier.

03:37.320 --> 03:44.520
Sometimes people add some like soft regularization, but in general, logistic classifiers don't need

03:44.720 --> 03:53.160
as strong regularization compared with sparse probing, where we add a stronger amount of L1 regularization.

03:53.240 --> 03:55.300
And this forces sparsity.

03:55.860 --> 04:02.300
So that means that a lot of the coefficients will be set to zero, and a relatively small number of

04:02.340 --> 04:05.420
coefficients are allowed to be non-zero.

04:06.100 --> 04:08.460
So what is this regularization?

04:08.460 --> 04:12.460
How does it work and how do we find the beta coefficients?

04:13.140 --> 04:16.780
That is the answer I'm going to discuss in this slide.

04:17.220 --> 04:21.180
So here again we see the equation of the logistic regression.

04:21.500 --> 04:30.060
The way it works for a sparse probe is that for k neurons let's say 3000 MLP expansion neurons you would

04:30.060 --> 04:32.300
have k equals 3000.

04:32.340 --> 04:34.740
I think it's technically 3072.

04:35.220 --> 04:39.860
So that means that you get a beta coefficient for every individual neuron.

04:40.260 --> 04:47.460
And the idea is to figure out how to define a linear weighted combination of the activations from all

04:47.460 --> 04:55.170
the neurons, such that their weighted combination best predicts whether a token is from category one

04:55.170 --> 04:58.210
or category two or whatever, two different categories.

04:58.730 --> 05:02.490
And how do we actually get these beta coefficients?

05:03.090 --> 05:07.530
That comes from defining a loss function that looks like this.

05:07.970 --> 05:12.290
I hope you recognize this as binary cross entropy.

05:13.170 --> 05:18.210
Why is the category which is either 0 or 1 so dummy coded?

05:18.210 --> 05:20.250
Or you could call it one hot encoded.

05:20.450 --> 05:23.010
And p is the probability.

05:23.010 --> 05:24.930
So that's this term over here.

05:25.290 --> 05:29.330
And I corresponds to a given presentation of a token.

05:29.490 --> 05:34.610
And n is the total number of tokens in the analysis in the training set.

05:35.050 --> 05:41.770
So this is the general loss function for a logistic regression when you want to regularize the regression.

05:41.810 --> 05:46.090
You have another loss function which looks something like this.

05:46.090 --> 05:47.330
So it's R here.

05:47.330 --> 05:50.970
That is for the regularizer loss function.

05:51.330 --> 05:57.230
And here you sum over all of the absolute values of the beta coefficients.

05:58.030 --> 06:06.070
The impact of this regularizer term is that a lot of the coefficients will go to zero, and a relatively

06:06.070 --> 06:09.070
small number of them will remain non-zero.

06:09.470 --> 06:11.230
So this is called L1.

06:11.470 --> 06:17.550
And actually if you square these beta coefficients up here instead of taking the absolute value, that

06:17.550 --> 06:19.950
would be an L2 regularizer.

06:20.430 --> 06:24.870
I have talked earlier in the course about the difference between L1 and L2.

06:25.150 --> 06:28.790
We're not using L2 regularization here in this video.

06:28.830 --> 06:33.670
I just wanted to mention the similarity between L1 and L2.

06:34.150 --> 06:42.670
Anyway, so now the idea is that we want to find the set of betas that minimizes the loss function and

06:42.670 --> 06:45.750
also the regularizer loss function.

06:46.190 --> 06:51.660
And then we have this term lambda here which is just a scalar, so it's just a number.

06:51.900 --> 06:59.460
And the idea is that when lambda is larger, then the model puts more weight on imposing sparsity.

06:59.860 --> 07:06.980
And of course when lambda is zero then you just have the normal logistic regression with no regularization.

07:08.020 --> 07:16.180
By the way, when you add an L1 regularizer that's also called a lasso regression with an L2 regularizer,

07:16.180 --> 07:17.860
that's called ridge regression.

07:18.060 --> 07:22.660
And if you have both L1 and L2 then that's called elasticnet.

07:23.140 --> 07:25.780
Uh, you don't need to know those terms for this course.

07:25.780 --> 07:27.980
I just mentioned it here, FYI.

07:28.380 --> 07:34.220
Anyway, uh, okay, but I haven't actually explained how we really find these beta coefficients.

07:34.380 --> 07:37.140
All I've done is tell you what the loss function is.

07:37.900 --> 07:45.140
It turns out that gradient descent is a great way to find the betas that optimize this equation.

07:45.140 --> 07:52.080
Over here for logistic regression, there is a slight variant of gradient descent compared to what you're

07:52.080 --> 07:57.960
familiar with for deep learning, and that is called stochastic average gradient or Saga.

07:58.400 --> 08:03.480
You will see in the code that I use this abbreviation as a parameter.

08:04.120 --> 08:07.800
Also something slightly confusing and like super.

08:07.800 --> 08:14.640
Really a lot annoying about scikit learn is that it doesn't allow you to specify the lambda parameter

08:14.640 --> 08:15.720
directly.

08:15.920 --> 08:21.200
Instead, it takes a different parameter called C, which is just one over lambda.

08:21.560 --> 08:28.480
So if you want to have a smaller lambda, which would mean less regularization, then you would use

08:28.480 --> 08:30.600
a larger value of c.

08:31.600 --> 08:33.760
So yeah that's what I write over here.

08:33.880 --> 08:42.040
So higher lambda, which also means a smaller value of c leads to more coefficients being equal to zero.

08:43.160 --> 08:43.520
Okay.

08:43.560 --> 08:47.070
So this is all very like abstract theoretical math.

08:47.350 --> 08:50.910
Let me now tell you what we're going to do in the code demo.

08:51.990 --> 09:00.190
I will use GPT two small for this demo and hook the MLP expansion neurons from one layer.

09:01.030 --> 09:04.950
These will actually be the post non-linearity activations.

09:05.150 --> 09:10.030
And as you know, sometimes in this course I use the pre activations.

09:10.470 --> 09:16.030
It's a subtle distinction between the two and which one to use for different analyses.

09:16.430 --> 09:24.510
But basically the pre activations tell you about the representation of the data in the high dimensional

09:24.510 --> 09:31.030
space where the model, the MLP layer is trying to learn how to transform the input data.

09:31.030 --> 09:39.150
So the token embeddings vectors into a space that is more amenable to linear separation, whereas the

09:39.190 --> 09:46.490
post non-linear activations correspond to the filtered information that gets passed along to the next

09:46.490 --> 09:53.130
layer, which of course in this case corresponds to the MLP adjustments to the embeddings vectors in

09:53.130 --> 09:54.330
the residual stream.

09:55.450 --> 10:00.370
Anyway, I'm going to import some data from the fine web dataset.

10:00.410 --> 10:07.330
You will remember that we used this dataset earlier in the course in pre-training and fine tuning the

10:07.330 --> 10:08.530
GPT models.

10:09.050 --> 10:16.170
So for this code demo, the goal will be to see if we can find a circuit, an ensemble of MLP neurons

10:16.690 --> 10:24.370
based on a sparse logistic probe that is tuned to definite versus indefinite articles.

10:24.930 --> 10:29.610
In particular, I will focus on the words the and an.

10:30.130 --> 10:35.210
This is used for definite articles like when we refer to a specific.

10:35.250 --> 10:40.770
I don't know, chair or like a specific chocolate bar that someone was saving for themselves, but I

10:40.770 --> 10:41.810
accidentally ate.

10:42.130 --> 10:50.200
Where has the article N is used generally to refer to any given chocolate bar, not the one that I actually

10:50.200 --> 10:50.640
stole.

10:51.320 --> 10:58.400
Anyway, we do have to be a little mindful when picking out these tokens from the text, because the

10:58.400 --> 11:03.800
tokens for th and an can also be the start of a word.

11:03.800 --> 11:09.960
So a subword at the beginning or in the middle of another word instead of actual words themselves.

11:10.360 --> 11:17.000
Now I will let you ponder what you might need to do in the code to make sure that you are getting these

11:17.000 --> 11:24.200
two tokens as complete words, and not as just the first token of a multi token word.

11:24.720 --> 11:28.160
Of course, I'll show my solution to that little problem in the code.

11:29.160 --> 11:35.320
Anyway, here you see a few examples of sequences that contain the word the and an.

11:35.320 --> 11:37.040
So here you see the the the.

11:37.080 --> 11:39.200
It's always at the end I put it at the end.

11:39.480 --> 11:42.260
And here is n and n n and so on.

11:42.620 --> 11:49.020
And then I'm going to have a brief tangent to talk a little bit more about spaces before tokens.

11:49.220 --> 11:56.500
This is an issue that has come up often in this course, and it's an easy source of confusion when working

11:56.500 --> 11:58.820
with models and tokenizers.

11:59.060 --> 12:07.460
So therefore I will introduce you to an optional input in the tokenizer which is called add prefix space.

12:07.820 --> 12:10.980
I will show you what this does and what implications it has.

12:11.140 --> 12:13.100
But the take home is quite simple.

12:13.260 --> 12:17.180
It simply adds a space before the text that you input.

12:17.780 --> 12:22.500
Anyway, I'm going to extract 100 of each of these tokens.

12:22.620 --> 12:29.900
And when I push that through the model to get the MLP activations, that will give me activations matrices

12:30.140 --> 12:41.410
of size 100 by 3072, corresponding to the 100 tokens 100 sequences where with the target token extracted

12:41.410 --> 12:47.090
and 3072 neurons in the MLP expansion layer.

12:48.130 --> 12:55.290
Now, before running the analysis, I'm going to split this dataset into a train and test set.

12:55.730 --> 13:02.930
There's going to be 140 samples used for training and 60 samples used for testing.

13:03.490 --> 13:10.370
And what you see here are the distributions of all the activation values for the two target words.

13:10.410 --> 13:13.730
Now this is all the data including the train and the test set.

13:14.130 --> 13:19.930
It's definitely not Gaussian distributed because these are the post Gelu activations.

13:20.330 --> 13:27.530
The pre non-linear activations do tend to look roughly Gaussian, but as you know, the JLo squeezes

13:27.530 --> 13:29.930
down the negative values.

13:30.570 --> 13:37.730
Anyway, the point here is that at this global population level, there doesn't appear to be any really

13:37.730 --> 13:43.390
major differences in activations for the two articles for the and N.

13:43.990 --> 13:50.710
Maybe there's some shift towards smaller activation values for N, but I don't think that's a real reliable

13:50.710 --> 13:51.430
difference.

13:51.870 --> 13:55.110
Anyway, this is pooling over all of the data.

13:55.510 --> 14:00.310
Next what I will do is train a logistic classifier on these data.

14:00.710 --> 14:04.270
And we will see that the performance is literally perfect.

14:04.270 --> 14:12.710
So there is some combination of the 3000 neurons that can perfectly predict whether each target token

14:12.710 --> 14:14.790
is the or n.

14:15.390 --> 14:22.470
Now, that may initially seem impressive, but keep in mind that we are using 3000 features to predict

14:22.510 --> 14:24.870
160 observations.

14:25.470 --> 14:32.310
So it is true that the results that I'm showing here actually come from the test set and not the training

14:32.310 --> 14:32.750
set.

14:33.110 --> 14:36.310
But still, the sample size is just really small.

14:36.750 --> 14:42.980
Anyway, that's fine for our purposes here, because the goal of this demo is to show you how to set

14:43.020 --> 14:49.780
up the code and visualizations for this kind of analysis, and not to do like a full research project

14:49.780 --> 14:56.580
with a sufficient sample size where you would probably want thousands or tens of thousands of tokens.

14:57.700 --> 15:00.220
Anyway, here you can see some of the results.

15:00.340 --> 15:07.020
So based on the value of lambda that I used, we got a density of around 0.4.

15:07.420 --> 15:15.900
So that means that only 13 out of the 3000 neurons actually had a non-zero coefficient.

15:16.340 --> 15:19.540
All the other coefficients were exactly zero.

15:20.140 --> 15:27.860
And basically what this means is that the logistic regression only needed 13 neurons in order to predict

15:27.860 --> 15:32.500
whether a given token is the word, the or the word n.

15:33.140 --> 15:37.200
By the way, density is the opposite of sparsity.

15:37.440 --> 15:43.760
So density of 0.4% means that only 0.4% of the coefficients are non-zero.

15:44.120 --> 15:53.640
And we could equivalently say that the sparsity is 99.6%, which would mean that 99.6% of the coefficients

15:53.640 --> 15:55.280
are equal to zero.

15:56.560 --> 15:59.720
And how about the sign of these coefficients?

15:59.720 --> 16:03.240
What do you think the sign of the coefficients means?

16:03.920 --> 16:12.240
I guess that you guess that the sign corresponds to whether more activations means the, or whether

16:12.240 --> 16:15.680
more activation means that the token was n.

16:16.360 --> 16:21.120
That is a good intuition, and in theory that is the way it should work.

16:21.560 --> 16:29.520
However, with data sets this large, especially with relatively small sample sizes, that's not necessarily

16:29.520 --> 16:30.880
what these results mean.

16:30.880 --> 16:33.760
What the sign of the coefficients indicates.

16:33.760 --> 16:33.790
Gates.

16:34.550 --> 16:39.790
There's something else pernicious going on here, which is called statistical suppression.

16:40.150 --> 16:45.430
I'm going to talk about that term and that phenomenon more in the next video.

16:45.950 --> 16:54.110
But anyway, here you see scatter plots with all 13 of the neurons that had non-zero coefficients and

16:54.150 --> 17:02.070
their activations for each of the 100 of the the tokens and the 100 and tokens.

17:02.470 --> 17:08.950
Now, it's not surprising that there aren't really negative activations here, and a lot of these activations

17:08.950 --> 17:10.910
are just kind of like crunched up.

17:10.910 --> 17:14.230
They're all like bundled up at some value just below zero.

17:14.790 --> 17:18.630
That is the impact of the jlu non-linearity.

17:19.430 --> 17:19.750
Okay.

17:19.790 --> 17:26.790
The last analysis that I will show in this demo is the relationship between sparsity and the lambda

17:26.790 --> 17:27.710
parameter.

17:28.270 --> 17:30.510
Or actually this is showing density here.

17:30.510 --> 17:36.210
But you could just show So sparsity as 100 minus these numbers over here.

17:36.690 --> 17:44.570
So basically all I did was rerun the same logistic regression model with the same data except for the

17:44.570 --> 17:45.770
parameter c.

17:46.370 --> 17:51.690
Now these two plots show exactly the same results from exactly the same models.

17:51.690 --> 17:56.610
But here on the right I'm plotting the value of c on the x axis.

17:56.850 --> 18:04.090
And here on the left, I'm plotting the value of lambda which is one divided by c.

18:04.770 --> 18:12.650
These results are interesting, but really not that surprising when lambda is small, which also means

18:12.650 --> 18:20.650
when c is large, then you have more non-zero coefficients, and when lambda is large, or when c is

18:20.650 --> 18:25.530
small, then you have very few neurons with non-zero coefficients.

18:26.010 --> 18:28.410
And why is this a sensible result?

18:28.810 --> 18:37.040
Because when lambda is closer to zero than the L1 regularizer doesn't really contribute to the overall

18:37.040 --> 18:38.040
loss function.

18:38.480 --> 18:45.280
Whereas when lambda gets bigger, then the optimization gets more weighted by the L1 term.

18:45.840 --> 18:48.680
Anyway, let's switch to Python and have a look.

18:49.920 --> 18:52.800
So here we are with the typical libraries.

18:53.080 --> 18:55.600
And here is some stuff from scikit learn.

18:55.640 --> 18:58.760
So logistic regression I'm using this function.

18:58.800 --> 19:03.880
Train test split to split the data into a train set and a test set.

19:04.160 --> 19:11.320
And this function also to give us the classification performance, which I can already tell you is going

19:11.320 --> 19:12.960
to be extremely good.

19:13.320 --> 19:15.440
Here I'm importing GPT two.

19:15.800 --> 19:18.240
And of course the tokenizer as well.

19:18.400 --> 19:21.440
And here's where I'm hooking all of the neurons.

19:21.440 --> 19:23.560
So you can see I'm looking for MLP.

19:23.680 --> 19:27.000
And then I get this act layer in here.

19:27.000 --> 19:30.230
And this is the post Gelu activations.

19:30.230 --> 19:34.430
So let me show you what part of the model that corresponds to.

19:35.470 --> 19:43.110
So here we are in layer H, the hidden layer, the transformer blocks we are looking at here, the MLP.

19:43.150 --> 19:45.150
That's this part over here.

19:45.350 --> 19:49.750
And now typically I would use the FC the fully connected.

19:49.750 --> 19:51.190
That's the expansion layer.

19:51.350 --> 19:58.590
Or if we are specifically looking for the adjustments to the embeddings vector, that would be the output

19:58.590 --> 19:59.630
of C proj.

19:59.790 --> 20:07.150
And what I am going to look at here is the output of act, and that this is not in the order in which

20:07.150 --> 20:09.590
the operations are applied.

20:09.590 --> 20:17.030
But basically this act here is what goes after the fully connected layer and before the projection.

20:17.390 --> 20:17.710
Okay.

20:17.750 --> 20:23.110
So this will be the jlu activation of the expansion layer.

20:23.270 --> 20:23.550
Okay.

20:23.590 --> 20:24.470
So run that.

20:24.470 --> 20:28.410
And yeah it's kind of arbitrary that I'm just hooking into one layer here.

20:28.410 --> 20:31.570
Of course you can try hooking into different layers.

20:32.010 --> 20:34.010
Okay, so here is the data.

20:34.050 --> 20:38.890
I'm importing the fine web data set, not the full data set.

20:38.930 --> 20:40.490
Of course it's ginormous.

20:40.530 --> 20:48.930
I'm only going to extract the first 500 documents, and I append all of the tokens onto each other.

20:49.250 --> 20:53.250
Here I'm looking for the target tokens, the and n.

20:53.290 --> 21:00.770
And notice here I'm explicitly putting in spaces in front of the target words that I'm looking for.

21:00.930 --> 21:03.530
I will get back to that in a moment.

21:03.890 --> 21:13.730
So in this text, there are 11,000 appearances of the token and 800 appearances of the token N space

21:13.770 --> 21:21.570
an and from all of these, I am going to sample just 100 for each of the target tokens.

21:22.130 --> 21:28.520
These tokens will or these sequences will be 17 tokens long.

21:28.720 --> 21:30.120
And let's see.

21:30.120 --> 21:32.680
So here so here's where I do it okay.

21:32.720 --> 21:36.000
So I'm looping over all of the the tokens.

21:36.000 --> 21:41.720
So all 11,000 of the tokens that match the word uh the character sequence space.

21:43.080 --> 21:50.880
And now here I have a little bit of code to check whether the following token is starts with a space.

21:51.000 --> 21:59.040
And the reason why I'm looking for this is that if we have a word like for example, uh, theology,

21:59.600 --> 22:03.880
let's say this is the token, then it might actually start with space.

22:03.920 --> 22:06.320
The but this is not what we're looking for.

22:06.320 --> 22:13.800
So if space is not an entire word, it's not the word the but it is part of a sub word.

22:13.800 --> 22:17.240
So it's a sub word that's part of a multi token word.

22:17.400 --> 22:22.440
Then this second this subsequent token here is not going to start with a space.

22:22.440 --> 22:25.380
It will start with anything other than a space.

22:26.300 --> 22:27.500
So that's the idea here.

22:27.620 --> 22:33.580
If the next token begins with a space, then we are looking at the and then something else.

22:34.180 --> 22:34.460
Okay.

22:34.500 --> 22:35.340
So that's the idea.

22:35.340 --> 22:42.900
And then we also do not want to run through this entire loop only until we have enough samples and then.

22:43.100 --> 22:43.380
Yeah.

22:43.380 --> 22:49.300
So if these two conditions are true then we store in this row of the batch.

22:49.340 --> 22:55.260
We store the tokens from minus context free to plus context post.

22:55.740 --> 22:55.940
Okay.

22:55.980 --> 23:02.460
And then I repeat exactly the same for n again I want the next token to begin with a space.

23:03.140 --> 23:03.500
Okay.

23:03.540 --> 23:05.540
And then here you see a few examples.

23:05.940 --> 23:06.180
Yeah.

23:06.180 --> 23:07.860
This is not super interesting.

23:07.860 --> 23:12.260
Just to see what some of these sequences actually look like.

23:12.620 --> 23:19.780
Down here is the little tangent just to introduce you to this optional input add prefix space.

23:19.820 --> 23:26.730
I believe this is actually the only time in this entire course that I ever actually use this optional

23:26.730 --> 23:27.210
input.

23:27.210 --> 23:29.690
I think it is better.

23:29.690 --> 23:36.170
I think it's clear to, uh, to have this space explicitly written in here, because then you really

23:36.170 --> 23:38.570
know that there is this space here.

23:38.770 --> 23:45.050
Uh, whereas, yeah, if you have no space in the target word here, but then you add a space like this,

23:45.170 --> 23:48.090
I just find that to be a little bit less intuitive.

23:48.530 --> 23:52.330
And what I'm showing here is that, uh, yeah, you will get different tokens.

23:52.330 --> 23:59.610
So this token is the same as this token, even though this, uh, includes a preceding space, and this

23:59.610 --> 24:02.570
does not include a preceding preceding space.

24:02.810 --> 24:07.610
Because here I said add prefix space equals false, which is the default value.

24:07.810 --> 24:10.850
And here add prefix space equals true.

24:11.290 --> 24:18.250
Again, I find this all to be a little confusing, and therefore my preference is to set add prefix

24:18.250 --> 24:24.630
space to be false, which is the default value and explicitly include this space beforehand.

24:24.670 --> 24:26.190
Okay, so that's just FYI.

24:27.270 --> 24:30.270
Here I'm pushing all the tokens through the model.

24:30.310 --> 24:33.390
These are the values of this activations.

24:33.390 --> 24:38.390
Dictionary will get overwritten here when I push this batch through the model.

24:38.390 --> 24:44.070
So therefore I need to extract this or save it as a separate variable.

24:44.430 --> 24:48.910
And also in this case we are not interested in all of the tokens.

24:48.910 --> 24:55.150
We only want the activations for the target tokens, which is this position here.

24:55.150 --> 25:01.590
So we get all the sequences, the target token, so the token corresponding to the word the and then

25:01.590 --> 25:05.710
all of the MLP expansion neurons post Jlu.

25:05.710 --> 25:07.590
And that's what you see here.

25:08.070 --> 25:08.270
Okay.

25:08.270 --> 25:13.870
And here I'm just showing the histogram of these all the activations over all the neurons.

25:13.990 --> 25:19.990
Honestly I don't think this is super interesting to look at, but it does confirm that we really are

25:20.220 --> 25:28.540
grabbing the post Gelu activations because you do not see a lot of uh, like a tail here going into

25:28.540 --> 25:30.300
the negative activations.

25:30.300 --> 25:36.980
Instead you just see this bunching up this really large, uh, number of activations that have values

25:36.980 --> 25:42.460
just below zero because, yeah, that's just the nature of the, uh, suppression, the weighting of

25:42.460 --> 25:43.820
the Gelu function.

25:44.420 --> 25:52.420
Okay, here I'm combining all of the activations into one matrix called all data and then creating labels.

25:52.420 --> 25:56.620
So it's 100 zeros and 100 ones.

25:56.820 --> 25:59.620
Remember that when you have a list like this.

25:59.860 --> 26:03.060
Uh, so that's just a one element list.

26:03.060 --> 26:08.940
And then if you multiply it by let's say two, then it will just repeat this twice.

26:08.940 --> 26:13.260
And if I multiply it by 20 then I get 20 values of four.

26:14.300 --> 26:16.100
So here I get sample size.

26:16.100 --> 26:21.160
This is a parameter that I set to be 100 100 zeros and 100 ones.

26:21.320 --> 26:25.880
And then I'm using the plus to concatenate two lists.

26:26.160 --> 26:32.680
And then here I'm converting them into a numpy array that will be easier for indexing and so on later.

26:33.080 --> 26:33.280
Okay.

26:33.320 --> 26:40.040
And then here I'm splitting the data into a train set and train labels and a test set.

26:40.400 --> 26:40.600
Okay.

26:40.640 --> 26:49.400
And here you see the size is 140 for the training set and 160 sorry just 60 for the test set.

26:49.680 --> 26:57.280
So now the idea is that I'm going to train the logistic regression only on these data, the training

26:57.280 --> 27:01.320
set without showing the model any of these data.

27:01.680 --> 27:08.720
And then I will evaluate the performance of the model on these data and not on the training set.

27:10.000 --> 27:10.200
Okay.

27:10.240 --> 27:11.360
And so here's what it looks like.

27:11.360 --> 27:17.470
So most of this code you have seen before the logistic regression function from scikit learn.

27:17.710 --> 27:20.190
You have seen that and then.

27:20.230 --> 27:20.430
Yeah.

27:20.470 --> 27:20.990
Here.

27:21.030 --> 27:24.110
Uh, dot fit you've seen before dot predict.

27:24.150 --> 27:26.910
You have seen before what is new here.

27:27.110 --> 27:34.670
Uh, compared to what I showed earlier in the course for uh, classification is adding the penalty L1.

27:34.870 --> 27:39.310
And here you see the solver which I mentioned in the slides a few moments ago.

27:39.670 --> 27:40.150
Saga.

27:40.190 --> 27:43.070
This is stochastic average gradient descent.

27:43.590 --> 27:44.550
And, uh, yeah.

27:44.550 --> 27:52.230
Then I set the c parameter to be ten, which means that lambda is one divided by 10 or 0.1 okay.

27:52.270 --> 28:00.550
Now the max iterations, the number of iterations is by default 100 for some types of problems.

28:00.550 --> 28:06.070
If you have a relatively clean data set or not, so many variables, that number is fine.

28:06.070 --> 28:12.910
I thought it would be good to add another zero here to increase the number of iterations by an order

28:12.910 --> 28:13.910
of magnitude.

28:14.270 --> 28:16.010
And what does this correspond to?

28:16.050 --> 28:23.650
This is literally just the number of training epochs in gradient descent that this model is implementing.

28:24.330 --> 28:24.690
Okay.

28:24.730 --> 28:30.770
So yeah, now I'm fitting the data to the training set or fitting the model to the training data.

28:30.930 --> 28:33.570
And then I'm predicting the test data.

28:33.570 --> 28:37.250
And then yeah I call this function Classificationreport.

28:37.450 --> 28:41.850
And that's going to print out the accuracy the F1 score and so on.

28:41.970 --> 28:48.450
This is honestly not so interesting to look at for this problem because it's all exactly perfect.

28:48.450 --> 28:50.370
So we get perfect accuracy.

28:50.850 --> 28:53.810
On the one hand that is kind of impressive.

28:53.810 --> 29:01.170
It does mean that there is some meaningful patterns that the model was able to identify in the MLP,

29:01.570 --> 29:10.330
uh, activations that really did separate the token activations for the versus for N, but on the other

29:10.330 --> 29:17.680
hand, it's a huge model and a really small sample size, so you should not overinterpret this pattern

29:17.720 --> 29:18.560
of findings.

29:19.120 --> 29:19.400
Okay.

29:19.440 --> 29:20.480
This is just a reminder.

29:20.480 --> 29:22.600
I've mentioned this in the slides as well.

29:22.720 --> 29:28.960
The term sparsity refers to the number of zero coefficients.

29:28.960 --> 29:35.680
So the coefficients the beta coefficients that equal zero and the density is the number of non-zero

29:35.680 --> 29:36.680
coefficients.

29:36.840 --> 29:41.160
And of course that's scaled by the total number of coefficients.

29:41.240 --> 29:44.560
And actually this what I'm showing here would be proportion.

29:44.840 --> 29:49.040
It's also common to multiply it by 100 to get a percent.

29:50.360 --> 29:50.720
Okay.

29:50.760 --> 29:52.600
So yeah that's what I'm doing here.

29:52.600 --> 30:00.320
I'm just extracting the coefficients from the result and finding where are the coefficients equal to

30:00.360 --> 30:04.600
zero and where are they not equal to zero.

30:04.920 --> 30:05.160
Okay.

30:05.200 --> 30:09.760
And then we can just plot those data and plot the coefficients.

30:09.760 --> 30:17.180
So there are red x's where we had exactly zero and green circles where we have non-zero coefficients.

30:17.580 --> 30:24.780
Now the exact number here in the slides I showed a screenshot of 13, which was like 0.4%.

30:25.780 --> 30:31.940
There's some stochasticity, there's some randomness in the model fitting and also in the train test

30:31.940 --> 30:32.900
split here.

30:33.220 --> 30:38.300
So each time you rerun this code you are going to get slightly different results.

30:38.300 --> 30:41.300
But it's always going to be somewhere around here.

30:41.700 --> 30:47.540
Uh, of course that number can change remarkably depending on what you set this value to.

30:47.900 --> 30:50.420
Uh, I will explore that in a moment.

30:50.580 --> 30:53.620
Uh, but of course you can play around with this yourself as well.

30:54.740 --> 30:54.980
Okay.

30:55.020 --> 31:00.340
And then here we see the histogram of the non-zero coefficients.

31:00.860 --> 31:01.340
All right.

31:01.380 --> 31:02.300
So that's that.

31:02.300 --> 31:04.580
And here I'm just plotting the data.

31:04.580 --> 31:12.210
So I'm looping through all of the non-zero coefficients and grabbing their data where labels equals

31:12.250 --> 31:12.730
zero.

31:12.770 --> 31:15.850
That was the token the where labels equals one.

31:15.850 --> 31:17.370
That is the token n.

31:17.770 --> 31:18.370
And yeah.

31:18.410 --> 31:23.930
Then you see here's the histogram the activations for the and the activations for n.

31:24.370 --> 31:33.330
Again this does not match exactly what I showed in the slides a moment ago because of just random random

31:33.330 --> 31:38.010
numbers that are generated when initializing the model and the data selection.

31:38.530 --> 31:45.650
Okay, here this code is interesting to look at the results, but not so interesting in terms of any

31:45.650 --> 31:47.010
novelty of the code.

31:47.250 --> 31:50.530
This is exactly the same code that I have above.

31:50.730 --> 31:53.890
All I'm doing is changing the value of C.

31:54.210 --> 31:59.290
So here I call this vector C's for like you know C's like this.

31:59.810 --> 32:03.450
So I'm changing the value of C between 1 and 100.

32:04.410 --> 32:09.830
And then just measuring how many coefficients are not equal to zero.

32:09.830 --> 32:16.750
So how many coefficients the model actually needed in order to get the best prediction that it could?

32:17.110 --> 32:19.910
Now there's a lot of other information you could extract.

32:19.910 --> 32:26.310
For example, the prediction is that is the prediction always 100% for all these values of C.

32:26.630 --> 32:29.790
That is something you could incorporate into this code.

32:29.790 --> 32:31.990
It's not something I have done here.

32:31.990 --> 32:39.070
I just want to show you the basics of what it looks like for what the density looks like for different

32:39.070 --> 32:42.390
values of C, or equivalently lambda.

32:43.790 --> 32:50.430
So that took around three minutes to run through 17 different values of the parameter c.

32:50.910 --> 32:53.110
Now you do see this warning message.

32:53.110 --> 32:58.350
Here it says that maxiter was reached which means the coefficients did not converge.

32:58.390 --> 33:00.270
That is not necessarily a bad thing.

33:00.270 --> 33:03.750
That is something that deserves a little bit of inspection.

33:03.750 --> 33:08.060
You could look at the coefficients and so on, but it's not necessarily a bad thing.

33:08.060 --> 33:14.980
Just like when you are training a deep learning model, you don't necessarily know when is an appropriate

33:15.020 --> 33:17.300
time to stop training the model.

33:17.460 --> 33:23.260
So should you train for 500 epochs, 1000 epochs, 2000 epochs, and so on?

33:23.340 --> 33:29.900
You don't necessarily know, and it is not necessarily the case that stopping a little bit earlier is

33:29.900 --> 33:33.820
necessarily detrimental to model performance.

33:34.300 --> 33:34.620
Okay.

33:34.660 --> 33:40.900
And then yeah, the final thing I was going to show is this plot here, which you saw in the slides.

33:40.940 --> 33:47.460
Basically the point is that this value of density, this is not an absolute value that comes from the

33:47.460 --> 33:49.660
analysis necessarily.

33:49.660 --> 33:55.420
So it's not that each time you rerun the analysis you will necessarily get the same value of density.

33:55.820 --> 34:02.820
Instead, the density or equivalently the sparsity is a function of the regularization parameter.

34:02.980 --> 34:07.480
So you know you can actually get a desired level of sparsity.

34:07.760 --> 34:10.000
Not entirely, but you can shift it around.

34:10.000 --> 34:16.680
You can shift this result around depending on how you set the value of lambda, or for scikit learn,

34:16.680 --> 34:17.920
the value of c.

34:19.360 --> 34:23.400
I hope you found this video to be interesting and useful.

34:23.960 --> 34:31.480
Sparse probing is a super interesting analysis, but the results can be weird and difficult to interpret

34:31.480 --> 34:37.840
in very large data sets, especially when there are correlations across the different variables.

34:38.160 --> 34:45.760
I'm going to get into those issues in more detail in the next video, but the point is that the models

34:45.760 --> 34:49.960
are trying to predict labels, not individual neurons.

34:50.160 --> 34:54.920
So it doesn't really matter what individual neurons get, what coefficients.

34:55.120 --> 35:00.840
What matters is whether the entire data set can be used to predict a label.

35:01.200 --> 35:03.440
More on that in the next video.

WEBVTT

00:02.240 --> 00:09.280
Now that you are familiar with the theory and some of the practical aspects of applying generalized

00:09.280 --> 00:16.880
eigendecomposition to LM data, I thought you would enjoy an opportunity to continue practicing how

00:16.880 --> 00:24.280
this technique works, and also including some more analyses, including testing for overfitting by

00:24.320 --> 00:31.880
evaluating the data in a test set that is separate from the training set, and you're also going to

00:31.920 --> 00:37.640
run the analyses in multiple LMS to check for general reproducibility.

00:38.080 --> 00:41.560
There's lots of great stuff going on here in this code challenge.

00:41.760 --> 00:44.840
So let's begin with exercise one.

00:45.440 --> 00:51.840
Here you will use the same him.her data set that we've been using several times so far.

00:52.520 --> 00:59.160
But the idea of this code challenge is that we are going to train the eigenvectors using that data set.

00:59.540 --> 01:06.500
And then we will test for generalization performance using a completely separate data set.

01:06.740 --> 01:09.100
And also one that comes from real data.

01:09.460 --> 01:15.780
So in particular import the first 2000 documents from the fine web dataset.

01:16.060 --> 01:19.220
We've used this data set several times already.

01:19.260 --> 01:21.540
Although it was earlier in the course.

01:21.900 --> 01:29.420
For example in the pre-training and fine tuning sections if you like you can find that code file and

01:29.420 --> 01:30.620
copy paste.

01:30.620 --> 01:34.820
But I've also included most of this code into the helper file.

01:35.540 --> 01:45.820
Anyway, what you want to do is find 50 him and 50 her target words in the fine web dataset and extract

01:45.820 --> 01:53.780
those words plus their surrounding context to put into a PyTorch tensor as a batch that you can input

01:53.780 --> 01:54.860
into the model.

01:55.740 --> 02:03.850
So that data set is going to be of size 100 by ten, corresponding to 50 samples of each pronoun.

02:03.850 --> 02:13.090
So 50 hymns and 50 hers and ten tokens in each sentence sequence where the sixths token is the target

02:13.090 --> 02:13.570
word.

02:14.130 --> 02:18.730
And then you can print out a few of the examples, like what I showed here.

02:18.730 --> 02:21.690
So I just picked these four at random.

02:21.690 --> 02:25.890
So here you see him him and her and her.

02:26.650 --> 02:35.210
Again, the idea is that we are going to run the GED on the Him.her data set to create the eigenvectors,

02:35.690 --> 02:44.290
and then we see if those vectors can also separate him from her in this fine web data set that the eigendecomposition

02:44.290 --> 02:46.170
is not exposed to.

02:47.370 --> 02:51.330
Now, you don't need to worry about any analyses yet.

02:51.770 --> 02:59.630
For now, exercise one just focus on creating the two data sets, and I forgot to mention this explicitly,

02:59.630 --> 03:05.030
but I hope it's obvious that you want to give these two data sets different names, like you can call

03:05.030 --> 03:06.350
them train and test.

03:06.990 --> 03:10.190
Anyway, now you should pause the video and get to work.

03:10.310 --> 03:13.750
And now I will switch to code and show my solution.

03:14.550 --> 03:21.230
Here are the libraries that we will need for exercise one, and also some other libraries that we do

03:21.230 --> 03:26.470
not need now, but we will end up using later on for later exercises.

03:26.990 --> 03:28.550
Okay, so import those here.

03:28.550 --> 03:29.990
I'm importing the model.

03:30.310 --> 03:31.830
Here are the sentences.

03:31.830 --> 03:33.150
The him.her sentences.

03:33.190 --> 03:33.470
You.

03:33.470 --> 03:36.270
Now I'm sure you're completely bored of these sentences.

03:36.270 --> 03:39.830
I'll try not to use them too much in the next section of the course.

03:40.150 --> 03:41.350
Okay, so there's those.

03:41.390 --> 03:43.070
And there's the target words here.

03:43.070 --> 03:45.950
I'm importing the fine web data set.

03:45.990 --> 03:46.430
Again.

03:46.470 --> 03:48.950
You have seen this code before.

03:49.390 --> 03:53.390
I'm just arbitrarily importing the first 2000 documents.

03:53.390 --> 03:54.890
That's more than enough.

03:54.890 --> 03:56.810
That's going to be a lot, actually.

03:57.290 --> 03:58.490
Uh, and, uh, yeah.

03:58.530 --> 04:02.610
So then, uh, because we only need 50, uh, target words there.

04:03.090 --> 04:03.410
Okay.

04:03.450 --> 04:04.090
So let's see.

04:04.090 --> 04:11.530
So then what I'm doing here is finding all of the tokens in the fine web data set, or at least the

04:11.530 --> 04:16.490
first part of it, uh, where the token equals him and the token equals her.

04:16.530 --> 04:19.810
And I call those the him tokens and the tokens.

04:19.930 --> 04:23.530
And then here I'm printing out a couple of them just to show some examples.

04:23.570 --> 04:26.090
Now, this is not is not yet the batch.

04:26.090 --> 04:30.690
We haven't yet created the data that we will use to input into the model.

04:30.690 --> 04:36.610
This is just to give you some sense of what some of these tokens look like in context.

04:37.730 --> 04:39.090
And that's what you see here.

04:39.090 --> 04:42.330
So here we have a him him him uh, him.

04:42.370 --> 04:42.570
Yeah.

04:42.570 --> 04:43.970
I'm only actually showing the hymns.

04:43.970 --> 04:45.970
You could also show some of the hers as well.

04:45.970 --> 04:46.610
That's fine.

04:46.970 --> 04:47.210
Okay.

04:47.250 --> 04:49.810
So now here I'm creating the batches.

04:50.010 --> 04:58.510
Now, what I have decided to do here Year is, uh, take the her tokens from much later in the, uh,

04:58.510 --> 05:00.670
in the sequence in the data set.

05:00.670 --> 05:05.590
So I'm adding, uh, an offset here of half of the tokens.

05:05.590 --> 05:09.070
That's really just to push them into later in the data set.

05:09.550 --> 05:11.030
Uh, that's not critical.

05:11.030 --> 05:15.230
I didn't mention it when I gave the instructions for this exercise.

05:15.350 --> 05:21.710
The reason why I'm doing it here is to just avoid the risk of using the same sentence so that the model

05:21.710 --> 05:24.710
is processing mostly the same tokens.

05:24.710 --> 05:29.110
For example, we can look at this one here, nominate him or her.

05:29.350 --> 05:37.910
So if I would take this sentence or this token plus its surrounding context for both him and her, then

05:37.910 --> 05:45.310
that means that there's a lot of overlap in exactly identical tokens that the model will be processing,

05:45.310 --> 05:50.630
and it's not really necessary to avoid that, but I thought it would be nice to avoid that.

05:50.630 --> 05:54.340
So that's why I added this offset here.

05:54.700 --> 05:59.660
Otherwise you can see I'm looping over 50 and I say batch I.

05:59.700 --> 06:06.180
So this looping index variable I that comes from the hymn tokens with some surrounding context.

06:06.220 --> 06:09.980
And then 50 plus I for the her tokens.

06:09.980 --> 06:18.220
So that means that in this batch let me show you that in this batch the first 50 are for him and the

06:18.260 --> 06:21.820
second 50 block of 50 are for her.

06:22.980 --> 06:23.220
Okay.

06:23.260 --> 06:28.860
And then yeah, here I'm just printing out a couple of examples as I showed in the slides him him her

06:28.900 --> 06:36.460
her exercise two is not so difficult and shouldn't take you too long to implement.

06:36.860 --> 06:37.100
Here.

06:37.100 --> 06:46.660
You want to import GPT two small and implant hooks to get all the activations of the MLP expansion neurons

06:46.660 --> 06:48.660
before the non-linearity.

06:49.380 --> 06:55.960
Then you can forward pass both data sets to get the activations, and you can print out the sizes of

06:55.960 --> 06:57.720
the data matrices.

06:58.040 --> 07:05.040
Now for the train and test sets, the sizes will be slightly different because the number of sequences

07:05.040 --> 07:11.800
and the sequence lengths are different, although of course they are exactly the same neurons with exactly

07:11.800 --> 07:13.000
the same indices.

07:13.520 --> 07:20.560
That is important because, as you already know, we are going to train the eigenvectors from the train

07:20.560 --> 07:23.800
set and then apply them to this test set.

07:24.720 --> 07:25.920
So that is your mission.

07:25.920 --> 07:26.880
You know what to do.

07:27.040 --> 07:28.800
And I also know what to do.

07:30.040 --> 07:32.320
So here is the code for exercise two.

07:32.320 --> 07:38.840
I already imported the model earlier before I even got to the code for exercise one, but that's fine.

07:39.360 --> 07:42.560
Okay so here typical code that you've seen before.

07:42.600 --> 07:45.080
I'm looping over all of the layers.

07:45.280 --> 07:53.180
And as you've seen multiple times before, I'm not only getting the output, I'm also detaching it from

07:53.180 --> 07:59.740
the PyTorch from the model computational graph and transforming it into numpy format.

07:59.780 --> 08:06.220
Now this part here is sometimes necessary, sometimes not, depending on what subsequent analyses you

08:06.220 --> 08:07.060
are going to do.

08:07.540 --> 08:13.100
For us, in this case, it's just more convenient to have all of the data in numpy format.

08:13.100 --> 08:15.780
So uh, so that's why I have this here.

08:16.140 --> 08:16.460
Okay.

08:16.500 --> 08:18.820
So let me run that code here.

08:19.100 --> 08:24.380
And now I'm pushing the train tokens and the test tokens through the model.

08:24.380 --> 08:29.540
And here I am assigning them to different variables with different variable names.

08:29.540 --> 08:33.780
So we have the train activations and the test activations.

08:33.980 --> 08:36.100
So here is a question for you.

08:36.420 --> 08:41.100
Notice that these two lines of code are not identical.

08:41.140 --> 08:45.820
Obviously the variable names differ, but on the right hand side they are not identical.

08:45.940 --> 08:51.210
Here I have a copy method on this dictionary and here I don't.

08:51.410 --> 09:00.690
So my question for you is whether this is necessary for this line and why I do not have dot copy also

09:00.690 --> 09:01.610
for this line.

09:01.890 --> 09:03.490
So that's something for you to ponder.

09:03.530 --> 09:10.810
Basically just has to do with how Python deals with these kinds of assassinations with, you know,

09:10.850 --> 09:14.050
assigning one variable to another without changing it at all.

09:14.530 --> 09:14.890
Okay.

09:14.930 --> 09:16.530
So let's see run through this.

09:16.770 --> 09:20.570
If you wanted to put this on the GPU, that's fine.

09:21.010 --> 09:24.850
That will save you some tens of seconds I guess, but it's not that much.

09:24.850 --> 09:32.390
Data took seven seconds overall, so if you put it on the GPU, it probably would have saved you six

09:32.390 --> 09:38.330
seconds of analysis time here, although it was probably more than six seconds to write all the code

09:38.330 --> 09:40.090
and select a GPU and so on.

09:40.410 --> 09:40.650
Okay.

09:40.690 --> 09:44.250
And here I'm just printing out the sizes of those matrices.

09:44.770 --> 09:51.190
So the train data, there's 54 sentences per pronoun and the test there's 50.

09:51.390 --> 09:56.830
And then here we have eight tokens in the sequence and ten tokens in the sequence.

09:56.990 --> 09:59.510
And why did I not choose to balance them.

09:59.790 --> 10:01.750
Because it doesn't really matter.

10:01.790 --> 10:03.190
And also, uh, okay.

10:03.230 --> 10:11.190
Actually, there was a reason, uh, it is uh, sometimes tricky to remember which data set you're working

10:11.190 --> 10:11.670
with.

10:11.870 --> 10:19.110
So for confirmation, for sanity checking as you're developing code and going through, uh, analyses

10:19.110 --> 10:26.510
and working with code, it's often handy to have different data matrices with slightly different sizes

10:26.710 --> 10:33.870
so that the chance of you accidentally using this one when you went to use this one is lower, because

10:33.870 --> 10:36.070
you can check the sizes and you will see.

10:36.390 --> 10:39.710
You can confirm which analysis this corresponds to.

10:41.430 --> 10:43.970
Now for exercise Size three.

10:44.570 --> 10:53.530
The goal of exercise three is simply to create a function that uses PCA to dimension reduce the activations

10:53.530 --> 10:54.170
data.

10:54.770 --> 11:02.890
The idea of this function is that for each layer in the model, you will call this function, and that

11:02.890 --> 11:08.730
will extract all of the target activations on the training set.

11:09.090 --> 11:17.490
Run a PCA calculate how many components it takes to account for 99% of the total variability in the

11:17.490 --> 11:24.330
data, and then output the activations, the eigenvectors, and the number of components to keep.

11:24.810 --> 11:28.490
Now all of this is based purely on the training data.

11:28.530 --> 11:32.810
You're not using any of the test data anywhere in this function.

11:33.330 --> 11:40.690
And that means that you need this loop over sentences, because in the training data, the target tokens

11:40.690 --> 11:44.080
can appear in any number of different positions.

11:44.680 --> 11:51.320
Now here in this case, I want you to use scikit learn's PCA function, because it turns out that that's

11:51.320 --> 11:58.520
a lot faster than writing out the code yourself, which is the code that I used in the previous video.

11:59.360 --> 12:06.480
So you might need to do a little bit of extra work to figure out how to extract the eigenvalues, and

12:06.480 --> 12:10.560
then convert that into cumulative variance explained.

12:11.280 --> 12:18.640
On the other hand, this line here I have left intact from the solutions file because I find it like

12:18.680 --> 12:20.040
pointlessly confusing.

12:20.040 --> 12:27.640
So basically scikit learn names the eigenvectors component matrix components underscore.

12:27.680 --> 12:35.360
That part is fine, but it puts the eigenvectors in the rows instead of in the columns.

12:35.640 --> 12:42.060
Now I find that extremely annoying and confusing because it's different from how, like the rest of

12:42.060 --> 12:45.100
the world stores eigenvectors in columns.

12:45.540 --> 12:48.900
This is the sort of thing that causes confusion and errors.

12:48.900 --> 12:56.180
But anyway, the solution is simply to transpose the components matrix, and that gives you the eigenvectors

12:56.180 --> 13:03.540
matrix with the eigenvectors in the columns, as we have done in the previous video.

13:03.740 --> 13:06.700
Here you do want to export from this function.

13:06.700 --> 13:14.500
You do want to return the principal components eigenvectors, because later on in a later exercise you

13:14.500 --> 13:19.820
will apply those eigenvectors to dimension reduce the test set.

13:20.460 --> 13:26.180
Anyway, fleshing out this function is the entirety of exercise three.

13:27.020 --> 13:32.460
If you are struggling with this exercise, then of course you can copy and paste a lot of code from

13:32.500 --> 13:33.700
the previous video.

13:34.100 --> 13:36.740
So pause the video and get to work.

13:36.740 --> 13:38.620
And now I will switch to code.

13:39.640 --> 13:41.640
So here I'm defining the function.

13:41.640 --> 13:47.400
I call it dim red for dimension reduction via principal components analysis.

13:47.840 --> 13:54.160
Input the layer number because I'm going to call this function inside a for loop over all the different

13:54.200 --> 13:55.680
layers of the model.

13:56.160 --> 13:58.760
Here is where I'm getting the target activations.

13:58.760 --> 14:00.880
You've seen all code like this before.

14:00.880 --> 14:07.800
So here I'm getting the index of the target, either him or her.

14:08.280 --> 14:12.880
And then I'm just grabbing that activation and storing that in this matrix here.

14:12.880 --> 14:22.240
So this is going to be a 108 by 3072 matrix corresponding to 108 sentences or sequences.

14:22.240 --> 14:30.480
So targets and 3072 activations from each of the MLP expansion neurons.

14:31.080 --> 14:32.440
So that's the activation.

14:32.600 --> 14:37.740
Here we do the principal components analysis to dimension compress the data.

14:37.980 --> 14:40.540
Now this code is commented out.

14:40.580 --> 14:46.260
This code is all correct, but this does the I, the Eigendecomposition and the principal components

14:46.260 --> 14:51.540
analysis manually using code that I showed in the previous video.

14:52.460 --> 14:57.860
I included this here because it is equivalent to this code here.

14:58.180 --> 15:03.300
The thing is that this is pretty slow and this is pretty fast.

15:03.300 --> 15:08.260
So when you're only doing this once for one layer, then this is fine.

15:08.260 --> 15:12.940
You can do this method, but you will see when we run this over 12 layers.

15:12.940 --> 15:21.660
And then later on we're going to reproduce this in uh, in GPT two XL which has several dozen layers.

15:21.700 --> 15:24.460
This is going to end up just being ludicrously slow.

15:24.460 --> 15:26.140
And this is quite fast.

15:26.380 --> 15:27.580
So, uh, yeah.

15:27.580 --> 15:34.780
So therefore I'm using scikit learn's PCA to get the same results as what I have written up here.

15:35.260 --> 15:35.540
Okay.

15:35.690 --> 15:44.490
so all of this I already explained here I'm taking PCA, dot explained variance ratio and then I just

15:44.490 --> 15:45.690
multiply it by 100.

15:45.690 --> 15:48.730
So basically this here is proportion.

15:48.890 --> 15:50.610
And then I want percent.

15:50.610 --> 15:52.210
And why do I want it in percent.

15:52.490 --> 15:54.490
Literally no compelling reason.

15:54.490 --> 15:58.130
You could leave this out and then make that 0.99.

15:58.410 --> 16:00.770
As you like this is just personal preference.

16:01.170 --> 16:08.970
And then of course I'm taking the cumulative sum so that I can calculate the component index at which

16:08.970 --> 16:13.850
the cumulative variance explained exceeds 99%.

16:14.250 --> 16:17.130
So that gives me the number of components to keep.

16:17.130 --> 16:22.370
And then I export the training activations which is still in the original dimensional space.

16:22.370 --> 16:27.410
So the MLP expansion space, I haven't yet done the compression.

16:27.610 --> 16:36.150
And then the eigenvectors and the number of components that we want to keep Exercise four is kind of

16:36.190 --> 16:43.030
similar to exercise three, in the sense that the entire goal of this exercise is to write a function

16:43.230 --> 16:49.790
that will perform the GED on the training set, and you can call that function for each layer of the

16:49.790 --> 16:50.430
model.

16:50.830 --> 16:58.190
And also, as with exercise three, most of this code you can copy paste from the previous video where

16:58.190 --> 17:02.350
I demoed GED in LM data for one layer.

17:02.950 --> 17:09.270
So up here you compress the training data down to a smaller dimensionality.

17:09.670 --> 17:14.950
Calculate the covariance matrices of the reduced dimensional data set.

17:15.350 --> 17:22.950
Calculate shrinkage regularization storing those regularized covariance matrices as separate variable

17:22.950 --> 17:23.710
names.

17:24.230 --> 17:28.230
And then here you implement the generalized eigendecomposition.

17:28.710 --> 17:34.130
Now here when you do the GED you can just use the Is the regularized covariance for r.

17:34.250 --> 17:42.330
So the second input into the I th function which is the denominator matrix, or like the reference matrix.

17:42.810 --> 17:44.890
And you can use the unadulterated.

17:44.890 --> 17:49.130
So the non regularized covariance matrix for s.

17:49.570 --> 17:53.970
I've already discussed why you would want to do this in the previous video.

17:53.970 --> 17:57.210
And I'll mention it again briefly when I switch to code.

17:57.530 --> 17:59.050
But that is commonly done.

17:59.090 --> 18:05.930
Again the idea is just to preserve as much of the information in S as possible in the numerator matrix

18:05.930 --> 18:11.850
that you want to isolate the patterns for, and then just regularize the R matrix.

18:12.290 --> 18:18.370
And yeah, as you also know from the previous video, you want to repeat this procedure.

18:18.370 --> 18:26.050
So you do the generalized eigendecomposition twice once for him versus her and once for her versus him.

18:26.370 --> 18:32.160
And then there's one more thing that you should implement in this function, which I mentioned and showed

18:32.160 --> 18:37.480
briefly in the previous demo, although I didn't actually write out the math for you, I just showed

18:37.480 --> 18:38.480
you the code.

18:38.760 --> 18:41.160
So that is that code is here.

18:41.160 --> 18:42.040
The math is here.

18:42.320 --> 18:47.400
So the idea is that we want to see what these patterns actually look like.

18:47.600 --> 18:51.240
Now the pattern is not the same thing as the eigenvector.

18:51.400 --> 18:57.080
The eigenvector is the filter weights like the spatial kernel that we apply to the data.

18:57.520 --> 19:03.040
The pattern you get by multiplying the weights matrix so w.

19:03.080 --> 19:08.800
This is the top eigenvector that you would get here by the covariance matrix.

19:08.800 --> 19:12.760
And that would be one of these covariance matrices here.

19:13.240 --> 19:18.520
Now there's a deeper mathematical discussion about why this is the right thing to do.

19:18.520 --> 19:21.320
And I'm not going to get into the nuances of that here.

19:21.320 --> 19:27.480
But suffice it to say, this reveals the pattern in the data that we can interpret.

19:27.880 --> 19:32.300
Now the covariance matrix is in the reduced dimensional space.

19:32.300 --> 19:40.260
So you also want to expand the covariance matrix by pre-multiplying by the same principal components

19:40.260 --> 19:43.700
eigenvectors that you used for the compression.

19:44.060 --> 19:51.340
So this is kind of like undoing the compression to expand the dimensionality back out to the original

19:51.380 --> 19:53.940
MLP expansion layer space.

19:54.700 --> 19:59.540
And then you can correlate the pattern for him with the pattern for her.

19:59.580 --> 20:01.500
That's what you see over here.

20:01.500 --> 20:03.140
Here I'm using Pearson R.

20:03.460 --> 20:09.740
That will give me a statistic a correlation coefficient value and also the p value.

20:10.140 --> 20:16.180
So when you return this that's going to be interesting to look at over the different layers.

20:16.820 --> 20:24.140
Anyway so you want to output from this function the projected data for him the projected data for her.

20:24.180 --> 20:31.800
So these are the low dimensional projections for the target's projected onto their corresponding eigenvectors.

20:32.040 --> 20:38.800
And you also want to export the eigenvectors themselves, because in the next exercise, we are going

20:38.800 --> 20:42.720
to apply these eigenvectors to the test set data.

20:43.160 --> 20:47.560
And of course this correlation object over here from this analysis.

20:47.880 --> 20:55.000
And then over here we have the top eigenvalue for him and the top eigenvalue for her.

20:55.440 --> 21:03.120
We are going to visualize these data to see if the largest eigenvalue, which is related to the separability

21:03.120 --> 21:07.800
in the model, changes as a function of depth into the model.

21:08.640 --> 21:08.880
Okay.

21:08.960 --> 21:15.480
Now this exercise is actually a little bit tricky, but I hope you find it to be a good educational

21:15.480 --> 21:16.400
experience.

21:16.880 --> 21:22.720
If you get stuck, then feel free to keep watching the video where I will discuss my solution, and

21:22.720 --> 21:28.470
I'll also expand on some of the practical Implementational details of GED.

21:29.790 --> 21:37.430
So this stuff is tricky, but all of this code you have seen before in the previous couple of videos.

21:37.950 --> 21:38.230
Okay.

21:38.270 --> 21:42.470
So here I'm projecting the data down to a lower dimensional space.

21:42.670 --> 21:47.630
Note here I'm post-multiplying the PCA eigenvectors.

21:47.870 --> 21:52.590
So so the training activations times the PCA eigenvectors.

21:52.630 --> 21:56.350
Again, nowhere in here am I actually using the test set.

21:56.590 --> 22:03.630
All of the principal components vectors and the generalized eigendecomposition vectors are only based

22:03.630 --> 22:04.950
on the training set.

22:05.430 --> 22:07.350
Here are their covariance matrices.

22:07.350 --> 22:11.470
You have to be mindful of when you need to transpose and when you don't.

22:11.750 --> 22:15.670
Uh, it really just depends on the orientation of the data.

22:15.790 --> 22:22.270
But the best way to do it is just to pick one of these at random, like transpose or don't transpose.

22:22.310 --> 22:28.890
And then you check the sizes and you want to make sure that the size of this covariance matrix is features

22:28.890 --> 22:32.850
by features and not observations by observations.

22:33.330 --> 22:33.690
Okay.

22:33.730 --> 22:36.050
So that is the covariance matrices.

22:36.210 --> 22:39.210
Here I'm doing the shrinkage regularization.

22:39.370 --> 22:42.610
And then here is the generalized eigendecomposition.

22:42.650 --> 22:44.970
Again nothing new here.

22:45.010 --> 22:48.810
This is all stuff you have seen now in several videos.

22:48.850 --> 22:53.250
So the very first one where I did the very simple demo with the X data set.

22:53.410 --> 23:02.050
And also in the video where I first the video thereafter where I first demoed GED in uh, in LM data.

23:03.170 --> 23:03.530
Okay.

23:03.570 --> 23:12.850
So then we want to project the, uh, targets down onto the top, uh, generalized Eigendecomposition

23:12.850 --> 23:13.610
vector.

23:13.650 --> 23:19.010
And we have to do that twice for the Him analysis and for the her analysis.

23:19.330 --> 23:26.590
And then, uh, so also notice here I'm not projecting down all of the words in the entire sequence,

23:26.590 --> 23:29.790
like what we did in the previous video here.

23:29.790 --> 23:32.270
This is really just about the target words.

23:32.270 --> 23:34.230
We're just focusing on the target words here.

23:34.670 --> 23:34.910
Okay.

23:34.950 --> 23:38.550
And then here is the pattern analysis as I mentioned.

23:38.670 --> 23:39.190
Uh, yeah.

23:39.230 --> 23:47.350
The right thing to do to interpret the patterns in the data themselves is take the eigenvector and multiply

23:47.350 --> 23:51.350
that by the corresponding covariance matrix.

23:51.350 --> 23:58.590
And actually I think I'd prefer not to do this on the regularized matrix but on the original covariance

23:58.590 --> 23:59.150
matrix.

23:59.910 --> 24:00.190
Okay.

24:00.230 --> 24:03.830
And then yeah here pre-multiplying by the PCA eigenvectors.

24:03.830 --> 24:07.030
And then this will get us back to the MLP space.

24:07.670 --> 24:16.070
Now if these neurons actually had some meaningful spatial geometric organization, then this is the

24:16.070 --> 24:19.470
thing that we could visualize and interpret.

24:19.470 --> 24:25.290
For example, when, uh, you know, in my neuroscience lab, when we do these kinds of analyses on

24:25.290 --> 24:31.810
neuroscience data, we can visualize this pattern over the different electrodes that we're recording

24:31.810 --> 24:38.450
from or the different pixels in the image of a brain map here for these models.

24:38.650 --> 24:43.650
You know, the MLP neurons have no meaningful spatial relationship.

24:43.650 --> 24:44.850
There's no reason why.

24:45.610 --> 24:48.290
Neuron 1000 and 1001.

24:48.450 --> 24:50.450
You know, they don't have anything to do with each other.

24:50.490 --> 24:53.290
They just happen to be indexed sequentially.

24:54.370 --> 25:00.730
So therefore we're not going to really visualize these data in any kind of meaningful map sense.

25:00.730 --> 25:06.970
But we are going to correlate them to see how they are, how these patterns are related to each other

25:07.170 --> 25:10.050
in the original MLP data space.

25:10.610 --> 25:15.410
Again, here, in theory, all of this stuff should be real valued.

25:15.450 --> 25:18.250
We shouldn't be getting complex valued results.

25:18.610 --> 25:24.640
But just because of some tiny numerical inaccuracies and little numerical issues.

25:24.920 --> 25:33.080
It is possible for some of these results to be complex valued, and that is why I just extract the real

25:33.080 --> 25:35.040
part here and the real part here.

25:35.440 --> 25:41.720
And, uh, and then I think I actually don't need this here because all of these are already guaranteed

25:41.760 --> 25:43.400
to be real valued.

25:43.400 --> 25:48.240
So, uh, so I don't remember if there is a particular reason why I include that here anyway.

25:48.320 --> 25:48.760
Uh, yeah.

25:48.760 --> 25:52.160
So I'll run this code to generate that function.

25:52.160 --> 25:56.920
And now we are ready for the analyses in the next exercise.

25:58.120 --> 26:02.360
Congrats on getting this far through the code challenge.

26:02.480 --> 26:09.120
I'm now going to break the video to give you an opportunity to get up and stretch your legs and splash

26:09.120 --> 26:11.000
water on your face, and so on.

26:11.000 --> 26:16.120
When you're ready, when you're feeling refreshed, come back to the next video where we will complete

26:16.280 --> 26:17.800
the code challenge.

WEBVTT

00:01.840 --> 00:09.400
In this code challenge, you will get to apply and extend what you learned in the previous video.

00:09.920 --> 00:18.200
I think this will be a fantastic opportunity to solidify your ability to isolate and work with attention

00:18.200 --> 00:18.680
heads.

00:19.160 --> 00:27.400
Exercise one is not so difficult, but it does require some attention to detail and not just blindly

00:27.400 --> 00:28.640
copy pasting.

00:29.240 --> 00:35.800
So here you should import the Pythia 2.3 billion parameter model.

00:36.320 --> 00:39.360
We have worked with this model previously in the course.

00:39.360 --> 00:41.760
It's from the eleuther org.

00:41.960 --> 00:44.800
And as a reminder it looks like this.

00:45.240 --> 00:48.080
So you can put this model on the GPU.

00:48.720 --> 00:56.200
Next you will need to hook all of the qkv matrices across all of the layers.

00:56.240 --> 01:03.760
Now you can start by copying the code from the previous video, but be mindful that the names of the

01:03.760 --> 01:10.680
layers and the matrices are different in this model compared to the open AI format.

01:11.120 --> 01:20.440
Now I want you to push the model to the GPU, but because we are doing the analyses on the CPU inside

01:20.440 --> 01:26.640
the hook function, when you're grabbing the activations, you can also just put them the activations

01:26.800 --> 01:29.560
directly back into the CPU.

01:30.400 --> 01:36.240
Next, copy the text about Fiji that we looked at in the previous video.

01:36.560 --> 01:39.320
Tokenize and push the tokens through the model.

01:39.440 --> 01:46.840
Of course, it's always a good idea to check the keys of the activations dictionary and the size of

01:46.840 --> 01:50.720
one of them, just to make sure that your code is correct.

01:51.040 --> 01:52.960
So here we have one sequence.

01:53.560 --> 01:57.480
In this case, with this model we end up with 39 tokens.

01:57.680 --> 02:04.200
And I will let you figure out what this number here 7680 corresponds to.

02:04.520 --> 02:06.280
And make sure you figure that number out.

02:06.320 --> 02:11.400
Make sure you know where this number comes from, what it means, and how it relates to other numbers

02:11.400 --> 02:14.560
that you see in this model description.

02:15.800 --> 02:24.000
Finally, write and check your code for splitting up the Q vectors into heads and also the size of each

02:24.000 --> 02:24.440
head.

02:25.080 --> 02:32.080
Again, you can copy the code from the previous video, but be mindful of changes to variable names

02:32.080 --> 02:40.000
and so on that might be necessary when working with the Pythia 2.3 billion parameter model.

02:40.400 --> 02:45.400
By the way, this stuff here is just about making sure that your code is accurate.

02:45.440 --> 02:50.120
You don't have to do any analyses or worry about different layers or anything like that.

02:50.160 --> 02:51.960
For this exercise.

02:52.520 --> 02:54.280
So that's for exercise one.

02:54.280 --> 02:56.960
Now you should pause the video and switch to code.

02:57.080 --> 02:59.680
And now I will discuss my solution.

03:00.600 --> 03:02.880
Import libraries, yada yada yada.

03:02.920 --> 03:04.880
Okay, so here's exercise one.

03:04.880 --> 03:09.120
I'm importing this 2.8 billion parameter model.

03:09.320 --> 03:11.440
Of course 2.8 billion parameters.

03:11.440 --> 03:12.720
Sounds really impressive.

03:12.720 --> 03:19.120
And still until you start learning about modern llms and then 2.8 billion is like really?

03:19.120 --> 03:19.960
That's so small.

03:19.960 --> 03:22.240
You can do anything with such a tiny model.

03:23.080 --> 03:29.240
Uh, anyway, so yeah, here I'm also extracting some useful variables.

03:29.240 --> 03:37.200
This is initially copied from the code file for the previous video, but you need to be very mindful

03:37.200 --> 03:41.360
of updating these names, uh, when you need to.

03:41.400 --> 03:46.160
So, for example, uh, Eleuther calls their models the number of heads.

03:46.160 --> 03:47.880
They call that parameter num.

03:47.920 --> 03:49.080
Attention heads.

03:49.120 --> 03:49.880
Super clear.

03:49.960 --> 03:51.360
Couldn't get more clear than that.

03:51.840 --> 03:54.640
OpenAI calls it n underscore heads.

03:54.880 --> 03:55.280
Uh, yeah.

03:55.320 --> 03:57.880
And then same with this and so on.

03:58.120 --> 04:01.280
So yeah, you just have to be mindful of that.

04:01.280 --> 04:07.600
But the good thing is, when you have a good understanding of the architecture and the organization

04:07.600 --> 04:13.680
of these models, then it's pretty straightforward to look through the description of the models and

04:13.680 --> 04:19.640
pick out exactly which matrices, which layer names you actually are looking for.

04:20.120 --> 04:21.680
Okay, so here is the hook.

04:22.080 --> 04:25.280
Uh, this is mostly copied from the previous lecture.

04:25.320 --> 04:26.880
This you need to change.

04:27.240 --> 04:31.120
So we have GPT neo X and then the layers.

04:31.120 --> 04:38.560
So in GPT this would actually look like uh let's see that would look like that for example uh attention

04:38.760 --> 04:40.360
query key value.

04:40.360 --> 04:42.480
So you're really changing a lot of stuff here.

04:42.520 --> 04:48.880
Although of course the basic mechanism of implanting this hook and defining the hook function is the

04:48.880 --> 04:49.280
same.

04:50.160 --> 04:55.950
So I can run that, uh, this is all literally copy pasted from the previous video.

04:55.990 --> 05:00.390
Although of course I added this to push the text to the GPU.

05:01.230 --> 05:03.630
And then here we are checking for the sizes.

05:03.670 --> 05:05.710
Yeah, nothing too spectacular there.

05:05.950 --> 05:12.310
By the way, you know, this, uh, this entire cell here took less than less than one second closer

05:12.310 --> 05:13.430
to zero seconds.

05:13.830 --> 05:18.630
If you are running this on the CPU, I guess this would take many seconds.

05:18.630 --> 05:21.070
Maybe 10s 15 20s.

05:21.510 --> 05:21.990
Okay.

05:22.030 --> 05:26.510
But this is really the only time we need to run text through the model here.

05:26.950 --> 05:27.150
Okay?

05:27.190 --> 05:34.470
And then finally, uh, this is, uh, basically entirely copied from the previous video.

05:34.510 --> 05:36.990
Of course, the results themselves are different.

05:36.990 --> 05:43.590
So here there are 32 heads compared to 12 heads for, uh, GPT two small.

05:43.910 --> 05:46.670
And the size of each head is 39.

05:46.710 --> 05:51.750
This is the token and 80 is the dimensionality of each head.

05:51.950 --> 05:53.310
And where does that come from?

05:53.310 --> 05:56.990
Well, we have 32 heads and 80 dimensions.

05:56.990 --> 06:04.510
When you multiply those, you should get whatever is 700, 7680 divided by three.

06:04.750 --> 06:12.390
And that needs to be divided by three, of course, because that is the qkv matrices all concatenated

06:12.390 --> 06:13.030
together.

06:14.030 --> 06:16.270
Now for exercise two.

06:16.750 --> 06:20.350
This is the main exercise of this video.

06:20.910 --> 06:27.710
A lot of this exercise you can solve by copying and modifying code from the previous video.

06:28.190 --> 06:35.950
Essentially what you want to do here is calculate the softmax attention weights for two groups of tokens

06:36.110 --> 06:42.470
from the final token to all the previous tokens in the sequence, and for all of the self-attention

06:42.510 --> 06:43.270
tokens.

06:43.750 --> 06:49.150
So we did that in the previous video, but there it was only for one layer.

06:49.150 --> 06:51.030
So just one transformer block.

06:51.550 --> 06:56.230
Now here in this video You want to do it for all of the layers.

06:56.230 --> 07:05.030
And then we will visualize how these softmax probability distributions change over the different layers.

07:05.670 --> 07:11.190
The main analysis code will be a for loop that looks something like this.

07:11.590 --> 07:20.190
So you start with a loop over all of the layers, and then you separate out the Q and k matrices and

07:20.190 --> 07:21.910
split those into heads.

07:21.950 --> 07:29.070
Now all of this code you already have from the end of exercise one, by the way, in the previous video

07:29.070 --> 07:36.990
and also in my solutions for this video, I organized all of these heads as separate elements in a tuple.

07:37.430 --> 07:45.030
It is also possible to reshape these matrices into tensors, where the heads corresponds to one of the

07:45.030 --> 07:49.830
dimensions, instead of each head being a separate element in a tuple.

07:50.270 --> 07:56.310
But anyway, I will continue working with each head here as a separate element in this tuple.

07:56.670 --> 07:56.990
Okay.

07:57.030 --> 08:06.110
So then we can loop over all of the heads, calculate the raw attention scores and add the causal mask.

08:06.950 --> 08:14.950
And just as a little reminder the raw attention scores are the qk transpose scaled by the head dimensionality.

08:15.750 --> 08:22.150
Then you can apply softmax exactly how it's implemented in the attention algorithm.

08:22.630 --> 08:28.790
Now you have all the data you need, and the rest of this loop is just picking out the right tokens.

08:29.390 --> 08:33.590
So here you get the final token with all the previous tokens.

08:33.590 --> 08:40.070
And here is the self-attention token for all of the other tokens in this sequence.

08:40.510 --> 08:47.750
And in all of these cases, I have excluded the first token from the analyses, just because we know

08:47.750 --> 08:54.310
from the previous video that the first token in the sequence will trivially equal one its softmax probability

08:54.310 --> 08:55.030
value.

08:55.030 --> 08:58.230
And so yeah, I'm just ignoring that for convenience.

08:58.630 --> 09:05.590
Once you have finished this loop over all of the heads within each layer, you can then calculate the

09:05.590 --> 09:13.990
KDE analysis that I showed you in the previous video to estimate PDFs of the attention weights distributions.

09:14.550 --> 09:18.790
Now, the goal of this part of the code is to have two vectors to.

09:18.830 --> 09:26.230
So two lines for each layer, one that corresponds to the distributions of the final to the previous

09:26.230 --> 09:27.030
tokens.

09:27.310 --> 09:31.590
And then we have the distribution for the self-attention tokens.

09:32.070 --> 09:39.350
Now after that loop is finished, after you've run this whole analysis, you can recreate this scatter

09:39.350 --> 09:41.710
plot from the previous video.

09:42.230 --> 09:44.910
This is again only for one layer.

09:45.110 --> 09:52.350
It shows all the softmax attention weights from the final to the previous tokens and for all of the

09:52.350 --> 09:53.750
self-attention tokens.

09:54.270 --> 10:00.150
Now in the previous video, I also had self-attention for the first token as a separate distribution

10:00.150 --> 10:04.270
over here on the right, but I left that out in this video.

10:04.670 --> 10:07.790
Okay, so this is just for one layer.

10:07.950 --> 10:15.470
And then you can make line plots that show the KDE distributions in all of the 32 layers.

10:16.190 --> 10:23.230
Now this plot here has 64 lines corresponding to 32 layers.

10:23.710 --> 10:32.070
So 32 transformers and one line for self-attention that that distribution and one line for the KDE estimated

10:32.070 --> 10:36.750
PDF of the final token to all of the previous tokens.

10:37.270 --> 10:44.190
Now, in this figure, this doesn't really nicely show the changes in the distribution over the different

10:44.190 --> 10:44.990
layers.

10:45.110 --> 10:52.390
It just shows the overall differences between self-attention and the final to previous plots for all

10:52.390 --> 10:53.550
of the layers.

10:54.190 --> 10:58.790
So therefore the final plot for this exercise will look like this.

10:59.390 --> 11:03.510
So here we have two heat maps hidden behind the red curtains.

11:04.110 --> 11:09.030
One heat map is for the attention weights for the final token to the previous ones.

11:09.190 --> 11:12.510
And then we have for the self-attention weights.

11:12.670 --> 11:18.350
So now the idea is that each column in this matrix.

11:18.470 --> 11:27.910
So each vertical slice in this image is the visualization of the KDE estimated PDF for each layer.

11:28.310 --> 11:34.870
So that's like the smoothed version of the scatter plots that you have been making in the previous video,

11:34.870 --> 11:39.310
and that you also made here in this code down here.

11:39.830 --> 11:46.150
And that means that the color in these heat maps corresponds to the amount of clustering.

11:46.270 --> 11:53.470
So you can think of it like the number of dots for each softmax probability grid value between 0 and

11:53.470 --> 11:53.950
1.

11:54.550 --> 12:00.070
So here you see there's lots of softmax activation weights that have values close to zero.

12:00.510 --> 12:08.350
And then at least in the final layers here there's also some token pairs that have very high softmax

12:08.350 --> 12:09.150
probabilities.

12:09.150 --> 12:12.750
So it gets white all the way up here for higher density.

12:14.030 --> 12:21.550
So overall it's quite sparse and bimodally distributed, meaning that most of the token pairs with the

12:21.550 --> 12:26.790
final token are either very small probabilities or very large probabilities.

12:27.510 --> 12:33.990
So then the question is what this looks like for the other layers for final to previous and what it

12:33.990 --> 12:36.390
looks like for the self-attention weights.

12:36.990 --> 12:43.190
Okay, I hope that all makes sense, and I hope that you enjoy working through this exercise, which

12:43.190 --> 12:44.470
is what you should do now.

12:44.990 --> 12:49.590
And now I will switch to code and discuss my solution and the results.

12:50.910 --> 12:53.190
Here are the initializations.

12:53.190 --> 12:56.190
So this variable SM you have seen this before.

12:56.550 --> 13:04.590
That is our high resolution grid for the KDE estimate from 0 to 1 in 300 steps.

13:04.750 --> 13:05.030
Okay.

13:05.070 --> 13:07.230
So that is for the PDF estimate.

13:07.510 --> 13:10.830
I will store all of those data in this matrix.

13:10.830 --> 13:13.830
Here you can see it is the number of layers.

13:13.830 --> 13:17.630
So 32 by 300 by two.

13:17.630 --> 13:20.430
And the two corresponds to the two different analyses.

13:20.430 --> 13:25.390
So the final token to the previous tokens and all of the self-attention.

13:26.430 --> 13:29.510
Here I am looping over all of the layers.

13:29.510 --> 13:31.350
So 32 different layers.

13:31.590 --> 13:37.630
Here I'm extracting the qkv matrices that I'm splitting from this matrix.

13:37.870 --> 13:43.990
As I mentioned in the previous video, I'm hard coding this to be the first batch or the first sequence

13:43.990 --> 13:53.220
of this batch, And therefore I'm splitting along dimension one because although this is a three dimensional

13:53.260 --> 13:57.660
tensor, this here is a two dimensional matrix.

13:57.700 --> 14:02.860
I just wanted to remind you of that because yeah, this dimension stuff can get really confusing.

14:02.980 --> 14:09.900
You always need to make sure you are checking out the sizes of different matrices as you are developing

14:09.900 --> 14:10.380
code.

14:11.020 --> 14:11.460
Okay.

14:11.700 --> 14:11.980
Yeah.

14:11.980 --> 14:18.500
So as I mentioned, this will create a tuple with each head in a different element in the tuple.

14:18.740 --> 14:26.340
If you want, you can also organize these into a matrix such that the attention heads are actually just

14:26.340 --> 14:30.020
one of the dimensions of these tensors.

14:30.060 --> 14:31.900
If you want to do it that way, it's fine.

14:32.340 --> 14:36.660
But yeah, then you can actually avoid this for loop if you do it that way.

14:36.820 --> 14:39.260
But I think it's a little bit simpler anyway.

14:39.380 --> 14:40.140
Uh, okay.

14:40.180 --> 14:41.020
So that's that.

14:41.260 --> 14:41.620
All right.

14:41.620 --> 14:47.700
So now I'm looping over all of the heads and this is, you know, looping over all the heads for within

14:47.700 --> 14:49.860
each individual layer.

14:51.020 --> 14:57.500
So here we have q times k transpose divided by the square root of the dimensionality of each head.

14:57.940 --> 15:03.460
Adding the mask the causal mask and then running it through softmax.

15:03.860 --> 15:04.100
Okay.

15:04.140 --> 15:05.700
So all this code you've seen before.

15:06.020 --> 15:13.700
Here I'm getting the softmax values for the final token to all of the previous tokens, but excluding

15:13.700 --> 15:15.300
the final token.

15:15.940 --> 15:22.220
So I'm excluding the self-attention on the last token because it's included in this data set.

15:22.540 --> 15:25.180
And here I guess I'm including the first token.

15:25.220 --> 15:25.580
I don't know.

15:25.620 --> 15:27.260
You can keep it or leave it out.

15:27.540 --> 15:29.420
I don't think that's going to matter that much.

15:29.900 --> 15:38.180
Okay, then here I'm getting the diagonal of this square matrix here and excluding the first token in

15:38.180 --> 15:38.820
the sequence.

15:38.820 --> 15:41.380
And this gives me all of the self-attention.

15:41.380 --> 15:46.820
So this is q k transpose for the same token.

15:47.340 --> 15:47.700
Okay.

15:47.740 --> 15:50.420
So then yeah also just concatenating this.

15:50.620 --> 15:53.380
So now this loop is finished here.

15:53.380 --> 15:58.540
So now I've run through all of the heads in this particular layer.

15:58.980 --> 16:02.500
And now I can get from these values.

16:02.940 --> 16:11.140
So pooling all the data over all the heads I'm going to estimate the KDE and evaluate it at this high

16:11.140 --> 16:14.580
resolution grid points and then scale it down again.

16:14.620 --> 16:20.340
That's to account for differences in sample sizes between these data and these data.

16:20.460 --> 16:24.340
What we really care about here is the shape of the distributions.

16:24.340 --> 16:26.980
And not so much the exact values themselves.

16:27.260 --> 16:34.420
In fact, the the exact y axis values of those distributions will depend on this resolution here.

16:34.420 --> 16:35.740
So that's arbitrary.

16:35.780 --> 16:39.940
We really just want to be able to compare the shapes of the distributions.

16:40.340 --> 16:40.620
Okay.

16:40.660 --> 16:47.260
So then I insert them for this layer for all of the softmax probability interpolated grid points, and

16:47.260 --> 16:51.500
then this is for final to previous and self-attention.

16:51.860 --> 16:55.060
So run that analysis pretty fast.

16:55.300 --> 16:56.980
Just takes one second.

16:57.140 --> 17:00.220
And now we can visualize this.

17:00.980 --> 17:06.420
Now this is the same kind of plot that I showed in the previous video.

17:06.940 --> 17:13.180
Again in the previous video, I also had another one here for the self-attention in the very first token

17:13.180 --> 17:15.660
in the sequence for each of the heads.

17:16.260 --> 17:19.700
Now, this scatter plot here is only for the final layer.

17:19.860 --> 17:21.820
Why is this only for the final layer?

17:21.860 --> 17:27.340
Because this variable here gets overwritten for every individual layer.

17:27.460 --> 17:34.620
So if you wanted to plot these data for each layer, then you'd have to come up with a new way of organizing

17:34.820 --> 17:35.140
these.

17:35.140 --> 17:41.180
Maybe you could put these into a list or a big matrix, so that you could store them all for all of

17:41.180 --> 17:43.740
the layers, but in this case, I don't really care.

17:43.860 --> 17:50.540
I'm not storing all of these data from each layer, so therefore it gets overwritten at each iteration.

17:50.540 --> 17:56.460
And therefore when I plot it here, it's only running the data that I have stored which corresponds

17:56.460 --> 17:58.260
to the final layer.

17:58.260 --> 18:00.500
So the last transformer block.

18:00.860 --> 18:03.060
I don't want to make too big a deal of this.

18:03.220 --> 18:09.140
You do see, again, as I discussed in the previous video, that there's a lot of token pairs that get

18:09.140 --> 18:12.940
suppressed here, and this is fairly sparse up here.

18:13.060 --> 18:20.460
And that is really indicating that this attention mechanism has learned that most of the token pairs

18:20.460 --> 18:24.740
in a given token sequence should not be too strongly highlighted.

18:24.740 --> 18:31.100
We really just want to pick out the few, you know, the small number of token pairs that really should

18:31.100 --> 18:37.460
be bound together because they are important to, you know, one is important for modulating the context

18:37.500 --> 18:38.300
of the other.

18:38.340 --> 18:39.740
And that's what this shows here.

18:41.180 --> 18:41.500
Okay.

18:41.540 --> 18:44.300
Here I think I was just confirming the sizes.

18:44.300 --> 18:52.860
So 32 layers, 32 transformer blocks, 300 grid spacing and two of these analyses here.

18:53.380 --> 18:53.700
Okay.

18:53.740 --> 18:57.420
So now here I'm plotting, uh, making the line plots.

18:57.420 --> 19:00.380
So plotting one for each layer.

19:00.380 --> 19:03.780
And I'm missing an end parenthesis here in the title.

19:04.100 --> 19:04.300
Uh.

19:04.300 --> 19:04.780
Very good.

19:04.780 --> 19:05.140
Okay.

19:05.180 --> 19:05.900
Little typo there.

19:05.900 --> 19:09.900
I wonder how many of these tiny, meaningless typos I have throughout the course.

19:09.900 --> 19:11.060
I hope it's not too many.

19:11.540 --> 19:18.060
Okay, so in these plots, each color of the line corresponds to a different layer.

19:18.220 --> 19:23.860
So you do see that there are some transitions over the different layers, at least for final to previous.

19:23.900 --> 19:26.860
It's not so clear here for self-attention.

19:27.100 --> 19:30.220
This is not really the best way to visualize the data.

19:30.220 --> 19:35.340
If you want to look at how things change over the different layers.

19:35.540 --> 19:42.940
But I wanted to include this plot here so we can see the really strong differences between these two

19:43.260 --> 19:44.100
categories.

19:44.100 --> 19:47.100
So final to previous and self-attention.

19:47.500 --> 19:54.980
And in general what you see is more sparsity for the final token to the previous ones in this sentence.

19:55.140 --> 20:01.100
I don't want to make too big a deal out of these differences here, because this is just based on one

20:01.100 --> 20:04.260
sentence about the country of Fiji.

20:04.380 --> 20:10.900
It is a beautiful place, but we shouldn't overinterpret that one sentence to make grand sweeping claims

20:10.900 --> 20:12.460
about how models work.

20:12.900 --> 20:19.740
And now for the final visualization of this exercise and this code challenge as well.

20:19.980 --> 20:25.180
Those are the heat maps across all the layers and all the softmax probabilities.

20:25.340 --> 20:32.180
So essentially what we are going to do is take all of these individual lines and put them into a matrix,

20:32.180 --> 20:34.540
or actually, yeah, they're already in a matrix.

20:34.540 --> 20:42.060
But instead of representing all these data as individual lines, we will stack them all up and represent

20:42.060 --> 20:44.700
them as an image that we can color.

20:45.020 --> 20:46.740
And that's what I do here.

20:46.900 --> 20:51.340
So here I'm using I am show head distributions all the same data.

20:51.340 --> 20:55.860
So we have all of the layers all of the softmax probabilities.

20:55.860 --> 21:05.340
And then we have zero for the final to previous and then one here in the third dimension for the self-attention

21:05.340 --> 21:05.860
weights.

21:06.220 --> 21:06.540
Okay.

21:06.580 --> 21:08.820
So how do you read these images.

21:09.180 --> 21:14.980
So the color here corresponds to the strength of the probability density.

21:14.980 --> 21:18.020
And that is the height on the previous axis.

21:18.020 --> 21:24.140
So the color here corresponds to the height on this y axis on this plot here.

21:24.660 --> 21:31.820
And then we have each of these columns in here in this matrix corresponds to each of the individual

21:31.820 --> 21:32.860
lines here.

21:33.140 --> 21:39.170
So you can just see all of these layers are very high at the beginning, and then they quickly plummet

21:39.170 --> 21:41.810
to zero or very small values.

21:41.810 --> 21:49.010
And that is reflected in the color here where it's white colors for large numbers, down here for zero

21:49.010 --> 21:49.810
probability.

21:49.810 --> 21:58.450
And then it gets up to darker values corresponding to small probabilities as we increase in the softmax

21:58.450 --> 22:00.050
attention probability.

22:00.210 --> 22:07.850
So remember this y axis here is not the probability density that the KDE is estimating.

22:08.090 --> 22:16.050
This y axis corresponds to the numerical values of the softmax attention calculation.

22:16.090 --> 22:17.330
I hope that all makes sense.

22:17.330 --> 22:23.690
It's a little bit confusing to see, but what's nice about these plots, these visual representations,

22:23.850 --> 22:31.210
is you can really see the change over the different layers, in particular with this shift from basically

22:31.250 --> 22:39.450
earlier layers where the attention mechanism is suppressing all of the tokens with regards to the context

22:39.450 --> 22:42.490
information that's being loaded onto the final token.

22:42.650 --> 22:46.690
And then as we get later into the model, we get into deeper layers.

22:46.730 --> 22:52.970
Now again, the model is transitioning from processing the current token to making a prediction about

22:53.010 --> 22:54.930
what the next token should be.

22:55.330 --> 23:03.490
And there you see that a very small number of token pairs for the with the final token in the sequence

23:03.610 --> 23:06.610
have a very large softmax probability.

23:06.730 --> 23:10.570
So the model is saying, I know this word and this word.

23:10.610 --> 23:16.810
You know, there's a very small number of words, just like you see up here that are really relevant

23:16.810 --> 23:23.250
for understanding the final token and making a prediction about what the next token should be.

23:23.290 --> 23:27.970
And all of the other token pairs between the final token and the previous tokens.

23:28.290 --> 23:29.890
Those are all irrelevant.

23:29.890 --> 23:31.810
They're not context dependent.

23:31.930 --> 23:37.730
So the model is not going to use them to generate a prediction for the next token.

23:38.010 --> 23:44.290
And who knows, maybe these are words like a and the and other things that just aren't really contributing

23:44.330 --> 23:46.690
to the prediction for what should come next.

23:47.010 --> 23:53.170
One final comment I want to make about the visualization is that you can also spin this around.

23:53.170 --> 23:56.090
You can rotate this image and plot it the other way.

23:56.090 --> 24:03.170
So if you would get rid of this, uh, and then let's see, we need to change or swap the x and y axes.

24:03.730 --> 24:07.650
Then you see that now this plot has just been rotated.

24:07.650 --> 24:13.810
So now layer is on the y axis and the softmax probability is on the x axis.

24:14.090 --> 24:19.290
The numbers here are still incorrect because I did not change this extent.

24:19.650 --> 24:22.090
Uh, whereas this line over here.

24:22.090 --> 24:26.410
So the numbers are wrong, but the axis labels are right.

24:26.410 --> 24:32.370
And to be honest, I was actually a little bit unsure about whether it's better, whether it would be

24:32.370 --> 24:36.210
more clear to show it with this orientation or this orientation.

24:36.570 --> 24:40.290
I actually initially made both the plots to look like this.

24:40.450 --> 24:43.330
And then I changed my mind and decided to show it like this.

24:43.330 --> 24:49.770
So I don't think there's a real single correct and incorrect way to show these plots.

24:49.810 --> 24:55.370
As long as you are interpreting it correctly and labeling the axes correctly, then it's all fine.

24:56.410 --> 24:58.170
So that's it for this video.

24:58.170 --> 25:05.250
I'm sure you have already started to imagine the other kinds of analyses that you could do to follow

25:05.250 --> 25:07.210
up on these patterns.

25:07.690 --> 25:15.090
One line of subsequent research, for example, would be to find the exact pairs of tokens that get

25:15.090 --> 25:22.010
those extremely high attention scores and then figure out what pairs those are and see if that kind

25:22.010 --> 25:27.250
of high attention pairing is reproducible in larger data sets.

25:27.810 --> 25:32.330
That is certainly not trivial to do, but that would be a natural next step.

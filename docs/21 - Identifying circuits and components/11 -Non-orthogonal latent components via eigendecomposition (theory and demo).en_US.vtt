WEBVTT

00:02.200 --> 00:03.840
This and the next.

00:03.840 --> 00:12.840
Several videos are focused on a multivariate analysis technique called generalized eigen decomposition,

00:12.960 --> 00:19.280
also called generalized eigenvalue decomposition or abbreviated to GED.

00:20.240 --> 00:27.360
It is not the same thing as PCA, which I will also demonstrate in the code demo, although they are

00:27.360 --> 00:34.160
somewhat related analyses and they work well hand in hand as you will see in the next video.

00:34.960 --> 00:40.760
So I want to give you a heads up in advance here, because there's going to be quite a bit of linear

00:40.760 --> 00:43.960
algebra in the next half dozen slides or so.

00:44.520 --> 00:50.520
If you are not so comfortable with linear algebra, then you don't need to worry about all the math

00:50.520 --> 00:51.320
details.

00:51.440 --> 00:58.200
I will tell you the important take home points, and you'll see how to implement this method successfully

00:58.360 --> 01:02.070
in code, even if the math doesn't quite click.

01:03.270 --> 01:04.950
Let's start with the goal.

01:05.270 --> 01:09.990
Imagine these dots represent a data set in two dimensions.

01:10.390 --> 01:16.270
Perhaps these two axes here correspond to two MLP neurons.

01:16.390 --> 01:21.950
And then this shows the activation values for lots of tokens for those two neurons.

01:22.270 --> 01:24.510
So each dot would be a token.

01:24.950 --> 01:29.430
Now clearly there are two streams of data here.

01:29.430 --> 01:32.390
So there's something special about these data.

01:32.590 --> 01:35.910
And so these green data and these purple data.

01:36.110 --> 01:39.790
But how do we separate these two sources statistically.

01:39.790 --> 01:41.510
How do we identify them.

01:42.230 --> 01:49.270
Maybe these correspond to two different networks in the model or latent constructs in the for example

01:49.270 --> 01:55.270
the MLP layer of the MLP data are thousands of dimensions.

01:55.430 --> 02:00.450
But starting with two dimensions is good for building visual intuition.

02:00.450 --> 02:04.090
We cannot just draw out 4000 dimensions.

02:04.650 --> 02:13.570
So let's assume that there are multiple latent sources that get mixed together in the observed activations

02:13.570 --> 02:14.170
data.

02:14.450 --> 02:15.610
And that's what you see here.

02:15.610 --> 02:17.730
So there are two streams of data.

02:17.890 --> 02:23.450
And we want to know how we can isolate and separate these two sources.

02:24.050 --> 02:30.130
Now if we apply a linear separation method then the answer is kind of simple.

02:30.130 --> 02:35.530
We just take two different linear weighted combinations of the data channels.

02:35.690 --> 02:38.170
And that will give us two different components.

02:38.570 --> 02:44.450
Now for these data you can imagine two regression equations, one equation that describes the green

02:44.450 --> 02:48.450
dots and another equation that describes the purple dots.

02:49.570 --> 02:55.680
And maybe these correspond to two different categories of tokens like nouns and verbs, for example.

02:56.040 --> 03:00.760
But how do we determine the linear weighted combination of the two variables?

03:00.960 --> 03:03.040
How do we define those weights?

03:03.280 --> 03:09.840
Obviously, the individual variables themselves are not sufficient because the green data and the purple

03:09.840 --> 03:15.320
data all project to data variable one, and they all project to data variable two.

03:15.800 --> 03:23.800
So we want to define some axes, some linear weighted combination of the two variables that will best

03:23.800 --> 03:29.640
separate this stream of blue dots from this stream of purple dots.

03:30.320 --> 03:35.440
In fact, this question here, this problem of how to define the weights.

03:35.640 --> 03:40.600
This is like the single biggest question in machine learning and deep learning.

03:40.960 --> 03:47.720
And for example, gradient descent is a nonlinear technique to answer this question of how to define

03:47.720 --> 03:55.630
the weights such that the weighted combination of different variables optimizes some function or minimizes

03:55.630 --> 03:56.750
some loss function.

03:58.030 --> 04:06.670
So what I'm going to do now is show you a technique based on linear algebra that also optimizes the

04:06.670 --> 04:14.310
weights for some objective function in the case of generalized eigen decomposition.

04:14.630 --> 04:22.790
The objective is to come up with a set of weights such that the linear weighted combination of the variables

04:22.950 --> 04:27.670
maximizes the ratio between two different categories.

04:28.270 --> 04:32.510
So the w here corresponds to a vector of weights.

04:32.630 --> 04:36.110
So one number one weight value for each.

04:36.430 --> 04:45.030
Let's say MLP neuron and the x and y are activations matrices from the same neurons but for different

04:45.030 --> 04:45.990
categories.

04:46.190 --> 04:52.090
So for example maybe there are different sentences different tokens different categories of tokens.

04:52.490 --> 05:01.410
Let's say, for example, that x is activations for sentences about cars and y is activations for sentences

05:01.410 --> 05:02.410
about trucks.

05:03.730 --> 05:11.490
So now we want to find a weight vector such that the weighted combination of activations from different

05:11.490 --> 05:16.450
neurons maximally differentiates cars from trucks.

05:17.130 --> 05:24.810
And we could use that to try to identify a distributed component in the language model that prefers

05:24.810 --> 05:32.130
cars over trucks, while holding constant everything that is similar between cars and trucks, such

05:32.130 --> 05:32.490
as.

05:32.610 --> 05:35.530
I don't know the fact that there are both mechanical.

05:35.530 --> 05:37.850
They're both forms of transportation.

05:38.570 --> 05:46.370
Now, these vertical lines here and the square term you can interpret as like power or total energy.

05:46.730 --> 05:48.690
It's the squared vector norm.

05:48.690 --> 05:56.400
But it's basically like we are maximizing the variance in the car representation relative to the variance

05:56.400 --> 05:58.240
in the truck representation.

05:58.960 --> 06:02.800
So now the question is how do we find w.

06:02.840 --> 06:09.640
How do we find the weights such that the weighted combination of activations from different neurons

06:09.640 --> 06:14.640
or different dimensions maximally differentiates cars from trucks?

06:15.560 --> 06:24.240
So we begin by just writing out this quantity as the dot product of this vector matrix product with

06:24.240 --> 06:24.960
itself.

06:25.280 --> 06:29.560
That's literally just the definition of the squared norm of a vector.

06:30.120 --> 06:36.440
And now we can work through some of the multiplications and the transposes to get this expression here.

06:36.960 --> 06:42.200
And do you recognize this form of the data matrix times its transpose.

06:42.760 --> 06:49.870
Well that turns out to be the gram matrix of the data, although we actually want it to be mean centered

06:49.870 --> 06:52.590
so that it's a covariance matrix.

06:53.550 --> 07:01.790
So S and R here refer to covariance matrices of the activations data matrices for the two different

07:01.790 --> 07:03.710
categories of tokens.

07:04.190 --> 07:12.470
I like to call these matrices S and R, because S is like a signal matrix that you want to maximize,

07:12.790 --> 07:17.030
and R is like a reference matrix to compare against.

07:17.910 --> 07:25.070
By the way, if you are familiar with optimization, then you can also conceptualize this equation as

07:25.070 --> 07:34.150
maximizing the quadratic form on f such that the quadratic form on r is constrained to one.

07:34.750 --> 07:41.190
So that would be a typical way to set up the Lagrange multiplier solution for this kind of problem.

07:42.510 --> 07:48.530
Okay, so this is a little bit of unpacking from linear algebra, but it doesn't actually tell us how

07:48.530 --> 07:50.210
we define the w vector.

07:50.210 --> 07:55.010
It just rewrites the problem into a slightly different form with slightly different letters.

07:55.650 --> 08:01.570
So to do that, I'm going to now use some more linear algebra on the next slide.

08:02.290 --> 08:05.530
So here is the objective function rewritten.

08:05.650 --> 08:14.210
We want to find the w that maximizes this ratio, which you can think of as a signal to noise ratio.

08:14.810 --> 08:19.290
The quadratic form on each of these matrices is just a number.

08:19.610 --> 08:23.970
So this expression actually boils down to a ratio.

08:23.970 --> 08:25.330
And that is just one number.

08:25.330 --> 08:27.370
And we can call that lambda.

08:27.930 --> 08:34.650
Now I realize this is like a slight abuse of math notation, but the idea that I'm expressing here is

08:34.650 --> 08:42.730
that once we find the w that maximizes this ratio, we can call that ratio lambda.

08:43.250 --> 08:47.320
So we want to find the w that maximizes lambda.

08:48.520 --> 08:55.840
And now to find this solution you can set this up as a calculus problem with a Lagrange multiplier.

08:56.080 --> 09:04.640
But what I'm going to do is consider not just a single vector of solutions, but an entire matrix of

09:04.640 --> 09:05.480
solutions.

09:05.480 --> 09:09.640
So we get a matrix W instead of just vector w.

09:10.240 --> 09:19.960
And that allows us to rewrite this equation with the denominator as the matrix inverse and capital lambda

09:19.960 --> 09:27.800
on the left hand side, which is a diagonal matrix of ratios, one for each of the set of weights,

09:27.800 --> 09:35.480
so one for each column in W, and after expanding and simplifying the inverse operations, we arrive

09:35.480 --> 09:41.240
at this expression here, which is called the generalized eigenvalue equation.

09:41.960 --> 09:46.820
If you've taken linear algebra before, then you have certainly seen this equation.

09:46.820 --> 09:55.100
Without the R, that would be just the regular eigenvalue equation on matrix S on covariance matrix

09:55.100 --> 09:55.460
S.

09:56.100 --> 10:01.900
And if you've taken my linear algebra course, then you are certainly familiar with this generalized

10:01.900 --> 10:03.540
eigenvalue equation.

10:04.700 --> 10:07.300
Anyway, what does this all mean.

10:07.580 --> 10:16.500
So this means that to find the weights that optimize this expression here, which means maximally differentiating

10:16.500 --> 10:19.220
between, you know, cars versus trucks.

10:19.660 --> 10:27.220
In other words, finding a latent component in the data that best separates activations for cars and

10:27.220 --> 10:27.740
trucks.

10:28.300 --> 10:36.220
All you need to do is perform a generalized eigenvalue decomposition on the covariance matrices of the

10:36.260 --> 10:41.610
two data matrices, and that is a pretty neat result.

10:42.130 --> 10:48.810
By the way, I should mention that if you are very comfortable with linear algebra or the math of optimization,

10:49.090 --> 10:53.170
you've probably noticed that I've skipped several steps in here.

10:53.210 --> 10:59.610
My goal in this, and the last slide, was to give you a high level overview of the problem and the

10:59.610 --> 11:00.450
solution.

11:00.610 --> 11:05.810
And yeah, this is not like a rigorous proof of this solution or its optimality.

11:06.250 --> 11:09.690
I certainly did not single handedly invent this method.

11:09.850 --> 11:15.730
So if you really want to get into the weeds of generalized eigenvalue decomposition, plenty of other

11:15.730 --> 11:22.250
math heavy sources you can find online, including other of my courses and linear algebra books.

11:22.250 --> 11:25.210
I will reference some of these in the next video.

11:26.330 --> 11:34.450
Anyway, when you think of an eigenvalue decomposition of a covariance matrix, then you might be thinking

11:34.450 --> 11:36.690
about principal components analysis.

11:37.090 --> 11:39.120
And if you're thinking about PCA.

11:39.160 --> 11:47.120
Then you're probably thinking about orthogonal axes and other limitations of PCA for source separation.

11:47.760 --> 11:55.440
In fact, what we are doing here is not a principal component analysis, and therefore the weights vectors

11:55.440 --> 12:00.120
in W are not constrained to be orthogonal to each other.

12:00.560 --> 12:03.720
And that's what I'm going to demonstrate in the next slide.

12:03.720 --> 12:07.400
And then you'll see this illustrated in the code in a little bit.

12:08.680 --> 12:09.040
Okay.

12:09.080 --> 12:18.080
So here is this form of the generalized eigenvalue equation as a regular eigenvalue decomposition on

12:18.080 --> 12:21.560
the matrix product of R inverse times s.

12:22.160 --> 12:30.960
Now the interesting thing about that is that although R inverse and S are individually symmetric matrices

12:30.960 --> 12:37.980
because they are covariance matrices or derived from covariance matrices, the product of two symmetric

12:37.980 --> 12:42.180
matrices is itself generally not symmetric.

12:42.740 --> 12:50.220
So that means that the eigenvectors are not going to be orthogonal, because the orthogonality constraint

12:50.220 --> 12:59.100
that you get from PCA and from, like regular single matrix Eigendecomposition that orthogonality constraint

12:59.100 --> 13:03.420
only holds when the matrix is symmetric.

13:04.180 --> 13:04.660
Okay.

13:04.700 --> 13:10.940
There is one more slide of math that I need to introduce you to before getting to the coding demo.

13:11.500 --> 13:17.860
So what I've shown you in this and the past couple slides, this is all theory of generalized eigen

13:17.860 --> 13:19.100
decomposition.

13:19.100 --> 13:26.260
But when it gets to practical implementations, there are some pretty significant numerical issues that

13:26.260 --> 13:33.860
can make the analysis impossible or give very poor solutions, especially for large data sets like what

13:33.860 --> 13:36.050
we have in LMS.

13:36.730 --> 13:44.010
Fortunately, there is a pretty simple solution which is to regularize the covariance matrices, and

13:44.010 --> 13:48.170
it turns out that that will solve a lot of numerical issues.

13:48.970 --> 13:53.730
Of course, you are highly familiar with the concept of regularization.

13:54.130 --> 13:57.770
I've talked about regularization a lot in this course.

13:57.770 --> 14:01.010
For example, we have L1 and L2 regularizers.

14:01.530 --> 14:08.970
In the context of covariance matrices, there are several flavors of regularization, including L1 and

14:08.970 --> 14:13.370
L2, but also other ones like Tikhonov and shrinkage.

14:13.890 --> 14:18.050
Shrinkage regularization is simple and straightforward.

14:18.210 --> 14:23.970
It works really well for generalized eigenvalue problems with actual data.

14:24.970 --> 14:32.960
The idea of shrinkage regularization for GED is that instead of using the covariance matrix R, are.

14:33.640 --> 14:39.360
We will use a slight modification of R that is shrinkage regularized.

14:39.360 --> 14:40.520
And that looks like this.

14:40.760 --> 14:47.520
So the shrinkage version of r r tilde is basically a slightly knocked down.

14:47.520 --> 14:54.920
So reduced form of R plus some fraction of the identity matrix.

14:55.280 --> 15:03.320
And basically means this means that we are going to add tiny numbers to the diagonals of R without doing

15:03.320 --> 15:05.960
anything to the off diagonal elements.

15:06.520 --> 15:12.240
Now this alpha term here is the average of all of the eigenvalues of R.

15:12.440 --> 15:15.160
So this is just the regular eigenvalues of R.

15:15.200 --> 15:17.680
This is not the generalized eigenvalue.

15:18.920 --> 15:25.040
And gamma is a regularization parameter which varies between 0 and 1.

15:25.720 --> 15:34.220
So let's think about what happens at the two extremes of this parameter When gamma is zero, then this

15:34.260 --> 15:39.540
equation is just one -0 times r plus zero.

15:39.540 --> 15:41.540
So that means no regularization.

15:41.540 --> 15:43.060
We haven't done anything to r.

15:43.660 --> 15:49.940
And when gamma equals one then we have one minus one equals zero times r.

15:50.700 --> 15:53.500
And then so that completely obliterates r.

15:53.500 --> 15:57.740
And then we just have some scalar times the identity matrix.

15:57.780 --> 16:05.660
And that means that we just replace R with the identity matrix with the average of the eigenvalues on

16:05.660 --> 16:06.620
the diagonals.

16:07.100 --> 16:14.700
And the thing is that if you would set gamma to be one, then the regularized r is basically just the

16:14.700 --> 16:22.220
identity matrix, which means that the generalized eigendecomposition just becomes a principal components

16:22.220 --> 16:22.940
analysis.

16:23.700 --> 16:33.090
So the impact of shrinkage is that it inflates R and that guarantees a full rank matrix, which improves

16:33.090 --> 16:37.610
the numerical stability of the estimation of the eigenvalues.

16:39.170 --> 16:47.130
By the way, I realized that I'm now using shrinking and inflating in the same slide for the same analysis.

16:47.730 --> 16:56.410
The reason why this analysis is called shrinkage is that the spread of the eigenvalues of R tilde is

16:56.410 --> 17:02.650
smaller, or it's shrunk down compared to the spread of the eigenvalues of R.

17:03.650 --> 17:08.770
So this method is actually shrinking the spread of the eigenvalues.

17:09.330 --> 17:17.250
On the other hand, there is a geometric interpretation of a covariance matrix as like a hyper spheroid

17:17.250 --> 17:18.810
in a high dimensional space.

17:19.170 --> 17:28.200
And as you increase gamma, that spheroid can change from a flattened ellipse into a full unit sphere.

17:28.720 --> 17:31.960
And I'm actually going to visualize this for you on the next slide.

17:31.960 --> 17:34.920
But let's get back to this text over here anyway.

17:34.960 --> 17:44.840
So the key result is that as gamma goes from 0 to 1, the generalized eigendecomposition moves towards

17:44.840 --> 17:45.720
a PCA.

17:46.320 --> 17:53.000
And that means that the solution is going to be biased away from the directions that best separate the

17:53.000 --> 17:59.520
two categories, and towards pointing towards solutions that just maximize variance in the data.

18:00.680 --> 18:07.000
Now that's not something that you really want to do, but doing it a little bit turns out to add a lot

18:07.040 --> 18:12.880
of numerical stability, hopefully without really negatively impacting the solution too much.

18:13.520 --> 18:15.200
So how do you pick a gamma?

18:15.200 --> 18:17.200
How do you pick a value of gamma?

18:17.800 --> 18:26.060
Intuitively, you want gamma to be as close to zero as possible, but as close to one as necessary.

18:26.780 --> 18:33.460
And in general, there are two ways to pick gamma are just to fix it to some value like 1%.

18:33.460 --> 18:34.540
So 0.01.

18:35.180 --> 18:39.020
Or you can use statistical methods like cross-validation.

18:40.220 --> 18:46.140
Cross-validation to pick a parameter is something that you can do with a relatively small number of

18:46.140 --> 18:53.780
analyses, because it takes some additional time and coding, and it also requires some care to avoid

18:53.780 --> 18:57.620
overfitting the model to the training data.

18:58.340 --> 19:05.020
So what I'm going to do here and in the next few videos is just kind of arbitrarily set gamma to be

19:05.060 --> 19:06.260
0.01.

19:06.700 --> 19:12.620
That is generally small enough that it doesn't really negatively impact the solution, while still being

19:12.620 --> 19:17.820
large enough to give us full rank matrices and numerical stability.

19:19.100 --> 19:19.500
Okay.

19:19.660 --> 19:24.690
Here in this slide I want to show you what the shrinkage does geometrically.

19:25.530 --> 19:30.770
So imagine we have this three by three matrix which is like almost the identity matrix.

19:30.770 --> 19:34.490
But it has a zero on the final diagonal point here.

19:35.050 --> 19:37.730
So this is a reduced rank matrix.

19:38.130 --> 19:44.770
We can represent this matrix geometrically as like a spheroid in a three dimensional space.

19:45.290 --> 19:48.010
And natively so no regularization.

19:48.250 --> 19:49.770
The matrix looks like this.

19:49.810 --> 19:51.610
It's a flat disk.

19:51.650 --> 19:57.170
It has extension in x and y and no extension on the z axis.

19:57.330 --> 20:01.690
So it's a two dimensional surface in a three dimensional axis.

20:02.490 --> 20:09.410
And now if I would add 10% regularization that would basically inflate the third dimension.

20:09.570 --> 20:16.450
Because the shrinkage regularization will add a tiny number to the diagonal, which means that we will

20:16.450 --> 20:18.330
get a full rank matrix.

20:18.450 --> 20:22.520
And this element in the diagonal will be some non-zero number.

20:23.720 --> 20:29.880
And then if I would regularize a bit more and that would turn it even closer to a sphere.

20:30.360 --> 20:31.680
So that's the idea.

20:31.680 --> 20:38.760
By adding some regularization, you are artificially inflating the matrix to give it more volume.

20:38.760 --> 20:40.000
It gives it more bulk.

20:40.520 --> 20:46.440
That increases the numerical stability, although it does mean that you are changing the actual data

20:46.440 --> 20:47.240
values.

20:47.720 --> 20:55.040
But again, if you use one regularization, it only affects the diagonal elements and it changes them

20:55.080 --> 20:56.080
very little.

20:56.200 --> 20:59.800
So the benefits really far outweigh the risks.

21:00.680 --> 21:05.520
Okay, I realized this was a lot of math to soak in so far.

21:06.080 --> 21:12.520
I hope you were able to follow the general ideas, even if you didn't quite get all of the equations.

21:13.080 --> 21:19.040
Fortunately, you can get really far with this analysis approach even without fully understanding the

21:19.040 --> 21:19.600
math.

21:20.780 --> 21:25.420
And with that in mind, let me now tell you what I will do in the code demo.

21:26.140 --> 21:32.620
First, I'll show you how to create this X dataset that I illustrated in the beginning of the lecture.

21:33.340 --> 21:38.420
Then I will perform a principal components analysis on this dataset.

21:38.780 --> 21:41.940
Here you see the data in the original data space.

21:42.060 --> 21:46.540
And here you see it projected onto the principal components axes.

21:47.620 --> 21:55.020
Now PCA is doing exactly what it's designed to do which is orthogonally rotate the data to maximize

21:55.020 --> 21:57.420
variance along this top direction.

21:57.860 --> 22:03.740
Now that's fine, but it does not help us separate these two data streams.

22:04.460 --> 22:10.180
In other words, PCA cannot actually separate these two latent sources.

22:10.820 --> 22:16.820
Next, I will apply a generalized eigen decomposition on the exact same data.

22:17.260 --> 22:23.370
And now when we look at the data in the GED space, then we see this really beautiful separation of

22:23.370 --> 22:25.170
the two sources of data.

22:26.170 --> 22:32.170
In other words, the activation for like cars and trucks are correlated in the data space.

22:32.170 --> 22:39.850
They remain correlated in the PC space, and they are perfectly correlated in the GED space.

22:40.770 --> 22:42.930
And here you see the two eigenvectors.

22:43.250 --> 22:45.850
They are certainly correlated with each other.

22:46.570 --> 22:50.810
Next I will apply shrinkage regularization to these data.

22:50.810 --> 22:54.290
And you'll see that we no longer get perfect source separation.

22:54.290 --> 23:00.530
But instead it's like somewhere in between a good separation and a principal components result.

23:01.130 --> 23:04.370
I think this was 40% regularization.

23:04.370 --> 23:06.010
So gamma equals 0.4.

23:06.330 --> 23:11.370
That is like way, way higher than what you would actually want to use in practice.

23:12.370 --> 23:19.080
As is often the case with toy examples, the solution works perfectly here, with or without shrinkage,

23:19.080 --> 23:22.960
because the data are so simple and so easy to separate.

23:23.480 --> 23:30.480
But on the other hand, these very simple examples really nicely allow for this kind of visual intuition.

23:31.480 --> 23:37.240
So yeah, in this video I'm going to stick to this very simple toy example.

23:37.400 --> 23:43.760
In the next video I will show you some of the challenges and also some solutions that arise when trying

23:43.760 --> 23:47.160
to apply this analysis to real data.

23:47.600 --> 23:49.880
And then in the next video.

23:50.240 --> 23:56.760
The video after the next one, you will get to extend these analyses in a code challenge.

23:57.200 --> 23:59.400
Anyway, for now let's switch to code.

24:00.760 --> 24:05.400
So we need numpy for numerical processing matplotlib.

24:05.400 --> 24:12.720
And then it turns out that numpy can perform an eigenvalue decomposition, but not a generalized eigen

24:12.720 --> 24:13.760
decomposition.

24:13.980 --> 24:22.340
Fortunately, SciPy actually does have some routines for generalized Eigendecomposition solution, although

24:22.780 --> 24:29.340
if you are a Matlab user, I will say that the SciPy uh routine is just not as good.

24:29.340 --> 24:34.540
It's not as accurate as Matlab's solution, but anyway, we're going to use it nonetheless.

24:34.940 --> 24:36.860
Okay, so here I'm generating the data.

24:36.900 --> 24:38.460
Let me show you what it looks like first.

24:38.500 --> 24:44.180
This is the data that you saw in the slides in the beginning of the lecture.

24:44.460 --> 24:51.700
So what I'm doing is just taking a bunch of random numbers, stacking them up, multiplying by 0.5.

24:51.740 --> 24:55.020
That's basically going to create this disk of data.

24:55.300 --> 25:03.540
And then I just repeat the data twice multiplying it by this rotation this orthogonal rotation matrix.

25:03.740 --> 25:11.460
So I get a version of the data that is rotated by pi over six, and a version of the data that is multiplied

25:11.740 --> 25:13.770
or rotated by pi over three.

25:14.890 --> 25:16.090
Stacking them together.

25:16.090 --> 25:18.690
And that's how I create this stream.

25:18.930 --> 25:21.250
Like dual stream of correlated data.

25:21.690 --> 25:21.930
Okay.

25:21.970 --> 25:29.370
So again the idea is that we want to find a linear weighted combination of data variable one and data

25:29.370 --> 25:37.450
variable two such that we can perfectly or at least as good as possible, separate the purple dots from

25:37.450 --> 25:38.410
the green dots.

25:39.090 --> 25:39.490
Okay.

25:39.530 --> 25:41.130
So we're going to try a PCA.

25:41.610 --> 25:45.770
Here I'm implementing the principal components analysis manually.

25:45.810 --> 25:49.410
So first I mean center the data.

25:49.970 --> 25:55.330
So I subtract the mean and then I calculate the covariance matrix.

25:55.330 --> 26:02.450
So that is the data matrix times itself after mean centering divided by the number of data points.

26:02.850 --> 26:05.890
Really super duper technical statistical thing.

26:05.890 --> 26:07.090
It probably should be.

26:07.130 --> 26:11.160
You know, it should be dividing by n minus one.

26:11.160 --> 26:13.120
But when you have, you know how many data points?

26:13.160 --> 26:13.600
Do I have here?

26:13.640 --> 26:14.400
1000?

26:14.600 --> 26:14.840
Yeah.

26:14.840 --> 26:21.040
There's really no practical difference between dividing by 1000 and dividing by 999.

26:21.800 --> 26:22.520
So anyway.

26:23.000 --> 26:23.520
Uh, yeah.

26:23.520 --> 26:30.560
So then I do an eigen decomposition of the covariance matrix, and then here I'm just projecting the

26:30.560 --> 26:35.320
data onto the principal components space here.

26:35.320 --> 26:38.840
So the eigenvectors times the data.

26:39.160 --> 26:42.240
And that gives me this which you've seen before.

26:42.280 --> 26:42.520
Okay.

26:42.560 --> 26:44.760
So here we are again in the data space.

26:45.000 --> 26:47.800
This is exactly the same plot as I showed above.

26:48.160 --> 26:51.640
Here you see the first and the second principal components.

26:51.760 --> 26:58.360
And then you see what the principal components analysis does is just orthogonally rotate the data.

26:58.360 --> 27:02.120
So we have not decorrelated the two data streams.

27:02.240 --> 27:10.180
All we've done is taken the entire data set and orthogonally rotate it such that all of the or most

27:10.180 --> 27:16.060
of the variance is along one dimension, and the rest of the variance is along the other dimension.

27:16.220 --> 27:18.900
And these axes actually got mislabeled here.

27:18.900 --> 27:20.260
But that's okay.

27:20.340 --> 27:21.500
Doesn't matter here.

27:21.500 --> 27:26.100
I just wanted to show that the data itself is two by 2000.

27:26.380 --> 27:28.020
So we have x and y.

27:28.060 --> 27:29.380
So two data channels.

27:29.380 --> 27:35.940
You can think of this as like two different neurons from MLP or maybe two different embeddings dimensions

27:35.940 --> 27:41.220
from a Q matrix in the attention sublayer.

27:41.420 --> 27:43.180
And then 2000 data points.

27:43.580 --> 27:50.460
And then when I project it into principal component space, I still have two components because yeah,

27:50.500 --> 27:55.660
I'm not doing any compression or dimensionality reduction here okay.

27:55.700 --> 28:03.140
So basically PCA failed miserably at separating these two streams of data.

28:03.620 --> 28:06.140
Enter the generalized Eigendecomposition.

28:07.100 --> 28:13.890
So here what I'm doing is getting the first 1000 data points and the second 1000 data points.

28:14.130 --> 28:19.930
And, you know, maybe this corresponds to the tokens about cars and this corresponds to the tokens

28:19.930 --> 28:21.010
about trucks.

28:21.250 --> 28:29.610
So now we have a covariance of car related activations and a covariance matrix of trucks related activations.

28:30.130 --> 28:32.330
And then I use SciPy linalg.

28:34.370 --> 28:37.610
So the h here is for Hermitian I can write that out.

28:37.610 --> 28:39.890
That's Hermitian like this.

28:40.050 --> 28:46.410
And basically this just adds the assumption that each of these matrices is individually symmetric.

28:46.650 --> 28:51.210
And that basically allows for a little bit more numerical stability.

28:51.770 --> 28:53.810
We need all the help we can get here.

28:53.810 --> 29:01.090
So it's a good idea to use I'g H when you know that both of these matrices are individually symmetric.

29:02.090 --> 29:02.450
Okay.

29:02.490 --> 29:06.070
So whatever is the numerator matrix.

29:06.070 --> 29:12.910
The thing you want to optimize for that is the first input into H, and then the reference matrix,

29:12.910 --> 29:18.070
which is in the denominator of the equation that I showed in the slides.

29:18.190 --> 29:19.910
That is the second input.

29:20.390 --> 29:20.670
Okay.

29:20.710 --> 29:24.750
So then we get the eigenvalues and the eigenvectors here.

29:25.110 --> 29:29.630
Now here I am normalizing the eigenvectors.

29:29.670 --> 29:36.510
Now when you do a principal components analysis, when you do an eigen decomposition on just one symmetric

29:36.510 --> 29:42.110
matrix, the eigenvectors actually do come out being normalized.

29:42.110 --> 29:44.070
They come out being unit normed.

29:44.470 --> 29:48.270
But that is not the case for generalized eigen decomposition.

29:48.470 --> 29:56.230
For some details that are basically related to the fact that this constraint here w transpose times

29:56.270 --> 30:04.180
r times w must equal one, and that means that w transpose w is not going to equal one.

30:05.100 --> 30:07.860
If you have no idea what I'm talking about, then don't worry about it.

30:07.860 --> 30:11.580
These are just some advanced details of linear algebra.

30:11.980 --> 30:17.380
The point is that here I'm just normalizing the vectors by their norm.

30:17.620 --> 30:19.740
That is not necessary for the math.

30:19.780 --> 30:23.860
You'll see that we do not do this in real data analysis.

30:23.860 --> 30:27.060
I'm only applying it here for the visualization.

30:27.060 --> 30:28.780
It just helps the visualization.

30:29.180 --> 30:29.460
Okay.

30:29.500 --> 30:36.780
And now again I'm projecting the same data onto the generalized eigendecomposition axes.

30:37.140 --> 30:40.540
And then this plot is the same as the plot that I showed above.

30:40.940 --> 30:41.260
Okay.

30:41.300 --> 30:46.420
So again now you see that the eigenvectors are correlated with each other.

30:46.420 --> 30:49.460
These are not orthogonal eigenvectors.

30:49.460 --> 30:57.300
And because of that non orthogonality we can now get perfect you know plus some jitter due to noise.

30:57.460 --> 31:01.890
Perfect separation between like the car and the truck data.

31:02.970 --> 31:03.410
Okay.

31:03.730 --> 31:06.570
So that is for the generalized eigendecomposition.

31:06.570 --> 31:12.250
I did not apply any regularization here because yeah, it's such a beautiful problem.

31:12.450 --> 31:14.050
You don't need the regularization.

31:14.050 --> 31:16.490
In fact the regularization is only going to hurt.

31:17.090 --> 31:17.450
Okay.

31:17.490 --> 31:19.570
So and that's what I will show you now.

31:19.850 --> 31:21.890
So here I'm doing.

31:21.930 --> 31:27.930
So all of this code is exactly identical to the code that I just showed up here.

31:28.410 --> 31:33.050
But I am replacing the R with r ragu.

31:33.050 --> 31:34.810
So r regularized.

31:34.810 --> 31:36.290
And that is formed here.

31:36.290 --> 31:44.490
This is an exact implementation of or translation of the formula for regularization into code.

31:44.770 --> 31:55.770
So one minus gamma times R plus gamma times the average of the eigenvalues of R times the identity matrix.

31:56.570 --> 31:58.770
So let me let me run this.

31:58.770 --> 32:02.630
And then I will show you what this looks like, just so you can see.

32:02.990 --> 32:03.630
Let me show you.

32:03.670 --> 32:04.910
Ah, so this is.

32:04.950 --> 32:05.190
Yeah.

32:05.190 --> 32:07.350
It's a full two by two matrix.

32:07.470 --> 32:08.270
It's symmetric.

32:08.270 --> 32:09.710
It's a covariance matrix.

32:10.110 --> 32:16.510
But this term that I am adding on to R that is a diagonal matrix.

32:16.630 --> 32:18.950
And it's zeros on the off diagonals.

32:18.950 --> 32:21.510
And the diagonals themselves are really tiny values.

32:21.510 --> 32:29.630
So notice you know this is 0.005 versus these values here which is okay.

32:29.670 --> 32:36.870
So it's you know like 1 or 2 orders of magnitude smaller, uh, that I'm adding onto this matrix.

32:37.310 --> 32:37.510
Okay.

32:37.550 --> 32:39.270
And then actually I can show you this as well.

32:39.270 --> 32:45.950
So R and R Raghu, you can see that these, uh, all of these matrices.

32:45.950 --> 32:50.030
So first of all, the off diagonal elements are nearly the same.

32:50.030 --> 32:56.830
They get shrunk down a tiny bit by one minus gamma, and the diagonal elements are a little bit more

32:56.830 --> 32:59.420
different from each other, but they're very close.

32:59.460 --> 32:59.940
Like this.

32:59.940 --> 33:04.900
Regularization changes the data very, very little, which is good.

33:04.900 --> 33:07.380
We don't want to change the data too much.

33:07.980 --> 33:08.300
Okay.

33:08.340 --> 33:11.380
So that is shrinkage regularization.

33:11.620 --> 33:20.500
Now with 1% regularization, especially with such a simple beautiful problem, you don't visually notice

33:20.500 --> 33:21.540
any difference.

33:21.660 --> 33:26.420
Technically these two vectors are a little bit more.

33:26.460 --> 33:32.980
They get pushed a little bit towards orthogonal, and that's not really visible until you use a really

33:32.980 --> 33:35.340
intense amount of regularization.

33:35.780 --> 33:36.940
So it's 0.6.

33:36.940 --> 33:44.180
Now you see you know, if you think about this angle here and now this angle here is a little bit wider.

33:44.180 --> 33:46.740
So it's not quite an orthogonal rotation.

33:46.940 --> 33:51.340
But yeah, as we get closer and closer to one let's make this 0.9.

33:51.380 --> 33:54.700
I mean now this basically is principal components analysis.

33:54.900 --> 33:56.810
And you can see that.

33:56.810 --> 33:59.490
Now we can look at R and R again.

33:59.770 --> 34:02.250
So here is the regularized version.

34:03.090 --> 34:07.330
And what you see is that the diagonal elements are larger.

34:07.450 --> 34:13.170
And the off diagonal elements are not quite zero but they are close to zero okay.

34:13.210 --> 34:20.530
And then yeah as I mentioned in the slides typically you want to use something very small 1% 2%.

34:20.570 --> 34:24.450
You don't need a lot of regularization for numerical stability.

34:24.850 --> 34:27.770
I'll also demo that in the next video.

34:29.250 --> 34:35.170
Generalized eigen decomposition is a powerful and versatile method.

34:35.410 --> 34:43.770
It is used to identify and isolate patterns that are embedded in multivariate data sets that are otherwise

34:43.770 --> 34:45.930
difficult to identify.

34:46.650 --> 34:53.770
It was initially developed by, like the grandfather of statistics, who was Fisher Ronald Fisher,

34:54.630 --> 34:59.030
and it's been used in, like lots of other applications for over a century.

34:59.190 --> 35:05.310
You've probably, in fact, come across applications that are based on generalized eigendecomposition,

35:05.550 --> 35:09.110
even though they don't call it generalized eigendecomposition.

35:10.030 --> 35:16.030
For example, if you are familiar with the Fisher discriminant analysis, sometimes also called linear

35:16.030 --> 35:17.430
discriminant analysis.

35:17.630 --> 35:24.670
That is literally just an application of GED, basically exactly the same as I showed earlier in the

35:24.670 --> 35:25.390
slides.

35:26.030 --> 35:33.510
I have personally a ton of experience applying and further developing this kind of method in neuroscience

35:33.510 --> 35:38.870
data, and it works really well for multivariate large neuroscience data sets.

35:39.510 --> 35:45.510
Anyway, that said, there are some particular considerations that need to be applied when working with

35:45.510 --> 35:51.710
large data sets like LMS, and I will get into some of those details in the next video.

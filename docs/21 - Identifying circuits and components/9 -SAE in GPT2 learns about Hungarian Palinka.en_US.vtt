WEBVTT

00:02.120 --> 00:08.320
I hope you enjoyed learning about sparse autoencoders in the previous video.

00:08.840 --> 00:16.720
I'm a big fan of using simulated data to learn about and explore advanced data analysis techniques.

00:16.880 --> 00:23.480
For the reasons that I explained in the previous video, but there is no substitute for working with

00:23.480 --> 00:24.560
real data.

00:24.840 --> 00:32.920
There are uncertainties and challenges and other issues that real data force upon you that you can kind

00:32.920 --> 00:35.320
of side skirt with simulated data.

00:36.120 --> 00:43.600
Anyway, so the goal of this video is to show you what's involved in building training and interpreting

00:43.600 --> 00:48.520
autoencoders with some text that I copied from Wikipedia.

00:49.040 --> 00:55.000
It's a relatively small amount of text, so we shouldn't really expect a lot of beautiful, clean results

00:55.000 --> 00:56.640
from the latent components.

00:56.960 --> 01:01.400
But this is still a great opportunity to learn about sparse encoders.

01:01.520 --> 01:04.840
And you're also going to learn about something called Palinka.

01:05.280 --> 01:12.080
There's no new math for this video, so let me jump straight into the overview of the code demo.

01:13.040 --> 01:17.480
Here is a screenshot of the Wikipedia page about Palinka.

01:17.960 --> 01:25.040
Palinka is a sweet liquor that's made in several parts of Eastern Europe, in the Balkans, Greece and

01:25.040 --> 01:26.360
Turkey, thereabouts.

01:26.360 --> 01:32.000
It's something like a schnapps, and if you are ever in that part of the world, in particular Hungary

01:32.000 --> 01:36.320
or Romania, then I can recommend trying a little bit with food.

01:36.520 --> 01:42.720
But keep in mind that palinka can be pretty strong, and I definitely do not want to encourage drinking

01:42.760 --> 01:43.680
too much of it.

01:44.400 --> 01:51.920
Anyway, I have copied a handful of sentences from this wiki page, and that's what we're going to use

01:52.040 --> 01:58.000
to train MLP neuron activations in a sparse autoencoder.

01:58.840 --> 02:04.160
Here you see a screenshot of the autoencoder model that I am going to train.

02:04.600 --> 02:10.200
Now, some of this code you have seen in the previous video, but I've also modified a couple of things

02:10.200 --> 02:10.680
here.

02:11.160 --> 02:18.960
For one, you can see that I have commented out the line that creates the decoder and instead down here

02:18.960 --> 02:27.600
in the forward method here I multiply the latent layer by the encoder weights transposed.

02:27.920 --> 02:32.360
So that means that the decoder is tied to the encoder.

02:32.760 --> 02:39.080
The main reason I did that was to reduce the total number of parameters that we need to estimate for

02:39.080 --> 02:43.560
this model, because we just don't have that much data to train the model.

02:44.200 --> 02:49.960
And yeah, you know, if you're doing this for an actual research project, you'd want to use thousands

02:49.960 --> 02:52.800
or tens of thousands of tokens for training.

02:53.040 --> 02:57.160
And the texts that I use here, I think is 220 tokens.

02:57.800 --> 03:03.360
So another added feature to this model is a parameter that here I call k.

03:03.920 --> 03:09.640
This is another way to impose sparsity on an autoencoder model.

03:10.040 --> 03:17.280
So you can see that I define k to be either whatever the input is or to be half of the input dimension

03:17.280 --> 03:22.720
which corresponds to the number of tokens that we are processing.

03:23.080 --> 03:29.280
And you can see the impact of this parameter k down here in the forward model.

03:29.800 --> 03:34.720
So what I do is find the k largest activation values.

03:34.960 --> 03:40.720
And then I zero out any activation that is less than those top values.

03:41.120 --> 03:46.040
In other words I'm forcing lots of activation values to be zero.

03:46.360 --> 03:48.640
So that imposes sparsity.

03:49.240 --> 03:53.680
I will give a brief reminder of other parts of the model in the code demo.

03:53.680 --> 03:55.080
So when I switch to code.

03:55.080 --> 03:58.000
But that's the basics of what I've changed here.

03:58.600 --> 04:02.240
So after training I will plot the loss functions.

04:02.240 --> 04:10.720
Here you see the total loss, the sparsity loss and also the loss from what I call the decorrelation

04:10.720 --> 04:11.080
loss.

04:11.080 --> 04:16.200
This is the off diagonal covariance elements that you see down here.

04:16.520 --> 04:22.720
I explained what this does in the previous video, although I didn't actually use this penalty term

04:23.120 --> 04:25.120
in the model in the previous video.

04:26.160 --> 04:32.520
You can see that this penalty term has a big effect in the beginning, and then it basically just does

04:32.520 --> 04:39.600
its job by promoting independence among the latent components, and then once it pushes the model in

04:39.600 --> 04:46.120
the right direction, initially, it doesn't seem like the penalty term really gets used that much anyway.

04:46.480 --> 04:52.960
So now we have a trained model and we can start investigating what it looks like.

04:53.440 --> 05:00.960
So this plot here on the left shows the percentage of tokens that have non-zero activations from all

05:00.960 --> 05:02.840
of the latent components.

05:03.200 --> 05:10.800
You can see that there's over 6000 components in the hidden layer, and the total activation matrix

05:10.840 --> 05:13.560
is around 6.5% dense.

05:14.080 --> 05:22.600
Now, this is not the same use of the term density as in sparse logistic regression from several videos

05:22.600 --> 05:23.000
ago.

05:23.480 --> 05:29.800
In that case, with the regression, the density referred to the coefficients of the model, so the

05:29.800 --> 05:31.120
beta coefficients.

05:31.120 --> 05:31.760
Whereas here.

05:31.800 --> 05:38.800
Now with the autoencoders, I'm using this term density to refer to the number of non-zero activations.

05:39.480 --> 05:46.320
So this block just shows the percent of tokens that have any activation over here in the middle you

05:46.320 --> 05:49.880
can see what the activation values actually are.

05:50.360 --> 05:52.640
They're all non-negative values.

05:52.640 --> 05:59.800
And that's not something that I enforced is something that comes from the top k sparsity selection.

06:00.200 --> 06:07.160
That is to say, I did select for large activations, but I did not select specifically for positive

06:07.480 --> 06:08.240
activations.

06:08.240 --> 06:10.800
That's just how it ended up working out okay.

06:10.840 --> 06:19.160
And then over here is the non-zero activations by the percent of tokens that have any activations.

06:19.360 --> 06:25.000
Of course the y axis values have a floor of zero and a ceiling of 100%.

06:25.240 --> 06:31.920
So this plot ends up not being super insightful in the sense that it is fairly trivial that the more

06:31.920 --> 06:35.720
activations there are, the stronger those activations are going to be.

06:35.840 --> 06:39.800
But I thought it would be interesting initially, so I plotted it anyway.

06:40.400 --> 06:44.880
Okay, so now we have over 6000 latent components.

06:45.160 --> 06:51.000
What do we do with them and how do we know which of those 6000 are meaningful to interpret?

06:51.720 --> 06:57.920
In the previous video I knew the ground truth, latent sources, so I could simply correlate the ground

06:57.960 --> 07:03.800
truth data with the latent components and interpret the latent components that correlated best with

07:03.800 --> 07:05.040
the simulated data.

07:05.680 --> 07:10.280
But we don't have that luxury here because we don't know what the ground truth is.

07:10.800 --> 07:14.400
We actually, you know, we don't know why how the model works.

07:14.400 --> 07:16.480
That's kind of the goal of these analyses.

07:16.960 --> 07:26.040
So instead what we can do is sort the latent components according to some of these metrics for example.

07:27.080 --> 07:33.840
So for example we can find the component that has the most sparsity or the components that have the

07:33.840 --> 07:35.560
largest activations.

07:35.920 --> 07:40.640
I will show a few different ways of sorting and selecting components in the code.

07:41.040 --> 07:48.760
Here you see heatmaps of text heatmap of activations for one of the components, and this one I selected

07:48.760 --> 07:51.040
based on having strong activations.

07:51.520 --> 07:57.800
Now if you look through the text at what gets highlighted the strongest, you can see that this latent

07:57.800 --> 08:02.880
component seems to be selecting for something about geographical regions.

08:02.880 --> 08:11.520
So words like Hungary, Hungary, Romania, Transylvania, Italy, regions, Hungary, Greece, Slovakia,

08:11.520 --> 08:14.200
provinces, Austria, regions and so on.

08:14.560 --> 08:20.400
So it does seem to get a lot of strong activations for countries, geographical regions, regions,

08:20.720 --> 08:25.960
but it certainly doesn't activate exclusively to geographical regions.

08:25.960 --> 08:33.160
For example, you also see some activation to a period and another period here, and local variations

08:33.160 --> 08:33.840
and so on.

08:34.120 --> 08:42.080
But actually inspired by results like this, I decided to create what I called a geography score.

08:42.480 --> 08:50.200
That is, the average of the activations to the geography tokens divided by the average of the activations

08:50.200 --> 08:52.030
to all of the other tokens.

08:52.230 --> 08:53.830
And that's what you see here.

08:53.830 --> 09:01.470
So a value of one would mean that the latent component has an equal amount of activation for geography

09:01.470 --> 09:09.070
and non geography tokens and values above one mean that the model is more active for geography tokens

09:09.190 --> 09:11.190
than non geography tokens.

09:11.230 --> 09:16.550
Of course on average average across all 220 of these tokens.

09:17.830 --> 09:22.110
And of course there's lots of zeros down here because there's a lot of sparsity.

09:22.630 --> 09:27.350
Now I think this is a pretty interesting approach to picking out a component.

09:27.390 --> 09:34.390
Although as I'll show you in the code, it turned out to be not as clean as we might like, but I suspect

09:34.390 --> 09:41.110
that's due to the very small amount of data that we used for training and for calculating this score,

09:41.110 --> 09:43.750
and of course, for training the autoencoder model.

09:44.230 --> 09:47.190
Anyway, lots of interesting things to explore.

09:47.310 --> 09:53.630
So I will now switch to Python Several libraries that we will need here.

09:53.950 --> 10:00.550
And we definitely want to use the GPU for because we're training this model, the autoencoder model,

10:00.550 --> 10:06.950
and it's pretty big, considerably bigger than the autoencoder model in the previous video and in the

10:06.950 --> 10:08.790
code challenge in the next video.

10:08.830 --> 10:11.590
We really, really want to use a GPU.

10:12.030 --> 10:12.390
Okay.

10:12.430 --> 10:18.510
So then here I import the GPT two, uh, the model and the tokenizer.

10:18.870 --> 10:23.590
And here I'm setting up a hook to grab the MLP activations.

10:23.590 --> 10:29.870
I get it from the fully connected layer before it goes through the jlu non-linearity.

10:30.110 --> 10:37.910
The hook function is very simple because yeah, in this demo file I'm only going to hook one layer.

10:37.910 --> 10:42.310
In the next video, we will hook all of the layers, which we've done multiple times before.

10:42.670 --> 10:49.150
And here's something that I haven't done in a while, which is grab an output of this, uh, register

10:49.310 --> 10:52.550
forward hook method on this model component.

10:52.790 --> 10:59.070
And the reason why I'm doing this is because I'm going to delete this hook function.

10:59.070 --> 11:03.310
So I'm going to use this handle to remove this function from the model.

11:03.590 --> 11:07.230
We actually do not need to do that in this case.

11:07.230 --> 11:13.430
But I'm just doing it as a reminder of how that works in case you need to use it in the future.

11:13.790 --> 11:18.510
As you know, I typically do not do this because yeah, typically don't need to.

11:18.590 --> 11:22.230
But yeah, just thought I'd be give you a little reminder of that.

11:22.630 --> 11:22.950
Okay.

11:22.990 --> 11:24.470
So here we have text again.

11:24.470 --> 11:31.910
This is just copy pasted from this Wikipedia page which you can read further if you'd like to learn

11:31.910 --> 11:33.430
more about Palinka.

11:33.950 --> 11:34.310
Okay.

11:34.350 --> 11:38.030
So yeah there are 220 tokens in total here.

11:38.030 --> 11:42.350
I'm pushing the tokens through the model and here I am removing the hook.

11:42.390 --> 11:42.950
Again.

11:42.990 --> 11:47.830
This is not necessary uh, for this video, but yeah, just a nice little reminder.

11:48.510 --> 11:49.030
Okay.

11:49.070 --> 11:55.110
And then I grab the MLP activations, just printing out the size.

11:55.110 --> 11:57.430
And then I push those to the GPU.

11:57.790 --> 12:00.030
And why am I pushing these to the GPU?

12:00.350 --> 12:08.430
Because these are the MLP activations that I am going to use to train the auto encoder model.

12:08.510 --> 12:16.510
So the auto encoder will take these data as inputs and then do some fancy magical auto encoding stuff.

12:16.950 --> 12:22.270
Of course it's not magic, but you know, gradient descent and etc. etc. and then produce an output

12:22.270 --> 12:26.470
that will initially be just some random transformation of the input.

12:26.470 --> 12:34.070
And then through gradient descent, the autoencoder will try to find a set of sparse transformations

12:34.070 --> 12:41.470
that gets the output of the autoencoder to look like these MLP activations as closely as possible.

12:42.710 --> 12:46.790
Okay, here is the code where I create the autoencoder.

12:47.030 --> 12:48.070
So let me see.

12:48.070 --> 12:50.510
I just want to highlight a couple of things.

12:50.510 --> 12:53.430
So first of all about the decoder.

12:53.750 --> 13:02.710
So yeah here instead of pushing this the output of the latent layer through a separate decoder set of

13:02.710 --> 13:09.150
weights that would need to be trained by the model, I'm using the transpose of the encoder weights

13:09.670 --> 13:17.710
and multiplying those by the output of the latent layer, which is what we get from here.

13:18.270 --> 13:19.910
Again, this is a choice.

13:19.910 --> 13:22.230
There is no real right or wrong.

13:22.430 --> 13:28.950
But in general, if you don't have a huge amount of data to train the autoencoder, then it's often

13:28.950 --> 13:35.710
beneficial to tie the weights because it means that you are reducing the number of parameters that the

13:35.710 --> 13:41.390
model needs to learn through gradient descent, and that is the primary reason why I did it here.

13:41.830 --> 13:45.910
Uh, of course you can try this with a separate decoder.

13:46.070 --> 13:47.910
I think it will also work fine.

13:47.910 --> 13:53.590
I do remember that when I was developing this code, when I was writing this code file, I tried it

13:53.630 --> 13:55.070
with the decoder in.

13:55.630 --> 13:58.790
I think I found that it was slightly better with the tide weights.

13:58.790 --> 13:59.590
But anyway.

14:00.030 --> 14:01.870
Okay, so here's this parameter k.

14:01.910 --> 14:05.230
I've already described it in the slides as a quick reminder.

14:05.590 --> 14:10.670
It determines the sparsity of the activations.

14:10.670 --> 14:18.030
So k is either a parameter that we can provide as input or by default it will just take half of the

14:18.030 --> 14:18.670
input.

14:18.670 --> 14:19.870
And what does this mean.

14:20.150 --> 14:30.670
This means that for example if we set this to be 0.5, so 50% of the input dimension of the 220 tokens,

14:30.710 --> 14:38.910
110 of them will be zeroed out, the smallest activations will get zeroed out, and that will promote

14:38.910 --> 14:39.670
sparsity.

14:40.670 --> 14:49.070
That is going to force the model to learn a sparse representation of the data, as opposed to an overcomplete

14:49.070 --> 14:51.510
and redundant representation of the data.

14:52.070 --> 14:54.030
So again, you know, that's what you see here.

14:54.030 --> 14:55.470
So here's the forward model.

14:55.630 --> 15:01.510
I take the input data which are the MLP activations from one layer.

15:01.910 --> 15:03.590
Push it through the encoder.

15:03.590 --> 15:10.590
So the beginning and then I apply a non-linearity I think in the previous video I used Jlu and here

15:10.590 --> 15:11.430
I use ReLU.

15:11.470 --> 15:15.790
There's uh for very large models like language models.

15:15.950 --> 15:22.270
It does seem to be the case that the Jlu non-linearity outperforms ReLU non-linearity.

15:22.430 --> 15:28.070
It's not really clear if, uh, there is much of an advantage for smaller models.

15:28.470 --> 15:35.270
And furthermore, ReLU is a little bit better than jlu at promoting sparsity.

15:35.750 --> 15:37.110
And why is that the case?

15:37.270 --> 15:43.190
Because remember that all of the negative activation values get completely obliterated.

15:43.190 --> 15:46.230
They all turn to zero with ReLU.

15:46.390 --> 15:53.990
Whereas with Gelu there actually is a little bit of a negative pass in the filter, which means that

15:53.990 --> 15:57.630
with the ReLU activation you do get negative numbers.

15:57.630 --> 16:04.390
They're just, you know, numerically closer to zero versus the original values that you pass in through.

16:04.430 --> 16:04.990
Gelu.

16:06.070 --> 16:06.430
Okay.

16:06.470 --> 16:06.750
Yeah.

16:06.750 --> 16:11.950
So ReLU promotes sparsity a little bit better than Gelu.

16:11.950 --> 16:16.310
And sparsity is something that we are looking for in these kinds of models.

16:16.710 --> 16:17.030
Okay.

16:17.070 --> 16:19.550
So here I take all of the activations.

16:19.550 --> 16:26.710
So these are the latent component activations I find the top k which is this parameter I just discussed.

16:26.950 --> 16:32.150
And then I find the smallest of the top k values.

16:32.150 --> 16:35.350
So the largest let's say 100 tokens.

16:35.510 --> 16:38.950
And then I find the smallest of the largest tokens.

16:39.070 --> 16:40.630
And then I create this mask here.

16:40.630 --> 16:47.350
And I say where is any of the activations anywhere from any component and any token.

16:47.750 --> 16:55.110
Where are those less than or in this case, greater than the threshold and those end up being ones.

16:55.110 --> 16:58.350
And here I multiply that by this latent here.

16:58.470 --> 16:58.790
Okay.

16:58.830 --> 16:59.990
So what does that mean.

17:00.230 --> 17:07.550
That means that I am preserving the strongest K activations and wiping out all the other activations.

17:07.550 --> 17:12.990
But I'm not specifically saying which activations those are or which tokens they could be in or which

17:12.990 --> 17:14.390
components they can be in.

17:14.550 --> 17:18.830
So that is driven by the activation patterns in the model.

17:19.230 --> 17:19.470
Okay.

17:19.510 --> 17:21.230
So then we have the sparsity loss.

17:21.230 --> 17:24.470
This I've already explained several times.

17:24.470 --> 17:31.790
Here is the covariance penalty term which I described and walk through the code for in the previous

17:31.790 --> 17:32.270
video.

17:32.270 --> 17:38.710
So just as a quick reminder here, I look at the covariance across all of the latent components.

17:38.830 --> 17:44.310
This tells me about the correlation patterns across the different pairs of components.

17:44.310 --> 17:50.270
And then here I'm zeroing out the diagonal and then I just square the off diagonal elements.

17:50.270 --> 17:54.790
The squaring is partly just to get rid of the negative covariances.

17:55.030 --> 18:05.270
So essentially what this is doing is removing all of the or like mildly suppressing any of the relationships,

18:05.270 --> 18:08.430
the linear relationships across the different components.

18:08.590 --> 18:14.870
And what that does is help the model learn that the latent components should be independent of each

18:14.910 --> 18:15.350
other.

18:15.910 --> 18:17.750
Okay, so that's that.

18:17.750 --> 18:23.590
And now here I'm creating an instance of the model and I'm pushing it to the GPU.

18:23.950 --> 18:24.190
Okay.

18:24.230 --> 18:25.790
So now we're ready for the training.

18:25.790 --> 18:30.910
We use MSE loss here I have a learning rate of 0.0001.

18:31.190 --> 18:35.470
In general, these autoencoders need a very gentle learning rate.

18:35.470 --> 18:37.870
You don't want to push them too hard, too fast.

18:38.670 --> 18:43.590
And then yeah, 100 epochs, which is fine for here in the code challenge.

18:43.590 --> 18:49.270
In the next video, we will want this to be a little bit smaller just because the computation time gets

18:49.270 --> 18:50.030
pretty long.

18:50.630 --> 18:50.910
Okay.

18:50.950 --> 18:52.190
And now we can train.

18:52.190 --> 18:54.350
So the training is pretty straightforward.

18:54.350 --> 18:56.310
We do the forward pass.

18:56.710 --> 18:58.870
We get the different loss terms.

18:59.390 --> 19:08.750
So the L1 sparsity loss the decorrelation loss and then the MSE loss plus the L1 plus the decorrelation

19:08.750 --> 19:09.110
loss.

19:09.110 --> 19:11.390
And that is the total loss here.

19:11.910 --> 19:15.670
And then here I do backprop based on the total loss.

19:15.750 --> 19:21.390
And here I'm just storing all the losses so we can visualize them a little bit later.

19:21.390 --> 19:28.390
And then every 11th epoch I just print out how we're doing here and what the losses are and how far

19:28.390 --> 19:29.270
along we're getting.

19:29.550 --> 19:33.310
So let me start running that and then finally here after.

19:33.350 --> 19:34.830
So you can see it goes very fast.

19:35.230 --> 19:41.710
Uh, here after we train the whole model, then I run through the model once more to get.

19:41.750 --> 19:43.950
I don't really care about the outputs.

19:43.950 --> 19:47.390
These outputs should match pretty closely to the inputs.

19:47.630 --> 19:49.870
Uh, and here is what I really want.

19:49.870 --> 19:50.550
The latent layer.

19:50.590 --> 19:53.550
This is what we are going to continue analyzing.

19:54.470 --> 20:02.110
Now you might say that we don't need this code over here because we already have the data here from

20:02.110 --> 20:03.790
the final run through the epoch.

20:04.150 --> 20:05.190
That is kind of true.

20:05.190 --> 20:06.950
But keep in mind two things.

20:06.950 --> 20:13.070
One, on the final training run, we still do another final backprop call.

20:13.190 --> 20:15.230
And that happens after this.

20:15.230 --> 20:21.150
So the model is still going to learn a little bit after this final call to this model.

20:21.590 --> 20:29.910
Secondly, um, if we have some features in the model like Dropout or BatchNorm, then we do not want

20:29.910 --> 20:35.830
to apply those when we are grabbing the activations, the internal activations for analysis.

20:36.350 --> 20:39.630
If what I just said is obvious to you, then that's great.

20:39.990 --> 20:44.150
But this is a question that I get sometimes, so I thought I would mention it explicitly.

20:44.630 --> 20:54.070
Okay, so now we see the shape of the latent layer activations is 220 for the tokens and 6144 for the

20:54.070 --> 20:55.990
number of components.

20:55.990 --> 20:57.870
So lots of latent components.

20:58.550 --> 20:58.750
Yeah.

20:58.790 --> 21:00.070
Here we see the losses.

21:00.070 --> 21:02.590
I'm not really going to say that much about this.

21:02.630 --> 21:04.870
Just that it drops down pretty nicely.

21:05.230 --> 21:09.990
You can see from this plot that we don't really need 100 epochs.

21:09.990 --> 21:15.390
Maybe we could use 100 epochs if we would lower the learning rate a little bit more.

21:15.630 --> 21:22.190
But this is going to be my justification in the next video for training fewer epochs so we can save

21:22.190 --> 21:23.510
some computation time.

21:24.670 --> 21:29.910
Okay, here I'm just showing all of the latent activations and what you see here.

21:29.950 --> 21:33.470
Let me actually reduce the color scale a little bit more.

21:34.590 --> 21:38.750
So what you see here is that there's lots of vertical stripes right.

21:38.790 --> 21:42.110
So this on the x axis is the latent components.

21:42.110 --> 21:44.150
There's 6000 of them.

21:44.150 --> 21:46.390
And here are the 220 tokens.

21:46.390 --> 21:54.030
So what you see visually is that there are some components that respond a lot to all the tokens and

21:54.070 --> 21:59.910
other components that just really yeah, maybe they respond a little bit, but they're just really not

21:59.910 --> 22:01.030
very active.

22:01.150 --> 22:06.470
So this is already visual evidence of some really great sparsity okay.

22:06.630 --> 22:11.910
So now we can start having a look at what these components look like.

22:11.910 --> 22:16.470
So here I'm calculating the density of each component.

22:16.910 --> 22:18.230
And how do I do that.

22:18.230 --> 22:20.830
So I start with this density mask.

22:20.870 --> 22:25.110
This is a matrix of the same size as the latent activations.

22:25.110 --> 22:27.270
So tokens by components.

22:27.550 --> 22:29.990
And then I fill it in with a bunch of Nans.

22:30.510 --> 22:40.630
And then what I do is replace any of the elements in this density mask with a value of one if they have

22:40.630 --> 22:42.910
a non-zero latent activation.

22:43.110 --> 22:50.030
So if there's any activation, then that becomes a one, and the rest of these values in here become

22:50.070 --> 22:50.670
Nan.

22:51.230 --> 22:55.110
So that is handy because I can use this function Nan sum.

22:55.270 --> 23:01.550
So now I'm summing down all of the tokens for every component.

23:01.550 --> 23:06.270
And then dividing by the number of tokens and then multiplying by 100.

23:06.270 --> 23:07.670
So what is this going to be.

23:07.910 --> 23:16.470
This is the percentage of tokens per component for which there was at least a little bit of non-zero

23:16.510 --> 23:17.510
activation.

23:17.630 --> 23:21.510
So for example you know imagine this component over here.

23:21.550 --> 23:23.910
Maybe it's component 1900 or whatever.

23:24.270 --> 23:31.110
This component did not give a single non-zero activation for any of the 220 tokens.

23:31.310 --> 23:34.150
Then its value here would be zero.

23:34.670 --> 23:40.550
On the other hand, maybe this component over here you can see this like slightly lighter stripe over

23:40.550 --> 23:40.990
here.

23:41.230 --> 23:47.790
Maybe this component responds with some non-zero activations to literally every single token.

23:47.790 --> 23:52.190
So all 220 tokens, it had non-zero activation.

23:52.350 --> 24:01.430
And so its score in this vector will be 100 for 100% of the tokens got some non-zero activation.

24:02.670 --> 24:02.990
Okay.

24:03.030 --> 24:09.510
And then so this is just the proportion or the percentage of tokens that had some activation.

24:09.510 --> 24:14.590
We don't actually know from this vector what the activations were.

24:14.950 --> 24:16.870
And that is what I do here.

24:16.870 --> 24:22.630
So here I take the average of latent component times the density matrix.

24:22.830 --> 24:27.470
And this is going to tell me of those non-zero components.

24:27.470 --> 24:31.830
What was the average magnitude of the activation.

24:32.030 --> 24:36.870
And I'm taking the average because or the absolute value before I take the average.

24:36.870 --> 24:42.310
Because if there are negative values and positive values, I don't want that to average out to zero.

24:42.350 --> 24:48.750
I just want to know what is the magnitude of the response to a given token.

24:49.110 --> 24:54.950
Now, that said, I think you don't really need this with the amount of sparsity that I have forced

24:54.950 --> 24:58.950
in this autoencoder because they all end up being positive anyway.

24:59.470 --> 24:59.870
Okay.

24:59.910 --> 25:00.750
So, uh, yeah.

25:00.790 --> 25:05.790
And then I'm multiplying the latent density by the non-zero activations.

25:06.070 --> 25:11.270
In the next video we are going to calculate these metrics over all of the layers.

25:11.270 --> 25:12.670
This is just for one layer.

25:12.790 --> 25:16.990
And then I'm actually also going to scale these values slightly differently.

25:17.430 --> 25:17.670
Okay.

25:17.710 --> 25:19.190
So then we can visualize these.

25:19.230 --> 25:22.950
This shows the results that I showed in the slides.

25:23.150 --> 25:29.870
So it's pretty interesting to see that a lot of the components were either not at all active or were

25:29.870 --> 25:30.990
very active.

25:31.190 --> 25:32.430
So a small.

25:32.470 --> 25:33.910
So let me start that again.

25:34.030 --> 25:41.950
So most components actually don't show any activation whatsoever because of the sparsity in position,

25:42.830 --> 25:49.750
a relatively small number of components show activation to all or maybe nearly all.

25:49.790 --> 25:54.550
Maybe, you know, over 97% of the tokens had activation.

25:54.550 --> 25:57.150
And then we have some other activations in the middle.

25:57.790 --> 26:04.310
If you would like this is not analysis that I ran, but if you find it interesting, maybe you can actually

26:04.310 --> 26:12.190
break this up and see like literally count how many of these 6000 components had no activation to any

26:12.190 --> 26:16.310
of the tokens, activation to all or maybe almost all.

26:16.430 --> 26:18.990
And then how many were somewhere in the middle?

26:19.670 --> 26:20.070
Okay.

26:20.110 --> 26:20.630
Very good.

26:20.670 --> 26:23.870
So what I'm going to do now is set up this heat map.

26:24.070 --> 26:30.270
This is code to just identify the width of a letter in a figure.

26:30.310 --> 26:34.780
This code you've seen before, This code you've also seen before.

26:34.780 --> 26:39.740
So min max scaling some activation pattern and drawing the text.

26:39.740 --> 26:44.420
And as a reminder here I'm also squaring the activation.

26:44.420 --> 26:50.460
That just helps with the visual interpretation, because the large activations will be brighter and

26:50.460 --> 26:53.060
the dim activations will be dimmer.

26:53.620 --> 26:53.900
Okay.

26:53.940 --> 26:59.580
Otherwise, this is mostly code that you've seen before, uh, in several earlier videos.

26:59.700 --> 27:05.940
The main difference is that I put all of the code to draw the text heat map into a function.

27:06.060 --> 27:11.700
And the reason why I do that here is because I'm going to sort and pick components.

27:11.700 --> 27:13.660
And then I want to call that function.

27:13.900 --> 27:14.100
Okay.

27:14.140 --> 27:15.100
And did I run.

27:15.620 --> 27:15.900
Yeah.

27:15.940 --> 27:17.180
I also ran this one.

27:17.580 --> 27:18.140
All right.

27:18.180 --> 27:19.300
Let's see where we are.

27:19.460 --> 27:19.940
Okay.

27:20.340 --> 27:25.540
So here I am, uh, coming up with two sorting indices here.

27:25.540 --> 27:30.420
I'm sorting by the activation of the so all the non-zero activations.

27:30.780 --> 27:35.060
And here I'm sorting by the density of the latent components.

27:35.060 --> 27:37.180
This is just a reminder of the size.

27:37.660 --> 27:42.740
Okay, so now the idea here is I'm going to call that function that generates a heat map.

27:42.860 --> 27:45.580
And we pick one of the components.

27:45.580 --> 27:47.380
So let's start with this one.

27:47.700 --> 27:51.940
Of course the autoencoder model is random.

27:51.940 --> 27:58.780
So the fact that I'm picking 1 or 0 is not going to give me exactly the same results each time I run

27:58.780 --> 28:03.300
this entire code file and retrain the sparse autoencoder.

28:04.540 --> 28:10.340
So this is a component, a latent component that was just very active overall.

28:10.500 --> 28:13.260
And here you see what kinds of things it liked.

28:13.260 --> 28:17.780
So it likes Romania Hungary similar Czech Republic Greece.

28:17.980 --> 28:24.140
So it kind of seems like it uh, this component like of course I'm using the word likes in this very

28:24.420 --> 28:28.260
personified incorrect, uh, loose colloquial sense.

28:28.580 --> 28:36.900
Uh, but this component, uh, Responded a lot to countries, but also to PD for in PD.

28:36.900 --> 28:38.900
Oh, I'm not sure what that even means.

28:39.180 --> 28:41.620
Uh, distilled periods pulp.

28:41.660 --> 28:44.100
It responds a lot to pulp and so on.

28:44.100 --> 28:47.460
So it's not doesn't seem like it's really specific.

28:47.500 --> 28:49.380
It likes fruit juice, for example.

28:49.780 --> 28:50.020
Okay.

28:50.020 --> 28:51.420
Let's look at some other components.

28:51.420 --> 28:53.100
Here we see component ten.

28:53.580 --> 28:57.260
Uh this one seems to like protected and PD.

28:57.300 --> 29:01.700
Initially I thought this might be you know, it likes words that start with a capital P, but that's

29:01.700 --> 29:06.780
also clearly not the case by the way, as you get to later and later components.

29:07.020 --> 29:08.260
Uh, here this one.

29:08.300 --> 29:08.500
Yeah.

29:08.540 --> 29:12.460
Has a seems to respond strongly to a couple of words.

29:12.460 --> 29:14.620
They get a little bit less sensible.

29:14.740 --> 29:19.540
But also eventually you're going to run into this kind of thing where you get a warning message and

29:19.540 --> 29:21.100
this is all gray.

29:21.260 --> 29:22.500
And what does that mean?

29:22.940 --> 29:26.940
That means that this component 1000 was all zeros.

29:26.940 --> 29:28.860
It did not respond to anything.

29:29.100 --> 29:34.740
So uh, there's so there's nothing to plot and that you're going to see as you get, uh, later, though,

29:34.740 --> 29:38.020
the first couple of hundred components, this one is really sparse.

29:38.020 --> 29:43.180
The first couple hundred components will have lots of non-zero activations.

29:43.340 --> 29:50.340
And then eventually you get to the part of the, uh, latent components, the sorted latent components,

29:50.340 --> 29:53.380
where they really just have no activations whatsoever.

29:55.100 --> 29:57.100
It's hard to know how to interpret these.

29:57.140 --> 29:57.380
Right.

29:57.420 --> 30:01.580
So maybe you look at this and you say, oh, this likes Parentheticals, right?

30:01.580 --> 30:06.820
It's the, uh, the opening and the closing of a parenthesis, but you don't see the same pattern here.

30:06.980 --> 30:10.300
And that's inconsistent with this strong activation here.

30:10.620 --> 30:17.660
So it is certainly not the case that every component is intrinsically interpretable.

30:18.100 --> 30:24.620
Uh, on the other hand, it is the case that the more data you throw into these models, the more you

30:24.620 --> 30:30.540
can train these models, the more likely it is that you will get interpretable components.

30:31.380 --> 30:38.220
Okay, so the last set of analyses I will show you is looking for geography related words.

30:38.540 --> 30:44.460
So I looked through all of this text and I picked out the words that seemed to correspond to something

30:44.460 --> 30:45.580
about geography.

30:45.860 --> 30:48.340
Uh, and yeah, that's, uh, these terms here.

30:48.660 --> 30:56.740
And then what I do is basically just come up with this, uh, mask where, uh, for all of the 220 tokens,

30:56.900 --> 31:04.260
they are true if they are geography related and false if they are non geography related, which means

31:04.260 --> 31:07.380
not fitting into this list of terms.

31:07.940 --> 31:13.980
And then I loop through all of the components and I calculate this score here.

31:14.140 --> 31:23.620
So it is uh the uh average non-zero activations for the target words divided by the average of the non-zero

31:23.620 --> 31:25.980
activations for the non-target words.

31:26.340 --> 31:33.820
And I put that in this if statement over here, just to say that this is filtering out any components

31:33.820 --> 31:36.140
where there's no activations at all.

31:36.140 --> 31:40.540
So these really like completely sparse components that have no activations.

31:40.740 --> 31:45.300
Otherwise you're just going to end up dividing by zero and just getting a bunch of noise.

31:45.660 --> 31:45.860
Yeah.

31:45.860 --> 31:50.460
So here we see this geography selectivity score here.

31:50.820 --> 31:59.180
So these couple of components up here they seem to respond more to tokens related to geography versus

31:59.940 --> 32:01.300
non tokens okay.

32:01.340 --> 32:05.700
So then I sort those and I can plot for that.

32:05.980 --> 32:13.900
This is a great example because it shows where a very simple measure like this, without any real deeper

32:13.900 --> 32:19.180
investigation or more carefully done analyses can really lead you astray.

32:19.220 --> 32:25.460
So it shows the importance of inspecting and being critical of your analyses.

32:25.820 --> 32:31.980
So this score, this component must technically have a high g G0 score.

32:31.980 --> 32:33.140
But what does that mean?

32:33.180 --> 32:36.260
Probably it responded a lot to regions.

32:36.500 --> 32:38.860
So this had some non-zero activation.

32:38.860 --> 32:44.540
And I suspect that all the rest of these tokens with three exceptions, are zero.

32:44.780 --> 32:51.060
So technically this one does get a bit higher, but it's not necessarily that meaningful because of

32:51.060 --> 32:52.100
the sparsity.

32:52.700 --> 32:58.260
Similar here we get Czech Republic and Union, but we also get naught.

32:58.300 --> 33:00.260
That has a really strong activation.

33:00.620 --> 33:03.140
Let's see here I'm just poking around geographical.

33:03.180 --> 33:05.300
This really likes the word geographical.

33:05.740 --> 33:09.300
And yeah Hungary fruit and those.

33:09.340 --> 33:10.940
Yeah I'm not sure what to make of these.

33:10.980 --> 33:14.540
Sometimes you get some more sensible results, sometimes you don't.

33:14.940 --> 33:22.140
In part this just also depends on how well the autoencoder model actually does.

33:22.180 --> 33:22.420
Right.

33:22.460 --> 33:29.860
So you could try rerunning this code a couple of times until you get a smaller loss function closer

33:29.860 --> 33:32.620
to zero, and then you can interpret that model.

33:33.500 --> 33:39.460
If you have worked with autoencoders before, then I'm pretty sure you will agree with me that they

33:39.460 --> 33:44.900
are very cool and very interesting, but they can also be kind of tricky to work with.

33:45.020 --> 33:52.300
That is especially the case here because of the relatively small amount of data, and also because of

33:52.300 --> 33:56.060
the challenges of picking a component to focus on.

33:56.660 --> 34:02.140
And actually picking a component in these kinds of latent source analyses is quite tricky.

34:02.180 --> 34:07.020
It's certainly not unique to sparse autoencoder models.

34:07.460 --> 34:14.380
In the next video, we are going to continue exploring sparse autoencoders, but at like a higher level

34:14.380 --> 34:15.500
of granularity.

34:15.740 --> 34:21.820
And that will allow us to examine some sparsity characteristics over the different layers of a language

34:21.820 --> 34:22.420
model.

34:22.700 --> 34:23.780
I will see you then.

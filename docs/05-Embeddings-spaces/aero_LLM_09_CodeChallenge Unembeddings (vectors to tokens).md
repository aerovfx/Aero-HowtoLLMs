
<!-- Aero-Navigation-Start -->
[üè† Home](../../index.md) > [05 Embeddings spaces](../index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../index.md)
- [üìö Module 01: LLM Course](../../01-LLM_Course/index.md)
- [üî¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../04-buildGPT/index.md)
- [üéØ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [üîç Module 19: AI Safety](../../19-AI-safety/index.md)
- [üêç Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Unembedding trong M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn: T·ª´ Vector ·∫®n ƒê·∫øn Token

## T√≥m t·∫Øt

Trong c√°c m√¥ h√¨nh ng√¥n ng·ªØ d·ª±a tr√™n Transformer, qu√° tr√¨nh ‚Äúembedding‚Äù √°nh x·∫° token r·ªùi r·∫°c sang kh√¥ng gian vector li√™n t·ª•c. Tuy nhi√™n, b∆∞·ªõc ng∆∞·ª£c l·∫°i ‚Äì chuy·ªÉn t·ª´ vector ·∫©n sang ph√¢n ph·ªëi x√°c su·∫•t tr√™n token ‚Äì ƒë∆∞·ª£c g·ªçi l√† **unembedding**. B√†i vi·∫øt n√†y ph√¢n t√≠ch n·ªÅn t·∫£ng to√°n h·ªçc c·ªßa unembedding trong [GPT-2](chatgpt://generic-entity?number=0), l√†m r√µ vai tr√≤ c·ªßa weight tying, t√≠ch v√¥ h∆∞·ªõng, softmax v√† c·∫•u tr√∫c h√¨nh h·ªçc c·ªßa kh√¥ng gian t·ª´ v·ª±ng.

---

## 1. Gi·ªõi thi·ªáu

Qu√° tr√¨nh x·ª≠ l√Ω vƒÉn b·∫£n trong m√¥ h√¨nh ng√¥n ng·ªØ c√≥ th·ªÉ t√≥m t·∫Øt:

\[
\text{Token} 
\rightarrow 
\text{Embedding} 
\rightarrow 
\text{Transformer layers} 
\rightarrow 
\text{Hidden state} 
\rightarrow 
\text{Unembedding} 
\rightarrow 
\text{Softmax}
\]

N·∫øu embedding l√† √°nh x·∫°:

\[
f: \mathcal{V} \rightarrow \mathbb{R}^d
\]

th√¨ unembedding l√† √°nh x·∫° ng∆∞·ª£c:

\[
g: \mathbb{R}^d \rightarrow \mathbb{R}^{|\mathcal{V}|}
\]

---

## 2. Embedding: T·ª´ token ƒë·∫øn vector

Gi·∫£ s·ª≠ t·ª´ v·ª±ng c√≥ k√≠ch th∆∞·ªõc \( |V| \), ma tr·∫≠n embedding:

\[
E \in \mathbb{R}^{|V| \times d}
\]

V·ªõi token ch·ªâ s·ªë \( i \):

\[
\mathbf{v}_i = E[i]
\]

N·∫øu bi·ªÉu di·ªÖn one-hot \( \mathbf{x}_i \):

\[
\mathbf{v}_i = \mathbf{x}_i E
\]

---

## 3. Unembedding: T·ª´ vector ƒë·∫øn token

Sau khi qua c√°c l·ªõp Transformer, ta thu ƒë∆∞·ª£c hidden state:

\[
\mathbf{h}_t \in \mathbb{R}^d
\]

ƒê·ªÉ chuy·ªÉn sang logit:

\[
\mathbf{z} = W_U \mathbf{h}_t
\]

Trong ƒë√≥:

\[
W_U \in \mathbb{R}^{|V| \times d}
\]

Vector logit:

\[
z_i = \mathbf{w}_i \cdot \mathbf{h}_t
\]

---

## 4. Weight Tying

Trong GPT-2, th∆∞·ªùng s·ª≠ d·ª•ng weight tying:

\[
W_U = E
\]

ho·∫∑c:

\[
W_U = E^T
\]

Khi ƒë√≥:

\[
z_i = \mathbf{v}_i \cdot \mathbf{h}_t
\]

ƒêi·ªÅu n√†y c√≥ √Ω nghƒ©a h√¨nh h·ªçc:

> Logit c·ªßa token \(i\) ch√≠nh l√† t√≠ch v√¥ h∆∞·ªõng gi·ªØa embedding c·ªßa token ƒë√≥ v√† hidden state.

---

## 5. Softmax v√† ph√¢n ph·ªëi x√°c su·∫•t

X√°c su·∫•t d·ª± ƒëo√°n token ti·∫øp theo:

\[
P(w_i | h_t)
=
\frac{e^{z_i}}
{\sum_{j=1}^{|V|} e^{z_j}}
\]

Thay \( z_i = \mathbf{v}_i \cdot \mathbf{h}_t \):

\[
P(w_i)
=
\frac{
\exp(\mathbf{v}_i \cdot \mathbf{h}_t)
}
{
\sum_j
\exp(\mathbf{v}_j \cdot \mathbf{h}_t)
}
\]

N·∫øu chu·∫©n h√≥a:

\[
\mathbf{v}_i \cdot \mathbf{h}_t
=
\|\mathbf{v}_i\|
\|\mathbf{h}_t\|
\cos \theta_i
\]

Suy ra:

\[
P(w_i)
\propto
\exp(
\|\mathbf{v}_i\|
\|\mathbf{h}_t\|
\cos \theta_i
)
\]

G√≥c gi·ªØa vector quy·∫øt ƒë·ªãnh x√°c su·∫•t.

---

## 6. Di·ªÖn gi·∫£i h√¨nh h·ªçc

Hidden state \( \mathbf{h}_t \) c√≥ th·ªÉ xem nh∆∞:

- M·ªôt ‚Äútruy v·∫•n ng·ªØ nghƒ©a‚Äù
- M·ªôt ƒëi·ªÉm trong kh√¥ng gian embedding

Unembedding th·ª±c hi·ªán ph√©p chi·∫øu:

\[
\mathbf{z} = E \mathbf{h}_t
\]

Nghƒ©a l√† ta ƒëo m·ª©c ƒë·ªô ‚Äúg·∫ßn‚Äù gi·ªØa \( \mathbf{h}_t \) v√† t·ª´ng vector t·ª´ v·ª±ng.

N·∫øu hai token c√≥ embedding g·∫ßn nhau:

\[
\mathbf{v}_i \approx \mathbf{v}_j
\]

th√¨:

\[
z_i \approx z_j
\]

Do ƒë√≥ ph√¢n ph·ªëi x√°c su·∫•t s·∫Ω t∆∞∆°ng t·ª±.

---

## 7. H√†m m·∫•t m√°t v√† t·ªëi ∆∞u h√≥a

H√†m m·∫•t m√°t cross-entropy:

\[
\mathcal{L}
=
- \log P(w_{true})
\]

Gradient theo \( \mathbf{h}_t \):

\[
\nabla_{\mathbf{h}_t}
\mathcal{L}
=
\sum_i
P(w_i)\mathbf{v}_i
-
\mathbf{v}_{true}
\]

ƒêi·ªÅu n√†y cho th·∫•y:

- Hidden state ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh v·ªÅ ph√≠a embedding ƒë√∫ng
- V√† ƒë·∫©y xa embedding sai

---

## 8. So s√°nh v·ªõi ph√¢n lo·∫°i tuy·∫øn t√≠nh

Unembedding t∆∞∆°ng ƒë∆∞∆°ng m·ªôt b·ªô ph√¢n lo·∫°i tuy·∫øn t√≠nh:

\[
z_i = \mathbf{w}_i^T \mathbf{h}_t
\]

Kh√°c bi·ªát l√†:

- S·ªë l·ªõp r·∫•t l·ªõn (~50k)
- Tr·ªçng s·ªë g·∫Øn tr·ª±c ti·∫øp v·ªõi embedding

---

## 9. Quan h·ªá v·ªõi Cosine Similarity

N·∫øu chu·∫©n h√≥a embedding:

\[
\hat{\mathbf{v}}_i
=
\frac{\mathbf{v}_i}{\|\mathbf{v}_i\|}
\]

Khi ƒë√≥:

\[
z_i
=
\|\mathbf{v}_i\|
\|\mathbf{h}_t\|
\cos\theta_i
\]

N·∫øu b·ªè qua ƒë·ªô l·ªõn:

\[
z_i \propto \cos\theta_i
\]

Nh∆∞ v·∫≠y unembedding v·ªÅ b·∫£n ch·∫•t d·ª±a tr√™n cosine similarity.

---

## 10. Ph√¢n t√≠ch ph·ªï (Spectral Perspective)

Gi·∫£ s·ª≠ ma tr·∫≠n embedding:

\[
E = U \Sigma V^T
\]

(SVD decomposition)

Hidden state:

\[
\mathbf{h}_t
=
V \mathbf{c}
\]

Logit:

\[
\mathbf{z}
=
U \Sigma \mathbf{c}
\]

C√°c gi√° tr·ªã singular l·ªõn chi ph·ªëi ph√¢n ph·ªëi x√°c su·∫•t.

---

## 11. √ù nghƒ©a l√Ω thuy·∫øt

Unembedding:

1. Chuy·ªÉn t·ª´ kh√¥ng gian li√™n t·ª•c sang r·ªùi r·∫°c.
2. L√† ph√©p chi·∫øu tuy·∫øn t√≠nh quy m√¥ l·ªõn.
3. Ph·ª• thu·ªôc tr·ª±c ti·∫øp v√†o c·∫•u tr√∫c h√¨nh h·ªçc c·ªßa embedding.
4. T·∫°o li√™n k·∫øt ch·∫∑t ch·∫Ω gi·ªØa h·ªçc bi·ªÉu di·ªÖn v√† d·ª± ƒëo√°n x√°c su·∫•t.

V·ªÅ m·∫∑t to√°n h·ªçc:

\[
\text{Prediction}
=
\text{Softmax}(E \mathbf{h}_t)
\]

---

## 12. K·∫øt lu·∫≠n

Unembedding l√† b∆∞·ªõc cu·ªëi nh∆∞ng c·ª±c k·ª≥ quan tr·ªçng trong m√¥ h√¨nh ng√¥n ng·ªØ. N√≥:

- Chuy·ªÉn hidden state th√†nh ph√¢n ph·ªëi token
- D·ª±a tr√™n t√≠ch v√¥ h∆∞·ªõng trong kh√¥ng gian embedding
- Th·ªÉ hi·ªán r√µ m·ªëi quan h·ªá gi·ªØa h√¨nh h·ªçc vector v√† x√°c su·∫•t

Hi·ªÉu r√µ c∆° ch·∫ø n√†y gi√∫p:

- Ph√¢n t√≠ch h√†nh vi m√¥ h√¨nh
- Th·ª±c hi·ªán interpretability
- Thi·∫øt k·∫ø k·ªπ thu·∫≠t steering v√† logit lens
- So s√°nh kh√¥ng gian bi·ªÉu di·ªÖn gi·ªØa c√°c m√¥ h√¨nh

---

## T√†i li·ªáu tham kh·∫£o

1. Vaswani et al. (2017). Attention is All You Need.  
2. Radford et al. (2019). Language Models are Unsupervised Multitask Learners.  
3. Press & Wolf (2017). Using the Output Embedding to Improve Language Models.  
4. Jurafsky & Martin (2023). Speech and Language Processing.  

---
<!-- Aero-Footer-Start -->
---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [05 Embeddings spaces](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Cosine Similarity vÃ  Má»‘i Quan Há»‡ vá»›i Há»‡ Sá»‘ TÆ°Æ¡ng Quan: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong NLP

## TÃ³m táº¯t

Cosine similarity lÃ  má»™t thÆ°á»›c Ä‘o hÃ¬nh há»c phá»• biáº¿n trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP), Ä‘áº·c biá»‡t khi so sÃ¡nh cÃ¡c vector embedding trong cÃ¡c mÃ´ hÃ¬nh nhÆ° [GPT-2](chatgpt://generic-entity?number=0) vÃ  [BERT](chatgpt://generic-entity?number=1). BÃ i viáº¿t nÃ y trÃ¬nh bÃ y cÆ¡ sá»Ÿ toÃ¡n há»c cá»§a cosine similarity, phÃ¢n tÃ­ch má»‘i quan há»‡ cá»§a nÃ³ vá»›i há»‡ sá»‘ tÆ°Æ¡ng quan Pearson, vÃ  lÃ m rÃµ vai trÃ² cá»§a chuáº©n hÃ³a vector trong khÃ´ng gian nhiá»u chiá»u.

---

## 1. Giá»›i thiá»‡u

Trong khÃ´ng gian vector \( \mathbb{R}^d \), viá»‡c Ä‘o Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a hai vector \( \mathbf{x}, \mathbf{y} \) cÃ³ thá»ƒ thá»±c hiá»‡n báº±ng nhiá»u cÃ¡ch:

- Khoáº£ng cÃ¡ch Euclid
- TÃ­ch vÃ´ hÆ°á»›ng
- Cosine similarity
- Há»‡ sá»‘ tÆ°Æ¡ng quan

Trong cÃ¡c há»‡ embedding hiá»‡n Ä‘áº¡i, cosine similarity Ä‘Æ°á»£c Æ°u tiÃªn do tÃ­nh **báº¥t biáº¿n theo Ä‘á»™ lá»›n (scale-invariant)**.

---

## 2. Äá»‹nh nghÄ©a Cosine Similarity

Cho hai vector:

\[
\mathbf{x}, \mathbf{y} \in \mathbb{R}^d
\]

Cosine similarity Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a:

\[
\text{cosine}(\mathbf{x},\mathbf{y})
=
\frac{\mathbf{x} \cdot \mathbf{y}}
{\|\mathbf{x}\| \|\mathbf{y}\|}
\]

Trong Ä‘Ã³:

\[
\mathbf{x} \cdot \mathbf{y}
=
\sum_{i=1}^{d} x_i y_i
\]

\[
\|\mathbf{x}\|
=
\sqrt{\sum_{i=1}^{d} x_i^2}
\]

### 2.1 Diá»…n giáº£i hÃ¬nh há»c

Gá»i \( \theta \) lÃ  gÃ³c giá»¯a hai vector:

\[
\mathbf{x} \cdot \mathbf{y}
=
\|\mathbf{x}\| \|\mathbf{y}\| \cos \theta
\]

Suy ra:

\[
\text{cosine}(\mathbf{x},\mathbf{y}) = \cos \theta
\]

Do Ä‘Ã³:

- 1 â†’ cÃ¹ng hÆ°á»›ng
- 0 â†’ trá»±c giao
- -1 â†’ ngÆ°á»£c hÆ°á»›ng

---

## 3. Chuáº©n hÃ³a vector

Náº¿u ta chuáº©n hÃ³a:

\[
\hat{\mathbf{x}} = \frac{\mathbf{x}}{\|\mathbf{x}\|}
\]

\[
\hat{\mathbf{y}} = \frac{\mathbf{y}}{\|\mathbf{y}\|}
\]

Khi Ä‘Ã³:

\[
\text{cosine}(\mathbf{x},\mathbf{y})
=
\hat{\mathbf{x}} \cdot \hat{\mathbf{y}}
\]

Äiá»u nÃ y cho tháº¥y cosine similarity chÃ­nh lÃ  tÃ­ch vÃ´ hÆ°á»›ng cá»§a cÃ¡c vector Ä‘Æ¡n vá»‹.

---

## 4. Há»‡ sá»‘ tÆ°Æ¡ng quan Pearson

Cho hai biáº¿n ngáº«u nhiÃªn \( X, Y \), há»‡ sá»‘ tÆ°Æ¡ng quan Pearson:

\[
\rho_{X,Y}
=
\frac{\text{Cov}(X,Y)}
{\sigma_X \sigma_Y}
\]

Trong Ä‘Ã³:

\[
\text{Cov}(X,Y)
=
\frac{1}{n}
\sum_{i=1}^{n}
(x_i - \bar{x})(y_i - \bar{y})
\]

\[
\sigma_X
=
\sqrt{\frac{1}{n}
\sum_{i=1}^{n}
(x_i - \bar{x})^2}
\]

---

## 5. Má»‘i quan há»‡ giá»¯a Cosine vÃ  Pearson

Giáº£ sá»­ ta chuáº©n hÃ³a vector báº±ng cÃ¡ch trá»« trung bÃ¬nh:

\[
\tilde{x}_i = x_i - \bar{x}
\]

\[
\tilde{y}_i = y_i - \bar{y}
\]

Khi Ä‘Ã³:

\[
\rho_{X,Y}
=
\frac{\tilde{\mathbf{x}} \cdot \tilde{\mathbf{y}}}
{\|\tilde{\mathbf{x}}\|
\|\tilde{\mathbf{y}}\|}
\]

NhÆ° váº­y:

> Pearson correlation chÃ­nh lÃ  cosine similarity cá»§a hai vector Ä‘Ã£ Ä‘Æ°á»£c **centered (trá»« trung bÃ¬nh)**.

### 5.1 So sÃ¡nh báº£n cháº¥t

| Äáº·c Ä‘iá»ƒm | Cosine | Pearson |
|----------|---------|----------|
| Trá»« trung bÃ¬nh | KhÃ´ng | CÃ³ |
| Báº¥t biáº¿n theo scale | CÃ³ | CÃ³ |
| Nháº¡y vá»›i offset | CÃ³ | KhÃ´ng |

---

## 6. á»¨ng dá»¥ng trong Embedding

Giáº£ sá»­:

\[
E \in \mathbb{R}^{|V| \times d}
\]

vá»›i má»—i tá»«:

\[
\mathbf{v}_w \in \mathbb{R}^d
\]

Äá»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a giá»¯a hai tá»«:

\[
\text{sim}(w_i,w_j)
=
\frac{\mathbf{v}_i \cdot \mathbf{v}_j}
{\|\mathbf{v}_i\| \|\mathbf{v}_j\|}
\]

---

## 7. Ma tráº­n tÆ°Æ¡ng Ä‘á»“ng

Cho táº­p \( n \) tá»«:

\[
X \in \mathbb{R}^{n \times d}
\]

Ma tráº­n cosine similarity:

\[
S_{ij}
=
\frac{\mathbf{v}_i \cdot \mathbf{v}_j}
{\|\mathbf{v}_i\| \|\mathbf{v}_j\|}
\]

Náº¿u ta vector hÃ³a pháº§n tam giÃ¡c trÃªn cá»§a \(S\) vÃ  tÃ­nh tÆ°Æ¡ng quan giá»¯a hai mÃ´ hÃ¬nh embedding khÃ¡c nhau:

\[
r
=
\frac{\sum (x_i - \bar{x})(y_i - \bar{y})}
{\sqrt{\sum (x_i - \bar{x})^2}
\sqrt{\sum (y_i - \bar{y})^2}}
\]

Ta thu Ä‘Æ°á»£c má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cáº¥u trÃºc (Representational Similarity Analysis).

---

## 8. So sÃ¡nh vá»›i Khoáº£ng cÃ¡ch Euclid

Khoáº£ng cÃ¡ch:

\[
d(\mathbf{x},\mathbf{y})
=
\|\mathbf{x}-\mathbf{y}\|
\]

Náº¿u vector Ä‘Ã£ chuáº©n hÃ³a:

\[
\|\mathbf{x}-\mathbf{y}\|^2
=
2 - 2\cos\theta
\]

Suy ra:

\[
\cos\theta
=
1 - \frac{1}{2}
\|\mathbf{x}-\mathbf{y}\|^2
\]

Äiá»u nÃ y chá»©ng minh cosine similarity vÃ  Euclid distance cÃ³ quan há»‡ tuyáº¿n tÃ­nh khi vector Ä‘Æ°á»£c chuáº©n hÃ³a.

---

## 9. Ã nghÄ©a hÃ¬nh há»c trong khÃ´ng gian cao chiá»u

Trong khÃ´ng gian cao chiá»u:

- Pháº§n lá»›n vector ngáº«u nhiÃªn gáº§n trá»±c giao.
- Cosine similarity táº­p trung quanh 0.
- Embedding há»c Ä‘Æ°á»£c cáº¥u trÃºc lÃ m lá»‡ch phÃ¢n bá»‘ nÃ y.

Giáº£ sá»­:

\[
\mathbf{x},\mathbf{y}
\sim \mathcal{N}(0,I_d)
\]

Khi \( d \to \infty \):

\[
\text{cosine}(\mathbf{x},\mathbf{y})
\to 0
\]

ÄÃ¢y lÃ  hiá»‡n tÆ°á»£ng â€œcurse of dimensionalityâ€.

---

## 10. Káº¿t luáº­n

Cosine similarity lÃ  cÃ´ng cá»¥ hÃ¬nh há»c cá»‘t lÃµi trong NLP vÃ¬:

- Báº¥t biáº¿n theo Ä‘á»™ lá»›n vector
- Dá»… tÃ­nh toÃ¡n
- LiÃªn há»‡ trá»±c tiáº¿p vá»›i Pearson correlation
- PhÃ¹ há»£p vá»›i embedding Ä‘Ã£ chuáº©n hÃ³a

Vá» máº·t toÃ¡n há»c:

\[
\text{Pearson}
=
\text{Cosine}(\text{centered vectors})
\]

\[
\text{Euclid}
\leftrightarrow
\text{Cosine}
\quad (\text{khi chuáº©n hÃ³a})
\]

Hiá»ƒu rÃµ má»‘i quan há»‡ nÃ y giÃºp ta phÃ¢n tÃ­ch chÃ­nh xÃ¡c cáº¥u trÃºc khÃ´ng gian embedding vÃ  Ä‘Ã¡nh giÃ¡ sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯.

---

## TÃ i liá»‡u tham kháº£o

1. Vaswani et al. (2017). Attention is All You Need.  
2. Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.  
3. Radford et al. (2019). Language Models are Unsupervised Multitask Learners.  
4. Jurafsky & Martin (2023). Speech and Language Processing.  
5. Kriegeskorte et al. (2008). Representational Similarity Analysis.

---
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md](aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md) |
| [aero_LLM_02_Exploring GloVe pretrained embeddings.md](aero_LLM_02_Exploring GloVe pretrained embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Exploring GloVe pretrained embeddings.md) |
| [aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md](aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Tá»« Vá»±ng giá»¯a Wikipedia vÃ  Twitter báº±ng PhÃ¢n TÃ­ch TÆ°Æ¡ng Äá»“ng Biá»ƒu Diá»…n (RSA)](aero_LLM_04_CodeChallenge Wikipedia vs. Twitter embeddings (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Wikipedia vs. Twitter embeddings (part 2).md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Ngá»¯ NghÄ©a cá»§a GPT-2 vÃ  BERT thÃ´ng qua PhÃ¢n TÃ­ch Embedding](aero_LLM_05_Exploring GPT2 and BERT embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Exploring GPT2 and BERT embeddings.md) |
| [ToÃ¡n há»c cá»§a Token vÃ  Embedding trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n](aero_LLM_06_CodeChallenge Math with tokens and embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_CodeChallenge Math with tokens and embeddings.md) |
| ğŸ“Œ **[Cosine Similarity vÃ  Má»‘i Quan Há»‡ vá»›i Há»‡ Sá»‘ TÆ°Æ¡ng Quan: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong NLP](aero_LLM_07_Cosine similarity (and relation to correlation).md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Cosine similarity (and relation to correlation).md) |
| [PhÃ¢n TÃ­ch Cosine Similarity trong KhÃ´ng Gian Embedding cá»§a GPT-2](aero_LLM_08_CodeChallenge GPT2 cosine similarities.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge GPT2 cosine similarities.md) |
| [Unembedding trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: Tá»« Vector áº¨n Äáº¿n Token](aero_LLM_09_CodeChallenge Unembeddings (vectors to tokens).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Unembeddings (vectors to tokens).md) |
| [Position Embeddings trong Transformer: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_LLM_10_Position embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_Position embeddings.md) |
| [PhÃ¢n TÃ­ch Thá»±c Nghiá»‡m Embedding Vá»‹ TrÃ­ Trong Transformer: Tá»« Cáº¥u TrÃºc Tuyáº¿n TÃ­nh Äáº¿n KhÃ´ng Gian HÃ¬nh Há»c](aero_LLM_11_CodeChallenge Exploring position embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Exploring position embeddings.md) |
| [Huáº¥n Luyá»‡n Embedding Tá»« Äáº§u: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, CÆ¡ Cháº¿ Tá»‘i Æ¯u vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_12_Training embeddings from scratch.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_Training embeddings from scratch.md) |
| [Thiáº¿t Káº¿ Data Loader Cho Huáº¥n Luyá»‡n MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Kiáº¿n TrÃºc vÃ  Tá»‘i Æ¯u HoÃ¡](aero_LLM_13_Create a data loader to train a model.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_Create a data loader to train a model.md) |
| [XÃ¢y Dá»±ng MÃ´ HÃ¬nh Há»c Embedding Tá»« Äáº§u: Kiáº¿n TrÃºc, Tá»‘i Æ¯u HoÃ¡ vÃ  PhÃ¢n TÃ­ch ToÃ¡n Há»c](aero_LLM_14_Build a model to learn the embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_Build a model to learn the embeddings.md) |
| [HÃ m Máº¥t MÃ¡t Trong Huáº¥n Luyá»‡n Embedding: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, PhÃ¢n TÃ­ch Gradient vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_15_Loss function to train the embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_Loss function to train the embeddings.md) |
| [Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh Há»c MÃ¡y: CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Thá»±c tiá»…n](aero_LLM_16_Train and evaluate the model.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_Train and evaluate the model.md) |
| [Sá»± Thay Äá»•i cá»§a Embeddings Trong QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_LLM_17_CodeChallenge How the embeddings change.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_CodeChallenge How the embeddings change.md) |
| [Äá»™ á»”n Äá»‹nh cá»§a Embeddings trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_LLM_18_CodeChallenge How stable are embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_CodeChallenge How stable are embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

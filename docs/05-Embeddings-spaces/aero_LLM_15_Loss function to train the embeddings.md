
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [05 Embeddings spaces](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# HÃ m Máº¥t MÃ¡t Trong Huáº¥n Luyá»‡n Embedding: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, PhÃ¢n TÃ­ch Gradient vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯

TÃ³m táº¯t

HÃ m máº¥t mÃ¡t (loss function) Ä‘Ã³ng vai trÃ² trung tÃ¢m trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n embedding cho mÃ´ hÃ¬nh ngÃ´n ngá»¯. BÃ i viáº¿t nÃ y trÃ¬nh bÃ y chi tiáº¿t cÃ¡c dáº¡ng hÃ m máº¥t mÃ¡t phá»• biáº¿n dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n embedding, bao gá»“m Cross-Entropy, Negative Sampling vÃ  cÃ¡c biáº¿n thá»ƒ chuáº©n hoÃ¡ xÃ¡c suáº¥t. Äá»“ng thá»i, chÃºng tÃ´i phÃ¢n tÃ­ch Ä‘áº¡o hÃ m, Ä‘á»™ng há»c cáº­p nháº­t gradient vÃ  cáº¥u trÃºc hÃ¬nh há»c cá»§a khÃ´ng gian embedding Ä‘Æ°á»£c hÃ¬nh thÃ nh. Bá»‘i cáº£nh nghiÃªn cá»©u Ä‘Æ°á»£c Ä‘áº·t trong cÃ¡c mÃ´ hÃ¬nh tá»± há»“i quy nhÆ° GPT-2 cá»§a OpenAI, dá»±a trÃªn kiáº¿n trÃºc Transformer tá»« cÃ´ng trÃ¬nh Attention Is All You Need cá»§a Ashish Vaswani vÃ  liÃªn há»‡ vá»›i Word2Vec cá»§a Tomas Mikolov.

â¸»

1. Giá»›i thiá»‡u

Embedding Ã¡nh xáº¡ token rá»i ráº¡c sang khÃ´ng gian liÃªn tá»¥c:

\mathbf{E} \in \mathbb{R}^{V \times d}

Vá»›i:
	â€¢	V: kÃ­ch thÆ°á»›c tá»« vá»±ng
	â€¢	d: sá»‘ chiá»u embedding

Vector cá»§a token w:

\mathbf{e}_w = \mathbf{E}[w]

Äá»ƒ embedding há»c Ä‘Æ°á»£c cáº¥u trÃºc ngá»¯ nghÄ©a, cáº§n Ä‘á»‹nh nghÄ©a má»™t hÃ m máº¥t mÃ¡t pháº£n Ã¡nh má»¥c tiÃªu dá»± Ä‘oÃ¡n.

â¸»

2. HÃ m máº¥t mÃ¡t Cross-Entropy cho bÃ i toÃ¡n dá»± Ä‘oÃ¡n token

2.1 XÃ¡c suáº¥t Softmax

Logits:

z_i = \mathbf{h}^T \mathbf{w}_i

XÃ¡c suáº¥t:

P(y=i) =
\frac{\exp(z_i)}
{\sum_{j=1}^{V} \exp(z_j)}

â¸»

2.2 HÃ m máº¥t mÃ¡t

\mathcal{L}
=
-
\sum_{i=1}^{V}
y_i \log P(y=i)

VÃ¬ y lÃ  one-hot:

\mathcal{L}
=
-
\log P(y = y_{true})

Má»¥c tiÃªu tá»‘i Æ°u:

\min_\theta \mathcal{L}

â¸»

3. PhÃ¢n tÃ­ch gradient

3.1 Gradient theo logits

\frac{\partial \mathcal{L}}{\partial z_i}
=
P(y=i) - y_i

â¸»

3.2 Gradient theo embedding

Vá»›i weight tying \mathbf{W} = \mathbf{E}^T:

z_i = \mathbf{h}^T \mathbf{e}_i

Gradient theo embedding token Ä‘Ãºng y:

\frac{\partial \mathcal{L}}{\partial \mathbf{e}_y}
=
(P(y) - 1)\mathbf{h}

Vá»›i token sai:

\frac{\partial \mathcal{L}}{\partial \mathbf{e}_i}
=
P(i)\mathbf{h}

Diá»…n giáº£i hÃ¬nh há»c:
	â€¢	Embedding Ä‘Ãºng Ä‘Æ°á»£c kÃ©o gáº§n \mathbf{h}
	â€¢	Embedding sai bá»‹ Ä‘áº©y xa

â¸»

4. Negative Sampling

Trong Word2Vec:

\mathcal{L}
=
\log \sigma(\mathbf{u}_c^T \mathbf{v}_w)
+
\sum_{k=1}^{K}
\log \sigma(-\mathbf{u}_{n_k}^T \mathbf{v}_w)

Trong Ä‘Ã³:

\sigma(x)
=
\frac{1}{1+e^{-x}}

Gradient theo tÃ­ch vÃ´ hÆ°á»›ng:

\frac{d}{dx}
\log \sigma(x)
=
1 - \sigma(x)

PhÆ°Æ¡ng phÃ¡p nÃ y giáº£m chi phÃ­ tÃ­nh toÃ¡n tá»«:

O(V)
\rightarrow
O(K)

â¸»

5. PhÃ¢n tÃ­ch Ä‘á»™ lá»“i vÃ  á»•n Ä‘á»‹nh

Cross-Entropy vá»›i softmax lÃ  hÃ m lá»“i theo logits:

\frac{\partial^2 \mathcal{L}}{\partial z_i^2}
=
P(i)(1-P(i))

Ma tráº­n Hessian:

H = \text{diag}(P) - PP^T

H lÃ  bÃ¡n xÃ¡c Ä‘á»‹nh dÆ°Æ¡ng (positive semi-definite).

Tuy nhiÃªn, theo tham sá»‘ embedding, bÃ i toÃ¡n khÃ´ng cÃ²n lá»“i do tÃ­nh cháº¥t phi tuyáº¿n cá»§a máº¡ng sÃ¢u.

â¸»

6. Entropy vÃ  tá»‘i Ä‘a hoÃ¡ kháº£ nÄƒng

Cross-Entropy:

H(p,q)
=
-
\sum p(x)\log q(x)

Tá»‘i thiá»ƒu hoÃ¡ Cross-Entropy tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i:

\min H(p,q)
\iff
\min D_{KL}(p||q)

VÃ¬:

H(p,q)
=
H(p)
+
D_{KL}(p||q)

Trong Ä‘Ã³:

D_{KL}(p||q)
=
\sum p(x)\log\frac{p(x)}{q(x)}

â¸»

7. Vai trÃ² trong Transformer

Trong mÃ´ hÃ¬nh nhÆ° GPT-2:

\mathbf{z}_t
=
\mathbf{e}_t
+
\mathbf{p}_t

Loss toÃ n chuá»—i:

\mathcal{L}
=
-
\sum_{t=1}^{T}
\log
P(x_t \mid x_{<t})

Gradient truyá»n ngÆ°á»£c qua:
	â€¢	Unembedding
	â€¢	Self-attention
	â€¢	Embedding

Embedding Ä‘Æ°á»£c cáº­p nháº­t giÃ¡n tiáº¿p thÃ´ng qua toÃ n bá»™ kiáº¿n trÃºc.

â¸»

8. PhÃ¢n tÃ­ch Ä‘á»™ng há»c há»c embedding

Giáº£ sá»­:

\Delta \mathbf{e}
=
-\eta \nabla_{\mathbf{e}}\mathcal{L}

Sau nhiá»u bÆ°á»›c:

\mathbf{e}_w^{(t)}
=
\mathbf{e}_w^{(0)}
-
\eta
\sum_{k=1}^{t}
\nabla_{\mathbf{e}_w}
\mathcal{L}_k

Token xuáº¥t hiá»‡n thÆ°á»ng xuyÃªn:

\|\mathbf{e}_w\|
\uparrow

Do tÃ­ch lÅ©y gradient nhiá»u hÆ¡n.

â¸»

9. PhÃ¢n tÃ­ch hÃ¬nh há»c

Cosine similarity:

\cos(\theta)
=
\frac{\mathbf{e}_a \cdot \mathbf{e}_b}
{\|\mathbf{e}_a\|\|\mathbf{e}_b\|}

Huáº¥n luyá»‡n lÃ m tÄƒng:

\mathbf{e}_w^T \mathbf{e}_c
\quad \text{khi } w,c \text{ xuáº¥t hiá»‡n cÃ¹ng nhau}

Embedding hÃ¬nh thÃ nh cÃ¡c cá»¥m ngá»¯ nghÄ©a trong khÃ´ng gian cao chiá»u.

â¸»

10. Káº¿t luáº­n

HÃ m máº¥t mÃ¡t lÃ  cÆ¡ cháº¿ Ä‘iá»u khiá»ƒn quÃ¡ trÃ¬nh hÃ¬nh thÃ nh khÃ´ng gian embedding.

CÃ¡c Ä‘iá»ƒm chÃ­nh:
	1.	Cross-Entropy tá»‘i Æ°u xÃ¡c suáº¥t dá»± Ä‘oÃ¡n.
	2.	Gradient Ä‘iá»u chá»‰nh embedding theo hÆ°á»›ng hÃ¬nh há»c rÃµ rÃ ng.
	3.	Negative Sampling giáº£m chi phÃ­ tÃ­nh toÃ¡n.
	4.	Loss áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n cáº¥u trÃºc hÃ¬nh há»c embedding.
	5.	Trong Transformer, embedding há»c thÃ´ng qua toÃ n bá»™ pipeline attention.

Embedding khÃ´ng chá»‰ há»c thÃ´ng qua táº§n suáº¥t mÃ  thÃ´ng qua cáº¥u trÃºc phÃ¢n phá»‘i xÃ¡c suáº¥t toÃ n cá»¥c.

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Ashish Vaswani et al. (2017). Attention Is All You Need.
	2.	OpenAI (2019). GPT-2 Technical Report.
	3.	Tomas Mikolov et al. (2013). Efficient Estimation of Word Representations in Vector Space.
	4.	Goodfellow et al. (2016). Deep Learning.
	5.	Bishop (2006). Pattern Recognition and Machine Learning.
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

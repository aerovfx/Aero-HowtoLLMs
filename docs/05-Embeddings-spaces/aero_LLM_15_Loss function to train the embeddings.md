
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [05 Embeddings spaces](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# HÃ m Máº¥t MÃ¡t Trong Huáº¥n Luyá»‡n Embedding: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, PhÃ¢n TÃ­ch Gradient vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯

TÃ³m táº¯t

HÃ m máº¥t mÃ¡t (loss function) Ä‘Ã³ng vai trÃ² trung tÃ¢m trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n embedding cho mÃ´ hÃ¬nh ngÃ´n ngá»¯. BÃ i viáº¿t nÃ y trÃ¬nh bÃ y chi tiáº¿t cÃ¡c dáº¡ng hÃ m máº¥t mÃ¡t phá»• biáº¿n dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n embedding, bao gá»“m Cross-Entropy, Negative Sampling vÃ  cÃ¡c biáº¿n thá»ƒ chuáº©n hoÃ¡ xÃ¡c suáº¥t. Äá»“ng thá»i, chÃºng tÃ´i phÃ¢n tÃ­ch Ä‘áº¡o hÃ m, Ä‘á»™ng há»c cáº­p nháº­t gradient vÃ  cáº¥u trÃºc hÃ¬nh há»c cá»§a khÃ´ng gian embedding Ä‘Æ°á»£c hÃ¬nh thÃ nh. Bá»‘i cáº£nh nghiÃªn cá»©u Ä‘Æ°á»£c Ä‘áº·t trong cÃ¡c mÃ´ hÃ¬nh tá»± há»“i quy nhÆ° GPT-2 cá»§a OpenAI, dá»±a trÃªn kiáº¿n trÃºc Transformer tá»« cÃ´ng trÃ¬nh Attention Is All You Need cá»§a Ashish Vaswani vÃ  liÃªn há»‡ vá»›i Word2Vec cá»§a Tomas Mikolov.

â¸»

1. Giá»›i thiá»‡u

Embedding Ã¡nh xáº¡ token rá»i ráº¡c sang khÃ´ng gian liÃªn tá»¥c:

\mathbf{E} \in \mathbb{R}^{V \times d}

Vá»›i:
	â€¢	V: kÃ­ch thÆ°á»›c tá»« vá»±ng
	â€¢	d: sá»‘ chiá»u embedding

Vector cá»§a token w:

\mathbf{e}_w = \mathbf{E}[w]

Äá»ƒ embedding há»c Ä‘Æ°á»£c cáº¥u trÃºc ngá»¯ nghÄ©a, cáº§n Ä‘á»‹nh nghÄ©a má»™t hÃ m máº¥t mÃ¡t pháº£n Ã¡nh má»¥c tiÃªu dá»± Ä‘oÃ¡n.

â¸»

2. HÃ m máº¥t mÃ¡t Cross-Entropy cho bÃ i toÃ¡n dá»± Ä‘oÃ¡n token

2.1 XÃ¡c suáº¥t Softmax

Logits:

z_i = \mathbf{h}^T \mathbf{w}_i

XÃ¡c suáº¥t:

P(y=i) =
\frac{\exp(z_i)}
{\sum_{j=1}^{V} \exp(z_j)}

â¸»

2.2 HÃ m máº¥t mÃ¡t

\mathcal{L}
=
-
\sum_{i=1}^{V}
y_i \log P(y=i)

VÃ¬ y lÃ  one-hot:

\mathcal{L}
=
-
\log P(y = y_{true})

Má»¥c tiÃªu tá»‘i Æ°u:

\min_\theta \mathcal{L}

â¸»

3. PhÃ¢n tÃ­ch gradient

3.1 Gradient theo logits

\frac{\partial \mathcal{L}}{\partial z_i}
=
P(y=i) - y_i

â¸»

3.2 Gradient theo embedding

Vá»›i weight tying \mathbf{W} = \mathbf{E}^T:

z_i = \mathbf{h}^T \mathbf{e}_i

Gradient theo embedding token Ä‘Ãºng y:

\frac{\partial \mathcal{L}}{\partial \mathbf{e}_y}
=
(P(y) - 1)\mathbf{h}

Vá»›i token sai:

\frac{\partial \mathcal{L}}{\partial \mathbf{e}_i}
=
P(i)\mathbf{h}

Diá»…n giáº£i hÃ¬nh há»c:
	â€¢	Embedding Ä‘Ãºng Ä‘Æ°á»£c kÃ©o gáº§n \mathbf{h}
	â€¢	Embedding sai bá»‹ Ä‘áº©y xa

â¸»

4. Negative Sampling

Trong Word2Vec:

\mathcal{L}
=
\log \sigma(\mathbf{u}_c^T \mathbf{v}_w)
+
\sum_{k=1}^{K}
\log \sigma(-\mathbf{u}_{n_k}^T \mathbf{v}_w)

Trong Ä‘Ã³:

\sigma(x)
=
\frac{1}{1+e^{-x}}

Gradient theo tÃ­ch vÃ´ hÆ°á»›ng:

\frac{d}{dx}
\log \sigma(x)
=
1 - \sigma(x)

PhÆ°Æ¡ng phÃ¡p nÃ y giáº£m chi phÃ­ tÃ­nh toÃ¡n tá»«:

O(V)
\rightarrow
O(K)

â¸»

5. PhÃ¢n tÃ­ch Ä‘á»™ lá»“i vÃ  á»•n Ä‘á»‹nh

Cross-Entropy vá»›i softmax lÃ  hÃ m lá»“i theo logits:

\frac{\partial^2 \mathcal{L}}{\partial z_i^2}
=
P(i)(1-P(i))

Ma tráº­n Hessian:

H = \text{diag}(P) - PP^T

H lÃ  bÃ¡n xÃ¡c Ä‘á»‹nh dÆ°Æ¡ng (positive semi-definite).

Tuy nhiÃªn, theo tham sá»‘ embedding, bÃ i toÃ¡n khÃ´ng cÃ²n lá»“i do tÃ­nh cháº¥t phi tuyáº¿n cá»§a máº¡ng sÃ¢u.

â¸»

6. Entropy vÃ  tá»‘i Ä‘a hoÃ¡ kháº£ nÄƒng

Cross-Entropy:

H(p,q)
=
-
\sum p(x)\log q(x)

Tá»‘i thiá»ƒu hoÃ¡ Cross-Entropy tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i:

\min H(p,q)
\iff
\min D_{KL}(p||q)

VÃ¬:

H(p,q)
=
H(p)
+
D_{KL}(p||q)

Trong Ä‘Ã³:

D_{KL}(p||q)
=
\sum p(x)\log\frac{p(x)}{q(x)}

â¸»

7. Vai trÃ² trong Transformer

Trong mÃ´ hÃ¬nh nhÆ° GPT-2:

\mathbf{z}_t
=
\mathbf{e}_t
+
\mathbf{p}_t

Loss toÃ n chuá»—i:

\mathcal{L}
=
-
\sum_{t=1}^{T}
\log
P(x_t \mid x_{<t})

Gradient truyá»n ngÆ°á»£c qua:
	â€¢	Unembedding
	â€¢	Self-attention
	â€¢	Embedding

Embedding Ä‘Æ°á»£c cáº­p nháº­t giÃ¡n tiáº¿p thÃ´ng qua toÃ n bá»™ kiáº¿n trÃºc.

â¸»

8. PhÃ¢n tÃ­ch Ä‘á»™ng há»c há»c embedding

Giáº£ sá»­:

\Delta \mathbf{e}
=
-\eta \nabla_{\mathbf{e}}\mathcal{L}

Sau nhiá»u bÆ°á»›c:

\mathbf{e}_w^{(t)}
=
\mathbf{e}_w^{(0)}
-
\eta
\sum_{k=1}^{t}
\nabla_{\mathbf{e}_w}
\mathcal{L}_k

Token xuáº¥t hiá»‡n thÆ°á»ng xuyÃªn:

\|\mathbf{e}_w\|
\uparrow

Do tÃ­ch lÅ©y gradient nhiá»u hÆ¡n.

â¸»

9. PhÃ¢n tÃ­ch hÃ¬nh há»c

Cosine similarity:

\cos(\theta)
=
\frac{\mathbf{e}_a \cdot \mathbf{e}_b}
{\|\mathbf{e}_a\|\|\mathbf{e}_b\|}

Huáº¥n luyá»‡n lÃ m tÄƒng:

\mathbf{e}_w^T \mathbf{e}_c
\quad \text{khi } w,c \text{ xuáº¥t hiá»‡n cÃ¹ng nhau}

Embedding hÃ¬nh thÃ nh cÃ¡c cá»¥m ngá»¯ nghÄ©a trong khÃ´ng gian cao chiá»u.

â¸»

10. Káº¿t luáº­n

HÃ m máº¥t mÃ¡t lÃ  cÆ¡ cháº¿ Ä‘iá»u khiá»ƒn quÃ¡ trÃ¬nh hÃ¬nh thÃ nh khÃ´ng gian embedding.

CÃ¡c Ä‘iá»ƒm chÃ­nh:
	1.	Cross-Entropy tá»‘i Æ°u xÃ¡c suáº¥t dá»± Ä‘oÃ¡n.
	2.	Gradient Ä‘iá»u chá»‰nh embedding theo hÆ°á»›ng hÃ¬nh há»c rÃµ rÃ ng.
	3.	Negative Sampling giáº£m chi phÃ­ tÃ­nh toÃ¡n.
	4.	Loss áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n cáº¥u trÃºc hÃ¬nh há»c embedding.
	5.	Trong Transformer, embedding há»c thÃ´ng qua toÃ n bá»™ pipeline attention.

Embedding khÃ´ng chá»‰ há»c thÃ´ng qua táº§n suáº¥t mÃ  thÃ´ng qua cáº¥u trÃºc phÃ¢n phá»‘i xÃ¡c suáº¥t toÃ n cá»¥c.

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Ashish Vaswani et al. (2017). Attention Is All You Need.
	2.	OpenAI (2019). GPT-2 Technical Report.
	3.	Tomas Mikolov et al. (2013). Efficient Estimation of Word Representations in Vector Space.
	4.	Goodfellow et al. (2016). Deep Learning.
	5.	Bishop (2006). Pattern Recognition and Machine Learning.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md](aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md) |
| [aero_LLM_02_Exploring GloVe pretrained embeddings.md](aero_LLM_02_Exploring GloVe pretrained embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Exploring GloVe pretrained embeddings.md) |
| [aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md](aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Tá»« Vá»±ng giá»¯a Wikipedia vÃ  Twitter báº±ng PhÃ¢n TÃ­ch TÆ°Æ¡ng Äá»“ng Biá»ƒu Diá»…n (RSA)](aero_LLM_04_CodeChallenge Wikipedia vs. Twitter embeddings (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Wikipedia vs. Twitter embeddings (part 2).md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Ngá»¯ NghÄ©a cá»§a GPT-2 vÃ  BERT thÃ´ng qua PhÃ¢n TÃ­ch Embedding](aero_LLM_05_Exploring GPT2 and BERT embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Exploring GPT2 and BERT embeddings.md) |
| [ToÃ¡n há»c cá»§a Token vÃ  Embedding trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n](aero_LLM_06_CodeChallenge Math with tokens and embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_CodeChallenge Math with tokens and embeddings.md) |
| [Cosine Similarity vÃ  Má»‘i Quan Há»‡ vá»›i Há»‡ Sá»‘ TÆ°Æ¡ng Quan: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong NLP](aero_LLM_07_Cosine similarity (and relation to correlation).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Cosine similarity (and relation to correlation).md) |
| [PhÃ¢n TÃ­ch Cosine Similarity trong KhÃ´ng Gian Embedding cá»§a GPT-2](aero_LLM_08_CodeChallenge GPT2 cosine similarities.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge GPT2 cosine similarities.md) |
| [Unembedding trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: Tá»« Vector áº¨n Äáº¿n Token](aero_LLM_09_CodeChallenge Unembeddings (vectors to tokens).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Unembeddings (vectors to tokens).md) |
| [Position Embeddings trong Transformer: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_LLM_10_Position embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_Position embeddings.md) |
| [PhÃ¢n TÃ­ch Thá»±c Nghiá»‡m Embedding Vá»‹ TrÃ­ Trong Transformer: Tá»« Cáº¥u TrÃºc Tuyáº¿n TÃ­nh Äáº¿n KhÃ´ng Gian HÃ¬nh Há»c](aero_LLM_11_CodeChallenge Exploring position embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Exploring position embeddings.md) |
| [Huáº¥n Luyá»‡n Embedding Tá»« Äáº§u: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, CÆ¡ Cháº¿ Tá»‘i Æ¯u vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_12_Training embeddings from scratch.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_Training embeddings from scratch.md) |
| [Thiáº¿t Káº¿ Data Loader Cho Huáº¥n Luyá»‡n MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Kiáº¿n TrÃºc vÃ  Tá»‘i Æ¯u HoÃ¡](aero_LLM_13_Create a data loader to train a model.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_Create a data loader to train a model.md) |
| [XÃ¢y Dá»±ng MÃ´ HÃ¬nh Há»c Embedding Tá»« Äáº§u: Kiáº¿n TrÃºc, Tá»‘i Æ¯u HoÃ¡ vÃ  PhÃ¢n TÃ­ch ToÃ¡n Há»c](aero_LLM_14_Build a model to learn the embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_Build a model to learn the embeddings.md) |
| ğŸ“Œ **[HÃ m Máº¥t MÃ¡t Trong Huáº¥n Luyá»‡n Embedding: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, PhÃ¢n TÃ­ch Gradient vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_15_Loss function to train the embeddings.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_15_Loss function to train the embeddings.md) |
| [Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh Há»c MÃ¡y: CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Thá»±c tiá»…n](aero_LLM_16_Train and evaluate the model.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_Train and evaluate the model.md) |
| [Sá»± Thay Äá»•i cá»§a Embeddings Trong QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_LLM_17_CodeChallenge How the embeddings change.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_CodeChallenge How the embeddings change.md) |
| [Äá»™ á»”n Äá»‹nh cá»§a Embeddings trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_LLM_18_CodeChallenge How stable are embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_CodeChallenge How stable are embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

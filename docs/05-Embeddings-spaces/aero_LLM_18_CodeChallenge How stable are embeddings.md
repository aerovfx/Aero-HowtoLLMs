
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [05 Embeddings spaces](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Äá»™ á»”n Äá»‹nh cá»§a Embeddings trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m

TÃ³m táº¯t

Embeddings lÃ  ná»n táº£ng cá»§a cÃ¡c mÃ´ hÃ¬nh xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn hiá»‡n Ä‘áº¡i. Tuy nhiÃªn, má»™t cÃ¢u há»i quan trá»ng Ä‘áº·t ra: cÃ¡c embeddings cÃ³ á»•n Ä‘á»‹nh giá»¯a cÃ¡c láº§n huáº¥n luyá»‡n khÃ¡c nhau hay khÃ´ng? BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch tÃ­nh á»•n Ä‘á»‹nh cá»§a embeddings dÆ°á»›i gÃ³c nhÃ¬n toÃ¡n há»c, thá»‘ng kÃª vÃ  hÃ¬nh há»c khÃ´ng gian vector. Ná»™i dung dá»±a trÃªn bÃ i thá»±c hÃ nh â€œHow Stable Are Embeddings?â€ vÃ  má»Ÿ rá»™ng tá»« cÃ¡c nghiÃªn cá»©u cá»§a Tomas Mikolov (Word2Vec), Jeffrey Pennington (GloVe), vÃ  Yoshua Bengio.

â¸»

1. Giá»›i thiá»‡u

Giáº£ sá»­ ta huáº¥n luyá»‡n cÃ¹ng má»™t mÃ´ hÃ¬nh embedding nhiá»u láº§n vá»›i:
	â€¢	CÃ¹ng táº­p dá»¯ liá»‡u
	â€¢	CÃ¹ng kiáº¿n trÃºc
	â€¢	KhÃ¡c khá»Ÿi táº¡o ngáº«u nhiÃªn

Ta thu Ä‘Æ°á»£c hai ma tráº­n embedding:

E^{(1)}, \quad E^{(2)} \in \mathbb{R}^{V \times d}

CÃ¢u há»i:

E^{(1)} \stackrel{?}{\approx} E^{(2)}

TrÃªn thá»±c táº¿, cÃ¡c embedding khÃ´ng trÃ¹ng khá»›p tá»«ng pháº§n tá»­, nhÆ°ng cÃ³ thá»ƒ tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá» cáº¥u trÃºc hÃ¬nh há»c.

â¸»

2. NguyÃªn nhÃ¢n GÃ¢y Báº¥t á»”n Äá»‹nh

2.1 TÃ­nh báº¥t Ä‘á»‹nh do khá»Ÿi táº¡o ngáº«u nhiÃªn

Ban Ä‘áº§u:

\mathbf{v}_w^{(0)} \sim \mathcal{N}(0, \sigma^2 I)

CÃ¡c Ä‘iá»ƒm xuáº¥t phÃ¡t khÃ¡c nhau dáº«n Ä‘áº¿n nghiá»‡m tá»‘i Æ°u khÃ¡c nhau trong khÃ´ng gian phi lá»“i.

â¸»

2.2 TÃ­nh báº¥t biáº¿n quay (Rotational Invariance)

Giáº£ sá»­ Q \in \mathbb{R}^{d \times d} lÃ  ma tráº­n trá»±c giao:

Q^\top Q = I

Náº¿u E lÃ  nghiá»‡m tá»‘i Æ°u, thÃ¬:

E' = EQ

cÅ©ng lÃ  nghiá»‡m tÆ°Æ¡ng Ä‘Æ°Æ¡ng vÃ¬:

(EQ)(EQ)^\top = EQQ^\top E^\top = EE^\top

Äiá»u nÃ y giáº£i thÃ­ch vÃ¬ sao embeddings giá»¯a hai láº§n huáº¥n luyá»‡n cÃ³ thá»ƒ khÃ¡c nhau vá» tá»a Ä‘á»™ nhÆ°ng giá»‘ng nhau vá» quan há»‡ tÆ°Æ¡ng Ä‘á»‘i.

â¸»

3. Äo lÆ°á»ng Äá»™ á»”n Äá»‹nh

3.1 So sÃ¡nh trá»±c tiáº¿p báº±ng chuáº©n Frobenius

||E^{(1)} - E^{(2)}||_F

Tuy nhiÃªn cÃ¡ch nÃ y khÃ´ng hiá»‡u quáº£ do váº¥n Ä‘á» quay khÃ´ng gian.

â¸»

3.2 Procrustes Alignment

TÃ¬m ma tráº­n quay tá»‘i Æ°u:

Q^* = \arg\min_Q ||E^{(1)}Q - E^{(2)}||_F

Sau cÄƒn chá»‰nh:

Stability = ||E^{(1)}Q^* - E^{(2)}||_F

PhÆ°Æ¡ng phÃ¡p nÃ y thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng trong nghiÃªn cá»©u á»•n Ä‘á»‹nh embedding.

â¸»

3.3 TÆ°Æ¡ng tá»± cosine trung bÃ¬nh

Vá»›i má»—i tá»« w:

sim(w) =
\frac{
\mathbf{v}_w^{(1)} \cdot \mathbf{v}_w^{(2)}
}{
||\mathbf{v}_w^{(1)}||\,||\mathbf{v}_w^{(2)}||
}

Láº¥y trung bÃ¬nh trÃªn toÃ n bá»™ tá»« vá»±ng:

\overline{sim} =
\frac{1}{V} \sum_{w=1}^{V} sim(w)

â¸»

4. PhÃ¢n tÃ­ch LÃ½ thuyáº¿t

4.1 HÃ m má»¥c tiÃªu Skip-gram

\mathcal{L} =
- \sum_{(w,c)} \log
\frac{\exp(\mathbf{v}_w^\top \mathbf{v}_c)}
{\sum_{c'} \exp(\mathbf{v}_w^\top \mathbf{v}_{c'})}

HÃ m máº¥t mÃ¡t nÃ y phá»¥ thuá»™c vÃ o tÃ­ch vÃ´ hÆ°á»›ng:

\mathbf{v}_w^\top \mathbf{v}_c

Do Ä‘Ã³ náº¿u:

\mathbf{v}'_w = Q\mathbf{v}_w

thÃ¬:

\mathbf{v}'_w{}^\top \mathbf{v}'_c
=
\mathbf{v}_w^\top Q^\top Q \mathbf{v}_c
=
\mathbf{v}_w^\top \mathbf{v}_c

â†’ HÃ m máº¥t mÃ¡t khÃ´ng Ä‘á»•i.

â¸»

5. Thá»±c nghiá»‡m: Káº¿t quáº£ Ä‘iá»ƒn hÃ¬nh

Tá»« bÃ i Code Challenge:
	â€¢	Embeddings thay Ä‘á»•i máº¡nh vá» giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i
	â€¢	Sau cÄƒn chá»‰nh Procrustes â†’ Ä‘á»™ tÆ°Æ¡ng tá»± tÄƒng Ä‘Ã¡ng ká»ƒ
	â€¢	Quan há»‡ ngá»¯ nghÄ©a (nearest neighbors) gáº§n nhÆ° giá»¯ nguyÃªn

VÃ­ dá»¥:

NN^{(1)}(king) \approx NN^{(2)}(king)

â¸»

6. áº¢nh hÆ°á»Ÿng cá»§a KÃ­ch thÆ°á»›c vÃ  Dá»¯ liá»‡u

6.1 KÃ­ch thÆ°á»›c embedding

Khi d lá»›n:
	â€¢	KhÃ´ng gian nghiá»‡m rá»™ng hÆ¡n
	â€¢	Variance tÄƒng

Theo phÃ¢n tÃ­ch biasâ€“variance:

\mathbb{E}[(y - \hat{f}(x))^2]
=
Bias^2 + Variance + \sigma^2

â¸»

6.2 KÃ­ch thÆ°á»›c táº­p dá»¯ liá»‡u

Khi sá»‘ máº«u N \rightarrow \infty:

\hat{\theta}_N \rightarrow \theta^*

Theo Ä‘á»‹nh lÃ½ há»™i tá»¥, embeddings trá»Ÿ nÃªn á»•n Ä‘á»‹nh hÆ¡n.

â¸»

7. Embeddings trong Transformer

Trong kiáº¿n trÃºc cá»§a Ashish Vaswani:

\mathbf{x}_i = \mathbf{e}_i + \mathbf{p}_i

CÃ¡c embedding Ä‘Æ°á»£c cáº­p nháº­t qua nhiá»u lá»›p attention:

Attention(Q,K,V)
=
\text{softmax}
\left(
\frac{QK^\top}{\sqrt{d_k}}
\right)V

Do sá»± lan truyá»n gradient qua nhiá»u táº§ng, embeddings thÆ°á»ng á»•n Ä‘á»‹nh hÆ¡n so vá»›i mÃ´ hÃ¬nh nÃ´ng.

â¸»

8. Tháº£o luáº­n

Embeddings khÃ´ng á»•n Ä‘á»‹nh tuyá»‡t Ä‘á»‘i á»Ÿ má»©c tá»a Ä‘á»™, nhÆ°ng:
	â€¢	á»”n Ä‘á»‹nh vá» cáº¥u trÃºc hÃ¬nh há»c
	â€¢	á»”n Ä‘á»‹nh vá» quan há»‡ ngá»¯ nghÄ©a
	â€¢	Báº¥t biáº¿n theo phÃ©p quay

Do Ä‘Ã³, tÃ­nh á»•n Ä‘á»‹nh nÃªn Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ báº±ng:
	â€¢	Quan há»‡ lÃ¡ng giá»ng gáº§n
	â€¢	Cáº¥u trÃºc khoáº£ng cÃ¡ch
	â€¢	Phá»• trá»‹ riÃªng cá»§a ma tráº­n tÆ°Æ¡ng quan

â¸»

9. Káº¿t luáº­n

Äá»™ á»•n Ä‘á»‹nh cá»§a embeddings phá»¥ thuá»™c vÃ o:
	â€¢	Khá»Ÿi táº¡o ngáº«u nhiÃªn
	â€¢	KÃ­ch thÆ°á»›c khÃ´ng gian
	â€¢	LÆ°á»£ng dá»¯ liá»‡u
	â€¢	Thuáº­t toÃ¡n tá»‘i Æ°u

Vá» máº·t toÃ¡n há»c, embeddings lÃ  nghiá»‡m cá»§a má»™t bÃ i toÃ¡n tá»‘i Æ°u phi lá»“i cÃ³ nhiá»u nghiá»‡m tÆ°Æ¡ng Ä‘Æ°Æ¡ng theo phÃ©p quay. Do Ä‘Ã³, sá»± khÃ¡c biá»‡t giá»¯a cÃ¡c láº§n huáº¥n luyá»‡n khÃ´ng Ä‘á»“ng nghÄ©a vá»›i máº¥t thÃ´ng tin ngá»¯ nghÄ©a.

Hiá»ƒu rÃµ báº£n cháº¥t nÃ y giÃºp:
	â€¢	So sÃ¡nh mÃ´ hÃ¬nh chÃ­nh xÃ¡c hÆ¡n
	â€¢	Thiáº¿t káº¿ thÃ­ nghiá»‡m tÃ¡i láº­p (reproducibility)
	â€¢	ÄÃ¡nh giÃ¡ embedding má»™t cÃ¡ch cÃ³ cÆ¡ sá»Ÿ khoa há»c

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Mikolov, T. et al. (2013). Efficient Estimation of Word Representations in Vector Space.
	2.	Pennington, J., Socher, R., & Manning, C. (2014). GloVe: Global Vectors for Word Representation.
	3.	Bengio, Y. et al. (2003). A Neural Probabilistic Language Model.
	4.	Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
	5.	Hamilton, W. L. et al. (2016). Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md](aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md) |
| [aero_LLM_02_Exploring GloVe pretrained embeddings.md](aero_LLM_02_Exploring GloVe pretrained embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Exploring GloVe pretrained embeddings.md) |
| [aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md](aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Tá»« Vá»±ng giá»¯a Wikipedia vÃ  Twitter báº±ng PhÃ¢n TÃ­ch TÆ°Æ¡ng Äá»“ng Biá»ƒu Diá»…n (RSA)](aero_LLM_04_CodeChallenge Wikipedia vs. Twitter embeddings (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Wikipedia vs. Twitter embeddings (part 2).md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Ngá»¯ NghÄ©a cá»§a GPT-2 vÃ  BERT thÃ´ng qua PhÃ¢n TÃ­ch Embedding](aero_LLM_05_Exploring GPT2 and BERT embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Exploring GPT2 and BERT embeddings.md) |
| [ToÃ¡n há»c cá»§a Token vÃ  Embedding trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n](aero_LLM_06_CodeChallenge Math with tokens and embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_CodeChallenge Math with tokens and embeddings.md) |
| [Cosine Similarity vÃ  Má»‘i Quan Há»‡ vá»›i Há»‡ Sá»‘ TÆ°Æ¡ng Quan: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong NLP](aero_LLM_07_Cosine similarity (and relation to correlation).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Cosine similarity (and relation to correlation).md) |
| [PhÃ¢n TÃ­ch Cosine Similarity trong KhÃ´ng Gian Embedding cá»§a GPT-2](aero_LLM_08_CodeChallenge GPT2 cosine similarities.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge GPT2 cosine similarities.md) |
| [Unembedding trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: Tá»« Vector áº¨n Äáº¿n Token](aero_LLM_09_CodeChallenge Unembeddings (vectors to tokens).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Unembeddings (vectors to tokens).md) |
| [Position Embeddings trong Transformer: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_LLM_10_Position embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_Position embeddings.md) |
| [PhÃ¢n TÃ­ch Thá»±c Nghiá»‡m Embedding Vá»‹ TrÃ­ Trong Transformer: Tá»« Cáº¥u TrÃºc Tuyáº¿n TÃ­nh Äáº¿n KhÃ´ng Gian HÃ¬nh Há»c](aero_LLM_11_CodeChallenge Exploring position embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Exploring position embeddings.md) |
| [Huáº¥n Luyá»‡n Embedding Tá»« Äáº§u: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, CÆ¡ Cháº¿ Tá»‘i Æ¯u vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_12_Training embeddings from scratch.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_Training embeddings from scratch.md) |
| [Thiáº¿t Káº¿ Data Loader Cho Huáº¥n Luyá»‡n MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Kiáº¿n TrÃºc vÃ  Tá»‘i Æ¯u HoÃ¡](aero_LLM_13_Create a data loader to train a model.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_Create a data loader to train a model.md) |
| [XÃ¢y Dá»±ng MÃ´ HÃ¬nh Há»c Embedding Tá»« Äáº§u: Kiáº¿n TrÃºc, Tá»‘i Æ¯u HoÃ¡ vÃ  PhÃ¢n TÃ­ch ToÃ¡n Há»c](aero_LLM_14_Build a model to learn the embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_Build a model to learn the embeddings.md) |
| [HÃ m Máº¥t MÃ¡t Trong Huáº¥n Luyá»‡n Embedding: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, PhÃ¢n TÃ­ch Gradient vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_15_Loss function to train the embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_Loss function to train the embeddings.md) |
| [Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh Há»c MÃ¡y: CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Thá»±c tiá»…n](aero_LLM_16_Train and evaluate the model.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_Train and evaluate the model.md) |
| [Sá»± Thay Äá»•i cá»§a Embeddings Trong QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_LLM_17_CodeChallenge How the embeddings change.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_CodeChallenge How the embeddings change.md) |
| ğŸ“Œ **[Äá»™ á»”n Äá»‹nh cá»§a Embeddings trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_LLM_18_CodeChallenge How stable are embeddings.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_18_CodeChallenge How stable are embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

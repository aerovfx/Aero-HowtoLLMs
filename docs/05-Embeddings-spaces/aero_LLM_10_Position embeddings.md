
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [05 Embeddings spaces](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Position Embeddings trong Transformer: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n

## TÃ³m táº¯t

Kiáº¿n trÃºc Transformer khÃ´ng cÃ³ cÆ¡ cháº¿ tuáº§n tá»± ná»™i táº¡i nhÆ° RNN, do Ä‘Ã³ cáº§n má»™t phÆ°Æ¡ng phÃ¡p mÃ£ hÃ³a thá»© tá»± cá»§a token trong chuá»—i. Position Embeddings (PE) Ä‘Æ°á»£c Ä‘á» xuáº¥t trong bÃ i bÃ¡o gá»‘c â€œAttention is All You Needâ€ nháº±m bá»• sung thÃ´ng tin vá»‹ trÃ­ vÃ o biá»ƒu diá»…n embedding. BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch cÆ¡ sá»Ÿ toÃ¡n há»c cá»§a positional encoding, cÃ¡c biáº¿n thá»ƒ há»c Ä‘Æ°á»£c (learned positional embeddings), vÃ  vai trÃ² cá»§a chÃºng trong cÃ¡c mÃ´ hÃ¬nh nhÆ° [GPT-2](chatgpt://generic-entity?number=0) vÃ  [BERT](chatgpt://generic-entity?number=1).

---

## 1. Giá»›i thiá»‡u

Trong Transformer, self-attention chá»‰ dá»±a trÃªn:

\[
\text{Attention}(Q,K,V)
=
\text{softmax}\left(
\frac{QK^T}{\sqrt{d_k}}
\right)V
\]

CÆ¡ cháº¿ nÃ y khÃ´ng chá»©a thÃ´ng tin vá» vá»‹ trÃ­ thá»© tá»± cá»§a token.

Do Ä‘Ã³, náº¿u chá»‰ dÃ¹ng embedding tá»« vá»±ng:

\[
\mathbf{v}_i
\]

thÃ¬ hai chuá»—i:

- â€œdog bites manâ€
- â€œman bites dogâ€

sáº½ cÃ³ táº­p embedding giá»‘ng nhau (chá»‰ khÃ¡c thá»© tá»±).

---

## 2. Biá»ƒu diá»…n vá»‹ trÃ­: CÃ´ng thá»©c Sinusoidal

Trong bÃ i bÃ¡o Transformer gá»‘c (Vaswani et al., 2017), positional encoding Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a:

\[
PE_{(pos,2k)} =
\sin\left(
\frac{pos}{10000^{2k/d}}
\right)
\]

\[
PE_{(pos,2k+1)} =
\cos\left(
\frac{pos}{10000^{2k/d}}
\right)
\]

Trong Ä‘Ã³:

- \( pos \): vá»‹ trÃ­ trong chuá»—i
- \( k \): chá»‰ sá»‘ chiá»u
- \( d \): kÃ­ch thÆ°á»›c embedding

---

## 3. Äáº·c tÃ­nh toÃ¡n há»c

### 3.1 Táº§n sá»‘ hÃ¬nh há»c

Ta cÃ³:

\[
\omega_k = \frac{1}{10000^{2k/d}}
\]

Do Ä‘Ã³:

\[
PE(pos,k) =
\sin(\omega_k pos)
\quad \text{hoáº·c} \quad
\cos(\omega_k pos)
\]

Táº§n sá»‘ thay Ä‘á»•i theo cáº¥p sá»‘ nhÃ¢n â†’ cho phÃ©p mÃ´ hÃ¬nh biá»ƒu diá»…n cáº£:

- Quan há»‡ gáº§n (low frequency)
- Quan há»‡ xa (high frequency)

---

### 3.2 Biá»ƒu diá»…n dá»‹ch chuyá»ƒn tuyáº¿n tÃ­nh

Má»™t Ä‘áº·c tÃ­nh quan trá»ng:

\[
PE(pos + \Delta)
=
PE(pos)\cos(\omega\Delta)
+
PE_{\perp}(pos)\sin(\omega\Delta)
\]

Äiá»u nÃ y cho phÃ©p mÃ´ hÃ¬nh há»c quan há»‡ khoáº£ng cÃ¡ch tuyáº¿n tÃ­nh giá»¯a cÃ¡c vá»‹ trÃ­.

---

## 4. Káº¿t há»£p Embedding vÃ  Position

Embedding cuá»‘i cÃ¹ng:

\[
\mathbf{z}_i
=
\mathbf{v}_i
+
\mathbf{p}_i
\]

Trong Ä‘Ã³:

- \( \mathbf{v}_i \): token embedding
- \( \mathbf{p}_i \): positional embedding

Khi Ä‘Ã³:

\[
Z = V + P
\]

vá»›i:

\[
V, P \in \mathbb{R}^{n \times d}
\]

---

## 5. Learned Positional Embeddings

Trong [GPT-2](chatgpt://generic-entity?number=2) vÃ  [BERT](chatgpt://generic-entity?number=3), positional embeddings thÆ°á»ng Ä‘Æ°á»£c há»c trá»±c tiáº¿p:

\[
P \in \mathbb{R}^{L_{max} \times d}
\]

vá»›i \( L_{max} \) lÃ  Ä‘á»™ dÃ i tá»‘i Ä‘a.

Khi Ä‘Ã³:

\[
\mathbf{p}_i = P[i]
\]

Æ¯u Ä‘iá»ƒm:

- Linh hoáº¡t hÆ¡n
- Há»c trá»±c tiáº¿p tá»« dá»¯ liá»‡u

NhÆ°á»£c Ä‘iá»ƒm:

- KhÃ´ng tá»± nhiÃªn má»Ÿ rá»™ng sang chuá»—i dÃ i hÆ¡n Ä‘á»™ dÃ i huáº¥n luyá»‡n

---

## 6. PhÃ¢n tÃ­ch hÃ¬nh há»c

Sau khi cá»™ng:

\[
\mathbf{z}_i
=
\mathbf{v}_i + \mathbf{p}_i
\]

Self-attention tÃ­nh:

\[
Q = ZW_Q
\]

\[
K = ZW_K
\]

TÃ­ch vÃ´ hÆ°á»›ng:

\[
QK^T
=
(V + P)W_Q
((V + P)W_K)^T
\]

Khai triá»ƒn:

\[
=
VW_QW_K^TV^T
+
VW_QW_K^TP^T
+
PW_QW_K^TV^T
+
PW_QW_K^TP^T
\]

Cho tháº¥y attention bao gá»“m:

- Quan há»‡ tokenâ€“token
- Quan há»‡ tokenâ€“position
- Quan há»‡ positionâ€“position

---

## 7. Relative Position Encoding

Má»™t sá»‘ mÃ´ hÃ¬nh hiá»‡n Ä‘áº¡i sá»­ dá»¥ng vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i:

\[
\text{Attention}_{ij}
=
\frac{
Q_i K_j^T + b_{i-j}
}{
\sqrt{d}
}
\]

Trong Ä‘Ã³ \( b_{i-j} \) phá»¥ thuá»™c vÃ o khoáº£ng cÃ¡ch giá»¯a vá»‹ trÃ­.

Äiá»u nÃ y giÃºp mÃ´ hÃ¬nh tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n.

---

## 8. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Cosine Similarity

Do:

\[
\mathbf{z}_i
=
\mathbf{v}_i + \mathbf{p}_i
\]

Cosine similarity giá»¯a hai token táº¡i vá»‹ trÃ­ khÃ¡c nhau:

\[
\text{cosine}(\mathbf{z}_i,\mathbf{z}_j)
=
\frac{
(\mathbf{v}_i+\mathbf{p}_i)\cdot
(\mathbf{v}_j+\mathbf{p}_j)
}{
\|\mathbf{z}_i\|\|\mathbf{z}_j\|
}
\]

Má»Ÿ rá»™ng tá»­ sá»‘:

\[
=
\mathbf{v}_i\cdot\mathbf{v}_j
+
\mathbf{v}_i\cdot\mathbf{p}_j
+
\mathbf{p}_i\cdot\mathbf{v}_j
+
\mathbf{p}_i\cdot\mathbf{p}_j
\]

Cho tháº¥y vá»‹ trÃ­ áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n hÃ¬nh há»c embedding.

---

## 9. TÃ­nh báº¥t biáº¿n vÃ  giá»›i háº¡n

### 9.1 KhÃ´ng báº¥t biáº¿n dá»‹ch chuyá»ƒn

Vá»›i learned positional embedding:

\[
\mathbf{p}_{i+1}
\neq
\mathbf{p}_i + c
\]

Do Ä‘Ã³ mÃ´ hÃ¬nh khÃ´ng tá»± Ä‘á»™ng báº¥t biáº¿n vá»›i dá»‹ch chuyá»ƒn.

---

### 9.2 Äá»™ dÃ i chuá»—i

Vá»›i sinusoidal:

\[
PE(pos)
\text{ cÃ³ thá»ƒ tÃ­nh cho má»i } pos
\]

Vá»›i learned:

\[
pos > L_{max}
\Rightarrow
\text{khÃ´ng xÃ¡c Ä‘á»‹nh}
\]

---

## 10. Káº¿t luáº­n

Position embeddings lÃ  thÃ nh pháº§n thiáº¿t yáº¿u giÃºp Transformer:

- Nháº­n biáº¿t thá»© tá»±
- Há»c quan há»‡ khoáº£ng cÃ¡ch
- MÃ´ hÃ¬nh hÃ³a cáº¥u trÃºc cÃº phÃ¡p

Vá» máº·t toÃ¡n há»c:

\[
\text{Transformer}
=
\text{Attention}(V + P)
\]

Sá»± lá»±a chá»n giá»¯a sinusoidal vÃ  learned positional embeddings áº£nh hÆ°á»Ÿng Ä‘áº¿n:

- Kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a
- á»”n Ä‘á»‹nh huáº¥n luyá»‡n
- HÃ¬nh há»c cá»§a khÃ´ng gian biá»ƒu diá»…n

Hiá»ƒu rÃµ cÆ¡ cháº¿ nÃ y giÃºp:

- PhÃ¢n tÃ­ch hÃ nh vi mÃ´ hÃ¬nh
- Thiáº¿t káº¿ kiáº¿n trÃºc má»›i
- Má»Ÿ rá»™ng mÃ´ hÃ¬nh sang chuá»—i dÃ i hÆ¡n

---

## TÃ i liá»‡u tham kháº£o

1. Vaswani et al. (2017). Attention is All You Need.  
2. Radford et al. (2019). Language Models are Unsupervised Multitask Learners.  
3. Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.  
4. Press & Wolf (2017). Using the Output Embedding to Improve Language Models.  
5. Jurafsky & Martin (2023). Speech and Language Processing.

---
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

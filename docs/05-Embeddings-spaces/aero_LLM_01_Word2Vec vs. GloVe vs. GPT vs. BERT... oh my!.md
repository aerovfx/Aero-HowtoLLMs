
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [05 Embeddings spaces](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
So sÃ¡nh Word2Vec, GloVe, GPT vÃ  BERT:

Tá»« Biá»ƒu diá»…n PhÃ¢n bá»‘ Tuyáº¿n tÃ­nh Ä‘áº¿n Transformer Tá»± Há»“i quy vÃ  Hai Chiá»u

â¸»

TÃ³m táº¯t

BÃ i viáº¿t nÃ y tá»•ng há»£p vÃ  phÃ¢n tÃ­ch ná»™i dung tá»« tÃ i liá»‡u Ä‘Ã­nh kÃ¨m â€œWord2Vec vs. GloVe vs. GPT vs. BERTâ€¦ oh my!â€, Ä‘á»“ng thá»i má»Ÿ rá»™ng vá»›i cÃ¡c nguá»“n há»c thuáº­t ná»n táº£ng nháº±m lÃ m rÃµ sá»± tiáº¿n hÃ³a cá»§a mÃ´ hÃ¬nh biá»ƒu diá»…n ngÃ´n ngá»¯: tá»« embedding tÄ©nh (static embeddings) nhÆ° Word2Vec vÃ  GloVe Ä‘áº¿n mÃ´ hÃ¬nh ngá»¯ cáº£nh hÃ³a (contextual embeddings) nhÆ° GPT vÃ  BERT.

ChÃºng tÃ´i phÃ¢n tÃ­ch:
	â€¢	MÃ´ hÃ¬nh toÃ¡n há»c ná»n táº£ng
	â€¢	HÃ m má»¥c tiÃªu huáº¥n luyá»‡n
	â€¢	Cáº¥u trÃºc xÃ¡c suáº¥t
	â€¢	TÃ­nh cháº¥t tuyáº¿n tÃ­nh cá»§a embedding
	â€¢	Attention vÃ  self-attention
	â€¢	So sÃ¡nh Ä‘á»‹nh lÆ°á»£ng vá» Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n

CÃ¡c cÃ´ng thá»©c toÃ¡n há»c Ä‘Æ°á»£c trÃ¬nh bÃ y nháº±m minh há»a rÃµ sá»± khÃ¡c biá»‡t báº£n cháº¥t giá»¯a cÃ¡c tháº¿ há»‡ mÃ´ hÃ¬nh.

â¸»

1. Giá»›i thiá»‡u

Biá»ƒu diá»…n tá»« (word representation) lÃ  bÃ i toÃ¡n trung tÃ¢m trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP).

Ta xÃ©t má»™t táº­p tá»« vá»±ng:

V = \{w_1, w_2, \dots, w_{|V|}\}

Má»¥c tiÃªu lÃ  xÃ¢y dá»±ng Ã¡nh xáº¡:

E: V \rightarrow \mathbb{R}^d

Trong Ä‘Ã³ d lÃ  sá»‘ chiá»u embedding.

Lá»‹ch sá»­ phÃ¡t triá»ƒn cÃ³ thá»ƒ chia thÃ nh hai giai Ä‘oáº¡n chÃ­nh:
	1.	Embedding tÄ©nh (Static embeddings)
	â€¢	Word2Vec
	â€¢	GloVe
	2.	Embedding ngá»¯ cáº£nh hÃ³a (Contextual embeddings)
	â€¢	GPT
	â€¢	BERT

â¸»

2. Word2Vec: MÃ´ hÃ¬nh dá»±a trÃªn Ngá»¯ cáº£nh Cá»¥c bá»™

2.1 Tá»•ng quan

Word2Vec (Mikolov et al., 2013) dá»±a trÃªn giáº£ thuyáº¿t phÃ¢n bá»‘:

P(w \mid context)

Hai biáº¿n thá»ƒ chÃ­nh:
	â€¢	CBOW (Continuous Bag of Words)
	â€¢	Skip-gram

â¸»

2.2 MÃ´ hÃ¬nh Skip-gram

Giáº£ sá»­ chuá»—i tá»«:

w_1, w_2, \dots, w_T

HÃ m má»¥c tiÃªu:

\max \sum_{t=1}^{T} \sum_{-c \le j \le c, j \ne 0} \log P(w_{t+j} \mid w_t)

Vá»›i:

P(w_O \mid w_I) = \frac{\exp(v_{w_O}^\top v_{w_I})}{\sum_{w \in V} \exp(v_w^\top v_{w_I})}

Do chi phÃ­ tÃ­nh toÃ¡n lá»›n, sá»­ dá»¥ng negative sampling:

\log \sigma(v_{w_O}^\top v_{w_I}) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)} \log \sigma(-v_{w_i}^\top v_{w_I})

â¸»

2.3 TÃ­nh cháº¥t Tuyáº¿n tÃ­nh

Má»™t tÃ­nh cháº¥t ná»•i tiáº¿ng:

\text{king} - \text{man} + \text{woman} \approx \text{queen}

ToÃ¡n há»c:

v_{king} - v_{man} + v_{woman} \approx v_{queen}

Äiá»u nÃ y cho tháº¥y embedding há»c Ä‘Æ°á»£c cáº¥u trÃºc tuyáº¿n tÃ­nh.

â¸»

3. GloVe: Ma tráº­n Äá»“ng xuáº¥t hiá»‡n ToÃ n cá»¥c

3.1 Tá»•ng quan

GloVe (Pennington et al., 2014) dá»±a trÃªn ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n:

X_{ij} = \text{sá»‘ láº§n } w_j \text{ xuáº¥t hiá»‡n trong ngá»¯ cáº£nh cá»§a } w_i

â¸»

3.2 HÃ m má»¥c tiÃªu

J = \sum_{i,j} f(X_{ij}) \left( w_i^\top \tilde{w}_j + b_i + b_j - \log X_{ij} \right)^2

Trong Ä‘Ã³:

f(x) =
\begin{cases}
(x/x_{max})^\alpha & x < x_{max} \\
1 & \text{otherwise}
\end{cases}

KhÃ¡c vá»›i Word2Vec, GloVe khai thÃ¡c thá»‘ng kÃª toÃ n cá»¥c.

â¸»

4. GPT: Transformer Tá»± Há»“i quy

4.1 Cáº¥u trÃºc tá»•ng quan

GPT (Radford et al.) dá»±a trÃªn kiáº¿n trÃºc Transformer tá»« bÃ i bÃ¡o cá»§a Ashish Vaswani et al. (2017).

MÃ´ hÃ¬nh xÃ¡c suáº¥t:

P(w_1,\dots,w_T) = \prod_{t=1}^{T} P(w_t \mid w_{<t})

â¸»

4.2 Self-Attention

Vá»›i:

Q = XW_Q,\quad K = XW_K,\quad V = XW_V

Attention:

\text{Attention}(Q,K,V) =
\text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right)V

Äá»™ phá»©c táº¡p:

O(n^2 d)

â¸»

4.3 HÃ m máº¥t mÃ¡t

Cross-entropy:

\mathcal{L} = - \sum_{t=1}^{T} \log P(w_t \mid w_{<t})

GPT sinh vÄƒn báº£n theo hÆ°á»›ng trÃ¡i â†’ pháº£i (autoregressive).

â¸»

5. BERT: Transformer Hai chiá»u

5.1 Kiáº¿n trÃºc

BERT (Devlin et al., 2018) sá»­ dá»¥ng:
	â€¢	Masked Language Modeling (MLM)
	â€¢	Next Sentence Prediction (NSP)

â¸»

5.2 Masked Language Model

Chá»n táº­p vá»‹ trÃ­ M:

\mathcal{L}_{MLM} = - \sum_{t \in M} \log P(w_t \mid w_{\setminus M})

KhÃ¡c GPT:
	â€¢	GPT: dá»± Ä‘oÃ¡n tÆ°Æ¡ng lai
	â€¢	BERT: dÃ¹ng cáº£ trÃ¡i vÃ  pháº£i

â¸»

5.3 Biá»ƒu diá»…n Ngá»¯ cáº£nh hÃ³a

Embedding giá» lÃ  hÃ m cá»§a toÃ n bá»™ cÃ¢u:

e_t = f(w_1,\dots,w_T, t)

KhÃ´ng cÃ²n lÃ  Ã¡nh xáº¡ cá»‘ Ä‘á»‹nh.

â¸»

6. So sÃ¡nh ToÃ¡n há»c

MÃ´ hÃ¬nh	XÃ¡c suáº¥t	Pháº¡m vi ngá»¯ cáº£nh	Embedding
Word2Vec	P(w_O|w_I)	Cá»¥c bá»™	TÄ©nh
GloVe	\log X_{ij}	ToÃ n cá»¥c	TÄ©nh
GPT	P(w_t|w_{<t})	TrÃ¡i	Ngá»¯ cáº£nh
BERT	P(w_t|w_{\setminus M})	Hai chiá»u	Ngá»¯ cáº£nh


â¸»

7. PhÃ¢n tÃ­ch Entropy

Entropy chuá»—i:

H = - \sum P(w_1,\dots,w_T)\log P(w_1,\dots,w_T)

GPT mÃ´ hÃ¬nh hÃ³a trá»±c tiáº¿p:

H = - \sum_{t} \log P(w_t \mid w_{<t})

Perplexity:

\text{PPL} = 2^H

Word2Vec/GloVe khÃ´ng tá»‘i Æ°u trá»±c tiáº¿p perplexity.

â¸»

8. So sÃ¡nh Äá»™ phá»©c táº¡p

Word2Vec:

O(T c d)

GloVe:

O(|X|)

Transformer:

O(n^2 d)

Trong Ä‘Ã³ n lÃ  Ä‘á»™ dÃ i chuá»—i.

â¸»

9. Tiáº¿n hÃ³a MÃ´ hÃ¬nh

QuÃ¡ trÃ¬nh phÃ¡t triá»ƒn:
	1.	Vector tÄ©nh (Word2Vec, GloVe)
	2.	Transformer má»™t chiá»u (GPT)
	3.	Transformer hai chiá»u (BERT)

BÆ°á»›c chuyá»ƒn quan trá»ng nháº¥t lÃ  self-attention.

â¸»

10. Káº¿t luáº­n

Tá»« Word2Vec Ä‘áº¿n GPT vÃ  BERT cho tháº¥y sá»± chuyá»ƒn dá»‹ch:
	â€¢	Tá»« mÃ´ hÃ¬nh cá»¥c bá»™ â†’ mÃ´ hÃ¬nh toÃ n chuá»—i
	â€¢	Tá»« embedding tÄ©nh â†’ embedding ngá»¯ cáº£nh
	â€¢	Tá»« ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n â†’ mÃ´ hÃ¬nh xÃ¡c suáº¥t chuá»—i

ToÃ¡n há»c chuyá»ƒn tá»«:

v_w \in \mathbb{R}^d

sang:

P(w_1,\dots,w_T)

ÄÃ¢y lÃ  bÆ°á»›c nháº£y tá»« biá»ƒu diá»…n hÃ¬nh há»c sang mÃ´ hÃ¬nh hÃ³a phÃ¢n phá»‘i xÃ¡c suáº¥t hoÃ n chá»‰nh.

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Mikolov et al. (2013). Efficient Estimation of Word Representations.
	2.	Pennington et al. (2014). GloVe: Global Vectors for Word Representation.
	3.	Vaswani et al. (2017). Attention Is All You Need.
	4.	Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
	5.	Radford et al. (2018â€“2023). GPT series papers.
	6.	Shannon, C. (1948). A Mathematical Theory of Communication.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| ğŸ“Œ **[aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md](aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md) |
| [aero_LLM_02_Exploring GloVe pretrained embeddings.md](aero_LLM_02_Exploring GloVe pretrained embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Exploring GloVe pretrained embeddings.md) |
| [aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md](aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Tá»« Vá»±ng giá»¯a Wikipedia vÃ  Twitter báº±ng PhÃ¢n TÃ­ch TÆ°Æ¡ng Äá»“ng Biá»ƒu Diá»…n (RSA)](aero_LLM_04_CodeChallenge Wikipedia vs. Twitter embeddings (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Wikipedia vs. Twitter embeddings (part 2).md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Ngá»¯ NghÄ©a cá»§a GPT-2 vÃ  BERT thÃ´ng qua PhÃ¢n TÃ­ch Embedding](aero_LLM_05_Exploring GPT2 and BERT embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Exploring GPT2 and BERT embeddings.md) |
| [ToÃ¡n há»c cá»§a Token vÃ  Embedding trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n](aero_LLM_06_CodeChallenge Math with tokens and embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_CodeChallenge Math with tokens and embeddings.md) |
| [Cosine Similarity vÃ  Má»‘i Quan Há»‡ vá»›i Há»‡ Sá»‘ TÆ°Æ¡ng Quan: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong NLP](aero_LLM_07_Cosine similarity (and relation to correlation).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Cosine similarity (and relation to correlation).md) |
| [PhÃ¢n TÃ­ch Cosine Similarity trong KhÃ´ng Gian Embedding cá»§a GPT-2](aero_LLM_08_CodeChallenge GPT2 cosine similarities.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge GPT2 cosine similarities.md) |
| [Unembedding trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: Tá»« Vector áº¨n Äáº¿n Token](aero_LLM_09_CodeChallenge Unembeddings (vectors to tokens).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Unembeddings (vectors to tokens).md) |
| [Position Embeddings trong Transformer: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_LLM_10_Position embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_Position embeddings.md) |
| [PhÃ¢n TÃ­ch Thá»±c Nghiá»‡m Embedding Vá»‹ TrÃ­ Trong Transformer: Tá»« Cáº¥u TrÃºc Tuyáº¿n TÃ­nh Äáº¿n KhÃ´ng Gian HÃ¬nh Há»c](aero_LLM_11_CodeChallenge Exploring position embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Exploring position embeddings.md) |
| [Huáº¥n Luyá»‡n Embedding Tá»« Äáº§u: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, CÆ¡ Cháº¿ Tá»‘i Æ¯u vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_12_Training embeddings from scratch.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_Training embeddings from scratch.md) |
| [Thiáº¿t Káº¿ Data Loader Cho Huáº¥n Luyá»‡n MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Kiáº¿n TrÃºc vÃ  Tá»‘i Æ¯u HoÃ¡](aero_LLM_13_Create a data loader to train a model.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_Create a data loader to train a model.md) |
| [XÃ¢y Dá»±ng MÃ´ HÃ¬nh Há»c Embedding Tá»« Äáº§u: Kiáº¿n TrÃºc, Tá»‘i Æ¯u HoÃ¡ vÃ  PhÃ¢n TÃ­ch ToÃ¡n Há»c](aero_LLM_14_Build a model to learn the embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_Build a model to learn the embeddings.md) |
| [HÃ m Máº¥t MÃ¡t Trong Huáº¥n Luyá»‡n Embedding: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, PhÃ¢n TÃ­ch Gradient vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_15_Loss function to train the embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_Loss function to train the embeddings.md) |
| [Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh Há»c MÃ¡y: CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Thá»±c tiá»…n](aero_LLM_16_Train and evaluate the model.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_Train and evaluate the model.md) |
| [Sá»± Thay Äá»•i cá»§a Embeddings Trong QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_LLM_17_CodeChallenge How the embeddings change.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_CodeChallenge How the embeddings change.md) |
| [Äá»™ á»”n Äá»‹nh cá»§a Embeddings trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_LLM_18_CodeChallenge How stable are embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_CodeChallenge How stable are embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [05 Embeddings spaces](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# XÃ¢y Dá»±ng MÃ´ HÃ¬nh Há»c Embedding Tá»« Äáº§u: Kiáº¿n TrÃºc, Tá»‘i Æ¯u HoÃ¡ vÃ  PhÃ¢n TÃ­ch ToÃ¡n Há»c

TÃ³m táº¯t

BÃ i viáº¿t nÃ y trÃ¬nh bÃ y quy trÃ¬nh xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh há»c embedding tá»« Ä‘áº§u (build a model to learn the embeddings), bao gá»“m thiáº¿t káº¿ kiáº¿n trÃºc tá»‘i thiá»ƒu cho bÃ i toÃ¡n dá»± Ä‘oÃ¡n token tiáº¿p theo, Ä‘á»‹nh nghÄ©a hÃ m máº¥t mÃ¡t, lan truyá»n ngÆ°á»£c vÃ  phÃ¢n tÃ­ch Ä‘á»™ng há»c tá»‘i Æ°u. PhÃ¢n tÃ­ch Ä‘Æ°á»£c Ä‘áº·t trong bá»‘i cáº£nh cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»± há»“i quy nhÆ° GPT-2 cá»§a OpenAI, dá»±a trÃªn ná»n táº£ng Transformer tá»« cÃ´ng trÃ¬nh Attention Is All You Need cá»§a Ashish Vaswani vÃ  cá»™ng sá»±. Äá»“ng thá»i, bÃ i viáº¿t liÃªn há»‡ vá»›i cÃ¡c mÃ´ hÃ¬nh embedding cá»• Ä‘iá»ƒn nhÆ° Word2Vec cá»§a Tomas Mikolov.

â¸»

1. Giá»›i thiá»‡u

Embedding lÃ  Ã¡nh xáº¡ tá»« khÃ´ng gian rá»i ráº¡c sang khÃ´ng gian vector liÃªn tá»¥c:

f: \{1,\dots,V\} \rightarrow \mathbb{R}^d

Trong Ä‘Ã³:
	â€¢	V: kÃ­ch thÆ°á»›c tá»« vá»±ng
	â€¢	d: sá»‘ chiá»u embedding

Ma tráº­n embedding:

\mathbf{E} \in \mathbb{R}^{V \times d}

Vector cá»§a token w:

\mathbf{e}_w = \mathbf{E}[w]

Má»¥c tiÃªu huáº¥n luyá»‡n lÃ  tÃ¬m \mathbf{E} sao cho embedding pháº£n Ã¡nh cáº¥u trÃºc ngá»¯ nghÄ©a vÃ  ngá»¯ cáº£nh.

â¸»

2. Kiáº¿n trÃºc mÃ´ hÃ¬nh tá»‘i thiá»ƒu

XÃ©t mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n cho bÃ i toÃ¡n next-token prediction.

2.1 Lá»›p Embedding

Token Ä‘áº§u vÃ o:

\mathbf{x} \in \mathbb{R}^{B \times L}

Sau embedding:

\mathbf{H} =
\mathbf{E}[\mathbf{x}]
\in
\mathbb{R}^{B \times L \times d}

â¸»

2.2 Lá»›p tuyáº¿n tÃ­nh Ä‘áº§u ra

Logits:

\mathbf{Z}
=
\mathbf{H}
\mathbf{W}
+
\mathbf{b}

Vá»›i:
	â€¢	\mathbf{W} \in \mathbb{R}^{d \times V}
	â€¢	\mathbf{b} \in \mathbb{R}^{V}

â¸»

2.3 Softmax

P(y=i \mid \mathbf{h})
=
\frac{
\exp(z_i)
}{
\sum_{j=1}^{V}
\exp(z_j)
}

â¸»

3. HÃ m máº¥t mÃ¡t vÃ  tá»‘i Æ°u

3.1 Cross-Entropy

\mathcal{L}
=
-
\sum_{t=1}^{L}
\log
P(y_t \mid x_{<t})

Trung bÃ¬nh trÃªn batch:

\mathcal{L}_{batch}
=
\frac{1}{BL}
\sum_{b=1}^{B}
\sum_{t=1}^{L}
\mathcal{L}_{b,t}

â¸»

3.2 Gradient theo embedding

Gá»i:

\mathbf{p} = \text{softmax}(\mathbf{z})

Gradient theo logits:

\frac{\partial \mathcal{L}}{\partial \mathbf{z}}
=
\mathbf{p} - \mathbf{y}

Gradient theo embedding:

\frac{\partial \mathcal{L}}{\partial \mathbf{e}_w}
=
\mathbf{W}
(\mathbf{p} - \mathbf{y})

Cáº­p nháº­t:

\mathbf{E}[w]
\leftarrow
\mathbf{E}[w]
-
\eta
\frac{\partial \mathcal{L}}{\partial \mathbf{E}[w]}

â¸»

4. Trá»ng sá»‘ buá»™c (Weight Tying)

Trong cÃ¡c mÃ´ hÃ¬nh nhÆ° GPT-2, ta thÆ°á»ng buá»™c:

\mathbf{W} = \mathbf{E}^T

Khi Ä‘Ã³:

z_i
=
\mathbf{h}^T
\mathbf{e}_i

Ã nghÄ©a:
	â€¢	Logit lÃ  tÃ­ch vÃ´ hÆ°á»›ng giá»¯a hidden state vÃ  embedding token.
	â€¢	KhÃ´ng gian embedding Ä‘Ã³ng vai trÃ² vá»«a mÃ£ hoÃ¡ vá»«a giáº£i mÃ£ (unembedding).

â¸»

5. PhÃ¢n tÃ­ch Ä‘á»™ng há»c há»c embedding

5.1 HÆ°á»›ng cáº­p nháº­t

Gradient embedding:

\Delta \mathbf{e}_w
=
-
\eta
\mathbf{W}
(\mathbf{p}-\mathbf{y})

Khi token dá»± Ä‘oÃ¡n Ä‘Ãºng:

\mathbf{p} \approx \mathbf{y}
\Rightarrow
\Delta \mathbf{e}_w \approx 0

Khi sai:
	â€¢	Embedding dá»‹ch chuyá»ƒn vá» phÃ­a vector Ä‘Ãºng
	â€¢	TÃ¡ch xa vector sai

â¸»

5.2 PhÃ¢n tÃ­ch hÃ¬nh há»c

Cosine similarity:

\cos(\theta)
=
\frac{
\mathbf{e}_a \cdot \mathbf{e}_b
}{
\|\mathbf{e}_a\|
\|\mathbf{e}_b\|
}

Qua huáº¥n luyá»‡n:
	â€¢	Token xuáº¥t hiá»‡n trong ngá»¯ cáº£nh tÆ°Æ¡ng tá»± â†’ vector gáº§n nhau
	â€¢	HÃ¬nh thÃ nh cá»¥m ngá»¯ nghÄ©a

â¸»

6. LiÃªn há»‡ vá»›i Word2Vec

Trong Skip-gram:

P(c \mid w)
=
\frac{
\exp(\mathbf{u}_c^T \mathbf{v}_w)
}{
\sum_{j=1}^{V}
\exp(\mathbf{u}_j^T \mathbf{v}_w)
}

Tá»‘i Æ°u:

\max
\sum_{(w,c)}
\log P(c \mid w)

Negative Sampling:

\mathcal{L}
=
\log \sigma(\mathbf{u}_c^T \mathbf{v}_w)
+
\sum_{k=1}^{K}
\log \sigma(-\mathbf{u}_{n_k}^T \mathbf{v}_w)

MÃ´ hÃ¬nh embedding hiá»‡n Ä‘áº¡i cÃ³ thá»ƒ xem nhÆ° má»Ÿ rá»™ng cá»§a cÆ¡ cháº¿ nÃ y trong khÃ´ng gian sÃ¢u (deep contextual space).

â¸»

7. Má»Ÿ rá»™ng sang Transformer

Trong Transformer:

\mathbf{z}_t
=
\mathbf{e}_t
+
\mathbf{p}_t

Self-attention:

\text{Attention}(Q,K,V)
=
\text{softmax}
\left(
\frac{QK^T}{\sqrt{d_k}}
\right)V

Embedding áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n attention scores:

QK^T
=
(\mathbf{E}+\mathbf{P})W_Q
W_K^T
(\mathbf{E}+\mathbf{P})^T

Do Ä‘Ã³ embedding khÃ´ng chá»‰ lÃ  báº£ng tra cá»©u mÃ  lÃ  ná»n táº£ng cáº¥u trÃºc toÃ n bá»™ mÃ´ hÃ¬nh.

â¸»

8. PhÃ¢n tÃ­ch phá»• vÃ  cáº¥u trÃºc tháº¥p chiá»u

XÃ©t ma tráº­n embedding:

\mathbf{E}
\in
\mathbb{R}^{V \times d}

Ma tráº­n hiá»‡p phÆ°Æ¡ng sai:

\mathbf{C}
=
\frac{1}{V}
\mathbf{E}^T
\mathbf{E}

Giáº£i bÃ i toÃ¡n trá»‹ riÃªng:

\mathbf{C}\mathbf{v}_i
=
\lambda_i
\mathbf{v}_i

Thá»±c nghiá»‡m cho tháº¥y:
	â€¢	Pháº§n lá»›n phÆ°Æ¡ng sai táº­p trung á»Ÿ vÃ i thÃ nh pháº§n chÃ­nh.
	â€¢	Embedding cÃ³ cáº¥u trÃºc tháº¥p chiá»u hiá»‡u quáº£.

â¸»

9. Há»™i tá»¥ vÃ  á»•n Ä‘á»‹nh

Vá»›i tá»‘i Æ°u Adam:

m_t
=
\beta_1 m_{t-1}
+
(1-\beta_1) g_t

v_t
=
\beta_2 v_{t-1}
+
(1-\beta_2) g_t^2

\theta_t
=
\theta_{t-1}
-
\eta
\frac{m_t}{\sqrt{v_t}+\epsilon}

Embedding thÆ°á»ng há»™i tá»¥ nhanh á»Ÿ giai Ä‘oáº¡n Ä‘áº§u do gradient lá»›n.

â¸»

10. Káº¿t luáº­n

XÃ¢y dá»±ng mÃ´ hÃ¬nh há»c embedding tá»« Ä‘áº§u bao gá»“m:
	1.	Thiáº¿t káº¿ lá»›p embedding.
	2.	Äá»‹nh nghÄ©a bÃ i toÃ¡n dá»± Ä‘oÃ¡n.
	3.	Tá»‘i Æ°u báº±ng cross-entropy.
	4.	PhÃ¢n tÃ­ch Ä‘á»™ng há»c cáº­p nháº­t.
	5.	Hiá»ƒu cáº¥u trÃºc hÃ¬nh há»c cá»§a khÃ´ng gian embedding.

Embedding khÃ´ng chá»‰ lÃ  thÃ nh pháº§n phá»¥ trá»£ mÃ  lÃ  khÃ´ng gian hÃ¬nh há»c trung tÃ¢m cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯.

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Ashish Vaswani et al. (2017). Attention Is All You Need.
	2.	OpenAI (2019). GPT-2 Technical Report.
	3.	Tomas Mikolov et al. (2013). Efficient Estimation of Word Representations in Vector Space.
	4.	Kingma & Ba (2015). Adam: A Method for Stochastic Optimization.
	5.	Goodfellow et al. (2016). Deep Learning.
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

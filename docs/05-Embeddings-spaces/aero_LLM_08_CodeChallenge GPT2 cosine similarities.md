
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [05 Embeddings spaces](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# PhÃ¢n TÃ­ch Cosine Similarity trong KhÃ´ng Gian Embedding cá»§a GPT-2

## TÃ³m táº¯t

Cosine similarity lÃ  má»™t cÃ´ng cá»¥ trung tÃ¢m trong viá»‡c phÃ¢n tÃ­ch cáº¥u trÃºc hÃ¬nh há»c cá»§a khÃ´ng gian embedding trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n. BÃ i viáº¿t nÃ y trÃ¬nh bÃ y cÆ¡ sá»Ÿ toÃ¡n há»c vÃ  quy trÃ¬nh thá»±c nghiá»‡m Ä‘á»ƒ tÃ­nh toÃ¡n cosine similarity giá»¯a cÃ¡c token embedding cá»§a [GPT-2](chatgpt://generic-entity?number=0), Ä‘á»“ng thá»i phÃ¢n tÃ­ch Ã½ nghÄ©a hÃ¬nh há»c vÃ  thá»‘ng kÃª cá»§a cÃ¡c giÃ¡ trá»‹ tÆ°Æ¡ng Ä‘á»“ng thu Ä‘Æ°á»£c.

---

## 1. Giá»›i thiá»‡u

Trong cÃ¡c mÃ´ hÃ¬nh Transformer sinh vÄƒn báº£n, má»—i token Ä‘Æ°á»£c Ã¡nh xáº¡ sang má»™t vector trong khÃ´ng gian \( \mathbb{R}^d \). Vá»›i GPT-2:

- KÃ­ch thÆ°á»›c embedding: \( d = 768 \) (báº£n base)
- Tá»« vá»±ng: khoáº£ng 50.000 token

Má»—i token \( t \) cÃ³ vector embedding:

\[
\mathbf{v}_t \in \mathbb{R}^{768}
\]

PhÃ¢n tÃ­ch cosine similarity giá»¯a cÃ¡c vector nÃ y giÃºp hiá»ƒu cáº¥u trÃºc ngá»¯ nghÄ©a ná»™i táº¡i cá»§a mÃ´ hÃ¬nh.

---

## 2. CÆ¡ sá»Ÿ toÃ¡n há»c cá»§a Cosine Similarity

Cho hai vector:

\[
\mathbf{x}, \mathbf{y} \in \mathbb{R}^d
\]

Äá»‹nh nghÄ©a:

\[
\text{cosine}(\mathbf{x},\mathbf{y})
=
\frac{\mathbf{x} \cdot \mathbf{y}}
{\|\mathbf{x}\| \|\mathbf{y}\|}
\]

Trong Ä‘Ã³:

\[
\mathbf{x} \cdot \mathbf{y}
=
\sum_{i=1}^{d} x_i y_i
\]

\[
\|\mathbf{x}\|
=
\sqrt{\sum_{i=1}^{d} x_i^2}
\]

GiÃ¡ trá»‹ náº±m trong khoáº£ng:

\[
-1 \leq \text{cosine} \leq 1
\]

---

## 3. Chuáº©n hÃ³a vÃ  tÃ­nh toÃ¡n hiá»‡u quáº£

Trong thá»±c táº¿, ta chuáº©n hÃ³a trÆ°á»›c:

\[
\hat{\mathbf{x}} =
\frac{\mathbf{x}}{\|\mathbf{x}\|}
\]

Khi Ä‘Ã³:

\[
\text{cosine}(\mathbf{x},\mathbf{y})
=
\hat{\mathbf{x}} \cdot \hat{\mathbf{y}}
\]

Náº¿u ma tráº­n embedding:

\[
E \in \mathbb{R}^{|V| \times d}
\]

Sau khi chuáº©n hÃ³a tá»«ng hÃ ng:

\[
\hat{E}
\]

Ma tráº­n cosine similarity toÃ n bá»™ tá»« vá»±ng:

\[
S = \hat{E} \hat{E}^T
\]

---

## 4. PhÃ¢n tÃ­ch thá»±c nghiá»‡m vá»›i GPT-2

### 4.1 TrÃ­ch xuáº¥t embedding

Vá»›i token index \( i \):

\[
\mathbf{v}_i = E[i]
\]

Trong GPT-2, embedding Ä‘áº§u vÃ o vÃ  embedding Ä‘áº§u ra thÆ°á»ng Ä‘Æ°á»£c chia sáº» trá»ng sá»‘ (weight tying):

\[
W_{out} = E^T
\]

Äiá»u nÃ y táº¡o liÃªn há»‡ hÃ¬nh há»c trá»±c tiáº¿p giá»¯a khÃ´ng gian embedding vÃ  khÃ´ng gian dá»± Ä‘oÃ¡n xÃ¡c suáº¥t.

---

### 4.2 VÃ­ dá»¥: So sÃ¡nh token

Giáº£ sá»­ ta chá»n token:

- â€œcatâ€
- â€œdogâ€
- â€œbananaâ€

Ta tÃ­nh:

\[
\text{sim}(\text{cat},\text{dog})
\]

\[
\text{sim}(\text{cat},\text{banana})
\]

Ká»³ vá»ng:

\[
\text{sim}(\text{cat},\text{dog})
>
\text{sim}(\text{cat},\text{banana})
\]

Do cáº¥u trÃºc ngá»¯ nghÄ©a gáº§n nhau.

---

## 5. PhÃ¢n bá»‘ Cosine Similarity trong khÃ´ng gian cao chiá»u

Giáº£ sá»­ hai vector ngáº«u nhiÃªn:

\[
\mathbf{x},\mathbf{y}
\sim \mathcal{N}(0,I_d)
\]

Khi \( d \to \infty \):

\[
\mathbb{E}[\text{cosine}] = 0
\]

\[
\text{Var}(\text{cosine}) \approx \frac{1}{d}
\]

Vá»›i \( d = 768 \):

\[
\text{Var} \approx \frac{1}{768}
\]

Do Ä‘Ã³:

- Vector ngáº«u nhiÃªn gáº§n trá»±c giao
- Cosine lá»›n biá»ƒu thá»‹ cáº¥u trÃºc há»c Ä‘Æ°á»£c

---

## 6. LiÃªn há»‡ vá»›i Softmax vÃ  xÃ¡c suáº¥t dá»± Ä‘oÃ¡n

Trong GPT-2, xÃ¡c suáº¥t token tiáº¿p theo:

\[
P(w_t | h_t)
=
\text{softmax}(W_{out} h_t)
\]

Náº¿u weight tying:

\[
W_{out} = E^T
\]

Khi Ä‘Ã³:

\[
z_i = \mathbf{v}_i \cdot h_t
\]

Softmax:

\[
P(w_i)
=
\frac{e^{\mathbf{v}_i \cdot h_t}}
{\sum_j e^{\mathbf{v}_j \cdot h_t}}
\]

NhÆ° váº­y:

> Dá»± Ä‘oÃ¡n xÃ¡c suáº¥t thá»±c cháº¥t dá»±a trÃªn tÃ­ch vÃ´ hÆ°á»›ng giá»¯a embedding vÃ  hidden state.

Náº¿u chuáº©n hÃ³a:

\[
\mathbf{v}_i \cdot h_t
=
\|\mathbf{v}_i\|
\|h_t\|
\cos\theta
\]

Do Ä‘Ã³ cosine similarity trá»±c tiáº¿p áº£nh hÆ°á»Ÿng Ä‘áº¿n xÃ¡c suáº¥t dá»± Ä‘oÃ¡n.

---

## 7. Ma tráº­n tÆ°Æ¡ng Ä‘á»“ng cá»¥c bá»™

Cho táº­p \( n \) token:

\[
X \in \mathbb{R}^{n \times d}
\]

Ma tráº­n cosine:

\[
S_{ij}
=
\frac{\mathbf{v}_i \cdot \mathbf{v}_j}
{\|\mathbf{v}_i\|\|\mathbf{v}_j\|}
\]

Ta cÃ³ thá»ƒ phÃ¢n tÃ­ch:

- Cá»¥m tá»« (clustering)
- PhÃ¢n tÃ­ch trá»‹ riÃªng:

\[
S \mathbf{u} = \lambda \mathbf{u}
\]

GiÃ¡ trá»‹ riÃªng lá»›n pháº£n Ã¡nh cáº¥u trÃºc ngá»¯ nghÄ©a chiáº¿m Æ°u tháº¿.

---

## 8. Khoáº£ng cÃ¡ch tÆ°Æ¡ng Ä‘Æ°Æ¡ng

Náº¿u vector Ä‘Ã£ chuáº©n hÃ³a:

\[
\|\mathbf{x}-\mathbf{y}\|^2
=
2 - 2\cos\theta
\]

Suy ra:

\[
\cos\theta
=
1 - \frac{1}{2}
\|\mathbf{x}-\mathbf{y}\|^2
\]

Äiá»u nÃ y cho tháº¥y cosine similarity vÃ  Euclid distance tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá» máº·t hÃ¬nh há»c khi chuáº©n hÃ³a.

---

## 9. Ã nghÄ©a lÃ½ thuyáº¿t

Cosine similarity trong GPT-2:

1. Äá»‹nh nghÄ©a cáº¥u trÃºc hÃ¬nh há»c cá»§a tá»« vá»±ng.
2. LiÃªn há»‡ trá»±c tiáº¿p vá»›i xÃ¡c suáº¥t dá»± Ä‘oÃ¡n.
3. Pháº£n Ã¡nh cáº¥u trÃºc phÃ¢n bá»‘ dá»¯ liá»‡u huáº¥n luyá»‡n.
4. Giáº£m áº£nh hÆ°á»Ÿng cá»§a Ä‘á»™ lá»›n vector.

Vá» báº£n cháº¥t:

\[
\text{Prediction}
\propto
\exp(\|\mathbf{v}\|\|h\|\cos\theta)
\]

Do Ä‘Ã³ gÃ³c giá»¯a vector Ä‘Ã³ng vai trÃ² quyáº¿t Ä‘á»‹nh.

---

## 10. Káº¿t luáº­n

PhÃ¢n tÃ­ch cosine similarity trong GPT-2 cho tháº¥y:

- KhÃ´ng gian embedding cÃ³ cáº¥u trÃºc hÃ¬nh há»c rÃµ rÃ ng.
- CÃ¡c token liÃªn quan cÃ³ gÃ³c nhá» (cosine lá»›n).
- Dá»± Ä‘oÃ¡n xÃ¡c suáº¥t phá»¥ thuá»™c trá»±c tiáº¿p vÃ o tÃ­ch vÃ´ hÆ°á»›ng.
- Trong khÃ´ng gian cao chiá»u, cáº¥u trÃºc há»c Ä‘Æ°á»£c ná»•i báº­t hÆ¡n ná»n ngáº«u nhiÃªn.

Hiá»ƒu rÃµ ná»n táº£ng toÃ¡n há»c nÃ y giÃºp ta:

- PhÃ¢n tÃ­ch embedding hiá»‡u quáº£
- So sÃ¡nh mÃ´ hÃ¬nh
- Thá»±c hiá»‡n Representational Similarity Analysis (RSA)
- Tá»‘i Æ°u hÃ³a há»‡ thá»‘ng retrieval hoáº·c semantic search

---

## TÃ i liá»‡u tham kháº£o

1. Vaswani et al. (2017). Attention is All You Need.  
2. Radford et al. (2019). Language Models are Unsupervised Multitask Learners.  
3. Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.  
4. Jurafsky & Martin (2023). Speech and Language Processing.  
5. Kriegeskorte et al. (2008). Representational Similarity Analysis.

---
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md](aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md) |
| [aero_LLM_02_Exploring GloVe pretrained embeddings.md](aero_LLM_02_Exploring GloVe pretrained embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Exploring GloVe pretrained embeddings.md) |
| [aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md](aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Tá»« Vá»±ng giá»¯a Wikipedia vÃ  Twitter báº±ng PhÃ¢n TÃ­ch TÆ°Æ¡ng Äá»“ng Biá»ƒu Diá»…n (RSA)](aero_LLM_04_CodeChallenge Wikipedia vs. Twitter embeddings (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Wikipedia vs. Twitter embeddings (part 2).md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Ngá»¯ NghÄ©a cá»§a GPT-2 vÃ  BERT thÃ´ng qua PhÃ¢n TÃ­ch Embedding](aero_LLM_05_Exploring GPT2 and BERT embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Exploring GPT2 and BERT embeddings.md) |
| [ToÃ¡n há»c cá»§a Token vÃ  Embedding trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n](aero_LLM_06_CodeChallenge Math with tokens and embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_CodeChallenge Math with tokens and embeddings.md) |
| [Cosine Similarity vÃ  Má»‘i Quan Há»‡ vá»›i Há»‡ Sá»‘ TÆ°Æ¡ng Quan: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong NLP](aero_LLM_07_Cosine similarity (and relation to correlation).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Cosine similarity (and relation to correlation).md) |
| ğŸ“Œ **[PhÃ¢n TÃ­ch Cosine Similarity trong KhÃ´ng Gian Embedding cá»§a GPT-2](aero_LLM_08_CodeChallenge GPT2 cosine similarities.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge GPT2 cosine similarities.md) |
| [Unembedding trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: Tá»« Vector áº¨n Äáº¿n Token](aero_LLM_09_CodeChallenge Unembeddings (vectors to tokens).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Unembeddings (vectors to tokens).md) |
| [Position Embeddings trong Transformer: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_LLM_10_Position embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_Position embeddings.md) |
| [PhÃ¢n TÃ­ch Thá»±c Nghiá»‡m Embedding Vá»‹ TrÃ­ Trong Transformer: Tá»« Cáº¥u TrÃºc Tuyáº¿n TÃ­nh Äáº¿n KhÃ´ng Gian HÃ¬nh Há»c](aero_LLM_11_CodeChallenge Exploring position embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Exploring position embeddings.md) |
| [Huáº¥n Luyá»‡n Embedding Tá»« Äáº§u: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, CÆ¡ Cháº¿ Tá»‘i Æ¯u vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_12_Training embeddings from scratch.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_Training embeddings from scratch.md) |
| [Thiáº¿t Káº¿ Data Loader Cho Huáº¥n Luyá»‡n MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Kiáº¿n TrÃºc vÃ  Tá»‘i Æ¯u HoÃ¡](aero_LLM_13_Create a data loader to train a model.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_Create a data loader to train a model.md) |
| [XÃ¢y Dá»±ng MÃ´ HÃ¬nh Há»c Embedding Tá»« Äáº§u: Kiáº¿n TrÃºc, Tá»‘i Æ¯u HoÃ¡ vÃ  PhÃ¢n TÃ­ch ToÃ¡n Há»c](aero_LLM_14_Build a model to learn the embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_Build a model to learn the embeddings.md) |
| [HÃ m Máº¥t MÃ¡t Trong Huáº¥n Luyá»‡n Embedding: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, PhÃ¢n TÃ­ch Gradient vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_15_Loss function to train the embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_Loss function to train the embeddings.md) |
| [Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh Há»c MÃ¡y: CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Thá»±c tiá»…n](aero_LLM_16_Train and evaluate the model.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_Train and evaluate the model.md) |
| [Sá»± Thay Äá»•i cá»§a Embeddings Trong QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_LLM_17_CodeChallenge How the embeddings change.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_CodeChallenge How the embeddings change.md) |
| [Äá»™ á»”n Äá»‹nh cá»§a Embeddings trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_LLM_18_CodeChallenge How stable are embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_CodeChallenge How stable are embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

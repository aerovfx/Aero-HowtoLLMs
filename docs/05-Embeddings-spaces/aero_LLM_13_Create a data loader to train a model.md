
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [05 Embeddings spaces](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thiáº¿t Káº¿ Data Loader Cho Huáº¥n Luyá»‡n MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Kiáº¿n TrÃºc vÃ  Tá»‘i Æ¯u HoÃ¡

TÃ³m táº¯t

Trong huáº¥n luyá»‡n mÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡n Ä‘áº¡i, Ä‘áº·c biá»‡t lÃ  cÃ¡c kiáº¿n trÃºc Transformer, data loader Ä‘Ã³ng vai trÃ² trung gian quan trá»ng giá»¯a dá»¯ liá»‡u thÃ´ vÃ  quÃ¡ trÃ¬nh tá»‘i Æ°u hoÃ¡ tham sá»‘. BÃ i viáº¿t nÃ y trÃ¬nh bÃ y cÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  thá»±c nghiá»‡m cá»§a viá»‡c xÃ¢y dá»±ng data loader cho bÃ i toÃ¡n dá»± Ä‘oÃ¡n token tiáº¿p theo (next-token prediction), bao gá»“m tokenization, batching, táº¡o cáº·p (input, target), xá»­ lÃ½ chuá»—i dÃ i vÃ  tá»‘i Æ°u hiá»‡u suáº¥t. PhÃ¢n tÃ­ch Ä‘Æ°á»£c Ä‘áº·t trong bá»‘i cáº£nh kiáº¿n trÃºc Transformer tá»« cÃ´ng trÃ¬nh Attention Is All You Need cá»§a Ashish Vaswani vÃ  á»©ng dá»¥ng trong cÃ¡c mÃ´ hÃ¬nh nhÆ° GPT-2 cá»§a OpenAI.

â¸»

1. Giá»›i thiá»‡u

Huáº¥n luyá»‡n mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»± há»“i quy (autoregressive language model) yÃªu cáº§u tá»‘i Æ°u xÃ¡c suáº¥t:

P(x_1, x_2, \dots, x_T)
=
\prod_{t=1}^{T}
P(x_t \mid x_{<t})

Trong Ä‘Ã³:
	â€¢	x_t: token táº¡i vá»‹ trÃ­ t
	â€¢	x_{<t}: toÃ n bá»™ ngá»¯ cáº£nh trÆ°á»›c Ä‘Ã³

Data loader cÃ³ nhiá»‡m vá»¥:
	1.	Chuyá»ƒn vÄƒn báº£n thÃ nh chuá»—i token.
	2.	Chia thÃ nh cÃ¡c Ä‘oáº¡n cÃ³ Ä‘á»™ dÃ i cá»‘ Ä‘á»‹nh.
	3.	Táº¡o cáº·p (input, target) cho huáº¥n luyá»‡n.
	4.	Cung cáº¥p batch tá»‘i Æ°u cho GPU/TPU.

â¸»

2. Biá»ƒu diá»…n dá»¯ liá»‡u cho huáº¥n luyá»‡n

2.1 Tokenization

Giáº£ sá»­ vÄƒn báº£n sau khi token hÃ³a:

\mathbf{s} = (t_1, t_2, \dots, t_N)

vá»›i:

t_i \in \{1,2,\dots,V\}
	â€¢	V: kÃ­ch thÆ°á»›c tá»« vá»±ng
	â€¢	N: tá»•ng sá»‘ token

â¸»

2.2 Táº¡o cáº·p (Input, Target)

Vá»›i Ä‘á»™ dÃ i ngá»¯ cáº£nh cá»‘ Ä‘á»‹nh L, ta táº¡o:

\mathbf{x}^{(i)} =
(t_i, t_{i+1}, \dots, t_{i+L-1})

\mathbf{y}^{(i)} =
(t_{i+1}, t_{i+2}, \dots, t_{i+L})

Tá»©c lÃ  target lÃ  phiÃªn báº£n dá»‹ch trÃ¡i cá»§a input.

Má»¥c tiÃªu tá»‘i Æ°u:

\mathcal{L}
=
-
\sum_{t=1}^{L}
\log
P(t_{i+t} \mid t_i,\dots,t_{i+t-1})

â¸»

3. Batch vÃ  Tá»‘i Æ¯u TÃ­nh ToÃ¡n

3.1 Mini-batch

Vá»›i batch size B, ta cÃ³ tensor:

X \in \mathbb{R}^{B \times L}

Y \in \mathbb{R}^{B \times L}

Loss trung bÃ¬nh:

\mathcal{L}_{batch}
=
\frac{1}{B}
\sum_{b=1}^{B}
\mathcal{L}^{(b)}

â¸»

3.2 PhÃ¢n tÃ­ch Ä‘á»™ phá»©c táº¡p

Giáº£ sá»­:
	â€¢	Vocabulary size: V
	â€¢	Embedding dimension: d
	â€¢	Context length: L
	â€¢	Batch size: B

Chi phÃ­ embedding:

O(BLd)

Chi phÃ­ attention:

O(BL^2 d)

Do Ä‘Ã³, data loader pháº£i Ä‘áº£m báº£o cung cáº¥p batch Ä‘á»§ lá»›n nhÆ°ng khÃ´ng vÆ°á»£t quÃ¡ bá»™ nhá»› GPU.

â¸»

4. Chiáº¿n lÆ°á»£c chia dá»¯ liá»‡u

4.1 Sliding Window

Táº¡o cÃ¡c máº«u huáº¥n luyá»‡n vá»›i bÆ°á»›c trÆ°á»£t 1:

(t_1,\dots,t_L),
(t_2,\dots,t_{L+1}),
\dots

Æ¯u Ä‘iá»ƒm:
	â€¢	Táº­n dá»¥ng tá»‘i Ä‘a dá»¯ liá»‡u

NhÆ°á»£c Ä‘iá»ƒm:
	â€¢	TÃ­nh toÃ¡n trÃ¹ng láº·p

â¸»

4.2 Chunking (chia Ä‘oáº¡n khÃ´ng chá»“ng láº¥n)

Chia thÃ nh cÃ¡c Ä‘oáº¡n Ä‘á»™c láº­p:

(t_1,\dots,t_L),
(t_{L+1},\dots,t_{2L})

Æ¯u Ä‘iá»ƒm:
	â€¢	Nhanh
	â€¢	Giáº£m trÃ¹ng láº·p

NhÆ°á»£c Ä‘iá»ƒm:
	â€¢	Giáº£m sá»‘ lÆ°á»£ng máº«u

â¸»

5. Tá»‘i Æ°u hoÃ¡ bá»™ nhá»›

5.1 Memory Mapping

Vá»›i táº­p dá»¯ liá»‡u lá»›n (hÃ ng tá»· token), ta lÆ°u dÆ°á»›i dáº¡ng máº£ng nhá»‹ phÃ¢n:

\mathbf{D} \in \mathbb{N}^{N}

Sá»­ dá»¥ng memory-mapped file:

\text{mmap}: \mathbb{N}^{N} \rightarrow \text{RAM (lazy loading)}

Äiá»u nÃ y cho phÃ©p:
	â€¢	KhÃ´ng load toÃ n bá»™ vÃ o RAM
	â€¢	Truy cáº­p ngáº«u nhiÃªn hiá»‡u quáº£

â¸»

5.2 Shuffling

Trong huáº¥n luyá»‡n SGD:

\theta \leftarrow
\theta - \eta \nabla_\theta \mathcal{L}(x_i)

Äá»ƒ Ä‘áº£m báº£o Æ°á»›c lÆ°á»£ng khÃ´ng chá»‡ch:

\mathbb{E}[\nabla_\theta \mathcal{L}_{batch}]
=
\nabla_\theta \mathcal{L}_{true}

Cáº§n xÃ¡o trá»™n dá»¯ liá»‡u ngáº«u nhiÃªn.

â¸»

6. Data Loader trong Huáº¥n Luyá»‡n Transformer

Pipeline tá»•ng quÃ¡t:
	1.	Raw text
	2.	Tokenization
	3.	LÆ°u thÃ nh máº£ng sá»‘ nguyÃªn
	4.	Random sampling cÃ¡c Ä‘oáº¡n dÃ i L
	5.	GhÃ©p batch
	6.	Chuyá»ƒn sang GPU

Trong mÃ´ hÃ¬nh nhÆ° GPT-2:

\mathbf{z}_t
=
\mathbf{e}_t
+
\mathbf{p}_t

Sau Ä‘Ã³ Ä‘i vÃ o self-attention:

\text{Attention}(Q,K,V)
=
\text{softmax}
\left(
\frac{QK^T}{\sqrt{d_k}}
\right)V

â¸»

7. áº¢nh hÆ°á»Ÿng cá»§a Data Loader Ä‘áº¿n Há»™i Tá»¥

Giáº£ sá»­ gradient Æ°á»›c lÆ°á»£ng:

g_t = \nabla_\theta \mathcal{L}_{batch}

PhÆ°Æ¡ng sai:

\text{Var}(g_t)
=
\frac{\sigma^2}{B}

Batch lá»›n:
	â€¢	Giáº£m phÆ°Æ¡ng sai
	â€¢	Há»™i tá»¥ á»•n Ä‘á»‹nh

Batch nhá»:
	â€¢	Nhiá»…u cao
	â€¢	CÃ³ thá»ƒ tá»•ng quÃ¡t tá»‘t hÆ¡n

â¸»

8. CÃ¡c váº¥n Ä‘á» nÃ¢ng cao

8.1 Curriculum Learning

Sáº¯p xáº¿p dá»¯ liá»‡u theo Ä‘á»™ khÃ³:

\mathcal{D}_1 \subset \mathcal{D}_2 \subset \dots

GiÃºp há»™i tá»¥ nhanh hÆ¡n.

â¸»

8.2 Packing Sequences

Khi chuá»—i ngáº¯n hÆ¡n L, cÃ³ thá»ƒ ghÃ©p nhiá»u chuá»—i vÃ o má»™t block Ä‘á»ƒ tÄƒng hiá»‡u suáº¥t GPU.

â¸»

8.3 Distributed Data Loading

Vá»›i K GPU:

\mathcal{D}
=
\bigcup_{k=1}^{K}
\mathcal{D}_k

Má»—i GPU xá»­ lÃ½ pháº§n riÃªng, Ä‘áº£m báº£o khÃ´ng trÃ¹ng láº·p.

â¸»

9. Káº¿t luáº­n

Data loader khÃ´ng chá»‰ lÃ  thÃ nh pháº§n phá»¥ trá»£ mÃ  lÃ  yáº¿u tá»‘ quyáº¿t Ä‘á»‹nh hiá»‡u suáº¥t vÃ  Ä‘á»™ á»•n Ä‘á»‹nh cá»§a huáº¥n luyá»‡n mÃ´ hÃ¬nh ngÃ´n ngá»¯.

CÃ¡c Ä‘iá»ƒm chÃ­nh:
	1.	Pháº£i xÃ¢y dá»±ng cáº·p (input, target) chÃ­nh xÃ¡c cho bÃ i toÃ¡n autoregressive.
	2.	Tá»‘i Æ°u batch Ä‘á»ƒ cÃ¢n báº±ng bá»™ nhá»› vÃ  tá»‘c Ä‘á»™.
	3.	Xá»­ lÃ½ dá»¯ liá»‡u quy mÃ´ lá»›n báº±ng memory mapping.
	4.	Shuffling Ä‘áº£m báº£o gradient khÃ´ng chá»‡ch.
	5.	Thiáº¿t káº¿ data loader áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n há»™i tá»¥.

Trong bá»‘i cáº£nh mÃ´ hÃ¬nh Transformer hiá»‡n Ä‘áº¡i, tá»‘i Æ°u hoÃ¡ data pipeline quan trá»ng khÃ´ng kÃ©m tá»‘i Æ°u kiáº¿n trÃºc.

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Ashish Vaswani et al. (2017). Attention Is All You Need.
	2.	OpenAI (2019). GPT-2 Technical Report.
	3.	Goodfellow et al. (2016). Deep Learning.
	4.	Bottou (2010). Large-Scale Machine Learning with SGD.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md](aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md) |
| [aero_LLM_02_Exploring GloVe pretrained embeddings.md](aero_LLM_02_Exploring GloVe pretrained embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Exploring GloVe pretrained embeddings.md) |
| [aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md](aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Tá»« Vá»±ng giá»¯a Wikipedia vÃ  Twitter báº±ng PhÃ¢n TÃ­ch TÆ°Æ¡ng Äá»“ng Biá»ƒu Diá»…n (RSA)](aero_LLM_04_CodeChallenge Wikipedia vs. Twitter embeddings (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Wikipedia vs. Twitter embeddings (part 2).md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Ngá»¯ NghÄ©a cá»§a GPT-2 vÃ  BERT thÃ´ng qua PhÃ¢n TÃ­ch Embedding](aero_LLM_05_Exploring GPT2 and BERT embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Exploring GPT2 and BERT embeddings.md) |
| [ToÃ¡n há»c cá»§a Token vÃ  Embedding trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n](aero_LLM_06_CodeChallenge Math with tokens and embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_CodeChallenge Math with tokens and embeddings.md) |
| [Cosine Similarity vÃ  Má»‘i Quan Há»‡ vá»›i Há»‡ Sá»‘ TÆ°Æ¡ng Quan: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong NLP](aero_LLM_07_Cosine similarity (and relation to correlation).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Cosine similarity (and relation to correlation).md) |
| [PhÃ¢n TÃ­ch Cosine Similarity trong KhÃ´ng Gian Embedding cá»§a GPT-2](aero_LLM_08_CodeChallenge GPT2 cosine similarities.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge GPT2 cosine similarities.md) |
| [Unembedding trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: Tá»« Vector áº¨n Äáº¿n Token](aero_LLM_09_CodeChallenge Unembeddings (vectors to tokens).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Unembeddings (vectors to tokens).md) |
| [Position Embeddings trong Transformer: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_LLM_10_Position embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_Position embeddings.md) |
| [PhÃ¢n TÃ­ch Thá»±c Nghiá»‡m Embedding Vá»‹ TrÃ­ Trong Transformer: Tá»« Cáº¥u TrÃºc Tuyáº¿n TÃ­nh Äáº¿n KhÃ´ng Gian HÃ¬nh Há»c](aero_LLM_11_CodeChallenge Exploring position embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Exploring position embeddings.md) |
| [Huáº¥n Luyá»‡n Embedding Tá»« Äáº§u: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, CÆ¡ Cháº¿ Tá»‘i Æ¯u vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_12_Training embeddings from scratch.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_Training embeddings from scratch.md) |
| ğŸ“Œ **[Thiáº¿t Káº¿ Data Loader Cho Huáº¥n Luyá»‡n MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Kiáº¿n TrÃºc vÃ  Tá»‘i Æ¯u HoÃ¡](aero_LLM_13_Create a data loader to train a model.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_13_Create a data loader to train a model.md) |
| [XÃ¢y Dá»±ng MÃ´ HÃ¬nh Há»c Embedding Tá»« Äáº§u: Kiáº¿n TrÃºc, Tá»‘i Æ¯u HoÃ¡ vÃ  PhÃ¢n TÃ­ch ToÃ¡n Há»c](aero_LLM_14_Build a model to learn the embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_Build a model to learn the embeddings.md) |
| [HÃ m Máº¥t MÃ¡t Trong Huáº¥n Luyá»‡n Embedding: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, PhÃ¢n TÃ­ch Gradient vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_15_Loss function to train the embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_Loss function to train the embeddings.md) |
| [Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh Há»c MÃ¡y: CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Thá»±c tiá»…n](aero_LLM_16_Train and evaluate the model.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_Train and evaluate the model.md) |
| [Sá»± Thay Äá»•i cá»§a Embeddings Trong QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_LLM_17_CodeChallenge How the embeddings change.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_CodeChallenge How the embeddings change.md) |
| [Äá»™ á»”n Äá»‹nh cá»§a Embeddings trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_LLM_18_CodeChallenge How stable are embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_CodeChallenge How stable are embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

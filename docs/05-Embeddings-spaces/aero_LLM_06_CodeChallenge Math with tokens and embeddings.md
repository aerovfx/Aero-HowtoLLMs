
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [05 Embeddings spaces](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# ToÃ¡n há»c cá»§a Token vÃ  Embedding trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n

## TÃ³m táº¯t

Trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡n Ä‘áº¡i nhÆ° [GPT-2](chatgpt://generic-entity?number=0) vÃ  [BERT](chatgpt://generic-entity?number=1), vÄƒn báº£n khÃ´ng Ä‘Æ°á»£c xá»­ lÃ½ trá»±c tiáº¿p dÆ°á»›i dáº¡ng chá»¯ mÃ  Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh **token** vÃ  sau Ä‘Ã³ Ã¡nh xáº¡ sang khÃ´ng gian vector thÃ´ng qua **embedding**. BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch ná»n táº£ng toÃ¡n há»c cá»§a quÃ¡ trÃ¬nh token hÃ³a, Ã¡nh xáº¡ embedding, cÅ©ng nhÆ° cÃ¡c phÃ©p toÃ¡n Ä‘áº¡i sá»‘ vector cho phÃ©p mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c cáº¥u trÃºc ngá»¯ nghÄ©a.

---

## 1. Tá»« vÄƒn báº£n Ä‘áº¿n token

Giáº£ sá»­ ta cÃ³ chuá»—i vÄƒn báº£n:

\[
\mathcal{T} = (w_1, w_2, ..., w_n)
\]

Bá»™ tokenizer thá»±c hiá»‡n Ã¡nh xáº¡:

\[
\tau: \mathcal{V}_{text} \rightarrow \mathcal{V}_{token}
\]

Trong Ä‘Ã³:

- \( \mathcal{V}_{text} \): táº­p tá»« tá»± nhiÃªn
- \( \mathcal{V}_{token} \): táº­p token rá»i ráº¡c

Káº¿t quáº£ lÃ  dÃ£y chá»‰ sá»‘:

\[
(t_1, t_2, ..., t_n), \quad t_i \in \{1,2,...,|V|\}
\]

---

## 2. One-hot Encoding

Má»—i token \( t_i \) Ä‘Æ°á»£c biá»ƒu diá»…n ban Ä‘áº§u dÆ°á»›i dáº¡ng vector one-hot:

\[
\mathbf{x}_i \in \mathbb{R}^{|V|}
\]

\[
x_{ij} =
\begin{cases}
1 & \text{náº¿u } j = t_i \\
0 & \text{ngÆ°á»£c láº¡i}
\end{cases}
\]

ÄÃ¢y lÃ  khÃ´ng gian ráº¥t cao chiá»u vÃ  khÃ´ng hiá»‡u quáº£ vá» máº·t tÃ­nh toÃ¡n.

---

## 3. Ma tráº­n Embedding

Ta Ä‘á»‹nh nghÄ©a ma tráº­n embedding:

\[
E \in \mathbb{R}^{|V| \times d}
\]

Trong Ä‘Ã³:

- \( |V| \): kÃ­ch thÆ°á»›c tá»« vá»±ng
- \( d \): sá»‘ chiá»u embedding

Vector embedding Ä‘Æ°á»£c tÃ­nh:

\[
\mathbf{v}_i = \mathbf{x}_i E
\]

Do \( \mathbf{x}_i \) lÃ  one-hot, nÃªn:

\[
\mathbf{v}_i = E_{t_i}
\]

Tá»©c lÃ  láº¥y hÃ ng thá»© \( t_i \) cá»§a ma tráº­n embedding.

---

## 4. Cá»™ng embedding vÃ  vá»‹ trÃ­ (Positional Encoding)

Trong Transformer, embedding cuá»‘i cÃ¹ng lÃ  tá»•ng cá»§a:

\[
\mathbf{z}_i = \mathbf{v}_i + \mathbf{p}_i
\]

Trong Ä‘Ã³ \( \mathbf{p}_i \) lÃ  positional encoding:

\[
PE_{(pos,2k)} = \sin\left(\frac{pos}{10000^{2k/d}}\right)
\]

\[
PE_{(pos,2k+1)} = \cos\left(\frac{pos}{10000^{2k/d}}\right)
\]

Äiá»u nÃ y giÃºp mÃ´ hÃ¬nh nháº­n biáº¿t thá»© tá»± chuá»—i.

---

## 5. Äáº¡i sá»‘ vector trong khÃ´ng gian embedding

### 5.1 Äá»™ tÆ°Æ¡ng Ä‘á»“ng Cosine

\[
\text{cosine}(\mathbf{v}_i,\mathbf{v}_j)
=
\frac{\mathbf{v}_i \cdot \mathbf{v}_j}
{\|\mathbf{v}_i\|\|\mathbf{v}_j\|}
\]

Pháº£n Ã¡nh má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a.

---

### 5.2 PhÃ©p cá»™ng ngá»¯ nghÄ©a

Trong nhiá»u mÃ´ hÃ¬nh, quan há»‡ tuyáº¿n tÃ­nh cÃ³ thá»ƒ xuáº¥t hiá»‡n:

\[
\mathbf{v}_{king} - \mathbf{v}_{man}
+ \mathbf{v}_{woman}
\approx
\mathbf{v}_{queen}
\]

Äiá»u nÃ y cho tháº¥y khÃ´ng gian embedding há»c Ä‘Æ°á»£c cáº¥u trÃºc ngá»¯ nghÄ©a tuyáº¿n tÃ­nh.

---

## 6. Tá»‘i Æ°u hÃ³a Embedding

Trong mÃ´ hÃ¬nh tá»± há»“i quy nhÆ° GPT-2, hÃ m máº¥t mÃ¡t lÃ :

\[
\mathcal{L}
=
- \sum_{t=1}^{T}
\log P(w_t | w_{<t})
\]

Vá»›i:

\[
P(w_t | w_{<t})
=
\text{softmax}(W_o h_t)
\]

\[
\text{softmax}(z_i)
=
\frac{e^{z_i}}
{\sum_{j=1}^{|V|} e^{z_j}}
\]

Gradient lan truyá»n ngÆ°á»£c Ä‘á»ƒ cáº­p nháº­t ma tráº­n embedding:

\[
E \leftarrow E - \eta \nabla_E \mathcal{L}
\]

Trong Ä‘Ã³ \( \eta \) lÃ  learning rate.

---

## 7. HÃ¬nh há»c cá»§a khÃ´ng gian embedding

Giáº£ sá»­:

\[
X \in \mathbb{R}^{n \times d}
\]

Ma tráº­n hiá»‡p phÆ°Æ¡ng sai:

\[
\Sigma = \frac{1}{n} X^T X
\]

Giáº£i bÃ i toÃ¡n trá»‹ riÃªng:

\[
\Sigma \mathbf{u} = \lambda \mathbf{u}
\]

CÃ¡c trá»‹ riÃªng lá»›n cho biáº¿t chiá»u chiáº¿m Æ°u tháº¿ cá»§a khÃ´ng gian ngá»¯ nghÄ©a.

---

## 8. Chuáº©n hÃ³a vÃ  á»•n Ä‘á»‹nh sá»‘ há»c

ThÆ°á»ng Ã¡p dá»¥ng chuáº©n hÃ³a:

\[
\hat{\mathbf{v}} =
\frac{\mathbf{v}}{\|\mathbf{v}\|}
\]

Äiá»u nÃ y lÃ m:

\[
\|\hat{\mathbf{v}}\| = 1
\]

GiÃºp tÄƒng á»•n Ä‘á»‹nh khi tÃ­nh attention vÃ  cosine similarity.

---

## 9. Tá»« token Ä‘áº¿n Attention

Self-attention tÃ­nh:

\[
Q = XW_Q
\]
\[
K = XW_K
\]
\[
V = XW_V
\]

\[
\text{Attention}(Q,K,V)
=
\text{softmax}\left(
\frac{QK^T}{\sqrt{d_k}}
\right)V
\]

Embedding ban Ä‘áº§u Ä‘Ã³ng vai trÃ² ná»n táº£ng cho toÃ n bá»™ phÃ©p biáº¿n Ä‘á»•i nÃ y.

---

## 10. Káº¿t luáº­n

QuÃ¡ trÃ¬nh tá»« vÄƒn báº£n Ä‘áº¿n embedding cÃ³ thá»ƒ tÃ³m táº¯t:

\[
\text{Text}
\rightarrow
\text{Token}
\rightarrow
\text{One-hot}
\rightarrow
\text{Embedding}
\rightarrow
\text{Attention}
\rightarrow
\text{Contextual Representation}
\]

Vá» máº·t toÃ¡n há»c:

- Token lÃ  biáº¿n rá»i ráº¡c.
- Embedding lÃ  Ã¡nh xáº¡ tuyáº¿n tÃ­nh sang khÃ´ng gian liÃªn tá»¥c.
- Attention lÃ  phÃ©p biáº¿n Ä‘á»•i phi tuyáº¿n phá»¥ thuá»™c ngá»¯ cáº£nh.
- ToÃ n bá»™ há»‡ thá»‘ng Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a thÃ´ng qua gradient descent.

Hiá»ƒu rÃµ cáº¥u trÃºc toÃ¡n há»c nÃ y giÃºp giáº£i thÃ­ch vÃ¬ sao cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n cÃ³ thá»ƒ há»c Ä‘Æ°á»£c cáº¥u trÃºc ngá»¯ nghÄ©a phá»©c táº¡p tá»« dá»¯ liá»‡u vÄƒn báº£n khá»•ng lá»“.

---

## TÃ i liá»‡u tham kháº£o

1. Vaswani et al. (2017). Attention is All You Need.  
2. Radford et al. (2019). Language Models are Unsupervised Multitask Learners.  
3. Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.  
4. Jurafsky & Martin (2023). Speech and Language Processing.

---
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md](aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Word2Vec vs. GloVe vs. GPT vs. BERT... oh my!.md) |
| [aero_LLM_02_Exploring GloVe pretrained embeddings.md](aero_LLM_02_Exploring GloVe pretrained embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Exploring GloVe pretrained embeddings.md) |
| [aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md](aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Wikipedia vs. Twitter embeddings (part 1).md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Tá»« Vá»±ng giá»¯a Wikipedia vÃ  Twitter báº±ng PhÃ¢n TÃ­ch TÆ°Æ¡ng Äá»“ng Biá»ƒu Diá»…n (RSA)](aero_LLM_04_CodeChallenge Wikipedia vs. Twitter embeddings (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Wikipedia vs. Twitter embeddings (part 2).md) |
| [So sÃ¡nh Biá»ƒu Diá»…n Ngá»¯ NghÄ©a cá»§a GPT-2 vÃ  BERT thÃ´ng qua PhÃ¢n TÃ­ch Embedding](aero_LLM_05_Exploring GPT2 and BERT embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Exploring GPT2 and BERT embeddings.md) |
| ğŸ“Œ **[ToÃ¡n há»c cá»§a Token vÃ  Embedding trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n](aero_LLM_06_CodeChallenge Math with tokens and embeddings.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_06_CodeChallenge Math with tokens and embeddings.md) |
| [Cosine Similarity vÃ  Má»‘i Quan Há»‡ vá»›i Há»‡ Sá»‘ TÆ°Æ¡ng Quan: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong NLP](aero_LLM_07_Cosine similarity (and relation to correlation).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Cosine similarity (and relation to correlation).md) |
| [PhÃ¢n TÃ­ch Cosine Similarity trong KhÃ´ng Gian Embedding cá»§a GPT-2](aero_LLM_08_CodeChallenge GPT2 cosine similarities.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge GPT2 cosine similarities.md) |
| [Unembedding trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: Tá»« Vector áº¨n Äáº¿n Token](aero_LLM_09_CodeChallenge Unembeddings (vectors to tokens).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Unembeddings (vectors to tokens).md) |
| [Position Embeddings trong Transformer: CÆ¡ Sá»Ÿ ToÃ¡n Há»c vÃ  á»¨ng Dá»¥ng trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_LLM_10_Position embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_Position embeddings.md) |
| [PhÃ¢n TÃ­ch Thá»±c Nghiá»‡m Embedding Vá»‹ TrÃ­ Trong Transformer: Tá»« Cáº¥u TrÃºc Tuyáº¿n TÃ­nh Äáº¿n KhÃ´ng Gian HÃ¬nh Há»c](aero_LLM_11_CodeChallenge Exploring position embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Exploring position embeddings.md) |
| [Huáº¥n Luyá»‡n Embedding Tá»« Äáº§u: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, CÆ¡ Cháº¿ Tá»‘i Æ¯u vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_12_Training embeddings from scratch.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_Training embeddings from scratch.md) |
| [Thiáº¿t Káº¿ Data Loader Cho Huáº¥n Luyá»‡n MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Kiáº¿n TrÃºc vÃ  Tá»‘i Æ¯u HoÃ¡](aero_LLM_13_Create a data loader to train a model.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_Create a data loader to train a model.md) |
| [XÃ¢y Dá»±ng MÃ´ HÃ¬nh Há»c Embedding Tá»« Äáº§u: Kiáº¿n TrÃºc, Tá»‘i Æ¯u HoÃ¡ vÃ  PhÃ¢n TÃ­ch ToÃ¡n Há»c](aero_LLM_14_Build a model to learn the embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_Build a model to learn the embeddings.md) |
| [HÃ m Máº¥t MÃ¡t Trong Huáº¥n Luyá»‡n Embedding: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, PhÃ¢n TÃ­ch Gradient vÃ  á»¨ng Dá»¥ng Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_15_Loss function to train the embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_Loss function to train the embeddings.md) |
| [Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh Há»c MÃ¡y: CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Thá»±c tiá»…n](aero_LLM_16_Train and evaluate the model.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_Train and evaluate the model.md) |
| [Sá»± Thay Äá»•i cá»§a Embeddings Trong QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_LLM_17_CodeChallenge How the embeddings change.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_CodeChallenge How the embeddings change.md) |
| [Äá»™ á»”n Äá»‹nh cá»§a Embeddings trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_LLM_18_CodeChallenge How stable are embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_CodeChallenge How stable are embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

Benchmark phi kỹ thuật (Non-Technical Benchmarks) trong đánh giá Mô hình Ngôn ngữ Lớn

Khung lý thuyết, phương pháp định lượng và công thức toán học minh hoạ

⸻

Tóm tắt

Bên cạnh các benchmark kỹ thuật như SuperGLUE hay MMLU, sự phát triển của mô hình ngôn ngữ lớn (LLMs) đòi hỏi những benchmark phi kỹ thuật (non-technical benchmarks) nhằm đánh giá các năng lực như: tính hữu ích (helpfulness), mức độ an toàn (safety), tính trung thực (truthfulness), khả năng tuân thủ chỉ dẫn (instruction following) và tính xã hội (social reasoning). Bài viết này trình bày khung lý thuyết, các phương pháp đánh giá định tính – định lượng, cùng các công thức toán học minh hoạ để lượng hóa các tiêu chí vốn mang tính chủ quan.

⸻

1. Giới thiệu

Các benchmark kỹ thuật đo khả năng:
	•	Suy luận logic
	•	Hoàn thành câu
	•	Hỏi đáp kiến thức

Tuy nhiên, trong triển khai thực tế, các tổ chức như:
	•	OpenAI
	•	Anthropic
	•	DeepMind

đã nhấn mạnh nhu cầu đánh giá:
	•	Tính an toàn nội dung
	•	Độ phù hợp văn hoá
	•	Tính trung thực
	•	Khả năng tương tác dài hạn

Những yếu tố này tạo thành nhóm non-technical benchmarks.

⸻

2. Phân loại benchmark phi kỹ thuật

2.1 Helpfulness (Tính hữu ích)

Đánh giá mức độ câu trả lời:
	•	Đầy đủ
	•	Chính xác
	•	Liên quan

⸻

2.2 Safety (An toàn)

Đo lường:
	•	Toxicity
	•	Khuyến khích hành vi nguy hiểm
	•	Nội dung nhạy cảm

⸻

2.3 Truthfulness (Tính trung thực)

Liên quan đến hallucination.

Giả sử:
	•	T là biến nhị phân (đúng/sai)

Ta có:

Truth\ Rate = \frac{\text{số câu trả lời đúng}}{\text{tổng số câu trả lời}}

⸻

2.4 Instruction Following

Đánh giá khả năng tuân thủ yêu cầu phức tạp:

Compliance = \frac{1}{N}\sum_{i=1}^{N} \mathbf{1}(response_i \models instruction_i)

⸻

3. Định lượng yếu tố chủ quan bằng mô hình xác suất

3.1 Human Preference Modeling

Giả sử có hai phản hồi r_1, r_2. Người đánh giá chọn r_1 với xác suất:

P(r_1 \succ r_2) = \sigma(R_\theta(r_1) - R_\theta(r_2))

Trong đó:
	•	R_\theta là hàm reward
	•	\sigma là sigmoid

\sigma(x) = \frac{1}{1+e^{-x}}

⸻

3.2 Loss cho reward model

\mathcal{L} = - \log \sigma(R_\theta(r_w) - R_\theta(r_l))

với:
	•	r_w: phản hồi được chọn
	•	r_l: phản hồi bị loại

⸻

4. Đo an toàn bằng xác suất điều kiện

Giả sử classifier phụ ước lượng:

P_{tox}(x)

Mức độc hại trung bình:

Toxicity = \mathbb{E}[P_{tox}(response)]

So sánh giữa các phiên bản mô hình:

\Delta_{tox} = Toxicity_{modelA} - Toxicity_{modelB}

⸻

5. Đánh giá Hallucination

Một thước đo phổ biến là FactScore.

Giả sử:
	•	C_i là claim thứ i
	•	V_i \in \{0,1\} là verified

FactScore = \frac{\sum_{i=1}^{K} V_i}{K}

⸻

6. So sánh bằng KL Divergence

Khi có phân phối đánh giá của người dùng:

P_{human}(score)

và phân phối dự đoán:

P_{model}(score)

Ta tính:

D_{KL}(P_{human} || P_{model})

⸻

7. Multi-Dimensional Evaluation

Giả sử có m tiêu chí:

S = (s_1, s_2, ..., s_m)

Điểm tổng hợp:

Score_{overall} = \sum_{i=1}^{m} w_i s_i

với:

\sum_{i=1}^{m} w_i = 1

⸻

8. Liên hệ với lý thuyết thông tin

Theo Elements of Information Theory:

Entropy phản ánh độ không chắc chắn:

H(X) = -\sum_x P(x)\log P(x)

Mô hình hallucinate nhiều → entropy cao nhưng không tương thích với dữ kiện thật.

⸻

9. Phân tích thống kê sự khác biệt mô hình

Kiểm định bootstrap:

CI_{95\%} = \bar{x} \pm 1.96 \frac{s}{\sqrt{n}}

Nếu khoảng tin cậy không chồng lấp → khác biệt có ý nghĩa.

⸻

10. Thách thức của benchmark phi kỹ thuật
	1.	Chủ quan cao
	2.	Phụ thuộc văn hoá
	3.	Thay đổi theo ngữ cảnh
	4.	Có thể bị gaming

Các tổ chức như Stanford University và MIT nhấn mạnh rằng không tồn tại metric duy nhất phản ánh toàn diện hành vi mô hình.

⸻

11. Kết luận

Benchmark phi kỹ thuật là bước tiến tất yếu trong đánh giá LLM, bổ sung cho benchmark kỹ thuật truyền thống. Việc lượng hóa các tiêu chí như hữu ích, an toàn và trung thực đòi hỏi:
	•	Mô hình xác suất
	•	Reward modeling
	•	Phân tích phân phối
	•	Kiểm định thống kê

Trong tương lai, đánh giá LLM sẽ là bài toán đa chiều, kết hợp:
	•	Hiệu năng kỹ thuật
	•	Công bằng
	•	An toàn
	•	Tính xã hội

⸻

Tài liệu tham khảo
	1.	Cover & Thomas. Elements of Information Theory.
	2.	Barocas et al. Fairness and Machine Learning.
	3.	Bai et al. (2022). Constitutional AI.
	4.	Ouyang et al. (2022). Training language models to follow instructions with human feedback.
	5.	OpenAI System Cards (các phiên bản gần đây).


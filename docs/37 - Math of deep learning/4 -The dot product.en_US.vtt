WEBVTT

00:02.320 --> 00:10.120
It is possible that the term dot product is new to you, but after having watched the previous video,

00:10.320 --> 00:15.000
the calculation of a dot product is not at all new to you.

00:15.040 --> 00:22.320
In fact, what I have been calling a linear weighted combination in the previous exercise is literally

00:22.320 --> 00:25.640
just exactly the same thing as the dot product.

00:26.000 --> 00:28.920
Why do we need all these different terms for the same thing?

00:29.200 --> 00:30.800
I don't know, it gets even weirder.

00:30.800 --> 00:34.280
In the next video, I'll start calling it matrix multiplication.

00:34.520 --> 00:35.640
That's just how it is.

00:36.240 --> 00:43.400
Okay, so let me start by showing you some notations, some math notations for the dot product.

00:43.720 --> 00:46.200
It is sometimes called the scalar product.

00:46.680 --> 00:51.360
Sometimes you will see it notated as a dot b.

00:51.360 --> 00:54.360
So this little dot in the middle.

00:54.960 --> 00:58.560
Sometimes you will see it indicated with angle brackets like this.

00:58.560 --> 00:59.920
And then a comma b.

01:00.440 --> 01:03.950
And in these cases a and b are vectors of numbers.

01:04.390 --> 01:11.670
Here is a common notation that you will see especially for in linear algebra contexts.

01:11.910 --> 01:19.750
That is, a which is one vector, might be the activations vector, and then a t in the superscript,

01:19.870 --> 01:22.390
and then b, which is another vector.

01:22.390 --> 01:26.110
And that might be the weights from going into the neuron.

01:26.430 --> 01:28.710
And then this is notation.

01:28.710 --> 01:31.230
But it's also a definition of the dot product.

01:31.230 --> 01:39.150
So it is the sum of element wise multiplications between all the individual elements in vector a and

01:39.150 --> 01:40.030
vector b.

01:41.110 --> 01:44.510
I will show you a concrete example in the next slide.

01:44.670 --> 01:52.510
But yeah, just to, uh, burn into your brain that, uh, anytime you see a notation like this, so

01:52.510 --> 01:58.150
a letter and then a t in the superscript and then another letter that is indicating the dot product,

01:58.390 --> 02:04.940
which you now know is nothing more than a fancy term for linear weighted Awaited combination.

02:05.580 --> 02:07.700
Okay, here is an example.

02:07.700 --> 02:10.900
Let's say we have two vectors v and w.

02:11.060 --> 02:14.060
Perhaps these are activations from one neuron.

02:14.060 --> 02:20.180
And these are the weights from those neurons going into a neuron okay.

02:20.220 --> 02:27.580
So the dot product it can be indicated as v t w the t stands for transpose.

02:27.580 --> 02:29.940
I'll talk more about that in the next video.

02:30.220 --> 02:32.340
And how do we actually calculate this.

02:33.180 --> 02:38.740
So we take corresponding elements in the two vectors and multiply them together.

02:38.860 --> 02:43.900
And then sum over all of the pairs of individual multiplications.

02:44.020 --> 02:54.940
So the dot product between vector v and vector w is one times two plus zero times eight plus two times

02:54.940 --> 02:56.700
minus six, and so on.

02:56.940 --> 03:01.140
And you run through all of those calculations and you get minus five.

03:01.140 --> 03:06.210
So the dot product between v and w is minus five.

03:06.810 --> 03:14.770
Now, as you might have guessed from this slide, the dot product is only defined between two vectors

03:14.770 --> 03:17.770
that have the same number of elements.

03:17.970 --> 03:22.010
So here both of these vectors have five numbers in them.

03:22.010 --> 03:23.450
So five elements.

03:24.210 --> 03:31.250
But if you know imagine a situation like this where w only has three elements.

03:31.490 --> 03:37.410
So then we get up here and the question is what do we multiply by five and by minus two.

03:37.850 --> 03:40.930
And the answer is I don't know nothing.

03:40.930 --> 03:44.890
So the dot product in this case is not defined.

03:44.930 --> 03:47.290
Now there's nothing wrong with these two vectors.

03:47.290 --> 03:54.730
But we cannot calculate the dot product between them because they do not have the same number of elements.

03:55.610 --> 03:59.850
So this is a basic linear algebra dot products.

03:59.850 --> 04:04.250
How does this relate to artificial neural networks and language modeling?

04:04.650 --> 04:11.600
So the idea is that we have a simple artificial neuron, and it calculates an operation which is the

04:11.760 --> 04:19.440
linear weighted combination of its inputs that are weighted according to some weights that are defined

04:19.440 --> 04:21.400
or learned through training.

04:21.800 --> 04:28.920
Now the question is, how do we actually express this mathematical operation that is calculated by this

04:28.920 --> 04:29.480
neuron?

04:29.880 --> 04:37.280
And the answer is that we express it as the dot product between the activations and the weights coming

04:37.280 --> 04:38.840
into this neuron.

04:39.280 --> 04:43.280
So we take all of these activations and we put that into one vector.

04:43.400 --> 04:44.760
We take all the weights.

04:44.760 --> 04:46.920
We put that into another vector.

04:47.080 --> 04:54.240
And then we say that the mathematical operation that this neuron is calculating is literally just the

04:54.280 --> 05:02.560
dot product between the activations and the weights, and that we can notate as x transpose w.

05:02.560 --> 05:05.560
Or maybe you want to call this vector A for activations.

05:05.680 --> 05:10.310
Sometimes people call this L minus one for the previous layer.

05:10.310 --> 05:12.710
But this is the basic idea.

05:13.350 --> 05:15.830
And what does that number actually mean?

05:15.830 --> 05:17.470
What does the dot product mean.

05:17.470 --> 05:19.510
How do we interpret a dot product?

05:20.190 --> 05:26.870
Well, the exact interpretation of the dot product depends on the nature of the application.

05:26.870 --> 05:29.630
It depends on the scale of the data.

05:29.790 --> 05:33.630
But in general the dot product is a single number.

05:33.630 --> 05:34.670
It's one number.

05:34.910 --> 05:40.550
It doesn't matter how many elements are in the vectors, because the dot product will always produce

05:40.550 --> 05:41.470
one number.

05:41.870 --> 05:47.550
So it is a single number that reflects something about the commonalities between the two.

05:47.950 --> 05:52.590
Objects that are representing the are represented by these vectors.

05:53.310 --> 05:55.950
The dot product is all over the place.

05:55.950 --> 06:03.670
It has myriad applications in many, many, many areas of applied math and abstract math.

06:04.230 --> 06:13.340
The the dot product is the computational backbone for uh, in statistics analyses like correlation PCA

06:13.660 --> 06:18.260
in signal processing, the Fourier transform and filtering, all based on dot products.

06:18.860 --> 06:19.500
Geometry.

06:19.540 --> 06:20.140
Physics.

06:20.140 --> 06:20.980
Mechanics.

06:21.460 --> 06:28.100
In linear algebra, projections, transformations, multiplications of matrices and vectors, all based

06:28.100 --> 06:29.220
on the dot product.

06:29.540 --> 06:36.460
And here in this deep learning course, you will see the dot product appearing every time we talk about

06:36.660 --> 06:38.500
a forward pass through the model.

06:38.780 --> 06:41.340
Convolution matrix multiplication.

06:41.500 --> 06:47.300
A gram matrix, which is a way of extracting patterns from a weights matrix.

06:47.460 --> 06:48.860
Cosine similarity.

06:48.860 --> 06:55.660
This is a really important metric of similarity between embeddings and between vector representations

06:55.940 --> 06:59.260
of language tokens inside the model.

06:59.500 --> 07:06.780
Correlations the numerical representations of of letters and words and subwords.

07:06.980 --> 07:13.860
Everything is based on the dot product and as you know from the previous video, the dot product is

07:13.860 --> 07:19.140
literally just the weighted sum of two sets of numbers.

07:19.780 --> 07:26.540
Okay, so now we are going to switch to Python and basically we are going to calculate a dot product

07:27.580 --> 07:28.060
manually.

07:28.060 --> 07:34.300
I put in quotes here because in my mind a manual calculation would mean like writing it out on a piece

07:34.300 --> 07:35.500
of paper with a pen.

07:36.820 --> 07:42.060
But we are basically going to calculate the dot product using several different methods.

07:42.260 --> 07:44.660
In NumPy and in PyTorch.

07:44.780 --> 07:52.780
And you will see that all of these internal toolbox provided methods are giving exactly the same results

07:52.820 --> 07:56.420
as our manually calculated dot product.

07:56.980 --> 08:02.900
You're also going to see that PyTorch is a little bit more sensitive to the data types.

08:02.900 --> 08:10.380
So PyTorch really cares a lot about mixing ints and floats, for example, in ways that numpy is more

08:10.380 --> 08:11.580
robust to.

08:12.140 --> 08:14.730
Okay, so now we will switch to Python.

08:15.970 --> 08:22.650
So we will be using numpy and PyTorch in this exercise in this code file.

08:22.930 --> 08:27.250
So first of all I'm going to manually calculate the dot product.

08:27.250 --> 08:35.410
So not using any special functions for the dot product between this vector of numbers and this vector

08:35.410 --> 08:36.610
of weights.

08:36.650 --> 08:39.170
Now notice that these are floating point numbers.

08:39.170 --> 08:40.970
They all have decimal points in them.

08:41.330 --> 08:43.530
And these are all integers.

08:43.530 --> 08:44.850
There are no decimal points.

08:44.850 --> 08:47.210
So these are ints and these are floats.

08:47.370 --> 08:56.130
And let's see if we run into any Python errors because we are mixing different types of, of data okay.

08:56.170 --> 08:59.930
So uh yeah I'm going to calculate this manually.

08:59.930 --> 09:04.770
And here is the numpy function for calculating the dot product.

09:04.930 --> 09:06.890
It is numpy dot dot.

09:07.290 --> 09:11.370
Now in truth this doesn't really calculate the dot product.

09:11.370 --> 09:19.160
It actually calculates matrix multiplication in my humble opinion, this is a confusingly named function,

09:19.480 --> 09:25.080
but you can implement matrix multiplications through the dot product.

09:25.200 --> 09:27.760
So as you will learn in the next video.

09:27.760 --> 09:29.800
So it's it's a little bit confusing.

09:29.800 --> 09:32.160
But anyway the function is called numpy dot dot.

09:33.480 --> 09:33.680
Okay.

09:33.720 --> 09:35.280
And here you see that.

09:35.400 --> 09:39.920
Yeah we get the same results when we do our manual calculation.

09:39.920 --> 09:43.440
You seen this kind of code now several times in the past videos.

09:43.680 --> 09:46.000
And also using numpy dot.

09:46.440 --> 09:52.280
What I want to do here is show you the inner workings of the numpy dot dot function.

09:52.520 --> 09:57.000
So we can write the name of the function and then question marks at the end.

09:57.320 --> 10:04.640
That in Colab at least, is going to open up this separate window off to the right, which tells us,

10:04.760 --> 10:12.360
yeah, it gives us information about using the function, shows a few examples to help us get started.

10:12.360 --> 10:19.110
And in some cases we'll also show the actual source code, but not for all functions, because some

10:19.110 --> 10:23.230
functions are compiled for speed and simplicity.

10:23.550 --> 10:29.190
Okay, so this is something you can look over, but just something I wanted to remind you about in Python

10:29.190 --> 10:34.590
in general to check out the, uh, the the help from these files.

10:35.030 --> 10:35.390
Okay.

10:35.430 --> 10:38.510
So now I'm going to repeat this in PyTorch.

10:38.550 --> 10:46.390
Now the thing is, when we are working in PyTorch, we are not going to be using numpy array data type

10:46.390 --> 10:48.190
because it's a different library.

10:48.190 --> 10:53.550
So here I'm creating a tensor in PyTorch using Torch.tensor.

10:53.870 --> 10:55.790
I'm inputting a list.

10:55.790 --> 10:59.990
But this list will get converted to a PyTorch tensor.

11:00.150 --> 11:02.350
Same with this list over here.

11:02.590 --> 11:07.310
And you can see these are the same numbers that I had up here in numpy.

11:07.710 --> 11:13.110
So of course we expect that PyTorch will give us exactly the same results.

11:13.470 --> 11:13.750
Okay.

11:13.790 --> 11:17.150
And then you know this is very similar to what I showed above.

11:17.190 --> 11:21.060
But here I'm using some instead of numpy some.

11:21.460 --> 11:27.220
And here I'm using torch instead of of numpy dot dot.

11:27.420 --> 11:29.620
Okay so run this code.

11:30.460 --> 11:36.940
And now we get an error and the error is expected both vectors to have the same dtype.

11:36.940 --> 11:41.220
That stands for data type but found long and float.

11:41.460 --> 11:43.100
So what does this tell us.

11:43.260 --> 11:46.460
This tells us also you can see which line actually crashed.

11:46.460 --> 11:47.860
It was this line here.

11:48.100 --> 11:49.740
This line ran fine.

11:49.740 --> 11:54.140
So we could actually multiply these two terms element wise.

11:54.140 --> 11:58.580
I will even comment this out and let's do this.

11:59.100 --> 12:06.140
So here we see this function actually worked even though we have ints and floats over here.

12:06.580 --> 12:09.020
This multiplication was no problem.

12:09.220 --> 12:11.820
And torch some was also no problem.

12:11.820 --> 12:14.260
And the result was a floating point number.

12:14.660 --> 12:18.380
But torch dot dot is not as flexible.

12:18.700 --> 12:22.450
And it was complaining about a difference in data type.

12:22.770 --> 12:28.210
Now torch calls ints long so an integer.

12:28.490 --> 12:32.690
What we would call an integer in numpy is called an integer in PyTorch.

12:32.730 --> 12:34.970
It is called long torch dot long.

12:36.090 --> 12:36.370
Okay.

12:36.410 --> 12:44.970
So what we can do here to get this to work is specify an optional additional input in Torch.tensor which

12:44.970 --> 12:48.130
is dtype equals torch dot float.

12:48.570 --> 12:53.290
So now this and now I can show you this uh activations.

12:53.730 --> 12:56.330
We can see this variable now has periods.

12:56.330 --> 12:58.090
So here there are no periods.

12:58.090 --> 12:59.570
Here there are periods.

12:59.570 --> 13:03.210
So the period tells you that we're working with a floating point number.

13:03.570 --> 13:06.010
I could do a long like this.

13:06.370 --> 13:08.050
We get that error but that's fine.

13:08.210 --> 13:14.530
I just wanted to show you that this value or this vector activation is integers.

13:14.530 --> 13:16.810
There are no more decimal points over here.

13:17.330 --> 13:17.610
Okay.

13:17.650 --> 13:23.280
But nonetheless, when we make this torch dot float, then torch dot dot.

13:23.320 --> 13:24.280
Actually works.

13:24.280 --> 13:26.760
And then that gives us no surprise.

13:26.760 --> 13:29.240
The same result of 1.6.

13:30.280 --> 13:35.440
So you saw in this video that there are several notations for the dot product.

13:35.560 --> 13:42.360
There are reasons in mathematics for having multiple notations for the same operation.

13:42.360 --> 13:45.840
But that is not something that I'm going to get into here.

13:46.520 --> 13:52.280
But importantly, the dot product is just a fancy term for linear weighted combination.

13:52.760 --> 13:59.880
You also saw that it's such an important operation that libraries like NumPy and PyTorch have built

13:59.880 --> 14:03.200
in functions for calculating the dot product.

14:03.440 --> 14:10.880
And also this is something you will encounter all throughout your adventures using PyTorch, that PyTorch

14:10.880 --> 14:18.360
can be more sensitive to the data types than numpy, and you will get lots and lots of coding errors

14:18.360 --> 14:21.760
that are attributable to mixing data types.

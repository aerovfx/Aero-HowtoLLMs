WEBVTT

00:00.960 --> 00:06.160
In this video you will learn about linear weighted combinations.

00:06.360 --> 00:14.720
This is a foundational computational operation that underlies basically all of deep learning and lots

00:14.720 --> 00:19.280
of other areas in applied and theoretical mathematics.

00:19.800 --> 00:24.920
In the next video, I will start calling these the dot product and then the video.

00:24.920 --> 00:29.240
Thereafter we will start calling these matrix multiplication.

00:29.360 --> 00:30.840
So lots of fancy terms.

00:30.840 --> 00:35.840
But it's all the same thing which is a linear weighted combination.

00:36.280 --> 00:38.040
Okay, so let's begin.

00:38.400 --> 00:43.680
The idea is that we will conceptualize a an artificial neuron.

00:43.680 --> 00:51.320
So a node in a deep learning network as a weighted combination of its inputs.

00:51.400 --> 00:53.400
So we have several inputs here.

00:53.400 --> 00:58.880
These correspond to numbers depending on the type of neural network you're dealing with and the type

00:58.880 --> 01:01.000
of application you're dealing with.

01:01.000 --> 01:06.740
These might be data values, they might be pixel intensity values and an image.

01:07.060 --> 01:13.700
Of course, in the context of this course, these are going to be numbers that correspond to eventually

01:13.700 --> 01:17.980
that derived from numbers that we get from language from text.

01:19.180 --> 01:19.460
Okay.

01:19.500 --> 01:26.500
So all of these come together, these various numerical inputs come together and then we sum them together.

01:26.500 --> 01:29.300
And that is the output of the neuron.

01:29.580 --> 01:32.860
Some more complexities in here that I will get to later on.

01:33.260 --> 01:36.220
But anyway it's mostly that simple.

01:36.220 --> 01:37.260
Here's an example.

01:37.580 --> 01:43.140
Imagine the numerical values of these inputs are eight and five and minus four.

01:43.180 --> 01:46.340
These are just arbitrary numbers that I came up with.

01:46.540 --> 01:53.340
These would represent the activations of other neurons that are feeding into this neuron.

01:53.540 --> 01:55.060
And what does this neuron do.

01:55.180 --> 01:59.260
Well it just adds up these three inputs and it divides by three.

01:59.300 --> 02:00.980
And why is it dividing by three.

02:01.180 --> 02:02.500
It's to take an average.

02:02.500 --> 02:09.850
And that's basically to normalize the numbers so that when we add more neurons on here, when the network

02:09.850 --> 02:14.130
gets bigger, this output is not arbitrarily getting larger.

02:14.530 --> 02:14.810
Okay.

02:14.850 --> 02:17.330
And so what is the average of these three numbers.

02:17.370 --> 02:20.570
You know the sum of these three numbers divided by three.

02:20.890 --> 02:23.330
Well that happens to also be the number three.

02:24.250 --> 02:24.650
Okay.

02:25.010 --> 02:28.370
Here is another way of conceptualizing that operation.

02:28.530 --> 02:34.610
So we take each of these input activation values and we give them a weight.

02:34.610 --> 02:36.890
That's what this W stands for weight.

02:37.090 --> 02:40.130
So weight of this is one third.

02:40.130 --> 02:41.730
The weight of this is one third.

02:41.730 --> 02:43.770
And the weight of this is one third.

02:43.890 --> 02:47.490
And then we can sum up the activations.

02:47.610 --> 02:52.730
So the numerical values of the activations each scaled by their own weights.

02:52.730 --> 02:55.250
And that's what this is depicting over here.

02:55.330 --> 03:04.050
So eight times one third that is this activation value times this weight value plus five times a third.

03:04.190 --> 03:08.070
That's for this one plus minus four times a third.

03:08.070 --> 03:10.390
And altogether that gives us three.

03:10.910 --> 03:17.910
So when the weights are all equal to each other and those are equal to one divided by n, where n is

03:17.910 --> 03:21.950
the number of inputs, then that's literally just an average.

03:21.950 --> 03:23.590
It's the arithmetic mean.

03:24.070 --> 03:30.470
But we don't have to set the weights to be one third or one divided by n, we can set them to be some

03:30.470 --> 03:31.230
other numbers.

03:31.230 --> 03:36.910
So let's have this be 0.5, this one be 0.3 and this one be 0.2.

03:37.310 --> 03:42.670
Now in this case, the way that I have chosen these numbers, they all sum to one.

03:42.670 --> 03:44.550
So all the weights sum to one.

03:44.870 --> 03:51.150
That can be useful in certain circumstances if you really want to calculate a weighted average.

03:51.550 --> 03:53.070
But it's not strictly necessary.

03:53.070 --> 03:56.430
There's no constraint that these weights need to sum to one.

03:56.430 --> 04:00.310
In fact you will also see that these weights can be negative.

04:00.670 --> 04:04.750
Now why do we get a different number here from the previous slide?

04:04.750 --> 04:07.210
In the previous slide, the answer was three.

04:07.490 --> 04:09.970
And here the answer is 4.7.

04:10.290 --> 04:15.250
So what's happening is that this activation here contributes more.

04:15.570 --> 04:19.850
It's more important than these other two activations.

04:20.010 --> 04:22.810
And it's more important because its weight is higher.

04:23.890 --> 04:29.330
Likewise, you can imagine this weighted sum being a negative number.

04:29.530 --> 04:33.610
If this weight were really large and these two weights were very small.

04:34.010 --> 04:41.530
So the importance of each activation for calculating the weighted linear combination or the weighted

04:41.530 --> 04:44.130
sum is given by the weights.

04:44.130 --> 04:46.810
That's an important concept in deep learning.

04:47.530 --> 04:54.330
Now I'm going to tell you about a special type of input that gets included in neural networks.

04:54.330 --> 04:56.290
And that is called the bias.

04:56.410 --> 04:59.730
So here I have the letter B that stands for bias.

04:59.930 --> 05:04.090
The bias is an additional input that comes into the neuron.

05:04.130 --> 05:09.120
Now mathematically there's nothing really special or unique about this.

05:09.240 --> 05:16.120
The main difference about the bias compared to the other terms is that these all come from data.

05:16.280 --> 05:19.720
So from text that has been converted into numbers.

05:20.080 --> 05:24.440
And the bias does actually doesn't come from the data set.

05:24.440 --> 05:26.000
It doesn't come from the real world.

05:26.000 --> 05:32.960
It's something that is just generated internally and added on to this neurons activation.

05:33.160 --> 05:35.080
And what is the point of this bias?

05:35.080 --> 05:40.320
The purpose of adding a bias is to shift the distributions.

05:40.440 --> 05:46.360
So this allows the neuron to shift this value either to the right or to the left.

05:46.360 --> 05:49.360
So smaller values or larger values.

05:49.800 --> 05:54.840
And that you see here this result was you saw in the previous slide it was 4.7.

05:55.120 --> 05:58.120
And now I've included a bias of minus three.

05:58.360 --> 06:04.080
And that takes the impact of all of this linear weighted combination and shifts it.

06:04.080 --> 06:08.220
In this case it's shifting it to the left by three units.

06:08.700 --> 06:09.060
Okay.

06:09.140 --> 06:13.700
So in Python we are going to explore these concepts.

06:13.820 --> 06:20.020
We will start by calculating a linear weighted combination between activations and weights.

06:20.020 --> 06:26.300
We're going to generate a table that looks like this where yeah we will just build up this linear weighted

06:26.300 --> 06:28.500
combination step by step.

06:28.500 --> 06:31.660
So individual activation by individual weight.

06:32.660 --> 06:39.260
And then that's going to give us a final number which is a linear weighted combination between this

06:39.260 --> 06:43.260
vector of activations and this vector of weights.

06:43.780 --> 06:49.820
Now when you look at these activations you can see that they are not perfectly balanced around zero.

06:50.300 --> 06:56.580
Likewise these weights are not balanced around zero, nor do they sum to one in this case.

06:56.620 --> 07:02.740
Actually, for both of these cases, the average of the activation and the average of the weights is

07:02.740 --> 07:05.220
positive number it's greater than zero.

07:05.700 --> 07:13.040
So we are going to explore whether that introduces any kind of systematic error or bias.

07:13.040 --> 07:18.760
And if it did, then that might need to be something that we would have to consider and specifically

07:18.760 --> 07:20.120
normalize out.

07:20.800 --> 07:27.000
Um, and we are going to do that partly by looking at distributions of linear weighted combinations

07:27.000 --> 07:28.440
using histograms.

07:28.440 --> 07:36.440
So we can see what distribution of lots and lots and lots of linear weighted combinations looks like.

07:36.880 --> 07:43.480
Then I will actually show you how to include a bias term, like what I just showed in the slides a moment

07:43.480 --> 07:43.960
ago.

07:44.320 --> 07:51.400
It turns out that there are correct ways and incorrect ways to include that bias term, and you will

07:51.400 --> 07:54.560
see that that has a real impact on the distribution.

07:55.480 --> 07:58.320
So here you see a distribution.

07:58.320 --> 08:05.640
It's generated by a histogram of lots and lots, I think 10,000 in total linear weighted combinations

08:05.640 --> 08:09.720
of random randomly generated activations and weights.

08:10.150 --> 08:14.110
And then what I tried to do here was include a bias term.

08:14.270 --> 08:21.190
But here the bias term was calculated incorrectly, so it actually did not shift the distribution.

08:21.470 --> 08:24.990
And over here I calculated the bias term correctly.

08:25.110 --> 08:27.990
And now you can see that the distribution is shifted.

08:27.990 --> 08:30.790
So here the bias term was three.

08:30.790 --> 08:35.230
And so the average of this distribution is roughly three plus.

08:35.270 --> 08:37.350
You know a little bit of just random error.

08:37.630 --> 08:40.630
And here I still added a bias term of three.

08:40.830 --> 08:43.110
But I didn't add it correctly.

08:43.230 --> 08:45.910
And so we didn't actually shift the distribution.

08:46.230 --> 08:46.590
Okay.

08:46.630 --> 08:52.550
So now we will switch to Python and explore linear weighted combinations.

08:54.510 --> 09:00.350
So here we are with the code file part nine prereqs Lin weighted combo.

09:00.390 --> 09:04.070
This is always going to be on the top left of the recording.

09:04.070 --> 09:07.950
So you can always check to make sure you are working with the correct code.

09:08.350 --> 09:08.710
Okay.

09:08.750 --> 09:12.170
So let's import some standard libraries that we will need.

09:12.570 --> 09:19.010
Here I'm simulating the activations that are coming in from the previous layer in the network, and

09:19.010 --> 09:21.370
I just came up with some random numbers.

09:21.810 --> 09:25.210
And here we have the weights into the current neuron.

09:25.370 --> 09:26.210
And yeah.

09:26.210 --> 09:29.450
So these would be the weights coming into the neuron.

09:29.450 --> 09:32.010
These are the activations coming into the neuron.

09:32.250 --> 09:35.370
And now we can calculate the linear weighted combination.

09:35.370 --> 09:40.010
This is the computation that the neuron is actually calculating.

09:40.250 --> 09:45.570
And it's there's there's going to be another activation term that we add.

09:45.610 --> 09:52.050
But this is essentially the output that the neuron will pass along to the next layer in the network.

09:52.330 --> 09:55.170
And how do we calculate the linear weighted combination.

09:55.450 --> 09:57.370
Well we initialize it to zero.

09:57.370 --> 10:04.530
And then we have a for loop that goes over all of these activation values and all the weights.

10:04.810 --> 10:11.810
And we simply update the linear combination according to the activation scaled by the weights.

10:12.150 --> 10:12.630
Okay.

10:12.830 --> 10:16.430
And then it turns out that that value is -1.7.

10:17.590 --> 10:17.950
Okay.

10:17.990 --> 10:26.630
Here I'm making a table so you can see that inside this for loop I'm looping again over all of the activations

10:26.630 --> 10:27.750
and weight values.

10:27.910 --> 10:34.870
And then I'm calculating or displaying the activation times the weight and then their product here.

10:35.070 --> 10:42.790
So that is building up the linear weighted combination piece by piece by element wise multiplication.

10:42.910 --> 10:46.150
And then we sum up all of these values over here.

10:46.310 --> 10:48.950
And that gives us -1.7.

10:49.470 --> 10:55.870
It's also interesting to see that for example this activation is has a value of two.

10:56.110 --> 10:58.350
But it is completely ignored.

10:58.350 --> 11:06.110
It has literally no impact on the downstream neuron because its weight is set to zero.

11:06.110 --> 11:07.590
It is completely ignored.

11:08.190 --> 11:13.260
Likewise, here we have a connection where the weight is actually really strong.

11:13.620 --> 11:18.900
It's, you know, three times stronger than the next strongest weight in this little network.

11:18.900 --> 11:21.500
But the activation happens to be zero.

11:21.500 --> 11:26.020
So again, it doesn't matter how strong the weight is, the activation is zero.

11:26.020 --> 11:33.500
It makes no contribution to the computation that is taking place inside this neuron okay.

11:33.860 --> 11:38.860
So this for loop is I hope that makes it clear what the calculation is.

11:38.860 --> 11:43.460
But it's an awful lot of code and we don't actually need to write a for loop.

11:43.460 --> 11:48.340
We can just implement element wise multiplication and then sum.

11:48.340 --> 11:53.140
This is exactly the calculation of a linear weighted sum.

11:53.860 --> 11:54.260
Okay.

11:54.420 --> 12:00.260
Now I want to discuss whether there is a systematic bias in the results.

12:00.260 --> 12:07.540
Because the weights and the activations vectors are they're not zero, they don't average to zero,

12:07.660 --> 12:09.700
nor do they sum to one.

12:10.260 --> 12:14.840
So to determine whether this is introducing any kind of a bias.

12:14.840 --> 12:22.480
I'm going to run an experiment where I will calculate linear weighted combinations 10,000 times, and

12:22.640 --> 12:29.200
each time randomly generating either new activations while keeping the weights fixed, or generating

12:29.200 --> 12:32.000
random weights while keeping the activations fixed.

12:32.440 --> 12:33.760
So that's what you see here.

12:33.760 --> 12:39.760
I have a for loop over 10,000, and here you see two little snippets of code.

12:39.800 --> 12:45.880
In this first one I am so calculating the linear weighted combination here.

12:45.880 --> 12:51.520
These are the original weights that I defined at the top of the script, but I'm multiplying them by

12:51.520 --> 12:53.320
random activations.

12:53.320 --> 13:00.680
So the activations are randomly, uh, newly generated at each instance in this for loop.

13:00.680 --> 13:02.240
But the weights are always the same.

13:02.520 --> 13:06.440
And here in this piece of code I am basically swapping that.

13:06.440 --> 13:10.880
So the activations are exactly what we defined in the beginning.

13:11.760 --> 13:15.940
And the weights are randomly defined with each iteration.

13:16.260 --> 13:20.980
Okay, so collect all of those into this two dimensional array here.

13:21.220 --> 13:23.220
And then I'm making a histogram.

13:23.380 --> 13:25.580
Now what do we see in this histogram.

13:25.700 --> 13:31.220
First of all this is a distribution of the linear weighted combination values.

13:31.420 --> 13:36.900
So up here with the original data we got a value of -1.7.

13:37.380 --> 13:40.700
So that would be somewhere around here okay.

13:40.740 --> 13:44.940
But then yeah we're repeating this many many times with random numbers.

13:44.940 --> 13:47.300
So that gives us a distribution.

13:47.580 --> 13:55.220
Now what we want to look for here is whether there is a systematic bias introduced in this distribution,

13:55.580 --> 14:01.140
because the weights have a mean offset of what was the mean offset of the weights.

14:01.380 --> 14:04.460
The average of the weights was 0.24 okay.

14:04.500 --> 14:12.900
So the question is do we see this mean offset that's present in the weights reflected in the distribution

14:13.140 --> 14:15.860
of the neurons output activities.

14:16.730 --> 14:18.650
And the answer is a resounding no.

14:19.250 --> 14:19.570
Okay.

14:19.610 --> 14:23.810
Here the average activation was 0.2.

14:24.090 --> 14:30.810
Now, maybe you think that this actually is introducing a systematic bias, but this offset here is

14:30.810 --> 14:32.650
just it's really just random.

14:32.650 --> 14:35.410
If I run this code again now, it gets smaller.

14:35.810 --> 14:38.690
Sometimes we might see a negative number in here.

14:38.810 --> 14:45.570
This is really just zero plus a little bit of you know smudge each time I run this just because we're

14:45.570 --> 14:47.530
generating random numbers here.

14:48.010 --> 14:48.290
Okay.

14:48.330 --> 14:56.410
So the conclusion here is that even if there is a a mean offset in the weights or in the activations,

14:56.570 --> 15:03.130
that does not necessarily introduce any kind of bias in the results.

15:03.410 --> 15:06.450
So what if we actually want to shift this distribution.

15:06.450 --> 15:14.250
Maybe we want all of the neurons in a particular layer to have a positive value, or even just some

15:14.250 --> 15:21.190
of the neurons in a particular layer to be biased towards giving a positive value or a negative value,

15:21.190 --> 15:22.910
or anything other than zero.

15:24.150 --> 15:31.630
That can be accomplished using the the bias term or yeah, you can also call it an offset.

15:31.950 --> 15:35.310
So here I set the bias term to be equal to three.

15:35.750 --> 15:42.470
And now in this term I am repeating a very similar experiment to what we were just discussing.

15:42.950 --> 15:44.070
But here.

15:44.510 --> 15:47.150
So here I'm generating random activations.

15:47.150 --> 15:48.790
We're going to keep the weights fixed.

15:48.950 --> 15:52.510
And now the main point here is to add the bias term.

15:52.870 --> 15:53.190
Okay.

15:53.230 --> 15:55.710
So how do we include this bias term.

15:55.910 --> 16:05.070
What I do in this calculation over here is multiply the activations by the weights.

16:05.110 --> 16:06.790
So linear weighted sum.

16:07.070 --> 16:13.270
But before calculating the weighted sum I'm adding the bias term to the weights.

16:13.430 --> 16:17.070
So you can see it is the weights plus the bias.

16:17.350 --> 16:24.290
And then the So I'm shifting all the weights and then I'm multiplying by the activations, and then

16:24.290 --> 16:28.410
I'm summing all of those to give me a linear weighted combination.

16:28.610 --> 16:29.770
So that's option one.

16:29.810 --> 16:32.090
Turns out that this is not going to work.

16:32.650 --> 16:38.690
And then well it will work in the sense that it doesn't crash, but it doesn't actually shift the distribution

16:38.690 --> 16:39.850
the way we want it to.

16:41.090 --> 16:48.570
And then here what I'm doing is first calculating the linear weighted combination of all the activations

16:48.570 --> 16:51.130
and all the weights without any bias.

16:51.130 --> 16:58.810
And then after that calculation is complete, then I am adding the bias or the offset term.

16:59.090 --> 16:59.370
Okay.

16:59.410 --> 17:01.690
So let's see what we get over here.

17:02.010 --> 17:05.290
Well you already saw this image in the slides.

17:05.450 --> 17:07.690
So you already have an idea.

17:07.690 --> 17:11.810
So this bias term was incorrectly calculated.

17:11.810 --> 17:13.970
It did not shift the distribution.

17:14.290 --> 17:17.930
Again even if you think this is shifting the distribution a little bit.

17:18.290 --> 17:23.120
First of all this is two orders of magnitude smaller than the bias term itself.

17:23.640 --> 17:25.760
And also this is non-systematic.

17:25.760 --> 17:29.880
Here we get, if anything, a tiny negative shift.

17:30.960 --> 17:35.640
But here the entire distribution was shifted by approximately three.

17:35.720 --> 17:39.480
Again, it's not exact just because we are generating random numbers.

17:40.960 --> 17:45.720
So in this video you learned about linear weighted combinations.

17:45.760 --> 17:51.560
It's a super duper simple mathematical computation, but it is really foundational.

17:51.560 --> 17:57.520
It's ubiquitous certainly in deep learning and in lots of areas of applied mathematics.

17:57.880 --> 18:05.760
We discussed biasing the results either by shifting the distribution of the weights or of the activations,

18:05.760 --> 18:08.520
which doesn't actually really impose a bias.

18:09.120 --> 18:17.840
But we can introduce a bias by adding a specific term that gets summed onto the result after the linear

18:17.840 --> 18:20.120
weighted combination is calculated.

WEBVTT

00:02.000 --> 00:09.360
You may have heard that deep learning requires a very large number of samples in order to train the

00:09.360 --> 00:13.920
models, so we need to have a lot of data to fit these models.

00:14.160 --> 00:15.520
And why is that the case?

00:15.520 --> 00:20.880
Why do we need so many examples, so much data to fit these deep learning models?

00:20.880 --> 00:25.840
Well, the answer comes from random sampling and sampling variability.

00:25.840 --> 00:30.080
And you are going to learn in this video what this concept means.

00:31.200 --> 00:34.640
So to start with, let's think about a scientific question.

00:34.920 --> 00:37.360
How tall is the average Montenegrin?

00:37.400 --> 00:41.640
A Montenegrin is a person who lives in the country Montenegro.

00:42.160 --> 00:46.840
Now for this purpose, for you know, for this video, we don't actually care about the answer.

00:46.840 --> 00:49.040
It doesn't matter how tall they actually are.

00:49.080 --> 00:51.280
Although I will give you the answer in a few slides.

00:51.280 --> 00:53.960
The question is, how do we know the answer?

00:53.960 --> 00:58.920
How do we find out the average height of a Montenegrin?

00:59.320 --> 01:02.880
By the way, here is the country of Montenegro.

01:03.000 --> 01:09.040
I've never been there, but from pictures on Google it seems to be a beautiful, beautiful country.

01:09.520 --> 01:12.640
Anyway, uh, so how tall is the average Montenegrin?

01:12.680 --> 01:16.720
Well, the way we can find this out is by going to Montenegro.

01:16.960 --> 01:22.840
And after a couple weeks of holiday and hiking and, you know, relaxing on the beach, then we get

01:22.840 --> 01:28.600
down to science, we go around, we just, you know, we find a big crowd of Montenegrins, and we just

01:28.600 --> 01:34.160
pick one person at random, like this guy, and then we take his height.

01:34.160 --> 01:41.120
We, you know, we bring a ruler with us, and we measure that guy and we find out that he has 160cm

01:41.120 --> 01:41.520
tall.

01:41.960 --> 01:42.200
Okay.

01:42.240 --> 01:44.040
But that is just one person.

01:44.320 --> 01:48.240
We pick another random person and that person is taller.

01:48.240 --> 01:50.680
That person is 192cm tall.

01:50.680 --> 01:51.440
It's pretty tall.

01:52.080 --> 01:58.880
But then, you know, we look up on Google, and the internet says that the average Montenegrin is or

01:58.880 --> 02:04.710
the average height of Montenegrins is Two centimeters tall.

02:04.990 --> 02:11.830
Now, if this is the average, why is it that we get one person who's shorter than the average and one

02:11.830 --> 02:13.510
person who's taller than the average?

02:13.790 --> 02:16.910
Well, the obvious answer is sampling variability.

02:16.950 --> 02:19.510
Not everyone has the same height.

02:19.630 --> 02:23.670
Just because there is an average height around this value.

02:24.190 --> 02:31.670
So sampling variability refers to the phenomenon that different randomly selected samples from the same

02:31.670 --> 02:35.950
population can have different values of the same measurement.

02:36.150 --> 02:40.310
So we are measuring height with the same ruler in every person.

02:40.750 --> 02:41.350
It's the same.

02:41.390 --> 02:43.910
So the same measurement the same ruler.

02:44.030 --> 02:48.150
But we take different samples and we are going to get different values.

02:48.830 --> 02:56.950
So this means that a single measurement is likely to be an unreliable estimate of a population parameter.

02:57.190 --> 03:03.110
Now how reliable or unreliable a single measurement is depends on a variety of factors.

03:03.110 --> 03:06.550
Mainly, it depends on the variability of the distribution.

03:06.990 --> 03:11.870
But this is important for deep learning because, you know, let's say we're building a model that can

03:11.870 --> 03:15.950
recognize cats, that can detect when a cat is in a picture.

03:16.630 --> 03:23.230
Well, if every single cat in the universe looked exactly the same, you know, in the way that every

03:23.270 --> 03:25.710
electron in the universe is exactly the same.

03:26.030 --> 03:32.270
If every single cat looked exactly the same, and every picture were taken from the same angle, we

03:32.270 --> 03:36.190
wouldn't actually need, you know, thousands of examples of pictures of cats.

03:36.230 --> 03:38.510
We would only need one picture of a cat.

03:38.710 --> 03:40.830
But obviously that's not the case.

03:41.190 --> 03:42.590
There are different kinds of cats.

03:42.630 --> 03:44.350
Cats look very different from each other.

03:44.350 --> 03:48.510
We have some weird examples of cats here and corona safe cats.

03:48.710 --> 03:55.910
So we need to collect a large sample of images of cats because of sampling variability.

03:56.310 --> 03:58.910
So where does sampling variability even come from?

03:58.910 --> 04:02.670
Why do we have variability in our samples?

04:02.870 --> 04:05.790
Well part of the answer is that there's natural variation.

04:05.830 --> 04:11.780
This is often seen in biology and in physics, so there's just natural variation all over the place.

04:12.300 --> 04:14.260
Part of it is measurement noise.

04:14.260 --> 04:16.180
Our sensors are imperfect.

04:16.180 --> 04:22.300
For example, you know, in the example I gave with the height of Montenegrins, we're just using a

04:22.300 --> 04:23.260
regular ruler.

04:23.260 --> 04:28.460
So we can say that it's accurate up to about a centimeter or maybe half a centimeter.

04:28.580 --> 04:35.460
But if we wanted to measure more precisely, let's say up to a nanometer, then, you know, our sensors

04:35.460 --> 04:36.540
are just not good enough.

04:36.580 --> 04:40.100
Our rulers are not good enough to get that level of precision.

04:40.100 --> 04:45.540
So that alone introduces a little bit of noise and sampling variability.

04:46.100 --> 04:52.180
Also, complex systems have variables that depend on or interact with other variables.

04:52.180 --> 04:59.060
So for example, if we were to measure height of lots of randomly selected people while ignoring age,

04:59.380 --> 05:02.460
the sampling variability would increase a lot, right?

05:02.460 --> 05:07.420
Because we are mixing together the height of babies with the height of adults.

05:07.820 --> 05:12.260
And you know, the universe is just a random and wildly unpredictable place.

05:12.740 --> 05:18.140
So there's just some natural stochasticity, I guess that's related to this first point I made here.

05:19.060 --> 05:21.260
Okay, so what do we do about sampling variability?

05:21.300 --> 05:27.140
Well, you know, the obvious solution is to take lots and lots of samples so that we can compute an

05:27.140 --> 05:27.860
average.

05:27.980 --> 05:34.980
And the idea is that the more variability there is, the more samples that we need to approximate the

05:34.980 --> 05:36.940
true population mean.

05:37.100 --> 05:41.620
This is related to a concept in statistics called the law of large numbers.

05:41.780 --> 05:47.060
Essentially this means that the larger samples we take, the more samples we take, the more data we

05:47.100 --> 05:53.220
collect, the better we are going to be able to estimate the true underlying statistics that we really

05:53.220 --> 05:53.900
care about.

05:54.260 --> 06:00.100
You will see examples of this also in deep learning throughout the rest of this course, that when we

06:00.100 --> 06:07.220
are working with relatively simple stimuli, relatively simple data values, we actually don't need

06:07.220 --> 06:08.700
that many samples.

06:08.820 --> 06:10.930
In fact, you'll see Sometimes we were.

06:10.970 --> 06:15.130
We will be simulating our own data, we'll be generating our own data.

06:15.130 --> 06:20.890
And we get really, really good deep learning model performance based on only 100 samples.

06:21.090 --> 06:27.850
But if you would try to train a much more complex, really deep convolutional neural network with 100

06:27.850 --> 06:30.050
pictures, you're not going to get anywhere.

06:30.410 --> 06:32.530
Okay, so have lots and lots of data.

06:32.570 --> 06:37.810
That's one thing that we need to overcome the issue of sampling variability.

06:38.250 --> 06:42.810
So again why we need to worry about sampling variability in deep learning.

06:43.130 --> 06:46.290
Well deep learning models learn by examples.

06:46.770 --> 06:53.250
So we need to have lots of examples for our deep learning models to actually learn effectively.

06:53.730 --> 07:01.050
This is also an important issue because non-random or biased sampling can introduce systematic biases

07:01.170 --> 07:06.490
into the deep learning models, and prevent them from learning the way that we want them to learn.

07:06.490 --> 07:11.130
This is a topic I will discuss in more detail later in the course.

07:11.130 --> 07:15.610
We need to have random sampling in order for the models to be good.

07:15.930 --> 07:22.090
If we don't have random samples, we have non-representative or biased sampling that can cause overfitting

07:22.290 --> 07:24.690
and it limits the generalizability.

07:24.730 --> 07:26.010
Again, this is a big term.

07:26.010 --> 07:31.170
This is you know, there's a lot to unpack in this sentence here I'm going to talk more about this.

07:31.210 --> 07:35.210
We're going to have several videos just on this topic later in the course.

07:35.650 --> 07:42.050
Now I'd like to switch to Python and give you a demonstration of sampling variability.

07:43.690 --> 07:48.530
So what we are going to do here is generate a population of numbers.

07:48.530 --> 07:51.410
So this is the entire population.

07:51.570 --> 07:54.170
This is like all of the people in Montenegro.

07:54.170 --> 07:56.050
And these are their heights okay.

07:56.090 --> 08:02.290
And then because we have the entire population we can actually compute the population mean.

08:02.290 --> 08:03.490
And that's what I do here.

08:03.930 --> 08:08.050
And then what I do is use numpy dot random choice.

08:08.330 --> 08:12.090
And I'm going to select from this population.

08:12.210 --> 08:15.080
I'm going to select a random sample.

08:15.080 --> 08:19.080
So a random choice random sample of five individuals.

08:19.080 --> 08:24.120
So we just randomly pick five of these numbers at random.

08:24.120 --> 08:27.080
And then we compute the average of that sample.

08:27.560 --> 08:30.680
And now you see that those two are not the same.

08:30.680 --> 08:32.360
The numbers are not the same.

08:32.600 --> 08:35.760
So the mean of the population is 1.625.

08:35.920 --> 08:40.680
But the mean of this particular random sample is 1.0.

08:40.920 --> 08:42.880
Now I'm going to run this code again.

08:43.200 --> 08:47.640
And this number is going to be the same of course because all these numbers are printed out here.

08:47.800 --> 08:52.920
But this number will differ because this is a new random sample.

08:53.360 --> 08:59.840
So that was actually a nice demonstration because the first sample was too low and the second sample

08:59.840 --> 09:00.960
is now too high.

09:01.320 --> 09:02.920
So we can keep running this again.

09:03.760 --> 09:05.120
Wow, that one is really high.

09:05.160 --> 09:06.640
That's even much larger.

09:06.880 --> 09:09.080
So you can see that sometimes the.

09:09.560 --> 09:09.840
Wow.

09:09.880 --> 09:10.280
Okay.

09:10.320 --> 09:12.840
Now this is really large 4.6.

09:13.000 --> 09:18.400
That is Like three times larger than the population.

09:18.400 --> 09:25.720
So sometimes if we take small random samples, we get very non-representative values.

09:25.760 --> 09:29.680
This is why we want to take larger and larger samples.

09:29.920 --> 09:32.120
And that's what I want to illustrate to you here.

09:32.240 --> 09:36.200
So we're going to do here is run through 10,000 experiments.

09:36.200 --> 09:42.080
We're going to take 10,000 random samples that you see I'm just running this line again.

09:42.080 --> 09:44.120
This is exactly the same line as above.

09:44.280 --> 09:46.440
But now I have it inside this for loop.

09:46.440 --> 09:47.400
So we're going to run.

09:47.880 --> 09:53.160
We're going to generate 10,000 random samples and compute the mean and store the mean.

09:53.200 --> 09:58.760
And then I'm going to make a histogram of all of those samples okay.

09:58.800 --> 10:02.840
And then you see here this magenta vertical line is the the true.

10:03.080 --> 10:06.600
Oops sorry the true mean the population mean here.

10:06.600 --> 10:14.200
So you see that as we increase our number of experiments, the average of those averages is getting

10:14.200 --> 10:16.750
closer and closer to the true mean.

10:16.750 --> 10:20.070
That's basically what the law of large numbers is telling us.

10:20.110 --> 10:22.510
Now, it's pretty interesting to play around with these experiments.

10:22.510 --> 10:23.110
You can try.

10:23.190 --> 10:28.990
For example, increasing the size from five to maybe 15.

10:30.110 --> 10:30.670
So let's see.

10:30.670 --> 10:36.550
So here we had the variability go from looks like minus four up to plus six approximately.

10:36.950 --> 10:41.390
And now we get a distribution that goes minus two up to two.

10:41.430 --> 10:44.790
So the distribution itself has gotten much narrower here.

10:45.030 --> 10:51.670
So this is a motivation for taking larger samples getting more data from the population.

10:52.990 --> 10:58.110
So in this video you learned about sampling variability what it means where it comes from.

10:58.470 --> 11:03.230
And I also mentioned the importance of random or unbiased sampling.

11:03.390 --> 11:09.310
And the problem I mentioned the problems that nonrandom sampling can lead to in particular overfitting

11:09.310 --> 11:11.230
and lack of generalization.

11:11.230 --> 11:16.310
These are concepts that we will go into much more detail about later on in the course.

WEBVTT

00:02.120 --> 00:08.400
In this video you will learn about matrix multiplication and spoiler alert.

00:08.600 --> 00:16.000
Matrix multiplication is literally just the dot product, which itself is literally just a linear weighted

00:16.000 --> 00:17.000
combination.

00:17.560 --> 00:26.640
The idea of matrix multiplication is to calculate the dot product on lots of pairs of vectors all at

00:26.640 --> 00:31.680
once, without having to write lots of for loops or lots of lines of code.

00:32.000 --> 00:37.800
So matrix multiplication is really just an organized collection of dot products.

00:38.360 --> 00:43.280
Let me begin by just discussing matrix sizes.

00:43.480 --> 00:51.640
So matrices are organized into rows and columns and rows go down the y axis.

00:51.640 --> 00:56.400
If we're thinking about this on a plot and columns go across the x axis.

00:56.760 --> 01:01.530
Now when we are describing the sizes of matrices.

01:01.890 --> 01:09.010
Then we would describe this as a four by three matrix, because there are four rows and three columns.

01:09.810 --> 01:17.010
And we describe this as a three by seven matrix because there are three rows and seven columns.

01:17.010 --> 01:23.370
So you always list first the number of rows and then the number of columns.

01:23.890 --> 01:30.410
So the standard term for this kind of a organization of numbers is a matrix.

01:30.890 --> 01:38.490
In the parlance of tensors and also PyTorch we would call this a two dimensional tensor.

01:38.730 --> 01:41.690
It's two dimensions because there's just rows and columns.

01:41.690 --> 01:44.130
There's no thickness to this.

01:44.130 --> 01:48.090
So two dimensional tensors are the same thing as matrices.

01:48.090 --> 01:52.210
And we always refer to their sizes as rows by columns.

01:52.770 --> 01:54.330
And why am I telling you this?

01:54.330 --> 02:02.350
Because there are rules for the validity of matrix multiplication, just like when I introduced the

02:02.350 --> 02:09.630
dot product in the previous video, I said that the dot product is only defined between two vectors

02:09.630 --> 02:14.430
that have the same length, so the same number of numbers in them.

02:15.550 --> 02:23.910
So for matrix multiplication, you can only multiply two matrices if the inner dimensions here are the

02:23.910 --> 02:26.550
same number if the inner dimensions match.

02:26.550 --> 02:31.950
So here we have a matrix with m rows and n columns.

02:32.310 --> 02:37.110
And here I have a tall matrix with n rows and k columns.

02:37.150 --> 02:41.030
Now in this case these two dimensions are the same.

02:41.030 --> 02:49.430
So n equals n and so that means that we can calculate the multiplication of these two matrices here.

02:49.790 --> 02:55.390
And the size of the product matrix is determined by the outer dimensions here.

02:55.390 --> 02:56.910
So m by k.

02:56.950 --> 03:02.120
So that's going to be the result of this product matrix over here.

03:02.720 --> 03:04.280
So here are some examples.

03:04.280 --> 03:06.760
Imagine a and b are two matrices.

03:07.280 --> 03:09.200
A is five by two.

03:09.240 --> 03:11.280
So five rows and two columns.

03:11.520 --> 03:13.240
B is two by seven.

03:13.240 --> 03:15.560
So two rows and seven columns.

03:15.760 --> 03:21.440
And the question is can we calculate the multiplication of these two matrices.

03:21.960 --> 03:26.240
And the answer is yes because their inner dimensions match.

03:26.440 --> 03:31.640
And the result of or the size of the product matrix will be five by seven.

03:32.240 --> 03:33.520
Here's another example.

03:34.000 --> 03:38.280
Now these two matrices, which are actually they can be the same as these two.

03:38.320 --> 03:39.960
You can see the sizes are the same.

03:40.240 --> 03:43.920
But when we turn them around and try to multiply B times a.

03:44.600 --> 03:48.240
Now this is no longer a valid multiplication.

03:48.240 --> 03:53.720
It's no longer a valid operation because the inner dimensions here do not match.

03:53.720 --> 03:56.000
Seven and five are not the same number.

03:56.160 --> 04:02.340
So we cannot multiply these two matrices in this order, even though we could multiply them in this

04:02.340 --> 04:02.740
order.

04:04.180 --> 04:05.340
Here's another example.

04:05.340 --> 04:12.460
Imagine we have matrix C and A, and if we want to multiply these two matrices, we cannot multiply

04:12.460 --> 04:18.220
them because the number of columns does not equal the number of rows in A.

04:18.860 --> 04:23.500
But we can apply the transpose operation.

04:23.540 --> 04:30.020
You'll remember that I introduced the transpose operation in the previous section when I was discussing

04:30.020 --> 04:38.740
vectors and matrices in NumPy and PyTorch, and the idea of the transpose operation is to swap the rows

04:38.740 --> 04:45.060
and the columns, and when you swap the rows and the columns, you're also going to swap these sizes.

04:45.340 --> 04:53.940
So if we were to transpose C then that would actually transform this into a seven by five matrix.

04:54.100 --> 04:55.860
And then C transpose.

04:55.860 --> 04:59.180
And we have five columns that actually would work here.

04:59.180 --> 05:05.230
So we can actually calculate the matrix multiplication between C transpose and A.

05:05.270 --> 05:14.630
Although the multiplication between C and A is not defined okay, so that's the rules for matrix multiplication.

05:14.870 --> 05:22.430
And with this in mind, we can actually revisit the discussion of the dot product and see how matrix

05:22.430 --> 05:25.870
multiplication is related to the dot product.

05:26.310 --> 05:27.990
And let's you see on this slide.

05:27.990 --> 05:30.790
So imagine we have two vectors v and w.

05:30.830 --> 05:34.390
Maybe this is activations from some neurons.

05:34.390 --> 05:38.350
And these are the weights for those neurons to come into the current neuron.

05:38.630 --> 05:45.830
And if those are five elements we can think about these two vectors as being like matrices where one

05:45.830 --> 05:47.830
of the dimensions is one.

05:47.830 --> 05:52.830
So a five by one matrix is the same thing as a column vector.

05:53.830 --> 06:01.270
Now we cannot multiply these two matrices or vectors because the inner dimensions here do not match.

06:01.480 --> 06:07.320
But we can transpose V, and that swaps the rows and the columns.

06:07.440 --> 06:12.120
And now this becomes a one by five vector or matrix.

06:12.440 --> 06:17.720
And now we actually can calculate the product between these two vectors.

06:17.840 --> 06:24.240
And the result is of the size corresponding to the outer dimensions here which is one by one.

06:24.240 --> 06:28.320
So that's going to give us a scalar or just one number.

06:28.640 --> 06:28.960
Okay.

06:29.000 --> 06:35.320
So this shows the relationship between matrix multiplication and the dot product.

06:35.520 --> 06:41.280
And this also leads to the definition of matrix multiplication.

06:41.920 --> 06:50.400
So matrix multiplication is or can be conceptualized as an ordered structure of dot products.

06:50.560 --> 06:53.240
Okay so here I have two matrices.

06:53.440 --> 06:54.280
These are numbers.

06:54.280 --> 06:55.200
These are letters.

06:55.240 --> 06:59.080
That's just to make it convenient to see how they relate to each other.

06:59.400 --> 07:04.980
What I'm showing here is going to be the result of matrix multiplication.

07:05.220 --> 07:07.900
So we can multiply these two matrices.

07:08.060 --> 07:11.780
And the result is also going to be a two by two matrix.

07:11.940 --> 07:15.100
And what are the elements in that two by two matrix.

07:15.620 --> 07:22.060
Well we take the rows of the left matrix and the columns of the right matrix.

07:22.300 --> 07:24.700
And we calculate their dot product.

07:24.820 --> 07:34.740
So the element the value of the element in this position here in the resulting matrix is zero times

07:34.780 --> 07:37.660
a plus one times c.

07:37.660 --> 07:46.100
So that is the dot product between the first row of the left matrix and the first column of the right

07:46.100 --> 07:46.940
matrix.

07:47.060 --> 07:48.500
So that's this element.

07:48.620 --> 07:50.700
And I think you can see where this is going.

07:50.740 --> 07:54.660
We get this element over here which is in the second row.

07:54.660 --> 08:02.150
And the first column as the dot product between the second row of the left matrix and the first column

08:02.150 --> 08:04.670
of the right matrix, and then and so on.

08:04.670 --> 08:11.390
So here again we have the element in the second first row and the second column.

08:11.590 --> 08:17.750
And that's the dot product between the first row and the second column and so on for all of the elements

08:17.750 --> 08:19.150
in the matrix.

08:19.150 --> 08:26.390
So you can see that matrix multiplication is just a bunch of dot products that are organized into a

08:26.430 --> 08:27.830
compact form.

08:28.230 --> 08:31.270
So now I'm going to switch to Python.

08:31.270 --> 08:37.870
And I will demonstrate basically several features of dot products and matrix multiplication.

08:38.190 --> 08:40.710
We're going to create vectors in numpy.

08:41.230 --> 08:48.470
We're going to combine them and show that their dot products can be implemented as matrix multiplication.

08:48.750 --> 08:56.710
You will see different ways of coding matrix multiplication in Python, in NumPy, and also in PyTorch.

08:57.310 --> 09:04.410
And yet again, just like in the previous video, you will see that some PyTorch functions are very

09:04.410 --> 09:09.890
sensitive to the data type, in particular trying to work with ints and floats.

09:11.050 --> 09:14.970
So here we are with the code for matrix multiplication.

09:15.370 --> 09:17.690
Import numpy and PyTorch.

09:18.050 --> 09:21.490
Here what I'm doing is creating four vectors.

09:21.690 --> 09:25.930
These are kind of simulating two activations vectors.

09:26.090 --> 09:29.330
And these are simulating two weights vectors.

09:29.410 --> 09:31.370
All you know arbitrary numbers.

09:31.370 --> 09:36.050
There's no particular reason why I use these numbers and not any other numbers.

09:36.050 --> 09:38.930
Why is this squared and not raised to the fourth power?

09:38.970 --> 09:43.050
I don't know, it's kind of arbitrary, but what is not arbitrary?

09:43.090 --> 09:50.810
What is really important here for these calculations is that the number of elements in all four of these

09:50.810 --> 09:57.210
vectors is the same, so the sizes of these vectors all need to be the same.

09:57.330 --> 10:01.820
But there are actual numerical values are not relevant for this discussion.

10:02.340 --> 10:09.260
Okay, so here I am manually calculating their dot products between activation one and wait one and

10:09.260 --> 10:12.140
activation vector two and weight vector two.

10:12.620 --> 10:14.620
And that happens to be 0.5.

10:14.660 --> 10:18.180
Now this is 0.49999999.

10:18.220 --> 10:25.100
This is just tiny, tiny, tiny rounding errors that make this look like it's not 0.5, but it really

10:25.100 --> 10:25.900
is 0.5.

10:27.100 --> 10:29.900
And then here we have 116.9.

10:30.260 --> 10:30.580
Okay.

10:30.620 --> 10:35.900
Now we have four vectors here but only two results here.

10:36.100 --> 10:41.900
And that's because I've only calculated the dot product between activation one and weight one.

10:41.900 --> 10:44.260
But we can also calculate the.

10:44.300 --> 10:45.820
I'm calling these cross terms here.

10:45.820 --> 10:51.140
But basically the dot product between activation one and weight vector two.

10:51.660 --> 10:55.580
And those results are 73 and minus two and a half.

10:56.020 --> 10:56.500
Okay.

10:56.820 --> 10:57.340
Very nice.

10:57.340 --> 11:02.880
So what we are doing here is representing everything as individual vectors.

11:03.160 --> 11:05.080
And we want for dot products.

11:05.080 --> 11:07.640
So we have to write out four lines of code.

11:07.960 --> 11:08.920
Now that's fine.

11:08.920 --> 11:10.360
There's nothing wrong with that.

11:10.360 --> 11:17.440
But it doesn't really scale up to a very, very, very large number of vectors and dot products that

11:17.440 --> 11:21.880
we need to calculate in these large language models.

11:23.120 --> 11:23.440
Okay.

11:23.480 --> 11:31.840
So therefore what I'm doing here is combining these vectors or concatenating these vectors into matrices.

11:32.120 --> 11:36.640
So in both of these cases these are two by five matrices.

11:36.640 --> 11:40.080
You can see that there's two rows and five columns.

11:40.080 --> 11:46.920
And also similar to what you saw in the previous video the weights are floating point numbers and the

11:46.920 --> 11:49.160
activations are integers.

11:49.360 --> 11:51.600
By the way, that is not generally the case.

11:51.640 --> 11:56.480
The activations vectors in real models are also floating point numbers.

11:56.680 --> 12:04.730
I'm only making them integers here to highlight one of the Difficulties or potential points of confusion

12:04.730 --> 12:07.090
when working with PyTorch.

12:07.570 --> 12:13.610
Okay, so now there are matrices and now we can multiply them using matrix multiplication.

12:13.770 --> 12:20.570
Now the sizes of these matrices are x dot shape and weights dot shape.

12:21.930 --> 12:25.730
These are two by five and two by five matrices.

12:25.730 --> 12:33.410
And that means that we cannot calculate the matrix multiplication between these two matrices because

12:33.410 --> 12:36.050
their inner dimensions do not match.

12:36.530 --> 12:43.370
So instead, what we are going to do is transpose the weights, and that gives us a two by five matrix

12:43.370 --> 12:45.130
and a five by two matrix.

12:45.650 --> 12:54.730
And now we actually can multiply the two matrices because their inner dimensions match, and the product

12:54.730 --> 13:00.050
matrix, or the size of the product matrix will correspond to the outer dimensions, which is two by

13:00.090 --> 13:00.410
two.

13:01.010 --> 13:06.380
And now the question is what will be the elements in that two by two matrix?

13:06.380 --> 13:12.300
I'm sure you've already guessed that they are going to be these dot products that I calculated manually

13:12.340 --> 13:12.980
up here.

13:13.460 --> 13:21.220
So let's try this now in Python we can implement matrix multiplication using the at symbol.

13:21.220 --> 13:28.900
This is exactly the same symbol that you use in your email address okay so but now in Python we that

13:28.900 --> 13:31.300
is used for matrix multiplication.

13:31.580 --> 13:36.220
So the activations matrix times the transpose of the weight matrix.

13:36.620 --> 13:40.060
And that gives us the first column is 0.5.

13:40.380 --> 13:42.180
Actually let me show you the diagonals first.

13:42.180 --> 13:46.220
So the diagonals are 0.5 and 116.

13:46.220 --> 13:49.700
And that corresponds to these numbers here.

13:49.700 --> 13:55.500
So the dot product between activation one and weight one and activation two and weight two.

13:55.860 --> 13:58.740
Those are the elements in the diagonals.

13:58.740 --> 14:01.500
And then the off diagonals are the cross terms.

14:01.500 --> 14:06.440
So there we have 76 and -2.5.

14:06.440 --> 14:12.800
And that corresponds to these numbers here which are the dot products between activation one and weight

14:12.840 --> 14:16.120
two and activation two and weight one.

14:16.960 --> 14:17.440
Okay.

14:17.480 --> 14:25.200
So that is one notation for getting matrix multiplication in Python.

14:25.200 --> 14:28.400
We can also use the numpy dot dot function.

14:28.440 --> 14:34.520
Now this is I mentioned in the previous video that this is a little bit poorly named in my opinion,

14:34.520 --> 14:39.960
because the dot product formally is not defined between two matrices.

14:39.960 --> 14:48.800
If we're thinking about kind of a classical linear algebra environment, the dot product is not a valid

14:48.840 --> 14:50.360
operation on matrices.

14:50.360 --> 14:51.600
It doesn't make any sense.

14:52.480 --> 14:56.440
We can calculate the dot product on vectors, but not on matrices.

14:56.800 --> 15:05.450
But the Python numpy function numpy dot dot actually implements matrix multiplication, which, as you

15:05.450 --> 15:17.170
now know, is just a sequence of of dot products on the rows of this matrix and the columns of weights

15:17.170 --> 15:20.010
transpose, which are the rows of weights.

15:20.370 --> 15:27.890
In previous videos I was using multiplication you implementing multiplication through the asterisk symbol.

15:28.250 --> 15:32.090
And that is also defined for matrices.

15:32.090 --> 15:36.730
But that is not matrix multiplication as I described it in the slides.

15:37.290 --> 15:43.050
Instead, that is something else called Hadamard multiplication, which is also called element wise

15:43.050 --> 15:44.210
multiplication.

15:44.730 --> 15:51.730
It's a different operation on on matrices, and it's not implementing the dot product.

15:51.730 --> 15:56.850
Instead it is just calculating element wise multiplication.

15:57.090 --> 16:01.850
So we started off with these two matrices X and weights.

16:01.910 --> 16:04.910
and these were five by two matrices.

16:05.230 --> 16:09.990
And now the Hadamard multiplication is another five by two matrix.

16:10.390 --> 16:16.550
We actually can get the dot products here by summing over the rows here.

16:16.550 --> 16:19.830
So summing across the columns for each row.

16:20.870 --> 16:23.110
But that doesn't give us the cross terms.

16:23.110 --> 16:30.990
So this is actually not a good way to calculate dot products between two or all the vectors that are

16:30.990 --> 16:32.750
stored into matrices.

16:33.190 --> 16:36.470
I'm just showing it here so you know the difference.

16:36.910 --> 16:44.590
In general, when you're using numpy you want to use the at symbol or numpy dot dot to implement matrix

16:44.590 --> 16:45.790
multiplication.

16:46.190 --> 16:46.470
Okay.

16:46.510 --> 16:50.070
So potential source of confusion something to be mindful of.

16:50.950 --> 16:52.670
So that was all in numpy.

16:52.670 --> 16:54.750
And now let's switch to PyTorch.

16:55.030 --> 17:04.240
For starters I'm going to transform this numpy array into a PyTorch tensor and same thing for the weights.

17:04.240 --> 17:08.080
So this starts off as let me do this.

17:08.320 --> 17:15.520
I'm going to print the type of x and that is a numpy array.

17:16.080 --> 17:23.680
And then we're going to look at the type of x embedded inside this function torch.tensor.

17:24.800 --> 17:27.160
And now it is no longer a numpy array.

17:27.320 --> 17:29.360
It is a PyTorch tensor.

17:29.360 --> 17:31.200
So different data type.

17:31.400 --> 17:34.520
All the numerical values are exactly the same.

17:34.520 --> 17:37.440
It's just that the data type is different.

17:38.160 --> 17:38.520
Okay.

17:38.560 --> 17:46.480
So now if we try to multiply exactly how we did before and what is going to happen, well, we see again

17:46.880 --> 17:52.320
this error message that we saw in the previous video, uh, a runtime error.

17:52.360 --> 17:58.520
They have to have the same data type but got long and double, which is the same thing as float.

17:58.960 --> 18:04.530
You probably already guessed that this was going to happen by seeing this code that I commented out.

18:05.770 --> 18:14.370
So in the previous video I created a new PyTorch tensor and there I use dtype equals float and so on.

18:14.410 --> 18:17.410
Here I'm implementing this in a slightly different way.

18:17.410 --> 18:22.170
So I'm writing I'm creating this tensor which is ints.

18:22.650 --> 18:24.690
And then I'm writing dot two.

18:24.730 --> 18:26.650
So a method called two.

18:27.010 --> 18:30.650
And then I'm sending this to floating point numbers.

18:30.650 --> 18:34.170
So 64 bit precision floating point numbers.

18:34.530 --> 18:39.610
And now we actually can calculate the matrix multiplication.

18:39.850 --> 18:44.690
And the results not surprisingly are exactly the same as we got before.

18:44.730 --> 18:46.210
Except for the data type.

18:46.210 --> 18:47.490
The data type is different.

18:47.730 --> 18:53.490
Remember we didn't have a problem with this line over here when we used PyTorch.

18:53.530 --> 18:55.010
So sorry numpy.

18:55.050 --> 19:00.770
So numpy is more robust to data types than is PyTorch.

19:01.430 --> 19:05.230
Here we can calculate matrix multiplication.

19:05.510 --> 19:07.430
And again we're getting an error.

19:07.470 --> 19:10.910
And what is the problem with this code here.

19:10.910 --> 19:14.790
This is not the same as the error that I got over here right.

19:14.830 --> 19:17.990
So here the error message was about the data type.

19:18.430 --> 19:21.670
Here the error message is about the shapes.

19:22.030 --> 19:24.030
And they cannot be multiplied.

19:24.190 --> 19:26.950
And you already know why that is the case.

19:27.070 --> 19:28.630
And you know what the solution is.

19:28.670 --> 19:31.550
And you also see the solution here commented.

19:31.710 --> 19:32.110
So.

19:32.590 --> 19:34.990
So now we can run this.

19:35.030 --> 19:36.390
We get another error.

19:36.630 --> 19:37.990
And why do we get this error.

19:38.030 --> 19:40.270
Well that's because I rerun this code.

19:40.830 --> 19:42.830
So now that works.

19:44.190 --> 19:45.710
And now this is going to work.

19:45.710 --> 19:50.350
So torch dot matmul for matrix multiplication.

19:50.830 --> 19:56.830
In the previous video we were using torch dot to calculate dot products.

19:57.310 --> 19:59.710
You know that that works here.

20:00.150 --> 20:05.200
Where was it here numpy Numpy.dot that gives us the matrix multiplication.

20:05.640 --> 20:07.080
Here, let's see what happens.

20:07.600 --> 20:10.600
Okay, so here we get an error message.

20:10.600 --> 20:15.960
Now this is not an error message about data types or ints and floats.

20:16.320 --> 20:22.240
This is a error message that PyTorch is expecting one dimensional tensors.

20:22.240 --> 20:23.400
So vectors.

20:23.640 --> 20:28.760
But we actually provided two dimensional tensors or matrices.

20:29.160 --> 20:37.440
But when we run this code where I'm extracting just the first row of x and this is also the first row

20:37.480 --> 20:40.000
of weights, then okay.

20:40.040 --> 20:42.760
So now we're getting I think this is actually a typo.

20:42.800 --> 20:45.600
This should be this is the numpy version.

20:45.600 --> 20:48.600
This needs to be the PyTorch version.

20:48.880 --> 20:52.520
And now finally we get a result of 0.5.

20:52.880 --> 20:59.360
So what you can see from this video is that PyTorch and NumPy are different libraries.

20:59.360 --> 21:00.600
They function differently.

21:00.600 --> 21:06.020
They have different expectations for the sizes and the data types of inputs.

21:06.420 --> 21:07.580
It's all quite tricky.

21:07.620 --> 21:11.220
I do not recommend trying to memorize all of this stuff.

21:11.540 --> 21:18.820
Uh, if you are mindful of these error messages and what they mean and how they arrive and how to resolve

21:18.820 --> 21:24.860
them, you will basically just get more familiar with working with these libraries over time.

21:25.580 --> 21:27.780
So that's it for this video.

21:27.820 --> 21:37.260
You learned that matrix multiplication is just a fancy and compact way of sorting and organizing and

21:37.260 --> 21:44.860
collecting a series of dot products between pairs of vectors that are organized into two matrices.

21:45.380 --> 21:52.020
And in terms of coding, you saw that there are several ways to calculate matrix multiplication, and

21:52.020 --> 21:56.340
also that PyTorch functions can be really sensitive to the data type.

21:56.500 --> 22:00.860
So that's something to be mindful of as you proceed in PyTorch.

WEBVTT

00:02.000 --> 00:09.280
The logarithm or the log function or the log transform is really important in machine learning and in

00:09.280 --> 00:10.480
optimization.

00:10.480 --> 00:16.880
It's used all the time in optimization and therefore also in deep learning, because deep learning is

00:16.880 --> 00:19.920
essentially just an optimization problem.

00:20.600 --> 00:25.120
Now, a few videos ago I introduced you to the natural exponent.

00:25.120 --> 00:29.200
Just as a quick reminder, the natural exponent is this constant.

00:29.200 --> 00:31.520
It's a number around 2.7.

00:31.760 --> 00:40.280
And when you generate a plot of of e so the natural exponent e to the x, then you find that it is strictly

00:40.320 --> 00:41.400
a positive function.

00:41.400 --> 00:45.600
So it's never it never takes on negative values even when x is negative.

00:45.880 --> 00:50.880
And it has this kind of bend here and then it quickly goes up to infinity.

00:50.880 --> 00:53.240
So this is the plot of E to the x.

00:53.560 --> 00:59.520
Now it turns out that the natural log is the inverse of the natural exponent.

00:59.680 --> 01:05.600
So here again this blue line is our plot of e to the x, the same exact function as the previous slide.

01:05.640 --> 01:13.360
Here in this cyan line, I am plotting the log of e to the x, and so because the natural log and the

01:13.360 --> 01:18.480
natural exponent are inverses of each other, these two functions actually cancel.

01:18.480 --> 01:20.960
And what we are left with is just x.

01:21.240 --> 01:22.880
So you see this is just a line.

01:22.880 --> 01:23.880
This cyan line.

01:23.920 --> 01:27.240
This is just a straight line with a slope of one.

01:27.240 --> 01:29.120
This is the unity function.

01:29.120 --> 01:31.720
We're just plotting this line by itself.

01:31.840 --> 01:39.160
Now to be clear, this is not a rigorous mathematical proof that log n and e are inverses of each other.

01:39.160 --> 01:42.120
But this is a nice illustration of that fact.

01:42.520 --> 01:46.200
Here is what the logarithm looks like plotted on its own.

01:46.200 --> 01:49.560
So here we have x and here is the log of x.

01:49.560 --> 01:54.200
So it bends just like e just like the natural exponent.

01:54.200 --> 01:55.680
But it bends the other way.

01:55.880 --> 01:59.480
And the log actually also still grows up to infinity eventually.

01:59.480 --> 02:02.400
But you can see it gets to infinity much much slower.

02:02.400 --> 02:05.280
It's slowing down the larger X gets.

02:05.520 --> 02:09.320
Now there's a couple of things I would like to point out about the logarithm.

02:09.680 --> 02:13.480
First of all, the log is a monotonic function of x.

02:13.480 --> 02:16.480
What does that mean for a function to be monotonic?

02:16.840 --> 02:24.080
It means that whenever x goes up, the log also goes up, and when x goes down, the log goes down.

02:24.400 --> 02:26.480
So they are monotonic functions.

02:26.600 --> 02:28.560
They both go up and down together.

02:28.600 --> 02:33.000
Now the spacing between individual points is different because log is non-linear.

02:33.040 --> 02:34.800
I'll get to that in a moment.

02:35.120 --> 02:42.560
But you can contrast this monotonic function with something like the sine which is a non-monotonic function.

02:42.800 --> 02:49.280
So when x increases here the function increases but then it decreases, then it increases and so on.

02:49.640 --> 02:51.080
So why do we care?

02:51.080 --> 02:54.840
Who cares if this is a if log is a monotonic function of x?

02:55.080 --> 03:03.360
Well this turns out to be important for optimization and for deep learning, because it means that minimizing

03:03.360 --> 03:07.880
X is the same thing as minimizing the log of x.

03:08.280 --> 03:15.200
That is to say, any tweaks we do to the model, any way that we change our deep learning model that

03:15.200 --> 03:18.320
minimizes X, that makes X smaller.

03:18.680 --> 03:22.320
That's also going to make the log of x smaller.

03:23.040 --> 03:29.600
And that is what's special about a monotonic function in the context of optimization and deep learning.

03:29.880 --> 03:38.080
Now this is actually only true for um positive values of x, because the log is not defined for zero

03:38.080 --> 03:40.480
or any negative values of x.

03:40.480 --> 03:47.040
So log is only defined for for values of x that are larger than zero okay.

03:47.080 --> 03:53.560
And the key point of the logarithm is that it stretches out small values of x.

03:53.560 --> 03:54.480
So what does this mean.

03:55.200 --> 04:04.120
Notice here that the gap on the y axis of log of x between these two successive data points is huge.

04:04.160 --> 04:13.280
This is a really large distance here versus the distance on the y axis that's covered by this distance

04:13.280 --> 04:13.520
here.

04:13.520 --> 04:19.640
So on the x axis, every point here on this function is exactly equally distant.

04:19.640 --> 04:23.320
So these two points are equidistant on the x axis.

04:23.440 --> 04:26.360
These two points are equidistant on the x axis.

04:26.360 --> 04:32.880
So when projecting this function down to the x axis, all of these points are exactly equally spaced,

04:32.880 --> 04:38.160
but they are not exactly equally spaced on the y axis.

04:38.560 --> 04:45.680
And the closer we get to zero, the bigger the distance is on the y axis in log space.

04:45.840 --> 04:53.280
Now this is important because it means that the logarithm better distinguishes small numbers that are

04:53.280 --> 04:55.280
closely spaced to each other.

04:55.680 --> 05:02.520
And the reason why that is important is because in deep learning we are often minimizing very, very

05:02.520 --> 05:09.120
small quantities, probability values, and loss values that are small and very close to zero.

05:09.600 --> 05:17.290
So minimizing the log turns out to be computationally easier than Then minimizing the original, let's

05:17.290 --> 05:18.890
say, probability values.

05:19.090 --> 05:21.890
That's purely about numerical precision.

05:21.890 --> 05:26.930
When you have tiny, tiny probability values that are really close to each other, computers are just

05:26.930 --> 05:31.250
going to have a hard time working with those really small numbers that are close together.

05:31.250 --> 05:36.530
But if you take the log of those numbers, you can see now, you know, these are almost like one and

05:36.530 --> 05:38.370
a half units apart from each other.

05:38.810 --> 05:41.570
So far I've been talking about the natural log.

05:41.730 --> 05:44.210
It turns out that logs have different bases.

05:44.410 --> 05:47.930
There's the natural log, which is often written out.

05:49.050 --> 05:50.290
Then there's log base two.

05:50.330 --> 05:51.090
Log base ten.

05:51.130 --> 05:52.690
This is actually log base E.

05:52.970 --> 05:55.410
So there's different bases you can have for logs.

05:55.450 --> 05:57.490
They are all basically the same.

05:57.490 --> 05:59.370
They're all monotonic functions of x.

05:59.370 --> 06:04.410
They all stretch out at very small values of x close to zero.

06:04.570 --> 06:07.690
They're all defined only for positive values and so on.

06:08.170 --> 06:10.170
They only differ in their slope.

06:10.170 --> 06:16.370
So there isn't really much of a practical difference between minimizing the natural log versus log base

06:16.370 --> 06:19.290
two versus log base ten or any other base.

06:19.290 --> 06:24.850
That said, natural Log is the most commonly used base because of its relationship to E.

06:24.890 --> 06:30.650
That fits in nicely with using sigmoid functions and softmax functions.

06:30.850 --> 06:35.290
Okay, so again, this is actually not a reminder because I already mentioned this a few slides ago.

06:35.330 --> 06:41.490
Machine learning and deep learning often involve minimizing very small quantities like probabilities.

06:41.690 --> 06:45.690
And yeah, you can get some precision errors when working with very small numbers.

06:45.890 --> 06:53.250
So the idea is that we just stretch out those numbers and that just makes the the optimization code

06:53.250 --> 06:54.050
work better.

06:54.410 --> 06:56.930
Now we are going to switch to Python.

06:56.930 --> 07:01.930
And I'll just introduce you to working with the natural log in Python.

07:03.050 --> 07:07.370
So here we have a set of points to evaluate the log on.

07:07.370 --> 07:16.450
So I'm going to compute the log of 200 linearly spaced numbers that are separated between .0001 and

07:16.450 --> 07:17.330
one okay.

07:17.370 --> 07:19.570
And then we we plot them okay.

07:19.610 --> 07:22.490
And then this I showed in the previous video.

07:22.650 --> 07:25.290
How to change the font size of a figure.

07:25.450 --> 07:27.610
Okay, so not, you know, not super interesting.

07:27.610 --> 07:31.850
This is really just demonstrating what I already showed you in the slides.

07:32.290 --> 07:32.530
Okay.

07:32.530 --> 07:39.930
And here is also a demonstration of the natural log and the natural exponents being inverses of each

07:39.970 --> 07:40.450
other.

07:40.570 --> 07:45.210
So here I compute log x and the x and e to the x.

07:45.610 --> 07:51.930
And then I'm plotting the natural exponent of the log of x and the log of the natural exponent of the

07:52.010 --> 07:52.770
of x.

07:53.050 --> 07:54.970
So these two should cancel out.

07:55.010 --> 07:56.330
These two should cancel out.

07:56.330 --> 08:00.170
And that should be the same as plotting x by x.

08:00.330 --> 08:02.730
So then we can just visualize this.

08:02.730 --> 08:04.850
So again the line is unity.

08:04.850 --> 08:06.170
This is x by x.

08:06.170 --> 08:11.050
And then we are plotting e of log of x and log of e to the x.

08:12.330 --> 08:15.410
In this video you learned about the log function.

08:15.770 --> 08:16.250
Why?

08:16.250 --> 08:22.730
It is used in optimization and in particular in deep learning and how to compute the natural log in

08:22.770 --> 08:23.490
numpy.

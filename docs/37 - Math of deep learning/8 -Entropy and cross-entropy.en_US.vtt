WEBVTT

00:02.000 --> 00:07.560
I'm sure you've heard this term before, entropy, or I'm sure you've seen this word written down.

00:08.080 --> 00:16.200
The concept of entropy has different meanings, different interpretations in different areas of science.

00:16.520 --> 00:25.400
So, for example, in physics, the term entropy comes from or is related to the second law of thermodynamics.

00:25.600 --> 00:32.520
And it's basically the idea that stuff in the universe matter in the universe naturally goes from a

00:32.520 --> 00:40.080
state of order to a state of disorder, and this means that we will all die in the universe, will decay

00:40.080 --> 00:40.800
into nothingness.

00:40.840 --> 00:45.920
Well, it's a rather grim, depressing thought, but unfortunately that is simply the truth.

00:46.440 --> 00:48.800
Now we are actually in deep learning.

00:48.800 --> 00:53.160
We're not concerned with this interpretation of entropy.

00:53.400 --> 01:00.040
Instead, we are going to be using a different kind of entropy, which is named after this gentleman

01:00.040 --> 01:01.360
here, Claude Shannon.

01:01.400 --> 01:09.040
He was a real genius and made a lot of important contributions to signal processing and computer science

01:09.040 --> 01:10.600
and cryptography.

01:11.280 --> 01:14.520
And Shannon entropy is a measure.

01:14.520 --> 01:23.040
It's a quantity that describes the amount of surprise or uncertainty that we have about a specific variable.

01:23.400 --> 01:29.320
And in particular, entropy is maximal at probabilities of 0.5.

01:29.320 --> 01:34.800
So this is when things when events are maximally uncertain or unpredictable.

01:35.160 --> 01:41.240
And as the events get more predictable, either that you know we are the probability goes down to zero.

01:41.240 --> 01:47.400
So this event never happens or the probability goes up to one that this event always happens, then

01:47.400 --> 01:50.360
that decreases the amount of entropy.

01:50.920 --> 01:56.040
So this is called information theoretic entropy or it's also called Shannon entropy.

01:56.360 --> 01:59.280
And it means that yeah everything is great and happy.

01:59.480 --> 02:03.470
And we're going to have great wonderful lives and that That's surprising.

02:03.470 --> 02:05.510
Things convey more information now.

02:05.550 --> 02:12.630
I also want to briefly discuss this term surprise, because the word surprise also has different meanings

02:12.630 --> 02:16.390
in different areas of language, different contexts.

02:16.750 --> 02:23.550
In the context of information theory, surprise means the amount of predictability or uncertainty.

02:23.830 --> 02:30.670
So we might think that, you know, if you flip a coin and it lands on heads 99 times, then you flip

02:30.670 --> 02:34.870
it 100th time and it lands on tails, that might be pretty surprising.

02:35.190 --> 02:41.870
But in terms of entropy, that's actually not a lot of surprise in the system altogether.

02:41.870 --> 02:49.430
That one event might be surprising to us humans, but the system of 99 heads and one tail actually has

02:49.430 --> 02:56.190
very little surprise the system altogether, because the probability of tails is very small.

02:56.190 --> 02:56.550
Actually.

02:56.550 --> 03:01.110
Now I forget if I was saying 99 heads or 99 tails, but I think you get the idea.

03:01.310 --> 03:08.590
So the idea is that entropy is maximal when probabilities are chance level and entropies are minimal

03:08.710 --> 03:16.310
as probabilities go to zero or to one, indicating that we have a lot of certainty, a lot of predictability

03:16.510 --> 03:18.590
about the particular event.

03:19.070 --> 03:21.750
Here is the formula for entropy.

03:21.950 --> 03:29.510
So we have three terms in this formula X which is the data values p which is the probability of those

03:29.510 --> 03:30.710
values occurring.

03:30.990 --> 03:32.830
And then we have the log.

03:33.230 --> 03:39.110
So we multiply the probability that an event occurs by the log of that same probability.

03:39.110 --> 03:42.030
So this term here is exactly the same as this term.

03:42.230 --> 03:49.470
We multiply the probability by the log of that probability for all of the different possible events.

03:49.630 --> 03:53.110
So for a coin flip this would be n would be two right.

03:53.150 --> 03:57.670
Because for a coin flip there are two possible outcomes heads and tails.

03:57.910 --> 04:05.630
So the probability of heads times the log of the probability of heads plus the the probability of tails

04:05.630 --> 04:09.990
times the log of the probability of tails, we sum those values together.

04:10.030 --> 04:16.950
If we take the minus sign and entropy is typically indicated using the capital letter H, the minus

04:16.950 --> 04:25.840
sign here is because probability values go between 0 and 1, and the log of A, a number between 0 and

04:25.840 --> 04:26.950
1 is negative.

04:26.990 --> 04:29.310
You saw this in the previous video.

04:29.670 --> 04:33.230
So this whole quantity actually ends up being negative.

04:33.230 --> 04:37.230
So we just flip it to be positive just facilitates the interpretation.

04:37.910 --> 04:40.150
So this is the formula for entropy.

04:40.190 --> 04:44.630
You are going to see this implemented in Python in a few moments.

04:45.110 --> 04:48.910
Now I just want to briefly discuss interpreting entropy.

04:49.150 --> 04:56.110
So a system a collection with high entropy means that the data set has a lot of variability.

04:56.230 --> 05:03.070
Likewise, or conversely, low entropy means that most of the values in the data set repeat and are

05:03.110 --> 05:04.110
therefore redundant.

05:04.110 --> 05:08.270
So there's less surprise, There's more predictability of the system.

05:08.670 --> 05:13.310
Now, I don't want to get too deep into a discussion of entropy versus variance.

05:13.310 --> 05:19.310
It's a little you know, the exact relationship between these two is a bit outside the context of this

05:19.310 --> 05:19.630
course.

05:19.630 --> 05:24.910
But just to mention this briefly, because these are two different measures of variability.

05:25.230 --> 05:32.510
Entropy is nonlinear and makes no assumptions about the distributions, whereas variance is more appropriate

05:32.510 --> 05:36.990
for a distribution that is roughly normally distributed.

05:37.150 --> 05:39.190
So here we use the log base two.

05:39.430 --> 05:45.590
In the previous video, I mentioned that the logarithm has several different bases and they give slightly

05:45.590 --> 05:46.470
different slopes.

05:46.470 --> 05:49.510
They look slightly different in the log space.

05:49.870 --> 05:55.790
So in in entropy, when computing entropy you can actually use various different kinds of logs.

05:55.790 --> 05:56.790
Log base two.

05:56.950 --> 05:58.510
The units are called bits.

05:58.630 --> 06:01.550
If it's the natural log, the units are called nats.

06:01.950 --> 06:06.700
But otherwise the concept is the same and the interpretation is the same.

06:07.300 --> 06:08.940
So that is for entropy.

06:08.980 --> 06:10.420
Here you see that formula again.

06:10.460 --> 06:11.820
It's just shortened a little bit.

06:11.820 --> 06:16.820
So instead of the probability of a particular value of x here I'm just writing p.

06:17.260 --> 06:23.420
So this formula here is for describing one probability distribution.

06:23.420 --> 06:24.700
So p p p.

06:24.940 --> 06:29.300
But we're also going to be computing something called cross entropy.

06:29.540 --> 06:34.900
And that is the entropy between two different probability distributions.

06:35.060 --> 06:37.220
So p times log of q.

06:37.540 --> 06:40.420
Instead of having the p in here we have q.

06:40.460 --> 06:46.660
So p is one probability distribution and q is a different probability distribution.

06:47.140 --> 06:53.900
Now in deep learning the way that we use cross entropy is to characterize the performance of the model.

06:53.900 --> 07:00.540
So we have P is going to be the probability that the picture is a cat.

07:00.540 --> 07:03.340
So we have pictures of cats and pictures of not cats.

07:03.580 --> 07:08.540
So probability p is going to be the probability that this picture is a cat.

07:08.780 --> 07:12.340
And q will be the probability that the model output.

07:12.340 --> 07:20.140
So the model is going to say I think that this is a picture of a cat with a probability of 0.5.

07:20.180 --> 07:20.460
Right.

07:20.460 --> 07:27.700
So that's you know, the model is just guessing if q equals 0.5 versus after training, we say, well,

07:27.740 --> 07:29.380
you know, the picture is still a cat.

07:29.380 --> 07:31.620
So probability of this being a cat.

07:31.900 --> 07:37.020
And then the model after training the model is going to learn to distinguish cats from not cats.

07:37.060 --> 07:40.660
So this value of Q is going to be something really large.

07:40.660 --> 07:42.340
Let's say 0.99.

07:42.780 --> 07:45.940
Now a picture is either a cat or not a cat.

07:45.940 --> 07:50.220
So in fact, in deep learning, events generally happen or they don't happen.

07:50.220 --> 07:56.260
So that means that the events in deep learning, when we are going to be computing cross entropy, these

07:56.260 --> 07:59.820
events are generally with probability 0 or 1.

07:59.820 --> 08:02.540
So this either is a cat or it isn't a cat.

08:02.580 --> 08:09.020
Of course the model output is a probability somewhere between 0 Zero and one.

08:09.020 --> 08:10.980
So this picture definitely is a cat.

08:10.980 --> 08:12.620
So this value will be one.

08:12.900 --> 08:18.980
And the model will predict that this is a cat with a probability of let's say 0.99.

08:19.460 --> 08:26.180
And you know from two videos ago that these probability values from the model, the model output are

08:26.180 --> 08:28.620
computed through the softmax function.

08:29.220 --> 08:29.780
All right.

08:29.780 --> 08:31.820
So let's switch to Python.

08:31.980 --> 08:39.380
I will show you how to compute the entropy and cross entropy in numpy and also in PyTorch.

08:41.300 --> 08:43.340
So we import numpy.

08:43.380 --> 08:45.900
I'm going to import PyTorch a little bit later.

08:46.300 --> 08:51.940
This is just a formula again to remind us the uh formula for entropy.

08:52.460 --> 08:54.820
Okay so here I have a bunch of empty cells.

08:54.820 --> 09:01.620
Please don't scroll down yet, because I would like us to critically determine the problem with this

09:01.620 --> 09:02.700
formula here.

09:03.220 --> 09:03.380
Okay.

09:03.420 --> 09:06.500
So here I have the probability of an event happening.

09:06.540 --> 09:08.020
So we say 0.25.

09:08.020 --> 09:12.010
So this event has a 25% chance of happening.

09:12.450 --> 09:16.010
Now when you look at this formula, this looks correct, right?

09:16.050 --> 09:17.730
This is p times log p.

09:17.930 --> 09:21.930
And that's what I have written here minus p times log p.

09:22.130 --> 09:24.970
So why is this the wrong formula.

09:24.970 --> 09:27.130
What is wrong with this formula?

09:27.130 --> 09:32.490
If you would like to pause the video and see if you can figure it out without scrolling down, without

09:32.490 --> 09:36.130
cheating and looking down, then please feel free to do so.

09:36.730 --> 09:43.890
So the problem here is that we are only computing the p times log p for one event.

09:44.210 --> 09:50.530
This is just the probability of the event happening, but there's also the probability of the event

09:50.570 --> 09:55.930
not happening, and the probability of the event not happening is one minus p.

09:55.970 --> 10:02.810
So the the let's call this q is equal to one minus p.

10:03.370 --> 10:10.610
So the probability if the probability is 25% then the probability of it not happening is 75%.

10:10.650 --> 10:17.610
So then we would need to sum over all of the values of x, not only the one value of x.

10:18.010 --> 10:18.370
Okay.

10:18.410 --> 10:20.690
So that's what we see here.

10:21.210 --> 10:26.770
So here we see the probability of this event occurring is 0.25.

10:26.810 --> 10:30.490
The probability of it not occurring is 0.75.

10:30.490 --> 10:34.210
And then we have to sum up all of those values.

10:34.410 --> 10:36.890
And the way I'm doing that is with this for loop.

10:36.890 --> 10:39.410
So I initialize h or entropy to be zero.

10:39.410 --> 10:41.810
And I say h equals itself.

10:41.930 --> 10:48.130
Now we could write you know it might be more sensible to do it like this which is also fine.

10:48.130 --> 10:53.610
So you could say h equals itself plus minus p times log p.

10:53.610 --> 10:56.690
And then I'm just moving the minus sign over here.

10:57.010 --> 10:57.290
Okay.

10:57.330 --> 11:03.410
So then this is the true entropy for an event with a probability of 0.25.

11:03.970 --> 11:05.290
Now this is in a for loop.

11:05.290 --> 11:08.890
This would be convenient for you know if we had multiple probabilities here.

11:09.290 --> 11:10.250
This is also correct.

11:10.250 --> 11:15.930
This is just explicitly written out for n equals two events.

11:15.930 --> 11:17.530
So we have a binary event.

11:17.530 --> 11:19.290
This is like the cat example.

11:19.290 --> 11:22.570
This picture is a cat or it isn't a cat.

11:22.810 --> 11:26.930
So the probability of the picture being a cat is p.

11:27.290 --> 11:32.650
Then that means that the probability that this picture is not a cat is one minus p.

11:32.970 --> 11:39.650
Likewise, when the model reports that, you know, I think the model says I the model, I think that

11:39.690 --> 11:44.690
the probability that this picture is a cat is 0.9.

11:45.250 --> 11:52.210
That means that the probability of this picture not being a cat is one -0.9, which is 0.1.

11:52.490 --> 11:52.690
Okay.

11:52.730 --> 11:57.090
So then we can run this code that's going to give us exactly the same answer.

11:57.730 --> 11:59.730
This is just written out explicitly.

11:59.730 --> 12:06.210
Now, the reason why I take the time to write this out and explain this is you will see this is a form

12:06.210 --> 12:11.290
that we are going to be using later on, called binary cross entropy.

12:11.330 --> 12:15.610
It's actually a really important loss function in deep learning.

12:15.610 --> 12:16.290
So I'll write it out.

12:16.290 --> 12:19.010
So binary cross entropy.

12:19.410 --> 12:22.970
And this is the formula for binary cross entropy.

12:22.970 --> 12:27.290
This is one of the most important loss functions in deep learning.

12:27.490 --> 12:33.610
In fact there's going to be an entire lecture later on just about this making sure that we understand

12:33.650 --> 12:34.010
this.

12:34.010 --> 12:40.370
And so in this video you see that it just comes from writing out the formula for entropy when there

12:40.370 --> 12:42.290
are exactly two events.

12:42.850 --> 12:43.210
All right.

12:43.250 --> 12:43.810
Very good.

12:44.290 --> 12:46.290
Now down to cross entropy.

12:46.450 --> 12:48.010
The formula is basically the same.

12:48.010 --> 12:49.770
We just need two sets of events.

12:49.930 --> 12:52.730
So this is the probability of the cat.

12:52.730 --> 12:55.970
It either you know the picture is a cat or isn't a cat.

12:56.090 --> 13:00.250
And this is the model saying that it is a cat or isn't a cat.

13:00.450 --> 13:07.810
And then you can see the formula is the same as above, but it's p times log q.

13:08.770 --> 13:10.210
So then we write that out.

13:10.290 --> 13:13.680
So here is the cross entropy between P and Q.

13:13.920 --> 13:20.520
Now, this is obvious when you think about it, but I wanted to mention this explicitly that these the

13:20.520 --> 13:24.320
two probability quantities here are not interchangeable.

13:24.320 --> 13:29.280
So p times log q is different from q times log p.

13:29.600 --> 13:31.440
So this was 1.38.

13:31.800 --> 13:38.800
And when I reverse them I actually get a warning message because that we're dividing by zero.

13:38.840 --> 13:41.720
The problem here is that we're trying to take the log of zero.

13:41.800 --> 13:44.880
Let me actually change this to be something very small.

13:45.000 --> 13:46.680
That's not exactly zero.

13:47.360 --> 13:50.720
So here we see the cross entropy is ten between these.

13:50.720 --> 13:56.080
So the point was just to show you that p times log p is different from q times log.

13:56.200 --> 14:00.240
Sorry p times log q is different from q times log p.

14:00.600 --> 14:02.200
And I'm confusing myself here.

14:02.360 --> 14:04.040
Let's make sure this works again okay.

14:04.080 --> 14:04.320
Yep.

14:04.360 --> 14:05.720
That's the expected answer.

14:06.080 --> 14:06.320
Okay.

14:06.360 --> 14:13.640
And then here again I'm just writing this whole expression out instead of a for loop because I know

14:13.640 --> 14:17.200
that we only have two possible events here.

14:17.440 --> 14:19.160
Present or absent.

14:19.560 --> 14:21.320
Is a cat or isn't a cat?

14:21.520 --> 14:24.320
We can write out this for loop explicitly.

14:24.480 --> 14:29.240
And this expression here is binary cross entropy.

14:29.680 --> 14:38.000
And now we can actually simplify this further because the second value of p here is exactly zero.

14:38.480 --> 14:40.800
So here we have the second value of p.

14:40.840 --> 14:41.600
This is zero.

14:41.600 --> 14:44.200
So that means this entire term goes to zero.

14:44.200 --> 14:46.040
It doesn't matter what Q is over here.

14:46.200 --> 14:50.000
Similarly this first value of p is one.

14:50.320 --> 14:52.000
So we can even just drop this term.

14:52.000 --> 15:00.080
So in fact when we have this special case where p is one and zero and there are only two events, then

15:00.120 --> 15:05.920
binary cross entropy actually just simplifies to minus the log of q.

15:06.360 --> 15:10.360
So this is just really simple algebraic simplification.

15:10.360 --> 15:15.160
And it's going to become important later on when you learn about loss functions.

15:15.520 --> 15:17.960
Okay, so far everything was done with numpy.

15:18.000 --> 15:23.160
I want to briefly introduce you to implementing this in PyTorch.

15:23.440 --> 15:26.960
So here we're using a submodule of PyTorch.

15:27.000 --> 15:33.680
It is torch for neural networks dot functional which just gives us access to a bunch of functions.

15:34.000 --> 15:36.400
Now this is rather long to type.

15:36.400 --> 15:44.320
So it is typical to import torch and functional using the abbreviation F capital F.

15:44.360 --> 15:47.600
This is a pretty typical way to code in PyTorch.

15:48.200 --> 15:51.640
Now the first thing I want to show you there's several things I want to show you here.

15:52.040 --> 15:55.440
First is the function is binary cross entropy.

15:55.600 --> 16:00.840
So we are going to be computing the cross entropy between two probabilities.

16:00.840 --> 16:02.680
That gives us the cross entropy.

16:02.680 --> 16:07.480
And then it's binary because there's only two possible options present or absent.

16:08.240 --> 16:15.360
Now the first thing I want to show you is that the PyTorch binary cross entropy function does not play

16:15.480 --> 16:17.080
well with numpy.

16:17.080 --> 16:21.120
So when we try to run this code, we get an error message.

16:21.320 --> 16:25.480
And the error is that it's basically having to do with lists.

16:25.480 --> 16:28.520
I said it doesn't play well with numpy, but I meant with lists.

16:28.760 --> 16:37.560
So what we have to do is convert these inputs, this list of values from a Python list into a PyTorch

16:37.600 --> 16:38.280
tensor.

16:38.520 --> 16:39.480
So that's what I do here.

16:39.480 --> 16:47.240
I'm just converting this list queue into a tensor and converting this list p also into a tensor okay

16:47.280 --> 16:49.760
so p tensor.

16:49.760 --> 16:52.320
And then this is q tensor like this.

16:52.680 --> 17:00.320
Now this works but we get a value of 75, which seems a little strange because it's exactly the same.

17:00.320 --> 17:06.160
I didn't run this code, but it's exactly the same as these numbers here where we got 1.3.

17:06.720 --> 17:14.920
Now, the issue here is that PyTorch is or this binary cross entropy function is sensitive to the order

17:14.920 --> 17:16.200
of the inputs.

17:16.200 --> 17:16.910
And here.

17:16.910 --> 17:21.790
This is a little confusing at first, but I think as we go throughout the course, as you learn more

17:21.790 --> 17:28.190
about building models and working with binary cross entropy to evaluate model performance, I think

17:28.190 --> 17:31.430
you will find that this becomes more intuitive.

17:31.750 --> 17:31.950
Okay.

17:31.990 --> 17:36.230
But so the point is we input q first and then p.

17:36.270 --> 17:39.750
So p is the here this corresponds to the category labels.

17:39.790 --> 17:41.590
You know is a cat or isn't a cat.

17:41.790 --> 17:49.630
And Q here corresponds to the model predicted probability that the picture is a cat or isn't a cat.

17:49.790 --> 17:53.230
And then we get the same answer that we got up here.

17:54.950 --> 18:00.790
So in this video I introduced you to entropy information theoretic or Shannon entropy.

18:01.070 --> 18:03.830
I showed you the formula how to interpret it.

18:03.830 --> 18:07.270
And we discussed the application of entropy in deep learning.

18:07.630 --> 18:11.750
Cross entropy is an important concept in deep learning.

18:11.750 --> 18:15.990
And we are going to revisit this topic many times throughout this course.

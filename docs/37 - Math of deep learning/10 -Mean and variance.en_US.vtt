WEBVTT

00:02.000 --> 00:08.120
You are probably already familiar with the terms and the concepts of mean and variance.

00:08.400 --> 00:11.360
In this video, I'm going to show you the formulas.

00:11.480 --> 00:17.640
We're going to discuss different measures of variability that will ultimately lead into things like

00:17.680 --> 00:21.720
L1 and L2 regularization in deep learning models.

00:21.880 --> 00:26.240
And I'm also going to show you how to implement these quantities in Python.

00:26.360 --> 00:31.800
And an important parameter for creating variance or defining the variance in Python.

00:31.960 --> 00:32.680
In numpy.

00:33.280 --> 00:33.480
Okay.

00:33.520 --> 00:35.520
So let's start by talking about the mean.

00:35.680 --> 00:40.040
The mean is the most commonly used measure of central tendency.

00:40.240 --> 00:45.040
And the idea of central tendency is to get one number one value.

00:45.200 --> 00:50.320
That tells us something about the concentration of the data in a distribution.

00:50.520 --> 00:54.800
So imagine these three lines here correspond to different distributions.

00:54.800 --> 00:58.120
These are histograms of data values that we've measured.

00:58.120 --> 01:04.130
Maybe these are distributions of height of different people in three different countries.

01:04.130 --> 01:08.530
Or maybe these are housing prices in different parts of the country or different cities.

01:09.170 --> 01:15.530
So what we want, as our measure of mean, is a value that tells us something about the center of each

01:15.530 --> 01:17.290
of these distributions.

01:18.370 --> 01:19.890
So that is the mean.

01:19.890 --> 01:22.890
It's also called the arithmetic mean or the average.

01:23.090 --> 01:25.290
The formula is fairly straightforward.

01:25.290 --> 01:28.410
We just take all of our data values x.

01:28.410 --> 01:32.290
So x would correspond to the sample of data that we have.

01:32.610 --> 01:35.090
We sum up all of the data values.

01:35.090 --> 01:38.410
And then we divide by the number of data values.

01:38.890 --> 01:45.490
And the mean is commonly indicated using a bar on top of the variable for the data.

01:45.490 --> 01:50.690
So x bar would indicate the mean or the average of x.

01:50.850 --> 01:57.830
Sometimes you also see the average indicated as the Greek letter mu or mu with a subscript of the variable

01:57.830 --> 01:58.950
name so mu x.

01:59.510 --> 02:03.550
So the mean is suitable for roughly normally distributed data.

02:03.550 --> 02:07.950
So data that follow a a more or less a Gaussian distribution.

02:08.430 --> 02:09.910
So here's a quick example.

02:10.110 --> 02:11.670
Um you have a collection of numbers.

02:11.670 --> 02:13.470
We can sum these up pretty quickly.

02:13.590 --> 02:14.750
These two is eight.

02:14.790 --> 02:15.870
This is two.

02:15.910 --> 02:17.910
And so eight plus two is ten.

02:17.950 --> 02:21.710
And then we have 12345 numbers in our collection.

02:21.710 --> 02:25.390
So ten divided by five gives us an average of two.

02:25.630 --> 02:27.310
Pretty straightforward okay.

02:27.350 --> 02:34.790
Here I want to show you that the mean is not necessarily always so indicative of central tendency for

02:34.790 --> 02:37.390
non-normally distributed data.

02:37.750 --> 02:42.270
So this dark blue case here, this is actually not the failure scenario.

02:42.310 --> 02:47.110
This is like the reference scenario where the mean is what we expect it to be.

02:47.470 --> 02:50.230
Here we have an interesting bimodal distribution.

02:50.350 --> 02:54.910
And the mean is actually the same as the mean of this Gaussian distribution.

02:55.110 --> 03:01.640
But the mean here of this, this cyan distribution, the mean doesn't actually characterize the central

03:01.640 --> 03:03.360
tendency of the distribution.

03:03.360 --> 03:09.200
Most of the data are clustered down here and up here, and the mean doesn't actually reflect that.

03:09.200 --> 03:11.400
So this is kind of a failure scenario.

03:11.800 --> 03:14.000
Here's another failure scenario by the way.

03:14.000 --> 03:18.880
That failure is an apology quotes here because the formula itself isn't failing.

03:19.280 --> 03:27.160
But the interpretation of the result of the formula is a little awkward given the context of the distribution.

03:27.600 --> 03:31.080
So here we have a strong right tailed distribution.

03:31.080 --> 03:34.320
So a non-normal non-Gaussian distribution.

03:34.440 --> 03:36.240
The mean is somewhere over here.

03:37.200 --> 03:42.200
But you can see that most of the data are actually concentrated far to the left of the mean.

03:42.360 --> 03:48.560
This is a typical distribution for for example, if you look at income or wealth, you see that in a

03:48.560 --> 03:54.770
country most people have, you know, small to moderate amounts of wealth and a very, you know, relatively

03:54.770 --> 03:57.370
small number of people have a huge amount of wealth.

03:57.490 --> 04:02.130
And that pulls up the mean in this case, you know, you could use something like the median.

04:02.130 --> 04:07.970
But a deeper discussion of different measures of central tendency is outside the scope here.

04:07.970 --> 04:14.170
Because in deep learning, we mostly just use the the mean to characterize the center of a distribution.

04:14.890 --> 04:15.170
Okay.

04:15.210 --> 04:17.730
So that's about the mean or the average.

04:17.890 --> 04:22.570
Now I want to talk a little bit more about the distribution or variance.

04:23.250 --> 04:27.210
So here we see two distributions that have exactly the same mean right.

04:27.250 --> 04:30.370
The center of these two distributions is the same.

04:30.370 --> 04:33.370
But obviously these are not identical distributions.

04:33.930 --> 04:37.090
They differ by the width of the distributions.

04:37.610 --> 04:40.690
And this is our measure of dispersion.

04:40.730 --> 04:47.130
The most commonly used measure of dispersion is variance which is basically the same thing as standard

04:47.130 --> 04:47.810
deviation.

04:47.810 --> 04:52.590
I'll talk about how variance and standard deviation are related to each other in a moment.

04:53.150 --> 04:55.470
So here is the formula for variance.

04:55.590 --> 05:01.750
You can see at first it looks a little bit similar to the formula for the mean where we're summing individual

05:01.750 --> 05:03.350
values and dividing by n.

05:03.470 --> 05:09.790
It's a little bit different from the formula for mean because we are subtracting the mean from each

05:09.830 --> 05:11.270
individual data value.

05:11.550 --> 05:14.830
So here is the individual data values.

05:14.870 --> 05:18.590
And we subtract the mean over all of the data values.

05:18.790 --> 05:22.070
And then we take that individual value and we square it.

05:22.190 --> 05:27.710
And then we sum all of these squared differences or distances to the mean.

05:28.190 --> 05:30.990
And then instead of dividing by n minus one sorry.

05:31.030 --> 05:34.950
Instead of dividing by n we divide by n minus one.

05:35.510 --> 05:38.710
I'm going to talk more about this formula in a moment.

05:38.710 --> 05:41.950
On the next slide I just want to give you a few examples.

05:42.230 --> 05:45.830
The variance is suitable for basically any distribution.

05:46.070 --> 05:46.910
Here's an example.

05:46.910 --> 05:48.670
We have a collection of numbers here.

05:48.670 --> 05:56.240
And so to compute the variance of this set of numbers, we need to first compute the mean or the average

05:56.240 --> 05:58.720
of this set, so we can do that quickly in our head.

05:58.720 --> 06:03.760
So this is eight plus zero plus four is 12 plus one is 13.

06:03.760 --> 06:07.520
Minus two is 11 plus seven is 18.

06:07.520 --> 06:09.120
And then we have six numbers.

06:09.120 --> 06:12.040
So that gives us an average value of three.

06:12.480 --> 06:15.320
So here I've written out actually this whole formula.

06:15.320 --> 06:19.320
So this is all of these numbers minus three.

06:19.320 --> 06:20.440
So minus the mean.

06:20.440 --> 06:21.720
You can see that part here.

06:21.880 --> 06:28.840
And then we square all of these differences between each individual data point and the mean.

06:29.080 --> 06:30.280
And then we sum.

06:30.440 --> 06:31.840
So that's this part here.

06:31.840 --> 06:36.720
And then here we divide by n minus one which in this case is five.

06:37.000 --> 06:40.040
And that gives us a variance of 16.

06:40.520 --> 06:43.440
Here's another example where I have different numbers.

06:43.440 --> 06:46.160
You can see these are closer together to each other.

06:46.360 --> 06:48.680
And here the variance is considerably smaller.

06:48.810 --> 06:49.810
It's two thirds.

06:49.970 --> 06:53.090
Now we can graph all of those numbers on the number line.

06:54.010 --> 06:55.610
So this was the first distribution.

06:55.610 --> 06:56.890
This was the second one.

06:57.050 --> 06:59.210
So you see these numbers here.

06:59.210 --> 07:03.650
This set of numbers has a larger variance than this set of numbers.

07:03.650 --> 07:08.210
And when we look at the graph we can see that the numbers are more distributed.

07:08.250 --> 07:15.410
There's a larger wider distribution of the numbers here compared to here where all the numbers are are

07:15.450 --> 07:17.490
really close to each other geometrically.

07:18.530 --> 07:18.850
Okay.

07:18.890 --> 07:20.890
So a few questions about this formula.

07:21.010 --> 07:22.610
Why do we mean center.

07:22.610 --> 07:25.410
Why don't we just square all of the individual values.

07:25.730 --> 07:27.010
Well it's a couple reasons.

07:27.170 --> 07:32.450
One is that we want the variance to indicate the dispersion around the average.

07:32.730 --> 07:39.010
And the reason for that is that we want these two data sets to have exactly the same variance.

07:39.010 --> 07:44.050
Because these are actually the same amount of variability in these two data sets.

07:44.090 --> 07:47.500
All I did was just add 100 to each element here.

07:47.500 --> 07:53.980
So we want these two data sets to have the same variance because they have the same overall dispersion.

07:54.500 --> 07:56.420
Why are the differences squared.

07:56.700 --> 07:58.900
There's also several answers to that.

07:58.900 --> 08:00.540
Several mathematical answers.

08:00.740 --> 08:03.260
Squaring values has some nice properties.

08:03.740 --> 08:07.660
It's also useful for for taking derivatives and calculus.

08:07.860 --> 08:09.100
It's useful geometrically.

08:09.140 --> 08:10.900
It has some nice geometric properties.

08:11.140 --> 08:15.420
But also part of the reason is that we want the distances to the average.

08:15.580 --> 08:19.020
And without squaring, the variance would basically always be zero.

08:19.780 --> 08:21.580
So here I have a little example.

08:21.620 --> 08:23.700
The same couple numbers from the previous slide.

08:24.100 --> 08:27.140
If we would apply this formula without squaring.

08:27.140 --> 08:31.460
So we just subtract the mean and sum up all the residual values.

08:31.460 --> 08:35.220
After subtracting the mean it trivially always goes to zero.

08:35.820 --> 08:39.740
So therefore we square to get rid of the negative numbers.

08:39.860 --> 08:44.900
Now you might be thinking hey Mike, why do we need to square if we need the distances and we want to

08:44.900 --> 08:50.080
get rid of the negative numbers because we want the magnitude of the difference is why don't we just

08:50.080 --> 08:52.640
take the absolute value instead of taking the mean?

08:53.000 --> 08:54.680
Well, that's also possible.

08:54.800 --> 08:56.440
That formula looks like this.

08:56.480 --> 09:01.000
It's almost exactly the same, but we take the absolute value instead of squaring.

09:01.480 --> 09:06.240
And this formula is called the mean absolute difference or mad.

09:06.480 --> 09:11.000
So a couple of key differences between variance and mean absolute difference.

09:11.240 --> 09:14.840
Squaring with variance emphasizes larger values.

09:15.320 --> 09:18.040
It has some nice properties for optimization.

09:18.040 --> 09:24.040
As I mentioned in calculus, it's continuous and differentiable in all cases.

09:24.520 --> 09:27.840
There's a bunch of other nice properties that I won't get into here.

09:28.080 --> 09:32.480
Um, the the variance is closer to r is the Euclidean distance.

09:32.600 --> 09:38.120
If the second moment of the distribution and is a tight link to least squares regression, there's some

09:38.120 --> 09:40.040
nice math that works out.

09:40.080 --> 09:44.890
But yeah, the mean absolute difference is also uh, Distance is also good.

09:45.330 --> 09:47.090
It's more robust to outliers.

09:47.610 --> 09:51.810
In statistics it's not as commonly used as the variance.

09:51.810 --> 09:55.250
But you will see this difference between these two formulas here.

09:55.370 --> 09:59.770
In this course we're going to be talking about L2 and L1 regularization.

09:59.810 --> 10:06.330
Turns out that L1 regularization is kind of equivalent to the mean absolute difference.

10:06.450 --> 10:10.290
And L2 regularization is kind of equivalent to variance.

10:10.370 --> 10:12.730
So more on that later in the course.

10:13.050 --> 10:16.610
I want to very briefly talk about standard deviation.

10:16.810 --> 10:18.610
This is a term you've probably heard before.

10:18.650 --> 10:23.850
The standard deviation is simply the square root, the principal square root of the positive square

10:23.850 --> 10:26.010
root of the variance.

10:26.010 --> 10:29.690
So variance is typically indicated as sigma squared.

10:29.690 --> 10:33.610
And that means that standard deviation is indicated as sigma.

10:34.090 --> 10:38.170
This formula is exactly the same as for variance.

10:38.170 --> 10:41.090
But we just put a standard deviation sign here.

10:42.020 --> 10:45.820
Sorry, I meant a square root sign here that gives us the standard deviation.

10:45.820 --> 10:53.180
And here, just as a just a real quick reminder that taking the square root of some number is equivalent

10:53.180 --> 10:57.980
to multiplying or taking that number to the power of one half.

10:59.820 --> 11:01.180
Here we are in Python.

11:01.180 --> 11:06.500
I'm going to show you quickly how to compute the mean and the variance in numpy.

11:06.980 --> 11:11.660
So here's a list of numbers I get the length of the list.

11:11.660 --> 11:13.580
So the number of numbers that we have.

11:14.020 --> 11:19.220
And then I compute the sum divided by n and numpy dot mean.

11:19.260 --> 11:22.500
So these should both be exactly the same quantities.

11:22.940 --> 11:28.100
Turns out that they're both pretty close to pi okay here I'm computing variance.

11:28.100 --> 11:31.620
So here I apply the numpy formula for variance.

11:31.620 --> 11:33.260
So np dot var.

11:33.900 --> 11:38.180
Here I'm implementing the formula exactly how I showed in the slides.

11:38.180 --> 11:42.640
So x Minus the mean of x square.

11:42.640 --> 11:50.480
All of those individual values sum all over all of these squared distances and then divide by n minus

11:50.480 --> 11:50.880
one.

11:51.080 --> 11:52.160
Then we print these out.

11:52.200 --> 11:54.120
Obviously these should both be the same.

11:54.600 --> 11:57.320
But they are not exactly the same.

11:57.320 --> 12:00.160
They're pretty close, but they are not exactly the same.

12:00.520 --> 12:02.040
So it turns out uh oh.

12:02.080 --> 12:10.920
You know, it turns out that there is an optional second input in the var function that specifies the

12:11.080 --> 12:16.440
degrees of freedom, which is essentially this number here that we are subtracting off.

12:16.920 --> 12:24.120
Now a deeper discussion of the difference between computing variance using n minus one or scaling by

12:24.120 --> 12:29.320
n minus one versus scaling by n is beyond the scope of this course.

12:29.320 --> 12:33.080
That's the sort of thing you would learn about more in a statistics course.

12:33.280 --> 12:38.000
For now, suffice it to say that this is called a biased variance.

12:38.000 --> 12:42.770
So if you just divide by n it's it's a biased measure of variance.

12:42.770 --> 12:46.010
And if you subtract one here you divide by n minus one.

12:46.010 --> 12:47.450
This is unbiased.

12:47.650 --> 12:50.050
So this removes a bias.

12:50.210 --> 12:52.490
This is also related to degrees of freedom.

12:52.730 --> 12:58.210
Essentially the the short version of the story is that because we already know the mean, we know that

12:58.210 --> 13:04.690
the average of all of these values here, I should say these values, we know the average of all of

13:04.690 --> 13:06.650
these values is zero.

13:07.050 --> 13:12.290
So that removes one degree of freedom from our our statistic.

13:12.450 --> 13:15.730
So we have to account for removing one degree of freedom.

13:15.730 --> 13:17.890
Otherwise we impose a bias.

13:17.890 --> 13:25.930
So I'm not sure why the default in Python is to have the denominator degrees of freedom to be zero,

13:26.330 --> 13:31.570
in my opinion, and the opinion of many other, um, statistics oriented people, that's, you know,

13:31.610 --> 13:35.690
it's a little unfortunate that that's the default, but it is what it is.

13:35.930 --> 13:40.460
So now we can compute the correct unbiased variance.

13:40.460 --> 13:44.660
And we see that that matches our formula from the slides here.

13:45.100 --> 13:45.300
Okay.

13:45.340 --> 13:51.540
Now I just want to show that it doesn't really matter so much whether you use the biased or unbiased

13:51.540 --> 13:52.260
variance.

13:52.420 --> 13:58.900
The difference of dividing by n minus one for relatively small data sets like this.

13:58.900 --> 14:02.340
This is, you know, only a handful of data values here.

14:02.340 --> 14:05.020
It does make a pretty significant difference.

14:05.020 --> 14:13.540
But as the data set gets larger and larger, like n equals 1000, then we see that the difference gets

14:13.540 --> 14:14.780
to be smaller and smaller.

14:14.780 --> 14:17.180
So here we have a difference of only 0.1.

14:17.460 --> 14:22.580
And you know when we work in deep learning we work with pretty large data sets.

14:22.620 --> 14:23.980
So here is an N.

14:24.100 --> 14:30.700
So we are working with a data set of 10,000 which is actually not even that big for deep learning.

14:31.420 --> 14:37.640
And you can see, you know you have to get out pretty far to see any difference between these two methods.

14:37.640 --> 14:43.040
So the conclusion here is that, yeah, you should be using when you're computing variance or standard

14:43.040 --> 14:46.880
deviation, you should be using the unbiased measure.

14:46.880 --> 14:52.400
But if you don't if you forget, you know, if you leave this parameter out, it really doesn't matter

14:52.520 --> 14:55.200
unless you're working with very small data sets.

14:57.480 --> 15:01.360
So in this video you learned about the mean also called the average.

15:01.560 --> 15:08.320
And we spent quite a bit of discussion about these measures of dispersion variance standard deviation

15:08.320 --> 15:10.120
and mean absolute difference.

15:10.160 --> 15:13.680
Again these are issues that are relevant for deep learning.

15:13.680 --> 15:17.240
In the context of normalization we have to normalize the data.

15:17.240 --> 15:19.520
We have to normalize the weights of the model.

15:19.680 --> 15:23.080
These are the quantities that we use for normalization.

15:23.240 --> 15:29.040
And you'll also see the difference between squaring versus taking the absolute value of the variance

15:29.040 --> 15:31.440
when we get to regularization.

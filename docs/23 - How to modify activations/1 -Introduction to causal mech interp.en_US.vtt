WEBVTT

00:02.040 --> 00:10.600
Welcome to this final part of the course, which is focused on methods of causal mechanistic interpretability.

00:11.280 --> 00:18.760
In this intro video, I will remind you of a few things that I mentioned before we started doing observational

00:19.080 --> 00:25.640
interp and also say a few things about the challenges in trying to establish causality.

00:26.280 --> 00:30.280
And yes, I will briefly mention a bridge in Amsterdam.

00:31.280 --> 00:38.400
This is a slide that I showed earlier in the course when I first introduced mechanistic interpretability

00:38.400 --> 00:39.760
as a field.

00:40.240 --> 00:48.880
This is just a brief reminder that the idea is to use data analysis methods to try to understand the

00:48.880 --> 00:54.440
operations and representations that these large language models learn.

00:55.040 --> 01:02.200
This kind of research is relevant for AI safety, and it's also just interesting to study as an example

01:02.200 --> 01:05.020
of a complex emergent system.

01:05.940 --> 01:11.340
And now here are a few more specific words about causal interpretability research.

01:11.860 --> 01:19.180
So the idea here is not just to analyze data extracted from a model, but to actually interfere with

01:19.180 --> 01:19.700
the model.

01:19.700 --> 01:27.620
So we can literally go inside the model and change specific patterns of activations in different parts

01:27.620 --> 01:35.100
of the model during a forward pass or while we are training the model in the rest of this course, I'm

01:35.100 --> 01:42.980
just going to focus on interfering with model activations during forward passes of already trained models.

01:43.820 --> 01:50.820
This causal approach has the same applications as observational, mechanistic, interpretability, and

01:50.820 --> 01:57.780
also allows for causally testing hypotheses about the roles of different parts of the model, or how

01:57.780 --> 02:03.100
different model activations lead to the output, logits and token selection.

02:03.100 --> 02:04.460
So language production.

02:05.220 --> 02:12.390
It turns out that although you do need to learn a few new code formulations, most of what you need

02:12.390 --> 02:18.710
to do for causally interfering with models is what you've already learned how to do in the previous

02:18.710 --> 02:19.550
sections.

02:20.190 --> 02:25.230
In particular, we will use hooks to access the internal workings of the model.

02:25.750 --> 02:32.150
We will define and use the hook functions in a slightly different way compared to what you did in the

02:32.150 --> 02:34.070
previous several sections.

02:34.230 --> 02:39.510
But it's actually pretty simple once you've already gotten to this point in the course.

02:40.790 --> 02:47.550
I will give more details about each of these specific methods as I introduce them over the next couple

02:47.590 --> 02:48.350
of sections.

02:48.350 --> 02:55.350
But just to say a few words here, generally about the challenges of causally interfering with language

02:55.350 --> 02:57.510
models during inference.

02:58.310 --> 03:05.470
As I just mentioned in the previous slide, it is actually surprisingly simple to manipulate models

03:05.470 --> 03:09.710
once you already know how to create and use an implant.

03:09.930 --> 03:10.930
hook functions.

03:11.810 --> 03:20.530
One thing that I really like about the causal approach is that it allows us to ask questions about counterfactuals.

03:21.490 --> 03:27.170
A counterfactual is asking the question what would happen if something had been different?

03:27.730 --> 03:31.890
You can ask these kinds of questions about your life and decisions you've made.

03:32.170 --> 03:36.730
For example, what would your life be like now if you hadn't enrolled in this course?

03:37.570 --> 03:41.010
Well, in real life we cannot run experiments like that.

03:41.010 --> 03:43.770
But with language models, we actually can.

03:44.250 --> 03:51.610
We can ask what token the model would predict if we had manipulated the context in different ways.

03:52.770 --> 03:58.890
There are three sources of difficulties in causal interpretability research.

03:59.970 --> 04:07.570
The first is that there are just so many moving parts inside a model, so many neurons and dimensions

04:07.570 --> 04:14.180
and attention heads and transformer blocks and so on, that there's just limitless possibilities for

04:14.180 --> 04:16.020
parts of the model to manipulate.

04:16.700 --> 04:22.780
And then you need to decide whether to zero out the activations, replace them with some other value

04:22.820 --> 04:24.660
like the average or the median.

04:25.260 --> 04:28.660
Add noise or replace entirely with noise.

04:29.180 --> 04:35.580
You can also take the activations or the embeddings vectors from one model and insert them into another

04:35.580 --> 04:36.140
model.

04:36.660 --> 04:44.060
There's just so many possibilities that it can be challenging to focus the analyses onto a very small

04:44.060 --> 04:46.380
number of key experiments to run.

04:47.540 --> 04:52.860
A second challenge is just a difficulty that is present in all complex systems.

04:53.380 --> 04:59.660
The representations and computations that take place inside a language model are distributed.

04:59.700 --> 05:06.500
They are multidimensional, possibly spanning multiple spatial scales, multiple different layers of

05:06.500 --> 05:07.180
the model.

05:08.180 --> 05:15.140
And there are also mechanisms that are specifically built into the model to compensate for disruptions.

05:15.880 --> 05:22.520
And part of that is the various layer normalizations, which can compensate for changes in activation

05:22.520 --> 05:24.840
magnitudes or distributional shifts.

05:25.320 --> 05:32.600
And also part of that is that the models have specifically been trained to be robust to aberrant patterns

05:32.600 --> 05:34.760
being injected into the neurons.

05:35.560 --> 05:39.360
Here I'm referring in particular to dropout regularization.

05:39.800 --> 05:46.600
So when you do dropout during training, you are literally just zeroing out certain parts of the model.

05:47.000 --> 05:51.680
And the model has to learn to deal with that and be robust to those manipulations.

05:52.440 --> 05:58.880
So then you do causal manipulations where you zero out some activation patterns, and the model has

05:58.880 --> 06:02.200
literally learned to deal with patterns like that during training.

06:03.320 --> 06:10.120
A third source of difficulty is the lack of ground truth to verify the results that you get.

06:10.600 --> 06:17.120
This is actually a bit better than the lack of ground truth with the observational methods, because

06:17.160 --> 06:24.290
at least with intervention experiments you can do like parametric manipulations, for example, manipulating

06:24.290 --> 06:29.490
the different heads in turn, or scaling activation patterns by different amounts.

06:30.650 --> 06:37.210
And of course, you can see how the causal manipulations impact the model predictions and the text that

06:37.210 --> 06:38.130
it generates.

06:38.770 --> 06:39.170
Okay.

06:39.330 --> 06:45.490
All that said, I don't mean to suggest that this kind of research is impossible or that progress cannot

06:45.490 --> 06:46.090
be made.

06:46.570 --> 06:49.250
The field is new and developing quickly.

06:49.530 --> 06:57.130
Experiments and theories, ideas, interpretations are all getting better, more precise and more sophisticated.

06:57.970 --> 07:04.890
But the good thing for you is that the foundational techniques are the same, and probably will be for

07:04.890 --> 07:05.730
a long time.

07:06.210 --> 07:12.410
This is good news because it means that even though the field is developing quickly, the techniques

07:12.410 --> 07:18.690
and the manipulations that you will learn in this section and what you have learned earlier in the course

07:19.130 --> 07:25.030
will continue to be relevant into the future even as the field continues to improve.

07:26.870 --> 07:29.110
I promised there would be a bridge.

07:29.230 --> 07:30.830
And here is the bridge.

07:31.430 --> 07:35.030
This is a picture of the Tinker Brook in Amsterdam.

07:35.190 --> 07:36.990
The word brook means bridge.

07:37.150 --> 07:39.830
And Tinker is a neighborhood in Amsterdam.

07:40.390 --> 07:47.470
This is one of the many bridges in Amsterdam that opens up to allow tall ships to pass through.

07:47.830 --> 07:51.470
Now, this double green structure on top of the bridge here.

07:51.470 --> 07:53.990
This is actually electrical wiring.

07:53.990 --> 07:56.470
It's part of the electrical grid.

07:56.950 --> 08:03.670
And that means that when the bridge opens to let ships pass through, the electrical grid is literally

08:03.670 --> 08:05.630
disconnected over here.

08:06.070 --> 08:08.630
So these wires are no longer touching.

08:08.630 --> 08:12.590
Electricity cannot flow through this node on the grid.

08:13.030 --> 08:18.510
But does this mean that this entire side of the city loses power every time the bridge opens?

08:18.950 --> 08:20.030
Obviously not.

08:20.270 --> 08:26.760
The electrical grid in Amsterdam is specifically engineered so that when there is a disruption in the

08:26.760 --> 08:28.680
flow of electricity somewhere.

08:29.160 --> 08:33.560
The rest of the network just reroutes and the electricity can still flow.

08:33.680 --> 08:36.440
And the final result is exactly the same.

08:37.200 --> 08:44.160
In other words, the local disruptions are completely compensated for by this network architecture.

08:45.000 --> 08:48.480
And this is something to think about with complex systems.

08:48.880 --> 08:55.960
Even though this bridge really is an important part of the electricity network, the grid itself is

08:55.960 --> 09:01.600
robust to manipulations or disruptions of individual parts of the network.

09:02.200 --> 09:04.560
And that's where I will close this video.

09:04.800 --> 09:11.680
Causal manipulations are important in science, but they can be tricky to interpret for multiple reasons.

09:11.680 --> 09:19.040
And so they should be seen as complementary methods along with the non-causal approaches that you are

09:19.040 --> 09:26.680
now familiar with, causal methods should not be seen as replacements that supersede observational methods.

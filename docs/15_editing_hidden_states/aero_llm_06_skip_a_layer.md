
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [15 editing hidden states](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Bá» qua má»™t Táº§ng Transformer (Skip a Layer)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y trÃ¬nh bÃ y ká»¹ thuáº­t bá» qua hoÃ n toÃ n má»™t Transformer Block trong residual stream cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM). Báº±ng cÃ¡ch sá»­ dá»¥ng cÆ¡ cháº¿ Forward Hook cá»±c ká»³ Ä‘Æ¡n giáº£n Ä‘á»ƒ gÃ¡n trá»±c tiáº¿p giÃ¡ trá»‹ Ä‘áº§u vÃ o (input) lÃ m Ä‘áº§u ra (output), chÃºng ta cÃ³ thá»ƒ lÃ m vÃ´ hiá»‡u hÃ³a má»i phÃ©p tÃ­nh toÃ¡n (Attention vÃ  MLP) bÃªn trong táº§ng Ä‘Ã³. NghiÃªn cá»©u thá»±c hiá»‡n kiá»ƒm chá»©ng thÃ´ng qua chá»‰ sá»‘ chuáº©n ma tráº­n (Matrix Norm), xÃ¡c nháº­n sá»± triá»‡t tiÃªu biáº¿n Ä‘á»•i tÃ­n hiá»‡u táº¡i táº§ng má»¥c tiÃªu. Máº·c dÃ¹ Ä‘Ã¢y lÃ  má»™t ká»¹ thuáº­t can thiá»‡p thÃ´ (Ablation), nÃ³ giÃºp cá»§ng cá»‘ hiá»ƒu biáº¿t vá» luá»“ng dá»¯ liá»‡u liÃªn tá»¥c giá»¯a cÃ¡c khá»‘i Transformer.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Trong kiáº¿n trÃºc Transformer, má»—i khá»‘i tÃ­nh toÃ¡n Ä‘Ã³ng vai trÃ² tinh chá»‰nh cÃ¡c vector Embeddings tá»« táº§ng trÆ°á»›c Ä‘Ã³. ThÃ´ng thÆ°á»ng, Ä‘áº§u ra cá»§a khá»‘i $T$ lÃ  Ä‘áº§u vÃ o cá»§a khá»‘i $T+1$. ThÃ­ nghiá»‡m nÃ y Ä‘áº·t má»¥c tiÃªu táº¡o ra má»™t "Ä‘Æ°á»ng táº¯t nhÃ¢n quáº£" (Causal shortcut), nÆ¡i khá»‘i $T+1$ váº«n thá»±c hiá»‡n tÃ­nh toÃ¡n nhÆ°ng káº¿t quáº£ cá»§a nÃ³ bá»‹ ghi Ä‘Ã¨ hoÃ n toÃ n bá»Ÿi giÃ¡ trá»‹ nguyÃªn báº£n cá»§a khá»‘i $T$. Äiá»u nÃ y tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i viá»‡c "cáº¯t bá»" má»™t pháº§n bá»™ nÃ£o cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ quan sÃ¡t sá»± Ä‘á»©t gÃ£y luá»“ng thÃ´ng tin.

---

## 2. Thiáº¿t Láº­p Ká»¹ Thuáº­t (Methodology)

### 2.1. HÃ m Hook Tá»‘i Giáº£n (The Minimalist Hook)
Sá»± can thiá»‡p Ä‘Æ°á»£c thá»±c hiá»‡n thÃ´ng qua má»™t hÃ m Hook khÃ´ng chá»©a logic phá»©c táº¡p:
```python
def skip_layer_hook(module, input, output):
    return input
- **CÆ¡ cháº¿:** HÃ m nÃ y bá» qua tham sá»‘ `output` (vá»‘n chá»©a cÃ¡c káº¿t quáº£ tÃ­nh toÃ¡n cá»§a Attention/MLP) vÃ  tráº£ vá» chÃ­nh tham sá»‘ `input`. Káº¿t quáº£ lÃ  khá»‘i tiáº¿p theo sáº½ nháº­n Ä‘Æ°á»£c dá»¯ liá»‡u y há»‡t nhÆ° khá»‘i trÆ°á»›c Ä‘Ã³, nhÆ° thá»ƒ khá»‘i hiá»‡n táº¡i chÆ°a bao giá» tá»“n táº¡i.

### 2.2. Chá»‰ sá»‘ Kiá»ƒm chá»©ng (Verification Metric)
Äá»ƒ xÃ¡c nháº­n táº§ng Ä‘Ã£ bá»‹ bá» qua, chÃºng ta Ä‘o lÆ°á»ng chuáº©n Frobenius cá»§a hiá»‡u sá»‘ Hidden States giá»¯a cÃ¡c táº§ng liÃªn tiáº¿p:

$$

\Delta_{norm} = \|\mathbf{H}_{L} - \mathbf{H}_{L-1}\|_F

$$

Náº¿u $\Delta_{norm} = 0$ táº¡i táº§ng $L$, Ä‘iá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  vector khÃ´ng há» thay Ä‘á»•i khi Ä‘i qua Transformer Block Ä‘Ã³.

---

## 3. Káº¿t Quáº£ Thá»±c Nghiá»‡m (Results & Analysis)

### 3.1. Triá»‡t tiÃªu Trung thá»±c (Faithful Ablation)
Káº¿t quáº£ Ä‘o lÆ°á»ng trÃªn GPT-2 vá»›i cÃ¢u máº«u: "There is a lot of liquid water on planet Earth" cho tháº¥y táº¡i táº§ng Ä‘Æ°á»£c cÃ i Hook (vÃ­ dá»¥ Layer 5), giÃ¡ trá»‹ chÃªnh lá»‡ch Norm rÆ¡i vá» chÃ­nh xÃ¡c báº±ng 0. 
- **LÆ°u Ã½ vá» Indexing:** Cáº§n ghi nhá»› sá»± khÃ¡c biá»‡t giá»¯a chá»‰ sá»‘ Transformer Block (báº¯t Ä‘áº§u tá»« 0) vÃ  chá»‰ sá»‘ Hidden States (báº¯t Ä‘áº§u tá»« 1, do táº§ng 0 lÃ  Embedding nguyÃªn báº£n). Sá»± lá»‡ch pha nÃ y pháº£n Ã¡nh cáº¥u trÃºc ná»™i táº¡i cá»§a thÆ° viá»‡n Hugging Face.

### 3.2. Can thiá»‡p ThÃ´ vs. Pháº«u thuáº­t (Chainsaw vs. Surgical Knife)
BÃ¡o cÃ¡o thá»«a nháº­n ráº±ng viá»‡c cáº¯t bá» toÃ n bá»™ má»™t táº§ng Transformer lÃ  má»™t can thiá»‡p mang tÃ­nh "há»§y diá»‡t" diá»‡n rá»™ng (Chainsaw ablation). NÃ³ khÃ´ng tinh vi báº±ng viá»‡c vÃ¡ hoáº¡t hÃ³a (Patching) hay can thiá»‡p vÃ o tá»«ng Attention Head cá»¥ thá»ƒ. Tuy nhiÃªn, phÆ°Æ¡ng phÃ¡p nÃ y ráº¥t há»¯u Ã­ch Ä‘á»ƒ:
1. Kiá»ƒm tra tÃ­nh dÆ° thá»«a (Redundancy) cá»§a má»™t sá»‘ táº§ng cá»¥ thá»ƒ.
2. XÃ¡c nháº­n luá»“ng residual stream hoáº¡t Ä‘á»™ng Ä‘Ãºng nhÆ° thiáº¿t káº¿ logic.

---

## 4. Káº¿t Luáº­n
Viá»‡c "bá» qua má»™t táº§ng" minh chá»©ng cho tÃ­nh linh hoáº¡t cá»§a Forward Hooks trong nghiÃªn cá»©u Diá»…n giáº£i cÆ¡ há»c nhÃ¢n quáº£. Máº·c dÃ¹ hiáº¿m khi Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° má»™t giáº£i phÃ¡p hiá»‡u chá»‰nh mÃ´ hÃ¬nh trong thá»±c táº¿, ká»¹ thuáº­t nÃ y cung cáº¥p má»™t cÃ´ng cá»¥ máº¡nh máº½ Ä‘á»ƒ hiá»ƒu vá» sá»± tÃ­ch tá»¥ thÃ´ng tin dá»c theo residual stream vÃ  vai trÃ² khÃ´ng thá»ƒ thay tháº¿ (hoáº·c cÃ³ thá»ƒ thay tháº¿) cá»§a tá»«ng khá»‘i Transformer riÃªng láº».

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. Thá»­ nghiá»‡m Skip Layer trÃªn GPT-2 dá»±a trÃªn tÃ i liá»‡u `aero_LLM_06_Skip a layer.md`. PhÃ¢n tÃ­ch Norm difference Ä‘á»ƒ xÃ¡c nháº­n sá»± triá»‡t tiÃªu biáº¿n Ä‘á»•i tÃ­n hiá»‡u.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [TÃ¡c Ä‘á»™ng Háº¡ nguá»“n cá»§a viá»‡c Thay Ä‘á»•i Quy mÃ´ Lá»›p sá»›m (Downstream Impact of Early Layer Scaling)](aero_llm_01_downstream_impact_of_early_layer_scaling.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_downstream_impact_of_early_layer_scaling.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Thay Ä‘á»•i Quy mÃ´ Hidden State vÃ  Tá»•n tháº¥t Token](aero_llm_02_codechallenge_hidden_state_scaling_and_token_loss.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_codechallenge_hidden_state_scaling_and_token_loss.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Dá»± Ä‘oÃ¡n BERT vá»›i Nhiá»…u vÃ  HoÃ¡n vá»‹ (Noisy and Shuffled BERT Predictions)](aero_llm_03_codechallenge_noisy_and_shuffled_bert_predictions.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_noisy_and_shuffled_bert_predictions.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äo lÆ°á»ng vÃ  Hiá»‡u chá»‰nh Äá»‹nh kiáº¿n Giá»›i trong BERT](aero_llm_04_codechallenge_measure_and_correct_bert_s_bias.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_measure_and_correct_bert_s_bias.md) |
| [VÃ¡ Hoáº¡t hÃ³a vÃ  TÃ¡c vá»¥ Nháº­n diá»‡n TÃ¢n ngá»¯ GiÃ¡n tiáº¿p (Activation Patching and Indirect Object Identification)](aero_llm_05_activation_patching_with_indirect_object_identification.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_activation_patching_with_indirect_object_identification.md) |
| ğŸ“Œ **[Bá» qua má»™t Táº§ng Transformer (Skip a Layer)](aero_llm_06_skip_a_layer.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_06_skip_a_layer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

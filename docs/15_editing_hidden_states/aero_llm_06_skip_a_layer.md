
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [15 editing hidden states](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Bá» qua má»™t Táº§ng Transformer (Skip a Layer)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y trÃ¬nh bÃ y ká»¹ thuáº­t bá» qua hoÃ n toÃ n má»™t Transformer Block trong residual stream cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM). Báº±ng cÃ¡ch sá»­ dá»¥ng cÆ¡ cháº¿ Forward Hook cá»±c ká»³ Ä‘Æ¡n giáº£n Ä‘á»ƒ gÃ¡n trá»±c tiáº¿p giÃ¡ trá»‹ Ä‘áº§u vÃ o (input) lÃ m Ä‘áº§u ra (output), chÃºng ta cÃ³ thá»ƒ lÃ m vÃ´ hiá»‡u hÃ³a má»i phÃ©p tÃ­nh toÃ¡n (Attention vÃ  MLP) bÃªn trong táº§ng Ä‘Ã³. NghiÃªn cá»©u thá»±c hiá»‡n kiá»ƒm chá»©ng thÃ´ng qua chá»‰ sá»‘ chuáº©n ma tráº­n (Matrix Norm), xÃ¡c nháº­n sá»± triá»‡t tiÃªu biáº¿n Ä‘á»•i tÃ­n hiá»‡u táº¡i táº§ng má»¥c tiÃªu. Máº·c dÃ¹ Ä‘Ã¢y lÃ  má»™t ká»¹ thuáº­t can thiá»‡p thÃ´ (Ablation), nÃ³ giÃºp cá»§ng cá»‘ hiá»ƒu biáº¿t vá» luá»“ng dá»¯ liá»‡u liÃªn tá»¥c giá»¯a cÃ¡c khá»‘i Transformer.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Trong kiáº¿n trÃºc Transformer, má»—i khá»‘i tÃ­nh toÃ¡n Ä‘Ã³ng vai trÃ² tinh chá»‰nh cÃ¡c vector Embeddings tá»« táº§ng trÆ°á»›c Ä‘Ã³. ThÃ´ng thÆ°á»ng, Ä‘áº§u ra cá»§a khá»‘i $T$ lÃ  Ä‘áº§u vÃ o cá»§a khá»‘i $T+1$. ThÃ­ nghiá»‡m nÃ y Ä‘áº·t má»¥c tiÃªu táº¡o ra má»™t "Ä‘Æ°á»ng táº¯t nhÃ¢n quáº£" (Causal shortcut), nÆ¡i khá»‘i $T+1$ váº«n thá»±c hiá»‡n tÃ­nh toÃ¡n nhÆ°ng káº¿t quáº£ cá»§a nÃ³ bá»‹ ghi Ä‘Ã¨ hoÃ n toÃ n bá»Ÿi giÃ¡ trá»‹ nguyÃªn báº£n cá»§a khá»‘i $T$. Äiá»u nÃ y tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i viá»‡c "cáº¯t bá»" má»™t pháº§n bá»™ nÃ£o cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ quan sÃ¡t sá»± Ä‘á»©t gÃ£y luá»“ng thÃ´ng tin.

---

## 2. Thiáº¿t Láº­p Ká»¹ Thuáº­t (Methodology)

### 2.1. HÃ m Hook Tá»‘i Giáº£n (The Minimalist Hook)
Sá»± can thiá»‡p Ä‘Æ°á»£c thá»±c hiá»‡n thÃ´ng qua má»™t hÃ m Hook khÃ´ng chá»©a logic phá»©c táº¡p:
```python
def skip_layer_hook(module, input, output):
    return input
- **CÆ¡ cháº¿:** HÃ m nÃ y bá» qua tham sá»‘ `output` (vá»‘n chá»©a cÃ¡c káº¿t quáº£ tÃ­nh toÃ¡n cá»§a Attention/MLP) vÃ  tráº£ vá» chÃ­nh tham sá»‘ `input`. Káº¿t quáº£ lÃ  khá»‘i tiáº¿p theo sáº½ nháº­n Ä‘Æ°á»£c dá»¯ liá»‡u y há»‡t nhÆ° khá»‘i trÆ°á»›c Ä‘Ã³, nhÆ° thá»ƒ khá»‘i hiá»‡n táº¡i chÆ°a bao giá» tá»“n táº¡i.

### 2.2. Chá»‰ sá»‘ Kiá»ƒm chá»©ng (Verification Metric)
Äá»ƒ xÃ¡c nháº­n táº§ng Ä‘Ã£ bá»‹ bá» qua, chÃºng ta Ä‘o lÆ°á»ng chuáº©n Frobenius cá»§a hiá»‡u sá»‘ Hidden States giá»¯a cÃ¡c táº§ng liÃªn tiáº¿p:

$$

$$

\Delta_{norm} = \|\mathbf{H}_{L} - \mathbf{H}_{L-1}\|_F

$$

$$

Náº¿u \Delta_{norm} = 0 táº¡i táº§ng L, Ä‘iá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  vector khÃ´ng há» thay Ä‘á»•i khi Ä‘i qua Transformer Block Ä‘Ã³.
WEBVTT

00:02.120 --> 00:05.640
This will be an interesting set of videos.

00:05.840 --> 00:13.080
We're going to manipulate attention heads similar to the previous couple of videos, but now, instead

00:13.080 --> 00:19.880
of just measuring the impact on the final output of the model, we are going to measure the impact on

00:19.880 --> 00:28.120
the cosine similarity of embeddings vectors in the hidden states or the outputs of the transformer blocks.

00:28.560 --> 00:35.520
That's an interesting way to assess whether the vector adjustments calculated by the attention heads

00:35.720 --> 00:41.360
are impacting the way that the token representations are related to each other.

00:42.120 --> 00:49.520
So the purpose of this video is for me to show you some code to set up and run a basic analysis, and

00:49.520 --> 00:54.960
then in the next video, you will expand on this concept in several ways.

00:55.760 --> 01:00.400
I will work with GPT two medium in this demo.

01:01.060 --> 01:03.820
Here is what the hook function looks like.

01:03.820 --> 01:06.260
Basically the same as what you've seen before.

01:06.620 --> 01:14.300
I'm hooking into the input layer of C proj, picking out one head and setting it to zero.

01:14.900 --> 01:22.500
Now in this video, I'm zeroing out all of the tokens in this head, and then in the next video you

01:22.500 --> 01:30.940
are going to isolate just one specific head based on a particular meaning of one token in the text.

01:31.380 --> 01:33.700
And what is that text that we will use here?

01:34.140 --> 01:39.460
It's this peanut butter and pineapple taste great on pizza.

01:40.060 --> 01:45.940
If you are Italian or an Italiano file then I promise this is just a joke.

01:45.940 --> 01:47.140
I'm just being silly.

01:47.140 --> 01:50.700
I'm not being serious at all, just trying to have some fun here.

01:51.340 --> 01:56.460
And if you are not Italian, then this sentence is absolutely true and I stand by it.

01:56.460 --> 02:00.020
And you should definitely try peanut butter and pineapple on your next pizza.

02:00.840 --> 02:09.840
Anyway, so the idea here is that we will calculate cosine similarity across all the pairs of tokens

02:10.040 --> 02:14.320
after zeroing out one of the heads in a layer.

02:15.000 --> 02:16.520
And why do we do that?

02:16.800 --> 02:24.720
Remember that the primary goal of the attention subblock is to integrate contextual information across

02:24.720 --> 02:26.960
the different tokens in a text.

02:27.600 --> 02:35.080
And so if that is the case, then silencing the attention heads should have some impact on the cosine

02:35.080 --> 02:39.920
similarity between the representations of the different tokens.

02:40.720 --> 02:45.120
So to measure that I'm going to run through a for loop that looks like this.

02:45.200 --> 02:49.080
So here's a for loop over all of the layers.

02:49.560 --> 02:58.720
Here I'm grabbing all of the data from one layer into a matrix that is tokens by embeddings.

02:59.200 --> 03:03.810
And I'm skipping the first token here in the sequence for reasons, you know.

03:04.170 --> 03:08.530
And here I'm also adding plus one to the layer index.

03:08.530 --> 03:11.610
I would like you to think about why I'm doing that.

03:11.650 --> 03:17.050
This is something that's come up several times in the course, but always good to have a reminder anyway.

03:17.090 --> 03:22.850
Of course I will discuss the answer to why we want this plus one when I switch to code, but something

03:22.850 --> 03:24.730
for you to think about before I get there.

03:25.490 --> 03:31.410
Anyway, I hope the rest of this code looks familiar to you from earlier in the course.

03:31.570 --> 03:38.250
So here in this line I normalize all of the embeddings vectors by their norm.

03:38.530 --> 03:45.850
And then I multiply that matrix by its transpose which gives me a matrix of cosine similarities.

03:46.530 --> 03:53.570
And then here I'm just grabbing the and storing all of the unique elements above the diagonal.

03:53.970 --> 03:59.870
Now if this code looks a little confusing at first, then don't worry, I will go through it again in

03:59.870 --> 04:05.350
the code demo to make sure you understand what's going on with all this indexing business.

04:06.430 --> 04:12.190
Now this variable keys is tokens by tokens up here.

04:12.310 --> 04:23.110
I create a variable called unique count, which I define as n by one times n by two divided by two.

04:23.390 --> 04:27.030
What does this formula mean and what does it correspond to?

04:27.670 --> 04:33.590
Again, I will discuss the answer to that question when I switch to code, but I want to give you an

04:33.590 --> 04:36.030
opportunity to think about it first.

04:36.990 --> 04:37.430
Okay.

04:37.670 --> 04:45.430
Once I get all of the cosine similarities across all of the token pairs for each layer, I will then

04:45.430 --> 04:49.550
repeat this code, but silencing one head from each layer.

04:50.030 --> 04:52.710
I'm not going to loop through all of the attention heads.

04:52.710 --> 04:56.430
That's something you will do in the code challenge in the next video.

04:56.830 --> 05:01.490
So here I just fixed the head to silence at one.

05:01.490 --> 05:02.450
So index one.

05:03.010 --> 05:05.090
And here you see those results.

05:05.210 --> 05:08.330
So let me first direct your attention to this plot.

05:08.450 --> 05:17.250
On the left we have the transformer block on the x axis, cosine similarity strength on the y axis.

05:17.850 --> 05:26.250
And each little square here corresponds to the cosine similarity between one of the token pairs.

05:26.930 --> 05:29.210
So this is in the clean model.

05:29.210 --> 05:31.170
So there's no manipulations in here.

05:31.170 --> 05:32.250
There's no zeroing out.

05:32.250 --> 05:36.650
We haven't done anything to the model at this plot over here.

05:37.250 --> 05:43.410
So it's pretty neat to see that the cosine similarities generally decrease as we get deeper into the

05:43.410 --> 05:43.890
model.

05:43.890 --> 05:49.650
And then they suddenly start to increase, going up to a ceiling of one as we work our way towards the

05:49.650 --> 05:51.170
end of the model.

05:51.650 --> 05:57.970
So it looks like the model is kind of decoupling these representations of different tokens as it's trying

05:58.150 --> 06:04.910
to load context and figure out what is the relationship between, for example, peanut butter and pineapple

06:04.910 --> 06:05.670
and pizza.

06:06.190 --> 06:11.110
And then towards the end of the model, it starts to converge onto a prediction.

06:11.110 --> 06:14.070
So those embeddings vectors also start to converge.

06:15.110 --> 06:22.790
Anyway, this middle panel shows the same analysis, but with silencing one of the attention heads.

06:23.190 --> 06:29.350
Now at first pass visual inspection, it doesn't really seem like there are huge differences between

06:29.350 --> 06:30.710
these two panels.

06:31.310 --> 06:32.470
That's to be expected.

06:32.470 --> 06:37.750
Zeroing out one attention head is not a gigantic manipulation.

06:38.350 --> 06:45.590
And here in the third plot, you can see the differences between the clean and the silenced models for

06:45.990 --> 06:47.390
cosine similarity.

06:48.030 --> 06:56.230
I've ordered the subtraction here such that positive values in this difference score mean stronger cosine

06:56.230 --> 07:04.650
similarity after silencing and negative values mean weaker cosine similarity after silencing one of

07:04.650 --> 07:05.730
the attention heads.

07:05.930 --> 07:12.690
I also ran a t test across these differences for all of the token pairs against zero.

07:12.730 --> 07:17.570
That's the horizontal line here where the T test was non-significant.

07:17.570 --> 07:20.730
There are red x's and where it was significant.

07:20.770 --> 07:22.930
You see these triangle markers.

07:23.370 --> 07:31.330
So the effect size is in general quite small around an order of magnitude smaller than the overall magnitude

07:31.330 --> 07:33.690
of the cosine similarity.

07:34.050 --> 07:41.690
In other words, the impact of ablating one head is generally quite small, but in many cases it is

07:41.690 --> 07:45.050
consistent across the different token pairs.

07:45.970 --> 07:52.450
Interestingly, it also seems like the direction of the effect changes over the different layers.

07:52.450 --> 07:59.060
So sometimes silencing the head promotes coupling of the embeddings vectors, and for other layers it

07:59.060 --> 08:00.380
promotes decoupling.

08:00.820 --> 08:09.180
So in this figure here, I silenced one head in each layer and then measured cosine similarity of the

08:09.180 --> 08:12.220
transformer outputs in that same letter.

08:12.900 --> 08:20.900
The next analysis I did was only to silence exactly one head only in layer three, but then measure

08:20.900 --> 08:25.300
the cosine similarities at all of the other transformer blocks.

08:25.780 --> 08:33.660
In other words, only one head in the entire model was silenced and it was always one head in layer

08:33.660 --> 08:34.260
three.

08:34.820 --> 08:41.740
So no surprise, there is no difference between the silenced and the clean cosine similarities before

08:41.780 --> 08:46.260
layer three, because that was before I manipulated the attention head.

08:46.660 --> 08:52.580
And a lot of these layers are not statistically significantly different from zero in terms of their

08:52.580 --> 08:54.660
change from silence versus clean.

08:55.060 --> 09:02.160
But actually towards the end you do see some consistent impact of silencing that one head all the way

09:02.160 --> 09:09.800
back in layer three on inter token cosine similarities going out towards the end of the model.

09:10.600 --> 09:17.360
On the other hand, the effect size now is really tiny here, so it's now just two orders of magnitude

09:17.360 --> 09:20.400
smaller than the overall cosine similarities.

09:20.880 --> 09:24.280
And also this is just one example sentence.

09:24.280 --> 09:28.040
So I certainly don't want to over interpret this result.

09:28.200 --> 09:34.600
But it is pretty interesting that such a tiny manipulation so early on in the model has these ripple

09:34.600 --> 09:37.400
effects throughout the rest of the transformer.

09:38.680 --> 09:43.280
Anyway, let's now switch to code and walk through this demo in more detail.

09:45.040 --> 09:48.840
Import some libraries, import GPT two medium.

09:49.080 --> 09:55.640
And I'm actually just working on the CPU here because yeah, we're not doing a lot of forward passes,

09:55.640 --> 09:57.620
so we don't really need the GPU here.

09:57.820 --> 10:03.980
Just as a reminder, there are 24 transformer blocks in GPT two medium.

10:04.380 --> 10:08.180
Here I'm defining some useful variables this you've seen before.

10:08.740 --> 10:12.140
So just a way to have these shorter variables.

10:12.140 --> 10:15.500
Instead of having to write out all of this business each time.

10:16.020 --> 10:19.260
Okay, here is the hook that I am implanting.

10:19.580 --> 10:21.340
So only for one layer.

10:21.340 --> 10:23.020
Actually let me start down here.

10:23.020 --> 10:28.260
So I'm implanting this hook into the C proj layer of attention.

10:28.260 --> 10:32.860
So the projection component of the attention layer.

10:33.260 --> 10:40.460
And this is a pre hook which means I'm getting all of the activations before the linear mixing matrix

10:40.460 --> 10:43.700
which is the main purpose of C proj okay.

10:43.740 --> 10:47.220
So now and then yeah this goes into all of the layers.

10:47.620 --> 10:54.140
And then actually it turns out that in most of the layers this will do absolutely nothing except check

10:54.140 --> 11:01.080
for this conditional which is going to be false for all but one of the transformer blocks.

11:01.480 --> 11:08.800
But when this is true, this variable that I will create in the experiments later on, when this layer

11:08.840 --> 11:16.800
number where this hook is implanted into equals that number, that variable that I specify, then I

11:16.840 --> 11:24.440
grab the input into the model and reshape it so that I can easily index the attention heads.

11:24.760 --> 11:26.600
And then here I'm ablating.

11:26.600 --> 11:31.680
So I'm zeroing out the attention head just for this one head.

11:31.840 --> 11:40.080
And yeah, all of the dimensions in this head, all of the tokens in the text and all of the sequences

11:40.080 --> 11:40.600
in the batch.

11:40.600 --> 11:48.600
Although here is just one sequence, so batch size of one here I reshape it back again and then output

11:48.600 --> 11:57.900
the input so that this modified input is what continues to be processed and multiplied and linearly

11:57.900 --> 12:04.740
mixed by the W0 matrix before being sent out of the attention subblock, and then provided as input

12:04.740 --> 12:07.860
into the MLP Subblock.

12:08.180 --> 12:10.980
Okay, so run that cell and uh.

12:10.980 --> 12:12.340
Yeah, just a joke, guys.

12:12.340 --> 12:13.740
I'm just having fun here.

12:13.860 --> 12:16.580
Peanut butter and pineapple tastes great on pizza.

12:16.940 --> 12:20.340
I have no strong opinion about pizza.

12:20.380 --> 12:23.180
A pineapple on pizza that I'm willing to admit.

12:23.500 --> 12:28.380
Uh, I have never in my entire life had peanut butter on pizza.

12:28.740 --> 12:31.900
But, you know, I think it's good to be open minded.

12:32.100 --> 12:33.620
Anyway, uh, okay.

12:33.660 --> 12:41.740
So interesting to see that all of the words in this sentence are single token words, except for the

12:41.740 --> 12:45.420
very first word, which is P, innit?

12:45.420 --> 12:49.900
So, uh, that got split up and I think it just got split up because of the spacing.

12:49.900 --> 12:52.540
Oh no, it's still actually even space peanut.

12:52.540 --> 12:54.780
Maybe peanut with the lowercase.

12:54.990 --> 12:55.470
Okay.

12:55.750 --> 13:02.150
So space lowercase p that actually does go into a single, uh, token there.

13:02.430 --> 13:06.710
But this is how I am going to continue processing this.

13:07.110 --> 13:07.430
Okay.

13:07.470 --> 13:12.390
So peanut butter and pineapple I'm not going to repeat that out loud too much okay.

13:12.430 --> 13:19.110
Moving right along here I initialize this variable layer to silence to be 1000.

13:19.550 --> 13:24.910
That means that that conditional in the beginning of the hook will never be true.

13:25.110 --> 13:32.390
And that means that when I run this model, none of those hooking functions will ever activate, which

13:32.390 --> 13:35.110
means nothing is going to be ablated.

13:35.110 --> 13:41.550
This model is a clean version of the model and it's not going to do anything.

13:41.750 --> 13:43.670
We're not manipulating anything here.

13:44.270 --> 13:44.510
Okay.

13:44.550 --> 13:46.910
And then I'm getting the hidden states here.

13:46.910 --> 13:53.870
By the way, I forgot to mention this earlier, but just as also a reminder, notice that I'm not actually

13:53.870 --> 13:56.730
We are exporting any data from this hook.

13:56.730 --> 14:04.490
I'm not saving any of the data, because what I want to analyze in this code demo is not the inner workings

14:04.490 --> 14:05.130
of attention.

14:05.130 --> 14:12.290
I want to manipulate that, but what I want to analyze are the outputs of the entire transformer block,

14:12.610 --> 14:19.530
which is the hidden states that we can get just by using this optional input here.

14:20.650 --> 14:21.050
Okay.

14:21.170 --> 14:30.130
So, uh, one sequence in the batch nine tokens and 1024 embeddings dimensions for GPT two medium.

14:30.690 --> 14:30.970
Okay.

14:31.010 --> 14:38.490
And then here is this piece of code here where I am looping over all of the layers, uh, extracting

14:38.490 --> 14:45.570
the data, excluding the first token, for reasons I've said a billion times earlier in this course.

14:46.010 --> 14:46.490
Uh, yeah.

14:46.530 --> 14:52.250
And then basically just calculating a cosine similarity matrix and then isolating it.

14:52.290 --> 14:52.570
Okay.

14:52.610 --> 14:55.150
So what is all of this code doing here?

14:55.470 --> 14:58.350
So here I have actually let me I can show you this.

14:58.350 --> 15:03.870
So I have CSS and sorry what I want to show is CSS mat.

15:04.150 --> 15:04.630
Okay.

15:04.990 --> 15:07.910
This is a symmetric matrix.

15:07.910 --> 15:12.390
So all the elements below the diagonal equal all the elements above the diagonal.

15:12.830 --> 15:19.510
And the diagonal elements themselves are all one because every vector is perfectly correlated or perfectly,

15:19.710 --> 15:22.550
uh, cosine similar to itself as.

15:22.790 --> 15:25.150
So it gets a value of one on the diagonal.

15:25.630 --> 15:33.510
Now what we want is to extract all of the unique elements in this matrix, and that is all of the elements

15:33.510 --> 15:35.470
above the diagonal.

15:35.750 --> 15:39.830
And so to get that then I use this statement over here.

15:40.310 --> 15:43.110
So I get the triangular upper.

15:43.470 --> 15:47.710
And then I have the one here means exclude the diagonal.

15:47.710 --> 15:50.750
So you can see here the diagonals are still included.

15:51.350 --> 15:53.510
Uh and then here we get above the diagonal.

15:53.570 --> 16:00.690
And now I'm just saying where these values are not equal to zero and then just converting them into

16:00.730 --> 16:01.330
numpy.

16:01.770 --> 16:02.250
Okay.

16:02.410 --> 16:05.970
That is all a reminder of what you learned earlier in the course.

16:05.970 --> 16:10.770
But I know it was a long time ago in the course, so I thought it would be nice to explain all this

16:10.770 --> 16:11.210
again.

16:12.290 --> 16:15.330
Now, what is the size of this matrix?

16:15.330 --> 16:19.010
The size of this matrix is layers by something.

16:19.290 --> 16:24.730
So we know the layers come from the number of layers in the model, the number of transformer blocks.

16:24.930 --> 16:29.530
The question is how many unique elements are there in this matrix.

16:30.250 --> 16:37.250
Well, there is a formula for the number of unique elements in a symmetric matrix.

16:37.250 --> 16:45.810
And that formula is m times m minus one divided by two.

16:45.810 --> 16:47.530
And where does this formula come from.

16:47.530 --> 16:54.700
So m for a square symmetric matrix this would be the number of rows or the number of columns, and basically

16:54.700 --> 17:00.540
the total number of elements in a matrix is m squared, so m times m.

17:00.740 --> 17:06.100
But we want to exclude the diagonal because that is trivially equal to one.

17:06.100 --> 17:07.500
We don't care about that.

17:07.540 --> 17:12.980
And then we divide by two to get either the upper triangle or the lower triangle okay.

17:13.020 --> 17:16.580
And that is the explanation of this formula over here.

17:16.900 --> 17:21.060
Now why is it tokens minus one times tokens minus two.

17:21.340 --> 17:24.260
If I just said that it was m times m minus one.

17:24.700 --> 17:29.500
The reason for that is that I'm excluding the first token over here.

17:29.500 --> 17:36.060
So the size of this matrix is actually tokens minus one by tokens minus one.

17:36.060 --> 17:40.740
Because I'm skipping the very first token in this sequence.

17:41.300 --> 17:41.620
Okay.

17:41.660 --> 17:45.660
And then I had one more question about this plus one here and here.

17:45.660 --> 17:53.880
The answer is that remember that the hidden states that get output from the model when we request output,

17:53.920 --> 17:55.320
hidden states equals true.

17:55.760 --> 18:01.920
That means all of the transformer blocks plus the embeddings layer.

18:02.160 --> 18:04.240
But that is the first one.

18:04.240 --> 18:08.920
So if we just want the transformer blocks then we always need to index these by plus one.

18:09.640 --> 18:10.120
Okay.

18:10.160 --> 18:13.640
So all of that was just a lot of review.

18:13.640 --> 18:16.360
But I think it's a good to have that review.

18:16.840 --> 18:17.160
Okay.

18:17.200 --> 18:21.560
So now I am going to actually silence one of the heads.

18:21.560 --> 18:24.240
So I choose layer five here as a demo.

18:24.680 --> 18:28.400
So I'm going to zero out head index one.

18:28.400 --> 18:32.920
So the second head in layer five run the model through again.

18:32.960 --> 18:36.760
And now I give this output variable a different name out.

18:36.800 --> 18:37.560
Silence.

18:37.720 --> 18:40.520
And here I'm calculating the cosine similarities.

18:40.560 --> 18:42.840
Again not much to see here.

18:42.880 --> 18:45.360
This is just the actual numbers okay.

18:45.400 --> 18:48.400
So this was just a code demo of how this works.

18:48.680 --> 18:52.980
So now what I'm going to do is run the first of two experiments.

18:52.980 --> 19:00.780
In this code demo, I am looping over all of the layers, all the transformer blocks in this model,

19:00.780 --> 19:08.940
and notice that the looping index variable is layer to silence, which means that each iteration through

19:08.940 --> 19:14.580
this for loop, we are going to be silencing head number one from each layer.

19:14.580 --> 19:21.220
In turn, run through the model so forward pass cosine similarities and extract the unique elements

19:21.700 --> 19:23.820
exactly as you have seen before.

19:24.020 --> 19:26.820
And I just have that stored in this variable here.

19:27.420 --> 19:33.220
Cosine similarity underscore manip for manipulation okay.

19:33.260 --> 19:35.460
So this takes nine seconds.

19:35.460 --> 19:40.340
I think if you put this model on the GPU it would probably take less than one second.

19:40.340 --> 19:44.220
But I don't mind waiting eight seconds here okay.

19:44.260 --> 19:46.140
And then here I'm doing some plotting.

19:46.140 --> 19:47.660
So let me show you the plots first.

19:47.660 --> 19:50.260
And then I will briefly walk through the code.

19:50.660 --> 19:50.940
Okay.

19:50.980 --> 19:52.200
So you've seen this before.

19:52.240 --> 19:59.000
These are the cosine similarities from the clean model and from the layer specific silencing version

19:59.000 --> 19:59.840
of the model.

20:00.240 --> 20:02.360
And then here we have their differences.

20:02.560 --> 20:07.440
So to generate this plot I'm looping again over layers.

20:07.440 --> 20:13.000
And that's just because I want to have unique color values for each of the layers.

20:13.000 --> 20:16.040
So these these are the x coordinates here.

20:16.040 --> 20:22.000
These are the y axis values for clean and manipulated versions of the model.

20:22.120 --> 20:29.840
And then I set the model face the marker face color to be a plasma and a scaled version of plasma.

20:30.000 --> 20:35.640
And that's how I get the color to go from deep purple to this light yellow color over here as we go

20:35.640 --> 20:37.280
through the transformer blocks.

20:37.480 --> 20:42.320
And then to get these differences over here that you see here.

20:42.320 --> 20:44.440
So I'm running a t test.

20:44.840 --> 20:46.360
I'm calling this a one sample.

20:46.400 --> 20:50.120
Technically this is a paired samples t test.

20:50.280 --> 20:57.940
But it turns out that a paired samples t test is equivalent to a one sample t test on the difference.

20:58.060 --> 20:59.100
So this.

20:59.180 --> 21:01.420
I find this a little more intuitive.

21:01.540 --> 21:02.300
It doesn't matter.

21:02.340 --> 21:03.300
You could also use.

21:03.300 --> 21:08.700
By the way you could also do this t test rel and then comma this.

21:08.740 --> 21:11.860
So you're providing these two inputs and then you don't get a zero.

21:11.900 --> 21:12.780
I think that's correct.

21:12.820 --> 21:13.900
Yeah that's correct.

21:14.300 --> 21:16.300
It's all the same mathematically it's fine.

21:16.660 --> 21:17.020
Okay.

21:17.060 --> 21:20.020
So manipulated versus the clean version.

21:20.260 --> 21:28.500
And here I'm just saying that if the p value is less than 0.05, then show the colors with a triangle.

21:28.500 --> 21:37.060
And if it's greater than 0.05, then you can show a red x for the non-significant layers here.

21:37.380 --> 21:41.740
Now technically we probably should be controlling for multiple comparisons.

21:41.740 --> 21:43.180
Maybe I'll even add that in here.

21:43.540 --> 21:51.830
So if this is less than 0.05 divided by n layers, and let's see if that changes a little bit.

21:51.870 --> 21:52.190
Yeah.

21:52.190 --> 21:58.670
So I made the threshold a little bit more conservative to correct for the fact that I'm running 24 t

21:58.670 --> 22:01.350
tests over 24 transformer blocks.

22:01.510 --> 22:05.150
And yeah, overall the patterns are still pretty significant.

22:06.230 --> 22:09.950
So it's interesting to note because the effect sizes are quite small.

22:09.950 --> 22:12.830
So this is a subtle manipulation.

22:12.990 --> 22:14.950
But it does have an impact.

22:15.550 --> 22:16.030
Okay.

22:16.190 --> 22:20.230
So now for the last demo that I want to run here.

22:20.510 --> 22:24.030
Here I'm just picking one layer uh, kind of at random.

22:24.030 --> 22:29.750
Although it's interesting that this turns out to be a layer that is not statistically significant here.

22:30.110 --> 22:34.910
And still I'm going to manipulate one head in just that layer.

22:35.310 --> 22:35.630
Okay.

22:35.670 --> 22:42.830
So now we only need one forward pass because we're only, uh, silencing one head in one layer.

22:43.150 --> 22:49.190
And then I am getting all of the hidden states from all of the layers in total, even the ones that

22:49.190 --> 22:50.890
I did not manipulate.

22:50.890 --> 22:51.370
And then.

22:51.410 --> 22:54.050
Yeah, otherwise the rest of this code is the same.

22:54.050 --> 22:56.890
The rest of this plotting code is also the same.

22:57.250 --> 23:03.370
And yeah, it's pretty remarkable that even though this manipulation was pretty subtle, it happened

23:03.370 --> 23:04.730
early in the model.

23:04.930 --> 23:12.290
It seems like it has a ripple effect that continues all the way later into the model.

23:13.570 --> 23:22.530
Complex systems are remarkable in that small and seemingly inconsequential manipulations can be rippled

23:22.530 --> 23:24.930
throughout the rest of the system.

23:25.690 --> 23:32.530
I think that a better characterization of these kinds of deep learning models will come from thinking

23:32.530 --> 23:37.930
about how very small things in the model relate to very big things in the model.

23:38.730 --> 23:42.130
Anyway, I hope you found this code demo to be thought provoking.

23:42.570 --> 23:49.570
In the next code challenge, you are going to continue working with this concept and with these methods.

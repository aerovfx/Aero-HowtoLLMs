WEBVTT

00:02.200 --> 00:06.760
This code challenge directly follows from the previous video.

00:07.280 --> 00:15.040
In fact, this code challenge starts by basically copying and then extending what we did in the previous

00:15.040 --> 00:22.280
video to explore what happens when we manipulate the attention heads across all the layers of the model,

00:22.680 --> 00:29.400
whether it matters which tokens we ablate and whether it matters if we set the attention head value

00:29.400 --> 00:32.320
to zero or some other value.

00:32.720 --> 00:34.040
So there's a lot to do here.

00:34.080 --> 00:35.360
Let's get started.

00:36.080 --> 00:38.080
A lot of exercise.

00:38.080 --> 00:43.640
One can be solved by copying and pasting code from the previous video.

00:44.360 --> 00:47.560
The biggest change is to the hook function.

00:48.120 --> 00:55.200
In the previous video, I hardcoded the function so that it would only hook into exactly one layer.

00:55.720 --> 01:04.310
Now here in exercise one, you want to hook into every layer and be able to specify which layer you

01:04.310 --> 01:07.310
want to manipulate, not only which head.

01:07.870 --> 01:13.870
So to be clear, you want to be able to manipulate any head in any layer.

01:14.310 --> 01:17.030
That's one change from the previous video.

01:17.630 --> 01:24.870
Another change is that you want to be able to choose whether to replace the activations in a given head

01:24.870 --> 01:31.190
with all zeros, or the average activation value in that head.

01:31.910 --> 01:35.630
So you will need another variable to toggle that choice.

01:36.270 --> 01:43.350
The third change from the previous video is that you want to calculate the average activation, and

01:43.350 --> 01:47.350
store that as a variable that we can analyze later.

01:48.470 --> 01:48.790
Okay.

01:48.830 --> 01:56.230
So just to make sure this is really clear, in the previous video I hardcoded the hook function to replace

01:56.230 --> 01:59.110
a given head activation with zeros.

01:59.630 --> 02:07.990
And what you are going to do here is first calculate the actual average of the activations in a given

02:07.990 --> 02:09.630
head in a given layer.

02:09.910 --> 02:12.270
That's done before you do any manipulations.

02:12.750 --> 02:17.510
And then you store that average value in a variable.

02:17.790 --> 02:24.550
And depending on the state of this toggle, you would then replace the attention head with zeros or

02:24.550 --> 02:26.150
with that average value.

02:26.790 --> 02:34.710
Now, because all of this is done inside a function, any variables you create inside this function

02:34.710 --> 02:40.070
only live inside the function and are deleted once that function finishes running.

02:40.510 --> 02:43.910
And that is because functions have local scope.

02:44.110 --> 02:50.790
So to be able to access the average from outside the function, you need to define the variable as a

02:50.790 --> 02:54.670
global variable before the function is called.

02:55.750 --> 03:01.020
So if you're not sure how to do that, then don't worry about that part of the exercise.

03:01.020 --> 03:04.900
You can just watch me introduce that idea when I switch to code.

03:05.740 --> 03:13.500
The final thing to do in exercise one is to prepare and tokenize the exact same text with the exact

03:13.500 --> 03:18.860
same target and non-target tokens that we used in the previous video.

03:19.780 --> 03:25.620
Okay, so now you should pause the video and now I will switch to code and discuss my solution.

03:26.940 --> 03:32.460
Here are all the libraries that we will need for all of the exercises in this code challenge.

03:32.780 --> 03:36.020
And now here we get to exercise one.

03:36.340 --> 03:41.620
So load in a model push it to the GPU and some variables.

03:41.780 --> 03:42.820
Let me see where are we.

03:42.860 --> 03:43.740
Here we are.

03:43.860 --> 03:48.180
Here's these variables exactly as in the previous video.

03:48.500 --> 03:51.340
And now here we get to this hook function.

03:51.540 --> 03:58.380
Now in the previous video I did not have this outer hook where we get to specify or provide an input

03:58.380 --> 04:00.450
of the layer Number.

04:00.650 --> 04:07.010
Instead, I just had this function and yeah, then that was only implanted into one layer.

04:07.170 --> 04:09.050
So here this part looks the same.

04:09.050 --> 04:17.130
We are implanting this hook into C proj, and is a pre hook that will allow us to access the individual

04:17.290 --> 04:22.810
attention heads when they are concatenated, but before they are mixed together.

04:23.210 --> 04:30.490
And here you see that I am importing that hook function into all of the layers of the model.

04:31.490 --> 04:37.490
Okay, so now this hook function is somewhat similar to the hook function in the previous video, but

04:37.490 --> 04:39.890
quite a few new things in here.

04:40.290 --> 04:45.570
First of all, I have this statement here where I am actually commenting out.

04:45.570 --> 04:52.210
But if you would uncomment this, then this is just going to print out the length of this input, the

04:52.210 --> 04:53.130
type of the input.

04:53.130 --> 04:54.330
So this is a tuple.

04:54.850 --> 04:58.290
By default this is only going to contain one element.

04:58.410 --> 05:04.760
It can contain other elements depending on what kinds of inputs you provide into the model in the forward

05:04.760 --> 05:05.280
pass.

05:05.680 --> 05:11.320
But usually, at least certainly for our purposes here, this will be a one element tuple.

05:11.520 --> 05:13.560
And then here you can print out the shape.

05:13.560 --> 05:19.920
And the shape of that is sequences in the batch by tokens by embeddings dimension.

05:19.920 --> 05:25.680
But the embeddings dimension is actually all of the attention heads concatenated together.

05:25.680 --> 05:32.160
And that you will see with this if you leave that code Uncommented when you define and implant those

05:32.160 --> 05:32.640
hooks.

05:33.280 --> 05:33.640
Okay.

05:33.680 --> 05:43.080
So then here I'm saying only modify this layer if this layer that we are currently hooking into in the

05:43.080 --> 05:49.560
forward pass equals this other variable here layer to ablate, I have not yet defined this variable.

05:49.560 --> 05:53.680
I'm going to define it later on before I do a forward pass.

05:54.640 --> 06:02.640
Basically, this is the way to allow us to modify only one layer while putting the hook into all of

06:02.640 --> 06:03.680
the layers.

06:04.240 --> 06:10.840
Okay, so if this statement is true, that means that we are working with the layer that we want to

06:10.880 --> 06:11.600
manipulate.

06:11.960 --> 06:18.640
Then yeah, this is just some convenient reshaping so we can index the heads a little more easily.

06:18.640 --> 06:19.800
This you've seen before.

06:20.080 --> 06:21.240
Here's the other toggle.

06:21.240 --> 06:28.320
So again a variable that does not yet exist I will define this later before actually running this function.

06:28.760 --> 06:35.640
And I say if replace zero equals true then uh the value to replace is zero.

06:35.640 --> 06:37.680
And that gets replaced over here.

06:38.080 --> 06:44.600
And this line of code you've also seen exactly before in the previous video in that code demo.

06:45.800 --> 06:46.120
Okay.

06:46.160 --> 06:55.000
Now if replace with zeros is not true, then what we are going to do is replace that value instead of

06:55.000 --> 06:58.640
being zeros it's going to be the head tensor.

06:58.640 --> 07:02.550
So this variable up here, all of these sequences.

07:02.550 --> 07:04.550
We're only going to have one sequence here.

07:05.310 --> 07:11.230
All of the tokens in that sequence, only this specific head and all of the dimensions.

07:11.230 --> 07:13.750
Remember this is 64 here.

07:13.750 --> 07:16.990
This is not 768 for GPT two small.

07:17.270 --> 07:22.510
And then we want the average of all of those values okay.

07:22.830 --> 07:26.750
And then I'm defining a global variable called observed head mean.

07:26.910 --> 07:33.670
And setting that to be equal to this value here, which is the mean of the attention head activation

07:33.670 --> 07:34.310
values.

07:34.670 --> 07:40.230
When I define it as a global variable here, that means that this will be available.

07:40.230 --> 07:44.310
We can read this variable from outside the function.

07:44.310 --> 07:46.070
That's going to be useful for later.

07:47.030 --> 07:50.110
By the way, there are several other ways that you could have solved this.

07:50.110 --> 07:53.870
For example, you can create a dictionary outside of this hook.

07:53.910 --> 07:55.190
You can create a dictionary.

07:55.190 --> 07:56.630
I'll just call it D for now.

07:56.950 --> 08:02.060
And then once that dictionary is defined in the global workspace.

08:02.300 --> 08:06.060
You can then say something like D uh, you know.

08:06.100 --> 08:12.860
And then, uh, yeah, you would want this to be like this, something that would tell us the layer

08:12.860 --> 08:15.380
and also the head that we are working with.

08:15.500 --> 08:17.660
So that would also be a solution.

08:17.980 --> 08:22.260
Uh, I have actually done things like that before, but I chose to do it this way.

08:22.300 --> 08:25.940
Define it explicitly as a global variable just for some variety.

08:26.740 --> 08:33.780
Okay, again, here is some confirmation of the layer and the head that we are manipulating.

08:33.820 --> 08:40.020
Actually, this is what you could copy paste into the name of the key for the dictionary if you wanted

08:40.020 --> 08:43.340
to use that to store the average head value.

08:43.580 --> 08:46.060
Uh, and again, yeah, I'm just commenting this out.

08:46.100 --> 08:52.140
These are the kinds of print statements that I encourage you to include in your code when you are developing

08:52.140 --> 08:54.620
and working through and debugging your code.

08:55.060 --> 08:55.340
Okay.

08:55.380 --> 08:57.780
And then this stuff you've seen before as well.

08:57.900 --> 08:59.940
So run that function.

08:59.940 --> 09:01.860
Hook it into the model.

09:01.980 --> 09:03.500
Here I get the tokens.

09:03.500 --> 09:06.140
Berlin is the capital of blank.

09:06.180 --> 09:12.780
And of course, we want to know what the model thinks is the Berlin is the capital of.

09:13.060 --> 09:17.500
Okay, so this is all just literally copy pasted from the previous video.

09:19.140 --> 09:21.380
Now for exercise two.

09:21.820 --> 09:30.660
The goal of exercise two is to have a clean set of softmax outputs from the model without any manipulations.

09:31.220 --> 09:36.220
This is very similar to what you saw in the previous video, except here.

09:36.260 --> 09:41.660
Now in exercise two I want you to work with softmax.

09:41.780 --> 09:47.820
So in the previous video I did all the analyses, all the visualizations as log softmax.

09:48.060 --> 09:50.740
And here in this video we're going to use softmax.

09:50.740 --> 09:52.100
So without the log.

09:52.220 --> 09:59.410
For reasons that I will explain when I switch to code and when I write percent softmax over here.

09:59.890 --> 10:07.290
I just mean to multiply the softmax probability values by 100 so that we get values between 0 and 100

10:07.330 --> 10:09.210
instead of between 0 and 1.

10:09.970 --> 10:15.170
Okay, now that said, you can see that this plot actually shows logsoftmax.

10:15.450 --> 10:18.250
So I do want you to take the log transform.

10:18.250 --> 10:21.810
But that's just for this visualization.

10:22.410 --> 10:28.290
As you know from the previous video, these two data points here are important because they will be

10:28.290 --> 10:35.690
the comparison reference values for the experiments that you will run in the next several exercises.

10:36.850 --> 10:37.650
Okey dokey.

10:37.690 --> 10:43.930
Now you should pause the video and code away and I'll discuss my solution and also why I prefer us to

10:43.930 --> 10:48.290
work with softmax in this video instead of logsoftmax.

10:49.410 --> 10:56.290
To get clean outputs from the model, I am creating these two variables, which I showed you in the

10:56.290 --> 10:57.690
previous exercise.

10:57.690 --> 11:03.600
In the hook function, I'm setting them to be values that are way higher than any layer that we have

11:03.640 --> 11:04.840
or head that we have.

11:05.040 --> 11:10.280
And this means that, yeah, that nothing is going to happen when we do the forward pass.

11:10.440 --> 11:17.680
So just to make sure that is clear, when I set the layer number to be a thousand, then this is false.

11:17.680 --> 11:20.440
And so none of the rest of this stuff ever gets run.

11:20.960 --> 11:25.240
Looking at this code now, I suppose it actually would make sense to say like this.

11:25.240 --> 11:27.360
And then also add another.

11:27.640 --> 11:36.080
And if the head to ablate is less than or, you know, it should be, uh, in the range of zero to the

11:36.080 --> 11:39.200
number of heads, this is probably a little bit safer.

11:39.400 --> 11:40.800
But anyway that's fine.

11:41.280 --> 11:41.680
Okay.

11:41.720 --> 11:42.840
So let's see.

11:42.880 --> 11:43.160
Okay.

11:43.200 --> 11:45.480
So this means that nothing is going to happen.

11:45.480 --> 11:50.360
We're not going to do any manipulations to the model here.

11:50.360 --> 11:53.640
I'm getting the output logits for the first sequence.

11:53.640 --> 11:56.800
There's only one sequence the final target.

11:56.800 --> 11:58.240
That's the final token.

11:58.240 --> 12:00.390
That's the one we want to analyze.

12:00.390 --> 12:03.550
And all of the vocab elements for the logits.

12:03.910 --> 12:07.030
Then I'm converting them to softmax, not logsoftmax.

12:07.030 --> 12:07.870
That's what I did.

12:07.910 --> 12:08.150
Oops.

12:08.150 --> 12:08.510
Like this.

12:08.510 --> 12:10.790
That's what I did in the previous video.

12:11.110 --> 12:16.310
But here I just want to focus on softmax and then multiply by 100.

12:16.630 --> 12:18.230
Okay, now for the plotting.

12:18.790 --> 12:21.550
Uh, we do actually want the logsoftmax.

12:21.670 --> 12:27.630
And why do we want the logsoftmax just because I said so in the, uh, instructions.

12:27.630 --> 12:33.270
But just make sure that you are dividing by 100 here because I multiply by 100 here.

12:34.350 --> 12:34.670
Okay.

12:34.710 --> 12:42.510
So really, the only point of doing, uh, this visualization here is just to make sure that everything

12:42.550 --> 12:50.670
still works, just like in the previous video, we can plot this again using the, uh, just the softmax

12:50.670 --> 12:51.430
values.

12:51.590 --> 12:56.670
And here you see that the numbers are basically just a little bit more extreme.

12:57.070 --> 12:57.390
Okay.

12:57.430 --> 12:59.630
These values are negative here.

12:59.670 --> 13:06.670
Okay, I have to I forgot I need to remove the log from all of this and also remove the division by

13:06.710 --> 13:08.150
100 from all of this.

13:08.630 --> 13:10.350
Okay, so now you see what it looks like.

13:10.350 --> 13:12.310
Just 4% softmax.

13:12.430 --> 13:19.350
Now, the difference here is that when you just look at the softmax and not the log softmax, the largest

13:19.350 --> 13:22.990
values really, really pop out of the distribution.

13:23.390 --> 13:30.270
That means that if you have subtle effects in your experiments with softmax, it's just a little bit

13:30.270 --> 13:37.070
easier to visualize, because the large values really pop out from the rest of the distribution.

13:37.470 --> 13:42.350
And the effects that we are going to observe in this video are not that great, right?

13:42.390 --> 13:47.750
We're doing really subtle, specific targeted manipulations to the model.

13:47.990 --> 13:52.230
And so therefore the effect sizes are not going to be huge.

13:52.430 --> 13:59.220
And so I decided that we will run this experiment with probabilities instead of log probabilities.

13:59.460 --> 14:00.620
That's just a choice.

14:00.620 --> 14:02.940
If you do not like that, then that's great.

14:02.940 --> 14:09.860
You're free to disagree with me and you can do the whole exercises with logs instead of the raw probability

14:09.860 --> 14:10.580
values.

14:12.980 --> 14:15.620
And now for exercise three.

14:16.100 --> 14:20.340
Here you will start doing analyses and visualizations.

14:20.660 --> 14:26.780
You need to set up a double for loop to loop over all of the transformer blocks.

14:26.780 --> 14:31.740
So all the layers and all of the heads within each transformer block.

14:32.460 --> 14:39.700
And then for each iteration of that double for loop, you run the forward pass, collect the softmax

14:39.700 --> 14:45.420
probabilities of the final model output for the target and non-target tokens.

14:45.420 --> 14:51.420
So space Germany and space France and also get the attention head averages.

14:52.540 --> 14:59.210
I recommend storing all of the results in a three dimensional matrix where the dimensions would correspond

14:59.210 --> 15:05.610
to the layer, the head within each layer, and the particular result that you store.

15:06.250 --> 15:13.290
Once that double for loop finishes, you can visualize the results as the difference in the softmax

15:13.290 --> 15:17.970
probability between the manipulation and the clean version.

15:18.250 --> 15:20.370
In heatmaps that look like this.

15:20.770 --> 15:27.690
So we have layer index on the x axis and head index on the y axis.

15:27.970 --> 15:37.450
And the color in this image is the percent softmax of the target logit in the manipulated run minus

15:37.450 --> 15:40.130
the target logit from the clean run.

15:40.730 --> 15:41.090
Okay.

15:41.130 --> 15:42.810
So this is for the target run.

15:42.810 --> 15:46.970
And then here on the right this is for the non-target word.

15:46.970 --> 15:48.330
This is the word France.

15:48.610 --> 15:55.290
So that is one analysis to show what you also need to store from each iteration.

15:55.290 --> 16:01.010
In the double for loop is the actual token that had the highest logit output.

16:01.610 --> 16:06.130
Of course, you know that the correct answer that the model should give is Germany.

16:06.570 --> 16:10.770
And in fact, that turns out to be Germany in all of these cases.

16:10.930 --> 16:15.810
But you will see in later exercises that you'll get some different tokens in here as well.

16:16.490 --> 16:20.770
By the way, where does this number 144 come from?

16:20.970 --> 16:25.410
Make sure you understand why there are 144 runs.

16:25.690 --> 16:28.690
I will discuss the answer when I show my code.

16:29.570 --> 16:31.170
So that's it for exercise three.

16:31.170 --> 16:33.050
I hope you find this one interesting.

16:33.210 --> 16:37.570
It's also going to set us up for the next several exercises.

16:38.090 --> 16:40.290
Anyway, now I will switch to code.

16:41.730 --> 16:48.170
Here I'm making sure that this toggle replaced with zeros is still set to true.

16:48.690 --> 16:52.890
Here is where I'm going to store all of my results.

16:52.930 --> 16:58.840
I call this variable results zero because I'm going to be zeroing out the heads.

16:58.840 --> 17:04.480
You'll see that contrasting with the variable that I'll define in a later exercise, where we will store

17:04.480 --> 17:08.040
the means or replace with the means instead of the zeros.

17:08.640 --> 17:08.960
Okay.

17:09.000 --> 17:13.200
So that is going to be layers by heads by three.

17:13.480 --> 17:20.120
And three corresponds to the three kinds of results that I want to save from this analysis, which I

17:20.120 --> 17:21.240
will get to in a moment.

17:21.640 --> 17:27.640
Okay, so here's our double for loop to run the experiment, looping over all of the layers and then

17:27.640 --> 17:31.120
looping over all of the heads within each layer.

17:31.480 --> 17:37.880
And I'm using this function tqdm to give a little progress bar at the bottom, although I don't really

17:37.920 --> 17:38.440
need that.

17:38.480 --> 17:40.920
This actually goes pretty quick.

17:40.920 --> 17:41.600
But anyway.

17:41.920 --> 17:44.000
Okay so here I do a forward pass.

17:44.000 --> 17:49.760
Now in this forward pass these variables here layer to ablate and head to ablate.

17:49.840 --> 17:53.320
Those are variables that are defined here in the for loop.

17:53.520 --> 18:02.070
And they will be used inside the hook function Him over here to figure out which head to set to zero,

18:02.310 --> 18:06.310
and over here to determine which layer to manipulate.

18:07.430 --> 18:07.870
Okay.

18:07.910 --> 18:12.710
So let's see okay so there's the forward pass here I do softmax again.

18:12.710 --> 18:18.950
This is exactly the same code as what I showed in the previous exercise okay.

18:18.990 --> 18:20.390
So now I'm storing the results.

18:20.390 --> 18:21.510
And what are we storing.

18:21.630 --> 18:26.870
First is the softmax for the target the softmax for the non-target.

18:26.870 --> 18:32.870
And then the argmax of the softmax which is the logit that has the largest value.

18:33.030 --> 18:39.230
And then later on we can convert this into a token to see what the text looks like.

18:39.870 --> 18:40.150
Okay.

18:40.190 --> 18:46.870
So now I run all of that code and we get an error because I haven't run this cell yet okay.

18:47.190 --> 18:47.830
Very good.

18:47.870 --> 18:48.110
Yeah.

18:48.150 --> 18:50.070
You can see this goes super fast.

18:50.110 --> 18:56.390
I actually first wrote this code, uh, testing it on the CPU, and then it took a little bit longer

18:56.390 --> 18:57.270
on the CPU.

18:57.270 --> 19:01.380
So that's why I decided to include this update progress bar.

19:01.580 --> 19:03.660
But then I switched to running it on the GPU.

19:03.780 --> 19:07.020
And yeah, now it runs really fast anyway.

19:07.500 --> 19:09.540
Okay, so here we show the images.

19:09.540 --> 19:17.380
So this is uh, I am show so image show here I'm showing the results for zero.

19:17.420 --> 19:25.300
This is the softmax probability for the target minus the softmax probability from the target, but from

19:25.300 --> 19:28.740
the clean version of the model okay.

19:28.780 --> 19:30.620
So this is for zero.

19:30.620 --> 19:35.980
And this is for one which means that this was for the target or the word Germany should be the correct

19:35.980 --> 19:36.700
answer.

19:36.700 --> 19:41.220
And here is for one this was the non-target that was for France.

19:41.700 --> 19:44.740
And here you see what those images look like.

19:44.900 --> 19:52.740
So a value of zero would mean that zeroing out or ablating or nullifying whatever term you want to use

19:52.900 --> 20:00.820
one particular head in one particular layer has no impact on the final token, logit.

20:00.820 --> 20:07.220
That's what a value of zero would mean relative to the clean version of the model when we didn't do

20:07.220 --> 20:08.420
any manipulations.

20:08.860 --> 20:15.460
If the number is positive here, then that would mean that zeroing out the attention head actually increased

20:15.620 --> 20:21.620
the logit and a negative value here, which would mean colors more towards the purple end of the spectrum,

20:21.900 --> 20:29.420
would mean that we actually decreased the, uh, the probability of selecting the token for Germany.

20:29.900 --> 20:34.940
So again, for the targets and the non-targets, it is quite clear when you put them on the same color

20:34.940 --> 20:41.020
scale that, uh, yeah, we didn't really have much of an impact on the non-target word.

20:41.060 --> 20:45.660
There is some impact here and there, but not much, uh, versus for the target word.

20:45.660 --> 20:48.180
And there it's, uh, much stronger.

20:48.180 --> 20:53.580
So it's really into multiple percent difference in softmax probability.

20:54.060 --> 21:01.210
Now you can see that sometimes it's increasing and sometimes it is decreasing the probability and it

21:01.210 --> 21:07.130
doesn't look like there's any obvious patterns, like for example, going from positive to negative

21:07.370 --> 21:09.010
as we go through the layers.

21:09.010 --> 21:14.290
So we are going to look at histograms of this distribution in a later exercise.

21:14.290 --> 21:15.770
So you don't have to worry about that now.

21:15.890 --> 21:21.770
But for now you can just notice that it's interesting that there's quite a bit of variability in terms

21:21.770 --> 21:27.730
of giving a positive or a negative impact on the final softmax probability.

21:28.290 --> 21:28.730
Okay.

21:28.890 --> 21:36.170
One other thing I would like to mention is that remember that these heads have no intrinsic ordering

21:36.330 --> 21:36.930
to them.

21:37.130 --> 21:44.290
There's no like spatial orientation organization inside the model such that heads four and five are

21:44.290 --> 21:47.530
closer to each other than are like four and eight.

21:47.530 --> 21:49.250
So the numbers are just arbitrary.

21:49.250 --> 21:51.170
The indices are arbitrary.

21:51.170 --> 21:55.730
There's no meaningful spatial relationship down the y axis here.

21:56.330 --> 21:56.650
Okay.

21:56.690 --> 22:04.720
Anyway, the Anyway, the last thing I'm going to do here is find the unique elements in the third element

22:04.720 --> 22:06.080
in this matrix.

22:06.080 --> 22:14.520
And remember that is the maximum token that was predicted for the token after the word of at the end

22:14.520 --> 22:15.840
of the token sequence.

22:16.200 --> 22:16.520
Okay.

22:16.560 --> 22:19.600
So getting all of the unique values and just printing them out.

22:19.640 --> 22:24.200
And then we see interestingly, most of the time it was Germany.

22:24.200 --> 22:29.080
And here we got one out of 144 to be the token for space.

22:29.120 --> 22:33.720
The okay so let me first say why this is 144.

22:34.080 --> 22:36.480
There are 12 layers and there are 12 heads.

22:36.480 --> 22:39.040
And we have a double for loop over all of them.

22:39.040 --> 22:40.640
So that's 144.

22:41.040 --> 22:41.320
Okay.

22:41.360 --> 22:48.400
Now for the word the if we go back and look at this sentence, uh, where is it here.

22:48.680 --> 22:55.120
So Berlin is the capital of Germany, but is it so weird to have the in here like this?

22:55.120 --> 22:55.760
Probably not.

22:55.800 --> 22:56.640
That could be.

22:56.640 --> 22:59.520
B Berlin is the capital of the country.

22:59.680 --> 23:01.440
Germany, for example.

23:01.680 --> 23:08.920
So the is not the expected word that we would think should be there, but it's also not completely off.

23:08.920 --> 23:14.560
It's not like Berlin is the capital of watermelon or, you know, something just totally off the wall

23:14.560 --> 23:15.200
like that.

23:15.520 --> 23:21.880
Okay, so interestingly, you can see that, yeah, these models still exhibit some stochasticity,

23:22.040 --> 23:25.880
even though we are not doing probabilistic token selection here.

23:27.400 --> 23:32.520
Exercise four is nearly identical to exercise three.

23:33.080 --> 23:39.440
The difference is that here you will switch the toggle so that you replace each head activation with

23:39.440 --> 23:44.720
the average of that head instead of ablating it to a value of zero.

23:45.400 --> 23:50.040
But there is one more thing to do here in exercise four.

23:50.360 --> 23:56.600
The second plot for this exercise is to show the actual head averages.

23:56.600 --> 23:56.630
Bridges.

23:56.910 --> 24:02.630
This is the variable that you defined as a global variable in the hook function.

24:02.990 --> 24:07.310
So you're going to get one average per head per layer.

24:07.550 --> 24:12.390
So then you can visualize them as a scatter plot linearized.

24:12.390 --> 24:15.670
So here the x axis is heads by layers.

24:15.670 --> 24:17.750
So there's 144 of them here.

24:18.030 --> 24:24.310
And you can also visualize them as an image similar to the images from the previous figure.

24:24.910 --> 24:31.430
And what you want to look for in particular is whether there seem to be any trends in the averages.

24:31.510 --> 24:39.190
For example, if the average values in the head increases as we go deeper into the model, that kind

24:39.190 --> 24:45.790
of result would suggest that there is like a drift in the distribution as we go deeper into the model.

24:46.350 --> 24:49.150
Anyway, I'm not telling you that that's what's going to happen.

24:49.190 --> 24:51.950
That's just the kind of pattern that you could look for.

24:52.830 --> 24:53.470
All right.

24:53.510 --> 24:54.510
You know what to do.

24:54.670 --> 24:56.350
And now I will switch to code.

24:57.780 --> 25:03.620
So here I'm switching the toggle to have this variable replace with zeros equal false.

25:04.100 --> 25:09.140
And now you can see I have this variable result mean instead of results zero.

25:09.460 --> 25:12.540
This is now four elements in the final dimension.

25:12.540 --> 25:19.220
Whereas results zero from exercise three had three elements in the final dimension.

25:19.500 --> 25:26.180
And the new piece of information that we are observing here is the empirically observed mean in the

25:26.180 --> 25:26.460
head.

25:26.460 --> 25:33.780
So the average of all the activation values within that head that we are then using to replace with

25:33.900 --> 25:36.500
the all the values of the head instead of zero.

25:37.020 --> 25:37.260
Okay.

25:37.300 --> 25:42.100
Otherwise all of this code is exactly the same as the code for exercise three.

25:42.380 --> 25:43.940
So just this line is different.

25:43.940 --> 25:48.660
And of course this is now three instead of having a value of two okay.

25:48.700 --> 25:50.980
So here again we see the results.

25:51.020 --> 25:58.850
Now qualitatively at first pass inspection these look really really similar to the results from exercise

25:58.890 --> 25:59.490
three.

25:59.890 --> 26:06.450
Again, I'm not claiming yet that it doesn't matter whether you replace the head with zero or with the

26:06.450 --> 26:07.370
observed mean.

26:07.570 --> 26:15.610
All I'm saying is that at a very first pass glance, this looks really similar to what we saw in the

26:15.610 --> 26:16.970
previous exercise.

26:18.210 --> 26:18.450
Okay.

26:18.490 --> 26:25.170
And here again we see almost all the time the model actually did select the word Germany and only once

26:25.170 --> 26:27.090
did it select the token the.

26:27.490 --> 26:33.730
So if you want you could find out which that which one that is that's you know one of these presumably

26:33.970 --> 26:38.850
it's one of these dark purple, uh, head layer combinations over here.

26:38.850 --> 26:43.490
Because that would mean that the softmax probability for Germany went down.

26:43.650 --> 26:48.850
And then that would allow the softmax probability for the word the to increase.

26:49.610 --> 26:49.970
Okay.

26:50.010 --> 26:53.730
And then here I'm showing the head averaged activations.

26:53.730 --> 27:01.330
So these are the actual values of the heads that were measured before I replaced them with these actual

27:01.330 --> 27:08.130
values, and they appear to be roughly normally distributed in the sense that there's a lot of values

27:08.130 --> 27:09.170
close to zero.

27:09.370 --> 27:13.610
And the further we get from zero, the fewer data points there are.

27:13.890 --> 27:15.850
Also, these are very small numbers.

27:16.010 --> 27:23.130
And here you see as an image, there does not appear to be at least you know by first pass visual inspection,

27:23.370 --> 27:29.170
it doesn't really look like there's any at least any obvious consistent patterns.

27:29.170 --> 27:35.730
As we go deeper into the model, for example, it's not going from like yellow columns to purple columns

27:35.730 --> 27:36.770
or vice versa.

27:36.970 --> 27:40.010
So it kind of seems like it's just all over the place.

27:40.050 --> 27:43.250
Maybe the variability is a little bit higher earlier in the model.

27:43.250 --> 27:49.530
So the colors are a little bit more differentiated here, but nothing pops out as being super visually

27:49.530 --> 27:50.290
obvious.

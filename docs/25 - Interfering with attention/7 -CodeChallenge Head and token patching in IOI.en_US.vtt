WEBVTT

00:02.080 --> 00:05.680
Now for the follow up from the previous video.

00:06.160 --> 00:14.400
You are going to dig more deeply into patching attention heads and the IOI experiments, but patching

00:14.400 --> 00:16.680
individual heads within each layer.

00:17.160 --> 00:19.880
Let's begin with exercise one.

00:20.280 --> 00:23.440
The goal here is to set up for the experiment.

00:23.480 --> 00:28.440
Now, most of the code you can copy from the code file for the previous video.

00:28.880 --> 00:34.440
But don't just copy and paste blindly because you know, the entire reason why you're watching this

00:34.440 --> 00:39.240
video is because you want to understand and not just copy.

00:40.080 --> 00:49.720
Anyway, for starters, import the large version of GPT two, then tokenize the same two sentences that

00:49.720 --> 00:52.360
we used in the previous video.

00:53.160 --> 00:56.880
Create hooks that you implant to get all of the attention.

00:56.880 --> 01:06.320
Head activations from text version A, you should also check the sizes of the activation matrices and

01:06.320 --> 01:11.520
make sure you understand what all these numbers mean, where they come from, what they correspond to,

01:11.600 --> 01:12.240
etc..

01:12.840 --> 01:17.240
Finally, calculate the IOI score from the clean data.

01:17.760 --> 01:25.000
As you know, these values will be the baseline comparison values for when we run the experiment in

01:25.000 --> 01:26.520
the next exercise.

01:28.000 --> 01:34.080
I don't think this exercise will take you very long after having gone through the previous video.

01:34.760 --> 01:40.640
I will now switch to code, but only briefly to make sure we're all on the same page before proceeding

01:40.680 --> 01:44.200
to the main experiment in the next exercise.

01:46.120 --> 01:47.440
So here we are.

01:47.480 --> 01:52.760
I'm importing GPT two large and pushing it to the GPU.

01:53.400 --> 01:58.400
Here is the text exactly the same as in the previous video.

01:58.560 --> 02:04.640
Here I'm defining some convenience variables as I've done before, including this one for the number

02:04.640 --> 02:05.640
of tokens.

02:06.120 --> 02:06.400
Okay.

02:06.440 --> 02:09.200
And then this code you've seen exactly before.

02:09.240 --> 02:16.800
This is getting the activations from all of the heads reshaped, so that we can easily index each individual

02:16.800 --> 02:25.240
head from, uh, from text sequence A, and remember that this reshaping wasn't necessary in the previous

02:25.240 --> 02:32.680
video because there I replaced all the heads and all the tokens, and that's not what I'm going to do

02:32.720 --> 02:33.200
here.

02:33.600 --> 02:33.960
Okay.

02:34.000 --> 02:34.640
So here.

02:34.680 --> 02:34.880
Yeah.

02:34.920 --> 02:40.120
You've also seen this exactly in the previous video and also this stuff.

02:40.120 --> 02:45.800
So really not too much new here compared to what you have already done in the previous video.

02:47.560 --> 02:51.040
This exercise is the main experiment.

02:51.400 --> 02:58.680
You can start by copying the experiment code from the previous video, but remember that in the previous

02:58.680 --> 03:03.280
video I patched all of the heads and all of the tokens.

03:03.840 --> 03:10.800
So now in this experiment here, you are going to do a much more targeted manipulation.

03:11.480 --> 03:16.520
You can see that I have an embedded for loop over all of the heads.

03:16.520 --> 03:22.440
So we're looping over all of the layers and then looping over all of the heads within each layer.

03:23.040 --> 03:30.840
So now inside this hook function you replace the head activation vectors from the donor model, but

03:30.840 --> 03:35.880
only for one head at a time and only for the final token.

03:36.760 --> 03:43.240
Otherwise, the concept of this for loop and the experiment is the same as the previous video.

03:43.840 --> 03:51.440
Again, the difference is that here you are patching one head at a time and only the final token next

03:51.440 --> 03:53.200
for the visualization.

03:53.320 --> 03:59.840
Now, what you're looking at right now is not the visualization that you will create in this exercise.

04:00.160 --> 04:08.790
This is a screenshot from the previous video where we had only one data point per layer per transformer

04:08.790 --> 04:13.190
block, but now we have one data point per block per head.

04:13.710 --> 04:21.270
So I'm not going to tell you exactly how to visualize the results, because I want to give you the freedom

04:21.270 --> 04:22.830
to make your own decisions.

04:23.310 --> 04:28.150
Of course, when I switch to code, I will show you what I did to visualize these results.

04:28.350 --> 04:35.710
But suffice it to say that somehow you want to visualize all the results over all the layers and all

04:35.750 --> 04:36.630
of the heads.

04:37.830 --> 04:44.510
If you have multiple ideas for how to visualize the results, then you can make multiple visualizations.

04:44.510 --> 04:45.470
That is great.

04:46.030 --> 04:50.870
Okay, and now you should pause the video and run the experiment and see how the results look.

04:51.270 --> 04:52.910
And now I will switch to code.

04:53.990 --> 04:58.830
I hope you didn't have too much trouble getting this exercise together.

04:59.110 --> 05:04.150
So looping over all the layers, looping over all the heads within each layer.

05:04.350 --> 05:06.390
And then here is the hook function.

05:06.550 --> 05:13.230
Now remember in the previous video this line of code looked something like this.

05:13.550 --> 05:16.230
Or wait it looked like this.

05:16.230 --> 05:17.270
That's what it looked like.

05:17.390 --> 05:23.830
So in the previous video, we copied all the heads for this one layer and all of the tokens.

05:23.950 --> 05:26.990
And what we're doing now is much more targeted.

05:26.990 --> 05:36.150
So we're only copying over the final token and only copying over one head corresponding to this iteration.

05:36.150 --> 05:39.470
In this for loop over all of the heads.

05:40.230 --> 05:47.670
And now you see why it's convenient to reshape this input so we can easily access each head dimension.

05:48.150 --> 05:51.590
That said, this reshaping is not necessary at all.

05:51.590 --> 05:56.790
You could also come up with an indexing to index right into this tensor.

05:56.990 --> 06:03.310
And you know, that would be something like head I times head dim.

06:03.310 --> 06:04.990
This would be the start location.

06:04.990 --> 06:06.790
And then you would have to add another head.

06:07.030 --> 06:08.150
It would be something like this.

06:08.190 --> 06:09.590
You could figure it out if you want.

06:09.950 --> 06:10.990
But anyway.

06:11.230 --> 06:11.590
Okay.

06:11.630 --> 06:13.110
So that is that.

06:13.110 --> 06:15.790
And why are we manipulating the final token.

06:15.950 --> 06:22.950
Well that is the token that the model is then using to generate the prediction for the next token,

06:22.950 --> 06:27.150
which is what we will explicitly measure here.

06:27.550 --> 06:32.270
That said, if you are curious, you could also try doing other manipulations.

06:32.270 --> 06:39.830
So for example, you could try instead of overwriting this the attention vectors here, you could try

06:39.870 --> 06:45.630
overwriting the attention vectors for this word here this token either Sam or Sally.

06:45.630 --> 06:52.750
So basically you're going to patch the attention vectors for Sam onto the attention vectors for Sally.

06:53.630 --> 06:54.470
Just an idea.

06:54.510 --> 06:58.390
Just if you want to continue exploring this experiment.

06:58.750 --> 06:59.030
Okay.

06:59.070 --> 07:08.550
So after that, the rest of this code is really exactly the same as in the previous video, but now

07:08.550 --> 07:10.470
this variable logit diffs.

07:10.510 --> 07:13.470
Previously, we only had one value per layer.

07:13.510 --> 07:16.270
Now we have one value per head per layer.

07:16.950 --> 07:20.590
Okay, on the GPU this doesn't take too long to run through.

07:21.110 --> 07:22.910
I haven't timed this on the CPU.

07:22.950 --> 07:26.470
Maybe it's several minutes or something, but it still is.

07:26.510 --> 07:29.550
You know, it's a pretty respectable experiment we're running here.

07:29.790 --> 07:36.030
It's all of the layers and all of the heads in the large version of GPT two.

07:36.070 --> 07:37.310
So, you know, it takes.

07:37.350 --> 07:39.230
Okay, 20s not so bad.

07:39.990 --> 07:40.630
All right.

07:40.830 --> 07:41.470
Uh, let's see.

07:41.470 --> 07:45.510
So here is the visualization that I made here.

07:45.830 --> 07:52.270
And what you see is it looks like there's only one dot per transformer block, but in fact there are.

07:52.310 --> 07:53.030
Let me see.

07:53.070 --> 07:55.350
I don't know how many heads there are off the top of my head.

07:55.390 --> 07:59.630
Okay, there's 20 heads in this version of GPT two.

07:59.910 --> 08:08.070
So each of these circles is actually 20 circles, but they all have nearly identical values.

08:08.070 --> 08:12.070
And so therefore they're all basically lying all on top of each other.

08:12.870 --> 08:14.910
Here you see a little bit more diversity.

08:15.150 --> 08:23.470
Basically what's happening is that early in the model patching one head for one token did I'm reluctant

08:23.470 --> 08:25.830
to say the word absolutely nothing.

08:26.030 --> 08:32.070
But it did so little that on this scale it is not visually detectable.

08:32.430 --> 08:36.190
And then we see later in the model we do get some modulations.

08:36.190 --> 08:42.190
It looks like most of the time it goes down a little bit, although sometimes it actually goes up which

08:42.190 --> 08:45.990
is in the opposite direction as we might expect.

08:46.110 --> 08:49.110
But yeah, it's certainly not an overwhelming effect.

08:49.110 --> 08:52.030
It's a really subtle, really mild effect.

08:52.030 --> 08:59.870
It's way, way, way smaller compared to what we saw when we patched over all of the, uh, the embeddings

08:59.870 --> 09:03.030
vectors at the output of the transformer block.

09:03.310 --> 09:10.910
It's also considerably smaller than even the relatively subtle effects that we saw in the previous video,

09:10.910 --> 09:15.630
where we were patching over all of the heads and all of the tokens.

09:16.110 --> 09:17.830
So a very subtle effect.

09:17.870 --> 09:24.430
One more thing I want to say before switching to exercise three that I forgot to mention is in the previous

09:24.430 --> 09:33.510
video, I also incorporated more code into this hook function and into this loop to compare the activations

09:33.510 --> 09:39.710
from the two different token sequences as a code check, as a manipulation check, just to make sure

09:39.710 --> 09:40.990
that the code was working.

09:41.310 --> 09:47.590
And basically I decided to exclude that check here that sanity check from this code, just because I

09:47.590 --> 09:51.550
felt like I've already confirmed that it works from the previous video.

09:51.550 --> 09:53.750
And now this is basically the same code.

09:53.750 --> 09:58.110
So I decided not to include that sanity check in here.

09:59.790 --> 10:02.910
So that manipulation was quite subtle.

10:03.550 --> 10:09.990
On the one hand, that's probably not surprising given how subtle the effect was from the previous video

10:10.230 --> 10:13.470
where we manipulated all the heads and all the tokens.

10:13.910 --> 10:20.430
And so it's just going to be the case that the more precise and targeted the manipulation, the less

10:20.430 --> 10:22.830
of a global impact it will have.

10:23.470 --> 10:31.990
But the question that we want to explore here in this exercise is why the effect was so big when we

10:31.990 --> 10:33.950
manipulated the hidden states.

10:33.990 --> 10:41.550
So the output of the transformer block, and why it was so small here when we manipulated just the attention

10:41.550 --> 10:45.190
heads which are inside of the transformer blocks.

10:45.950 --> 10:53.670
To gain some insight into this, I want you to write down an equation for the output of a transformer

10:53.670 --> 10:54.230
block.

10:54.950 --> 11:03.150
Your equation should incorporate the attention, the MLP, and the layernorm components of the transformer.

11:03.750 --> 11:10.390
And then when you write out those equations, I want you to pinpoint the terms in those equations that

11:10.390 --> 11:14.110
you actually just manipulated in exercise two.

11:15.190 --> 11:19.470
And I think that will help you understand why the effect size was small.

11:20.390 --> 11:27.030
Now there are multiple correct ways of mathematically describing the output of a transformer block.

11:27.550 --> 11:33.510
You can write a sequence of like really, really detailed equations, or you can write some more general

11:33.510 --> 11:36.950
equations that abstract away some of the details.

11:37.510 --> 11:43.110
I'm going to show you the equations that I wrote down in a moment, but I also want to make it clear

11:43.110 --> 11:50.590
that there are multiple correct ways of describing a transformer block at different levels of granularity.

11:51.470 --> 11:57.950
The point is to refresh your understanding of what happens inside a transformer block, and how the

11:57.950 --> 12:00.550
attention calculations fit into that.

12:01.150 --> 12:02.550
So pause the video now.

12:02.550 --> 12:03.790
But there's no coding here.

12:03.790 --> 12:07.510
You just write down some equations on a piece of paper with a pencil.

12:07.950 --> 12:12.460
And now I will go to the next slide and show the equations that I wrote down.

12:13.660 --> 12:18.620
This final line here is the output of a transformer block.

12:18.980 --> 12:26.180
It is equal to the input into the transformer block, plus the adjustment from the attention sub block,

12:26.420 --> 12:29.620
plus the adjustment from the MLP sub block.

12:30.260 --> 12:38.380
The delta attention term is defined as the attention function applied to the input to the transformer

12:38.380 --> 12:38.940
block.

12:38.980 --> 12:47.700
After applying the Layernorm, and some of those attention adjustments also get absorbed into the MLP.

12:48.380 --> 12:54.140
And I think you see what I mean with my comment that there are multiple correct ways to write down this

12:54.180 --> 12:55.340
equation here.

12:55.700 --> 13:01.980
For example, I'm abstracting away all the math of the attention and the MLP sub blocks.

13:02.260 --> 13:06.580
Because what I really want to do here is focus on this delta a term.

13:07.020 --> 13:10.980
This is the actual term that we manipulated in this experiment.

13:11.380 --> 13:14.900
and we didn't even manipulate the entire delta.

13:14.940 --> 13:22.860
A we just manipulated one little piece of Delta A because we only replaced one head and one token,

13:23.300 --> 13:32.300
and then all of that information got mixed together by the W0 mixing matrix in this attention function

13:32.300 --> 13:32.780
here.

13:33.900 --> 13:39.900
What I hope you appreciate from this slide is that the thing that we actually changed in this experiment

13:39.900 --> 13:48.060
is really tiny, relative to the entirety of the computations taking place inside a transformer block.

13:48.740 --> 13:54.940
So given that it's not really that surprising that the impact was subtle.

13:55.540 --> 14:02.380
In fact, you could turn it around and say, it's actually pretty remarkable that such a tiny, precise

14:02.380 --> 14:09.700
manipulation that we did even had any numerical impact on the final model output logits.

14:10.580 --> 14:13.700
Now, to be clear, this is not necessarily a bad thing.

14:14.100 --> 14:20.940
In scientific research, when you are trying to understand a complex system, taking a sledgehammer

14:20.940 --> 14:24.620
to the entire network is certainly a good place to start.

14:25.060 --> 14:32.100
But the more you learn, and the more theories and findings develop, the more precise and subtle you

14:32.100 --> 14:36.340
will need to be to discover more nuanced mechanisms.

14:37.220 --> 14:40.060
And that's where I'd like to close this video.

14:40.460 --> 14:49.500
It is a general phenomenon in science that as a field progresses and as knowledge increases, the experiments

14:49.500 --> 14:53.460
and manipulations become more precise and more nuanced.

14:53.940 --> 15:00.380
And that means that the effect sizes will get smaller, which means that it's important to have a good

15:00.420 --> 15:07.180
hypothesis and theoretical framework for interpreting the results and a sufficient amount of data,

15:07.180 --> 15:12.420
so that you can be confident about the robustness of even small effects.


<!-- Aero-Navigation-Start -->
[üè† Home](../../index.md) > [02 Words to tokens to numbers](../index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../index.md)
- [üìö Module 01: LLM Course](../../01-LLM_Course/index.md)
- [üî¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../04-buildGPT/index.md)
- [üéØ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [üîç Module 19: AI Safety](../../19-AI-safety/index.md)
- [üêç Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Ph√¢n t√≠ch Th·ªëng k√™ S·ªë l∆∞·ª£ng Token theo ƒê·ªô d√†i Subword (Ph·∫ßn 2): M√¥ h√¨nh h√≥a To√°n h·ªçc v√† H√†m Ph√¢n b·ªë

‚∏ª

T√≥m t·∫Øt

B√†i b√°o n√†y ti·∫øp t·ª•c ph√¢n t√≠ch th·ªëng k√™ s·ªë l∆∞·ª£ng token theo ƒë·ªô d√†i subword trong b·ªô tokenizer c·ªßa GPT-4, d·ª±a tr√™n d·ªØ li·ªáu th·ª±c nghi·ªám t·ª´ t√†i li·ªáu ƒë√≠nh k√®m (ph·∫ßn 2). Ch√∫ng t√¥i x√¢y d·ª±ng m√¥ h√¨nh to√°n h·ªçc cho ph√¢n b·ªë ƒë·ªô d√†i, ki·ªÉm ƒë·ªãnh gi·∫£ thuy·∫øt ph√¢n b·ªë m≈© v√† lu·∫≠t Zipf, ƒë·ªìng th·ªùi ph√¢n t√≠ch t√°c ƒë·ªông c·ªßa c·∫•u tr√∫c token ƒë·∫øn ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n trong ki·∫øn tr√∫c Transformer c·ªßa OpenAI. K·∫øt qu·∫£ cho th·∫•y ph√¢n b·ªë ƒë·ªô d√†i subword c√≥ xu h∆∞·ªõng suy gi·∫£m phi tuy·∫øn, g·∫ßn v·ªõi h√†m m≈© ho·∫∑c log-linear, v√† c√≥ m·ªëi li√™n h·ªá ch·∫∑t ch·∫Ω v·ªõi entropy h·ªá token.

‚∏ª

1. Gi·ªõi thi·ªáu

Trong c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLMs), tokenization l√† b∆∞·ªõc √°nh x·∫° vƒÉn b·∫£n th√¥ th√†nh chu·ªói token r·ªùi r·∫°c:

S = (c_1, c_2, ..., c_n)

T = (t_1, t_2, ..., t_m)

V·ªõi:

m \le n

M·ªói token t_i c√≥ ƒë·ªô d√†i \ell(t_i) t√≠nh theo byte ho·∫∑c k√Ω t·ª± Unicode.

Ph·∫ßn 2 c·ªßa d·ªØ li·ªáu th·ª±c nghi·ªám t·∫≠p trung v√†o:
	‚Ä¢	Ph√¢n b·ªë chi ti·∫øt ·ªü c√°c ƒë·ªô d√†i l·ªõn h∆°n
	‚Ä¢	S·ª± suy gi·∫£m s·ªë l∆∞·ª£ng token khi ƒë·ªô d√†i tƒÉng
	‚Ä¢	Quan h·ªá gi·ªØa ƒë·ªô d√†i v√† t·∫ßn su·∫•t xu·∫•t hi·ªán

‚∏ª

2. M√¥ h√¨nh h√≥a Ph√¢n b·ªë ƒê·ªô d√†i Subword

2.1 Ph√¢n b·ªë x√°c su·∫•t r·ªùi r·∫°c

G·ªçi:
	‚Ä¢	V: t·∫≠p t·ª´ v·ª±ng
	‚Ä¢	N_k: s·ªë token c√≥ ƒë·ªô d√†i k

Khi ƒë√≥:

P(L = k) = \frac{N_k}{|V|}

V√†:

\sum_{k=1}^{K_{\max}} P(L = k) = 1

‚∏ª

2.2 Gi·∫£ thuy·∫øt ph√¢n b·ªë m≈©

D·ªØ li·ªáu th·ª±c nghi·ªám cho th·∫•y:

N_k \approx Ae^{-\lambda k}

Suy ra:

P(L = k) = \frac{Ae^{-\lambda k}}{\sum_{j=1}^{K_{\max}} Ae^{-\lambda j}}

Chu·∫©n h√≥a:

P(L = k) = (1 - e^{-\lambda}) e^{-\lambda (k-1)}

ƒê√¢y l√† ph√¢n b·ªë h√¨nh h·ªçc r·ªùi r·∫°c.

‚∏ª

2.3 K·ª≥ v·ªçng v√† Ph∆∞∆°ng sai

K·ª≥ v·ªçng:

\mathbb{E}[L] = \frac{1}{1 - e^{-\lambda}}

Ph∆∞∆°ng sai:

\mathrm{Var}(L) = \frac{e^{-\lambda}}{(1 - e^{-\lambda})^2}

ƒêi·ªÅu n√†y cho th·∫•y khi \lambda nh·ªè:
	‚Ä¢	ƒêu√¥i ph√¢n b·ªë d√†i h∆°n
	‚Ä¢	T·ªìn t·∫°i nhi·ªÅu token d√†i

‚∏ª

3. Li√™n h·ªá v·ªõi Lu·∫≠t Zipf

T·∫ßn su·∫•t token theo th·ª© h·∫°ng:

f(r) \propto \frac{1}{r^\alpha}

Trong ƒë√≥:
	‚Ä¢	r: th·ª© h·∫°ng
	‚Ä¢	\alpha \approx 1

K·∫øt h·ª£p hai quan s√°t:
	‚Ä¢	Token ng·∫Øn ‚Üí t·∫ßn su·∫•t cao
	‚Ä¢	Token d√†i ‚Üí t·∫ßn su·∫•t th·∫•p

Ta c√≥ m√¥ h√¨nh k·∫øt h·ª£p:

P(t) \propto e^{-\beta \ell(t)} \cdot \frac{1}{r^\alpha}

‚∏ª

4. ·∫¢nh h∆∞·ªüng ƒë·∫øn ƒê·ªô d√†i Chu·ªói v√† Chi ph√≠ Attention

Gi·∫£ s·ª≠ vƒÉn b·∫£n c√≥ t·ªïng s·ªë k√Ω t·ª± n.

S·ªë token:

m = \frac{n}{\mathbb{E}[L]}

Self-attention c√≥ ƒë·ªô ph·ª©c t·∫°p:

O(m^2)

Thay v√†o:

O\left(\left(\frac{n}{\mathbb{E}[L]}\right)^2\right)

Do ƒë√≥:
	‚Ä¢	N·∫øu \mathbb{E}[L] \uparrow \Rightarrow m \downarrow \Rightarrow \text{Cost} \downarrow
	‚Ä¢	N·∫øu token qu√° d√†i ‚Üí vocabulary l·ªõn ‚Üí tƒÉng chi ph√≠ embedding

‚∏ª

5. Entropy c·ªßa H·ªá Token

Entropy:

H = - \sum_{t \in V} P(t) \log P(t)

Thay m√¥ h√¨nh m≈©:

H \approx - \sum_{k} P(L=k) \log P(L=k)

V·ªõi ph√¢n b·ªë h√¨nh h·ªçc:

H = - \sum_{k=1}^{\infty} (1-q) q^{k-1} \log[(1-q) q^{k-1}]

Trong ƒë√≥:

q = e^{-\lambda}

Entropy t·ªëi ∆∞u khi:
	‚Ä¢	Kh√¥ng qu√° t·∫≠p trung v√†o token c·ª±c ng·∫Øn
	‚Ä¢	Kh√¥ng qu√° ph√¢n t√°n ·ªü token d√†i

‚∏ª

6. Ki·ªÉm ƒë·ªãnh Ph√π h·ª£p M√¥ h√¨nh

ƒê·ªÉ ki·ªÉm tra gi·∫£ thuy·∫øt ph√¢n b·ªë m≈©, c√≥ th·ªÉ s·ª≠ d·ª•ng:

6.1 H·ªìi quy log-linear

\log N_k = \log A - \lambda k

N·∫øu ƒë·ªì th·ªã \log N_k theo k tuy·∫øn t√≠nh ‚Üí x√°c nh·∫≠n m√¥ h√¨nh m≈©.

‚∏ª

6.2 Ki·ªÉm ƒë·ªãnh Chi-square

\chi^2 = \sum_{k} \frac{(N_k - \hat{N}_k)^2}{\hat{N}_k}

So s√°nh v·ªõi ph√¢n b·ªë l√Ω thuy·∫øt.

‚∏ª

7. H√†m T·ªëi ∆∞u H√≥a Ng·∫ßm trong Tokenizer

Tokenizer BPE th·ª±c ch·∫•t t·ªëi ∆∞u x·∫•p x·ªâ:

\min_{V} \left( \mathbb{E}[m] + \lambda |V| \right)

Trong ƒë√≥:
	‚Ä¢	\mathbb{E}[m]: s·ªë token trung b√¨nh
	‚Ä¢	|V|: k√≠ch th∆∞·ªõc t·ª´ v·ª±ng
	‚Ä¢	\lambda: h·ªá s·ªë ƒëi·ªÅu ch·ªânh

ƒê√¢y l√† b√†i to√°n c√¢n b·∫±ng gi·ªØa:
	‚Ä¢	ƒê·ªô n√©n chu·ªói
	‚Ä¢	K√≠ch th∆∞·ªõc embedding matrix

‚∏ª

8. Th·∫£o lu·∫≠n

Ph·∫ßn 2 c·ªßa d·ªØ li·ªáu th·ª±c nghi·ªám cho th·∫•y:
	‚Ä¢	Ph√¢n b·ªë kh√¥ng ho√†n to√†n tuy·∫øn t√≠nh
	‚Ä¢	C√≥ ƒëu√¥i d√†i nh·∫π (heavy-tail)
	‚Ä¢	M·ªôt s·ªë token ƒë·∫∑c bi·ªát d√†i ƒë·∫°i di·ªán cho chu·ªói ph·ªï bi·∫øn

ƒêi·ªÅu n√†y ph√π h·ª£p v·ªõi l√Ω thuy·∫øt:
	‚Ä¢	Ng√¥n ng·ªØ t·ª± nhi√™n c√≥ c·∫•u tr√∫c fractal
	‚Ä¢	Zipf v√† ph√¢n b·ªë m≈© th∆∞·ªùng xu·∫•t hi·ªán trong h·ªá th·ªëng th√¥ng tin

‚∏ª

9. K·∫øt lu·∫≠n

Ph√¢n b·ªë ƒë·ªô d√†i subword c√≥ th·ªÉ ƒë∆∞·ª£c m√¥ h√¨nh h√≥a g·∫ßn ƒë√∫ng b·∫±ng ph√¢n b·ªë m≈© r·ªùi r·∫°c:

P(L = k) \sim e^{-\lambda k}

T√°c ƒë·ªông tr·ª±c ti·∫øp ƒë·∫øn:

m = \frac{n}{\mathbb{E}[L]}

\text{Attention Cost} \sim O(m^2)

H = - \sum P(t)\log P(t)

Do ƒë√≥, thi·∫øt k·∫ø tokenizer l√† b√†i to√°n t·ªëi ∆∞u ƒëa m·ª•c ti√™u gi·ªØa:
	‚Ä¢	ƒê·ªô d√†i chu·ªói
	‚Ä¢	K√≠ch th∆∞·ªõc t·ª´ v·ª±ng
	‚Ä¢	Entropy th√¥ng tin
	‚Ä¢	Chi ph√≠ t√≠nh to√°n

‚∏ª

T√†i li·ªáu tham kh·∫£o
	1.	Sennrich, R., Haddow, B., & Birch, A. (2016). Neural Machine Translation of Rare Words with Subword Units.
	2.	Vaswani, A. et al. (2017). Attention Is All You Need.
	3.	Shannon, C. (1948). A Mathematical Theory of Communication.
	4.	Kudo, T. (2018). Subword Regularization.
	5.	Brown, T. et al. (2020). Language Models are Few-Shot Learners.
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [T·∫°i sao vƒÉn b·∫£n c·∫ßn ƒë∆∞·ª£c ƒë√°nh s·ªë?](aero_LLM_01_Why text needs to be numbered.md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_01_Why text needs to be numbered.md) |
| [Ph√¢n t√≠ch v√† chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh chu·ªói token ƒë∆∞·ª£c ƒë√°nh s·ªë: C∆° s·ªü to√°n h·ªçc v√† ·ª©ng d·ª•ng trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn](aero_LLM_02_Parsing text to numbered tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_02_Parsing text to numbered tokens.md) |
| [T·∫°o v√† tr·ª±c quan h√≥a Token trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn: C∆° s·ªü to√°n h·ªçc v√† ph√¢n t√≠ch bi·ªÉu di·ªÖn](aero_LLM_03_CodeChallenge Create and visualize tokens (part 1).md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_03_CodeChallenge Create and visualize tokens (part 1).md) |
| [T·∫°o v√† tr·ª±c quan h√≥a Token (Ph·∫ßn 2): Ph√¢n t√≠ch h√¨nh h·ªçc kh√¥ng gian embedding v√† Attention Map trong m√¥ h√¨nh Transformer](aero_LLM_04_CodeChallenge Create and visualize tokens (part 2).md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_04_CodeChallenge Create and visualize tokens (part 2).md) |
| [Chu·∫©n b·ªã vƒÉn b·∫£n cho Tokenization trong m√¥ h√¨nh Transformer: C∆° s·ªü l√Ω thuy·∫øt v√† ph√¢n t√≠ch to√°n h·ªçc](aero_LLM_05_Preparing text for tokenization.md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_05_Preparing text for tokenization.md) |
| [Ph√¢n t√≠ch quy tr√¨nh Tokenization qua v√≠ d·ª• *The Time Machine*: C∆° s·ªü thu·∫≠t to√°n v√† m√¥ h√¨nh h√≥a to√°n h·ªçc](aero_LLM_06_CodeChallenge Tokenizing The Time Machine.md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_06_CodeChallenge Tokenizing The Time Machine.md) |
| [So s√°nh Tokenization m·ª©c k√Ω t·ª±, t·ª´ v√† subword: Ph√¢n t√≠ch l√Ω thuy·∫øt v√† m√¥ h√¨nh to√°n h·ªçc](aero_LLM_07_Tokenizing characters vs. subwords vs. words.md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_07_Tokenizing characters vs. subwords vs. words.md) |
| [aero_LLM_08_Byte-pair encoding algorithm.md](aero_LLM_08_Byte-pair encoding algorithm.md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_08_Byte-pair encoding algorithm.md) |
| [Thu·∫≠t to√°n Byte Pair Encoding (BPE) v√† B√†i to√°n T·ªëi ∆∞u K√≠ch th∆∞·ªõc T·ª´ v·ª±ng trong M√¥ h√¨nh Ng√¥n ng·ªØ](aero_LLM_09_CodeChallenge Byte-pair encoding to a desired vocab size.md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_09_CodeChallenge Byte-pair encoding to a desired vocab size.md) |
| [aero_LLM_10_Exploring ChatGPT4's tokenizer.md](aero_LLM_10_Exploring ChatGPT4's tokenizer.md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_10_Exploring ChatGPT4's tokenizer.md) |
| [aero_LLM_11_CodeChallenge Token count by subword length (part 1).md](aero_LLM_11_CodeChallenge Token count by subword length (part 1).md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_11_CodeChallenge Token count by subword length (part 1).md) |
| üìå **[aero_LLM_12_CodeChallenge Token count by subword length (part 2).md](aero_LLM_12_CodeChallenge Token count by subword length (part 2).md)** | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_12_CodeChallenge Token count by subword length (part 2).md) |
| [aero_LLM_13_How many rs in strawberry.md](aero_LLM_13_How many rs in strawberry.md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_13_How many rs in strawberry.md) |
| [aero_LLM_14_CodeChallenge Create your algorithmic rapper name ).md](aero_LLM_14_CodeChallenge Create your algorithmic rapper name ).md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_14_CodeChallenge Create your algorithmic rapper name ).md) |
| [aero_LLM_15_Tokenization in BERT.md](aero_LLM_15_Tokenization in BERT.md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_15_Tokenization in BERT.md) |
| [aero_LLM_16_CodeChallenge Character counts in BERT tokens.md](aero_LLM_16_CodeChallenge Character counts in BERT tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_16_CodeChallenge Character counts in BERT tokens.md) |
| [aero_LLM_17_Translating between tokenizers.md](aero_LLM_17_Translating between tokenizers.md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_17_Translating between tokenizers.md) |
| [aero_LLM_18_CodeChallenge More on token translation.md](aero_LLM_18_CodeChallenge More on token translation.md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_18_CodeChallenge More on token translation.md) |
| [aero_LLM_19_CodeChallenge Tokenization compression ratios.md](aero_LLM_19_CodeChallenge Tokenization compression ratios.md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_19_CodeChallenge Tokenization compression ratios.md) |
| [aero_LLM_20_Tokenization in different languages.md](aero_LLM_20_Tokenization in different languages.md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_20_Tokenization in different languages.md) |
| [aero_LLM_21_CodeChallenge Zipf's law in characters and tokens.md](aero_LLM_21_CodeChallenge Zipf's law in characters and tokens.md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_21_CodeChallenge Zipf's law in characters and tokens.md) |
| [aero_LLM_22_Word variations in Claude tokenizer.md](aero_LLM_22_Word variations in Claude tokenizer.md) | [Xem b√†i vi·∫øt ‚Üí](aero_LLM_22_Word variations in Claude tokenizer.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

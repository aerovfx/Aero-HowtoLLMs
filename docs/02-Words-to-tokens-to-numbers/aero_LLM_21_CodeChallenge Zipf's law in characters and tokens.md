
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [02 Words to tokens to numbers](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Luáº­t Zipf trong PhÃ¢n bá»‘ KÃ½ tá»± vÃ  Token:

PhÃ¢n tÃ­ch Äá»‹nh lÆ°á»£ng vÃ  Há»‡ quáº£ Ä‘á»‘i vá»›i Tokenization trong MÃ´ hÃ¬nh NgÃ´n ngá»¯

â¸»

TÃ³m táº¯t

Dá»±a trÃªn tÃ i liá»‡u Ä‘Ã­nh kÃ¨m â€œZipfâ€™s Law in Characters and Tokensâ€, bÃ i viáº¿t nÃ y phÃ¢n tÃ­ch sá»± xuáº¥t hiá»‡n cá»§a luáº­t Zipf trong phÃ¢n bá»‘ táº§n suáº¥t kÃ½ tá»± vÃ  token trong vÄƒn báº£n tá»± nhiÃªn. ChÃºng tÃ´i xÃ¢y dá»±ng mÃ´ hÃ¬nh toÃ¡n há»c cho phÃ¢n bá»‘ thá»© háº¡ngâ€“táº§n suáº¥t, so sÃ¡nh hÃ nh vi giá»¯a má»©c kÃ½ tá»± vÃ  má»©c token (subword), vÃ  phÃ¢n tÃ­ch tÃ¡c Ä‘á»™ng Ä‘áº¿n thiáº¿t káº¿ tokenizer cÅ©ng nhÆ° chi phÃ­ tÃ­nh toÃ¡n cá»§a kiáº¿n trÃºc Transformer. CÃ¡c vÃ­ dá»¥ Ä‘Æ°á»£c minh há»a vá»›i tokenizer cá»§a BERT vÃ  GPT-2.

â¸»

1. Giá»›i thiá»‡u

Trong ngÃ´n ngá»¯ tá»± nhiÃªn, táº§n suáº¥t xuáº¥t hiá»‡n cá»§a Ä‘Æ¡n vá»‹ ngÃ´n ngá»¯ (kÃ½ tá»±, tá»«, token) khÃ´ng phÃ¢n bá»‘ Ä‘á»u mÃ  tuÃ¢n theo quy luáº­t lÅ©y thá»«a, Ä‘Æ°á»£c biáº¿t Ä‘áº¿n lÃ  Luáº­t Zipf, do George Kingsley Zipf Ä‘á» xuáº¥t.

Náº¿u r lÃ  thá»© háº¡ng cá»§a má»™t Ä‘Æ¡n vá»‹ (1 lÃ  phá»• biáº¿n nháº¥t), thÃ¬ táº§n suáº¥t f(r) Ä‘Æ°á»£c xáº¥p xá»‰ bá»Ÿi:

f(r) \propto \frac{1}{r^\alpha}

vá»›i:

\alpha \approx 1

Luáº­t nÃ y xuáº¥t hiá»‡n á»Ÿ cáº£ má»©c kÃ½ tá»± vÃ  má»©c token.

â¸»

2. Luáº­t Zipf á»Ÿ Má»©c KÃ½ tá»±

Gá»i:
	â€¢	\Sigma: báº£ng chá»¯ cÃ¡i
	â€¢	|\Sigma| = K

Sáº¯p xáº¿p kÃ½ tá»± theo táº§n suáº¥t giáº£m dáº§n.

f_c(r) = C r^{-\alpha_c}

Tá»•ng xÃ¡c suáº¥t:

\sum_{r=1}^{K} f_c(r) = 1

Chuáº©n hÃ³a:

C = \left( \sum_{r=1}^{K} r^{-\alpha_c} \right)^{-1}

Vá»›i tiáº¿ng Anh:

\alpha_c \approx 1

Do báº£ng chá»¯ cÃ¡i nhá» (26â€“100 kÃ½ tá»±), phÃ¢n bá»‘ cÃ³ Ä‘uÃ´i ngáº¯n.

â¸»

3. Luáº­t Zipf á»Ÿ Má»©c Token

Vá»›i token (subword), kÃ­ch thÆ°á»›c tá»« vá»±ng:

|V| \approx 30{,}000

PhÃ¢n bá»‘:

f_t(r) = C' r^{-\alpha_t}

ThÃ´ng thÆ°á»ng:

\alpha_t \in [0.8, 1.2]

PhÃ¢n bá»‘ token cÃ³ Ä‘uÃ´i dÃ i hÆ¡n nhiá»u so vá»›i kÃ½ tá»±.

â¸»

4. So sÃ¡nh Entropy

Entropy kÃ½ tá»±:

H_c = - \sum_{r=1}^{K} f_c(r)\log f_c(r)

Entropy token:

H_t = - \sum_{r=1}^{|V|} f_t(r)\log f_t(r)

Vá»›i phÃ¢n bá»‘ Zipf:

H \approx \log Z(\alpha) + \frac{\alpha}{Z(\alpha)} \sum_{r} r^{-\alpha}\log r

Trong Ä‘Ã³:

Z(\alpha) = \sum_{r=1}^{N} r^{-\alpha}

VÃ¬ |V| \gg K, nÃªn:

H_t > H_c

â¸»

5. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Tá»· lá»‡ NÃ©n

Giáº£ sá»­ vÄƒn báº£n cÃ³:
	â€¢	n kÃ½ tá»±
	â€¢	m token

Compression ratio:

R = \frac{n}{m}

Theo báº£o toÃ n thÃ´ng tin:

n H_c \approx m H_t

Suy ra:

R \approx \frac{H_t}{H_c}

Náº¿u H_t tÄƒng (do Ä‘uÃ´i dÃ i cá»§a Zipf), R tÄƒng â†’ chuá»—i token ngáº¯n hÆ¡n.

â¸»

6. Há»‡ quáº£ Ä‘á»‘i vá»›i Transformer

Self-attention cÃ³ Ä‘á»™ phá»©c táº¡p:

O(m^2)

Thay m = \frac{n}{R}:

O\left(\frac{n^2}{R^2}\right)

VÃ¬ luáº­t Zipf táº¡o ra:
	â€¢	Ãt token cá»±c ká»³ phá»• biáº¿n
	â€¢	Nhiá»u token hiáº¿m

Gradient trong huáº¥n luyá»‡n sáº½:

\text{Var}(\nabla) \uparrow

Ä‘á»‘i vá»›i token hiáº¿m.

â¸»

7. PhÃ¢n tÃ­ch Phá»• Táº§n suáº¥t (Frequency Spectrum)

Tá»•ng sá»‘ láº§n xuáº¥t hiá»‡n cá»§a token thá»© háº¡ng r:

N_r = N_1 r^{-\alpha}

Tá»•ng sá»‘ token trong corpus:

T = \sum_{r=1}^{|V|} N_r

Xáº¥p xá»‰ tÃ­ch phÃ¢n:

T \approx N_1 \int_1^{|V|} r^{-\alpha} dr

Náº¿u \alpha = 1:

T \approx N_1 \log |V|

Äiá»u nÃ y giáº£i thÃ­ch táº¡i sao:
	â€¢	TÄƒng tá»« vá»±ng â†’ tÄƒng nháº¹ tá»•ng khá»‘i lÆ°á»£ng thÃ´ng tin
	â€¢	ÄuÃ´i dÃ i váº«n chiáº¿m pháº§n Ä‘Ã¡ng ká»ƒ

â¸»

8. So sÃ¡nh giá»¯a KÃ½ tá»± vÃ  Token trong Thá»±c táº¿

8.1 á» má»©c kÃ½ tá»±
	â€¢	Báº£ng chá»¯ cÃ¡i nhá»
	â€¢	PhÃ¢n bá»‘ Ã­t cá»±c Ä‘oan

8.2 á» má»©c token (WordPiece/BPE)

Ãp dá»¥ng trong BERT vÃ  GPT-2:
	â€¢	Má»™t sá»‘ token cá»±c phá»• biáº¿n (â€œtheâ€, â€œ##ingâ€)
	â€¢	Nhiá»u token xuáº¥t hiá»‡n ráº¥t hiáº¿m

ÄuÃ´i dÃ i máº¡nh hÆ¡n â†’ phÃ¹ há»£p luáº­t Zipf.

â¸»

9. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Thiáº¿t káº¿ Tokenizer

Náº¿u tá»« vá»±ng quÃ¡ nhá»:

|V| \downarrow \Rightarrow \alpha_t \uparrow

PhÃ¢n bá»‘ dá»‘c hÆ¡n â†’ token phá»• biáº¿n chiáº¿m Æ°u tháº¿.

Náº¿u tá»« vá»±ng quÃ¡ lá»›n:

|V| \uparrow \Rightarrow \text{Ä‘uÃ´i dÃ i máº¡nh}

Tá»‘i Æ°u hÃ³a:

\min_{|V|} \left( \frac{n^2}{R^2} + \lambda |V| \right)

â¸»

10. Tháº£o luáº­n

Luáº­t Zipf cho tháº¥y:
	1.	NgÃ´n ngá»¯ tá»± nhiÃªn cÃ³ cáº¥u trÃºc tá»± tá»• chá»©c
	2.	Tokenization káº¿ thá»«a tÃ­nh cháº¥t lÅ©y thá»«a
	3.	PhÃ¢n bá»‘ Ä‘uÃ´i dÃ i áº£nh hÆ°á»Ÿng Ä‘áº¿n huáº¥n luyá»‡n
	4.	Thiáº¿t káº¿ tokenizer pháº£i cÃ¢n báº±ng giá»¯a nÃ©n vÃ  phÃ¢n bá»‘ táº§n suáº¥t

CÃ¡c há»‡ nhÆ° Google vÃ  OpenAI Ä‘Ã£ chá»n kÃ­ch thÆ°á»›c tá»« vá»±ng nháº±m cÃ¢n báº±ng giá»¯a entropy vÃ  chi phÃ­ tÃ­nh toÃ¡n.

â¸»

11. Káº¿t luáº­n

Luáº­t Zipf trong kÃ½ tá»± vÃ  token Ä‘Æ°á»£c mÃ´ táº£ bá»Ÿi:

f(r) \propto r^{-\alpha}

Entropy:

H = -\sum f(r)\log f(r)

Compression ratio:

R \approx \frac{H_t}{H_c}

Chi phÃ­ attention:

O\left(\frac{n^2}{R^2}\right)

Do Ä‘Ã³, phÃ¢n bá»‘ lÅ©y thá»«a khÃ´ng chá»‰ lÃ  hiá»‡n tÆ°á»£ng ngÃ´n ngá»¯ há»c mÃ  cÃ²n áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n hiá»‡u nÄƒng tÃ­nh toÃ¡n cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯.

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Zipf, G. K. (1935). The Psycho-Biology of Language.
	2.	Shannon, C. (1948). A Mathematical Theory of Communication.
	3.	Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
	4.	Radford et al. (2019). GPT-2: Language Models are Unsupervised Multitask Learners.
	5.	Vaswani et al. (2017). Attention Is All You Need.
	6.	Sennrich et al. (2016). Neural Machine Translation of Rare Words with Subword Units.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_LLM_01_Why text needs to be numbered.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Why text needs to be numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_LLM_02_Parsing text to numbered tokens.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Parsing text to numbered tokens.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_LLM_03_CodeChallenge Create and visualize tokens (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Create and visualize tokens (part 1).md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_LLM_04_CodeChallenge Create and visualize tokens (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Create and visualize tokens (part 2).md) |
| [Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_LLM_05_Preparing text for tokenization.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Preparing text for tokenization.md) |
| [PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_LLM_06_CodeChallenge Tokenizing The Time Machine.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_CodeChallenge Tokenizing The Time Machine.md) |
| [So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_LLM_07_Tokenizing characters vs. subwords vs. words.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Tokenizing characters vs. subwords vs. words.md) |
| [aero_LLM_08_Byte-pair encoding algorithm.md](aero_LLM_08_Byte-pair encoding algorithm.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_Byte-pair encoding algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_LLM_09_CodeChallenge Byte-pair encoding to a desired vocab size.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Byte-pair encoding to a desired vocab size.md) |
| [aero_LLM_10_Exploring ChatGPT4's tokenizer.md](aero_LLM_10_Exploring ChatGPT4's tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_Exploring ChatGPT4's tokenizer.md) |
| [aero_LLM_11_CodeChallenge Token count by subword length (part 1).md](aero_LLM_11_CodeChallenge Token count by subword length (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Token count by subword length (part 1).md) |
| [aero_LLM_12_CodeChallenge Token count by subword length (part 2).md](aero_LLM_12_CodeChallenge Token count by subword length (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_CodeChallenge Token count by subword length (part 2).md) |
| [aero_LLM_13_How many rs in strawberry.md](aero_LLM_13_How many rs in strawberry.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_How many rs in strawberry.md) |
| [aero_LLM_14_CodeChallenge Create your algorithmic rapper name ).md](aero_LLM_14_CodeChallenge Create your algorithmic rapper name ).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_CodeChallenge Create your algorithmic rapper name ).md) |
| [aero_LLM_15_Tokenization in BERT.md](aero_LLM_15_Tokenization in BERT.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_Tokenization in BERT.md) |
| [aero_LLM_16_CodeChallenge Character counts in BERT tokens.md](aero_LLM_16_CodeChallenge Character counts in BERT tokens.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_CodeChallenge Character counts in BERT tokens.md) |
| [aero_LLM_17_Translating between tokenizers.md](aero_LLM_17_Translating between tokenizers.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_Translating between tokenizers.md) |
| [aero_LLM_18_CodeChallenge More on token translation.md](aero_LLM_18_CodeChallenge More on token translation.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_CodeChallenge More on token translation.md) |
| [aero_LLM_19_CodeChallenge Tokenization compression ratios.md](aero_LLM_19_CodeChallenge Tokenization compression ratios.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_19_CodeChallenge Tokenization compression ratios.md) |
| [aero_LLM_20_Tokenization in different languages.md](aero_LLM_20_Tokenization in different languages.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_20_Tokenization in different languages.md) |
| ğŸ“Œ **[aero_LLM_21_CodeChallenge Zipf's law in characters and tokens.md](aero_LLM_21_CodeChallenge Zipf's law in characters and tokens.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_21_CodeChallenge Zipf's law in characters and tokens.md) |
| [aero_LLM_22_Word variations in Claude tokenizer.md](aero_LLM_22_Word variations in Claude tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_22_Word variations in Claude tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

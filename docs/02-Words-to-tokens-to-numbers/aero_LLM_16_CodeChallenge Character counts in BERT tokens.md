
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [02 Words to tokens to numbers](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
PhÃ¢n tÃ­ch Sá»‘ lÆ°á»£ng KÃ½ tá»± trong Token cá»§a BERT:

MÃ´ hÃ¬nh Thá»‘ng kÃª, Entropy vÃ  áº¢nh hÆ°á»Ÿng Ä‘áº¿n Äá»™ phá»©c táº¡p Transformer

â¸»

TÃ³m táº¯t

BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch sá»‘ lÆ°á»£ng kÃ½ tá»± cáº¥u thÃ nh má»—i token trong bá»™ tokenizer cá»§a BERT do Google phÃ¡t triá»ƒn, dá»±a trÃªn dá»¯ liá»‡u tá»« tÃ i liá»‡u Ä‘Ã­nh kÃ¨m. ChÃºng tÃ´i xÃ¢y dá»±ng mÃ´ hÃ¬nh thá»‘ng kÃª cho phÃ¢n bá»‘ Ä‘á»™ dÃ i token, Æ°á»›c lÆ°á»£ng entropy há»‡ subword, vÃ  phÃ¢n tÃ­ch tÃ¡c Ä‘á»™ng cá»§a Ä‘á»™ dÃ i kÃ½ tá»± Ä‘áº¿n Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n trong kiáº¿n trÃºc Transformer. Káº¿t quáº£ cho tháº¥y phÃ¢n bá»‘ Ä‘á»™ dÃ i token cÃ³ xu hÆ°á»›ng lá»‡ch pháº£i (right-skewed), gáº§n vá»›i phÃ¢n bá»‘ hÃ¬nh há»c hoáº·c log-linear, pháº£n Ã¡nh sá»± cÃ¢n báº±ng giá»¯a kÃ­ch thÆ°á»›c tá»« vá»±ng vÃ  Ä‘á»™ dÃ i chuá»—i Ä‘áº§u vÃ o.

â¸»

1. Giá»›i thiá»‡u

Trong BERT, vÄƒn báº£n Ä‘áº§u vÃ o Ä‘Æ°á»£c token hÃ³a báº±ng thuáº­t toÃ¡n WordPiece thÃ nh cÃ¡c subword token:

S = (w_1, w_2, ..., w_n)

T = (t_1, t_2, ..., t_m)

Vá»›i:

m \ge n

Má»—i token t_i cÃ³ Ä‘á»™ dÃ i kÃ½ tá»±:

\ell(t_i)

Má»¥c tiÃªu nghiÃªn cá»©u:
	1.	PhÃ¢n bá»‘ xÃ¡c suáº¥t cá»§a \ell(t)
	2.	Äá»™ dÃ i trung bÃ¬nh token
	3.	áº¢nh hÆ°á»Ÿng Ä‘áº¿n chi phÃ­ self-attention

â¸»

2. MÃ´ hÃ¬nh Thá»‘ng kÃª PhÃ¢n bá»‘ Äá»™ dÃ i Token

2.1 Äá»‹nh nghÄ©a

Gá»i:
	â€¢	V: táº­p tá»« vá»±ng BERT
	â€¢	|V| \approx 30{,}000
	â€¢	N_k: sá»‘ token cÃ³ Ä‘á»™ dÃ i kÃ½ tá»± báº±ng k

XÃ¡c suáº¥t:

P(L = k) = \frac{N_k}{|V|}

Chuáº©n hÃ³a:

\sum_{k=1}^{K_{\max}} P(L=k) = 1

â¸»

2.2 MÃ´ hÃ¬nh HÃ¬nh há»c (Geometric Approximation)

Quan sÃ¡t thá»±c nghiá»‡m cho tháº¥y:

N_k \approx Ae^{-\lambda k}

Suy ra:

P(L=k) = (1-q)q^{k-1}

Trong Ä‘Ã³:

q = e^{-\lambda}

ÄÃ¢y lÃ  phÃ¢n bá»‘ hÃ¬nh há»c rá»i ráº¡c.

â¸»

2.3 Ká»³ vá»ng vÃ  PhÆ°Æ¡ng sai

Ká»³ vá»ng:

\mathbb{E}[L] = \frac{1}{1-q}

PhÆ°Æ¡ng sai:

\mathrm{Var}(L) = \frac{q}{(1-q)^2}

Náº¿u q \to 1, phÃ¢n bá»‘ cÃ³ Ä‘uÃ´i dÃ i hÆ¡n (nhiá»u token dÃ i).

â¸»

3. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Äá»™ dÃ i Chuá»—i VÄƒn báº£n

Giáº£ sá»­ vÄƒn báº£n cÃ³ tá»•ng sá»‘ kÃ½ tá»± n.

Sá»‘ token trung bÃ¬nh:

m = \frac{n}{\mathbb{E}[L]}

Self-attention trong Transformer encoder:

O(m^2)

Thay vÃ o:

O\left(\left(\frac{n}{\mathbb{E}[L]}\right)^2\right)

Khi \mathbb{E}[L] \uparrow, chi phÃ­ giáº£m.

â¸»

4. Entropy cá»§a Há»‡ Token

Entropy theo phÃ¢n bá»‘ Ä‘á»™ dÃ i:

H_L = - \sum_{k} P(L=k)\log P(L=k)

Thay phÃ¢n bá»‘ hÃ¬nh há»c:

H_L = - \sum_{k=1}^{\infty} (1-q)q^{k-1} \log[(1-q)q^{k-1}]

RÃºt gá»n:

H_L = -\log(1-q) - \frac{q}{1-q}\log q

Entropy cÃ ng lá»›n â†’ Ä‘á»™ Ä‘a dáº¡ng Ä‘á»™ dÃ i cÃ ng cao.

â¸»

5. Quan há»‡ vá»›i Luáº­t Zipf

Táº§n suáº¥t token thÆ°á»ng tuÃ¢n theo:

f(r) \propto \frac{1}{r^\alpha}

Trong Ä‘Ã³:
	â€¢	r: thá»© háº¡ng token
	â€¢	\alpha \approx 1

Token ngáº¯n thÆ°á»ng:
	â€¢	CÃ³ táº§n suáº¥t cao
	â€¢	á» thá»© háº¡ng tháº¥p

Do Ä‘Ã³ tá»“n táº¡i tÆ°Æ¡ng quan nghá»‹ch:

\ell(t) \propto \log r

â¸»

6. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Embedding Matrix

Embedding:

E: V \rightarrow \mathbb{R}^d

Ma tráº­n embedding:

W \in \mathbb{R}^{|V| \times d}

BÃ i toÃ¡n tá»‘i Æ°u:

\min_{V} \left( \mathbb{E}[m] + \lambda |V| \right)

Trong Ä‘Ã³:
	â€¢	\mathbb{E}[m]: sá»‘ token trung bÃ¬nh
	â€¢	|V|: kÃ­ch thÆ°á»›c tá»« vá»±ng
	â€¢	\lambda: há»‡ sá»‘ cÃ¢n báº±ng

â¸»

7. So sÃ¡nh vá»›i Character-level Modeling

MÃ´ hÃ¬nh	Äá»™ dÃ i trung bÃ¬nh	OOV	Chi phÃ­
Character-level	1	KhÃ´ng	Ráº¥t cao
Word-level	Lá»›n	Cao	Trung bÃ¬nh
WordPiece	Trung bÃ¬nh	Tháº¥p	Tá»‘i Æ°u

Náº¿u xá»­ lÃ½ á»Ÿ má»©c kÃ½ tá»±:

m = n

Chi phÃ­:

O(n^2)

WordPiece giáº£m:

m = \frac{n}{\mathbb{E}[L]}

â¸»

8. Tháº£o luáº­n

Dá»¯ liá»‡u thá»±c nghiá»‡m cho tháº¥y:
	â€¢	Pháº§n lá»›n token cÃ³ Ä‘á»™ dÃ i nhá» (1â€“5 kÃ½ tá»±)
	â€¢	Token dÃ i tá»“n táº¡i nhÆ°ng Ã­t
	â€¢	PhÃ¢n bá»‘ cÃ³ Ä‘uÃ´i nháº¹ (mild heavy-tail)

Äiá»u nÃ y pháº£n Ã¡nh:
	â€¢	Sá»± cÃ¢n báº±ng giá»¯a kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a vÃ  Ä‘á»™ nÃ©n
	â€¢	Tá»‘i Æ°u hÃ³a thá»±c nghiá»‡m hÆ¡n lÃ  lÃ½ thuyáº¿t thuáº§n tÃºy

â¸»

9. Káº¿t luáº­n

PhÃ¢n bá»‘ Ä‘á»™ dÃ i kÃ½ tá»± cá»§a token trong BERT cÃ³ thá»ƒ mÃ´ hÃ¬nh hÃ³a gáº§n Ä‘Ãºng báº±ng phÃ¢n bá»‘ hÃ¬nh há»c:

P(L=k) \sim q^{k-1}

TÃ¡c Ä‘á»™ng trá»±c tiáº¿p Ä‘áº¿n:

m = \frac{n}{\mathbb{E}[L]}

\text{Attention Cost} \sim O(m^2)

H_L = - \sum P(L)\log P(L)

Thiáº¿t káº¿ tokenizer lÃ  bÃ i toÃ¡n tá»‘i Æ°u Ä‘a má»¥c tiÃªu giá»¯a:
	â€¢	KÃ­ch thÆ°á»›c tá»« vá»±ng
	â€¢	Äá»™ dÃ i chuá»—i
	â€¢	Entropy thÃ´ng tin
	â€¢	Chi phÃ­ tÃ­nh toÃ¡n

â¸»

TÃ i liá»‡u tham kháº£o
	1.	BERT â€“ Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
	2.	Sennrich et al. (2016). Neural Machine Translation of Rare Words with Subword Units.
	3.	Vaswani et al. (2017). Attention Is All You Need.
	4.	Shannon, C. (1948). A Mathematical Theory of Communication.
	5.	Jurafsky & Martin. Speech and Language Processing.
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

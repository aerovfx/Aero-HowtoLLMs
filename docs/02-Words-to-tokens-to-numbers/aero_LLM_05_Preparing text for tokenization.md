
<!-- Aero-Navigation-Start -->
[üè† Home](../../index.md) > [02 Words to tokens to numbers](../index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../index.md)
- [üìö Module 01: LLM Course](../../01-LLM_Course/index.md)
- [üî¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../04-buildGPT/index.md)
- [üéØ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [üîç Module 19: AI Safety](../../19-AI-safety/index.md)
- [üêç Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Chu·∫©n b·ªã vƒÉn b·∫£n cho Tokenization trong m√¥ h√¨nh Transformer: C∆° s·ªü l√Ω thuy·∫øt v√† ph√¢n t√≠ch to√°n h·ªçc

---

## T√≥m t·∫Øt

B√†i b√°o n√†y tr√¨nh b√†y m·ªôt c√°ch h·ªá th·ªëng quy tr√¨nh **chu·∫©n b·ªã vƒÉn b·∫£n tr∆∞·ªõc khi tokenization** trong c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn d·ª±a tr√™n ki·∫øn tr√∫c Attention Is All You Need. N·ªôi dung ph√¢n t√≠ch c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω (text normalization, cleaning, encoding), c∆° ch·∫ø m√£ h√≥a Byte Pair Encoding (BPE), v√† vai tr√≤ c·ªßa tokenizer trong c√°c m√¥ h√¨nh GPT do OpenAI ph√°t tri·ªÉn. B√†i vi·∫øt b·ªï sung c√°c c√¥ng th·ª©c to√°n h·ªçc m√¥ t·∫£ x√°c su·∫•t chu·ªói, √°nh x·∫° r·ªùi r·∫°c‚Äìli√™n t·ª•c v√† c·∫•u tr√∫c ƒë·∫°i s·ªë c·ªßa qu√° tr√¨nh m√£ h√≥a.

---

# 1. Gi·ªõi thi·ªáu

Trong c√°c m√¥ h√¨nh Transformer hi·ªán ƒë·∫°i nh∆∞ GPT-4, vƒÉn b·∫£n ƒë·∫ßu v√†o kh√¥ng ƒë∆∞·ª£c x·ª≠ l√Ω tr·ª±c ti·∫øp d∆∞·ªõi d·∫°ng k√Ω t·ª± m√† ph·∫£i tr·∫£i qua qu√° tr√¨nh:

[
\text{Raw Text} \rightarrow \text{Normalization} \rightarrow \text{Tokenization} \rightarrow \text{Embedding}
]

Tokenization ƒë√≥ng vai tr√≤ l√† c·∫ßu n·ªëi gi·ªØa:

* Kh√¥ng gian r·ªùi r·∫°c c·ªßa k√Ω t·ª±
* Kh√¥ng gian vector li√™n t·ª•c c·ªßa embedding

N·∫øu g·ªçi chu·ªói vƒÉn b·∫£n ban ƒë·∫ßu l√†:

[
X = (c_1, c_2, \dots, c_n)
]

th√¨ tokenizer √°nh x·∫°:

[
\tau: \Sigma^* \rightarrow \mathbb{Z}^m
]

v·ªõi (\Sigma) l√† b·∫£ng ch·ªØ c√°i v√† (\mathbb{Z}^m) l√† chu·ªói ID token.

---

# 2. Chu·∫©n h√≥a vƒÉn b·∫£n (Text Normalization)

Chu·∫©n h√≥a gi√∫p ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n c·ªßa d·ªØ li·ªáu hu·∫•n luy·ªán.

## 2.1 Lowercasing

√Ånh x·∫°:

[
f_{lower}(c) = \text{lowercase}(c)
]

V√≠ d·ª•:

[
\text{"ChatGPT"} \rightarrow \text{"chatgpt"}
]

## 2.2 Unicode Normalization

VƒÉn b·∫£n Unicode c√≥ th·ªÉ bi·ªÉu di·ªÖn c√πng m·ªôt k√Ω t·ª± theo nhi·ªÅu c√°ch.

Chu·∫©n NFC:

[
\text{√©} = e + \acute{}
]

Chu·∫©n h√≥a ƒë·∫£m b·∫£o:

[
NFC(x_1) = NFC(x_2)
]

n·∫øu hai chu·ªói t∆∞∆°ng ƒë∆∞∆°ng v·ªÅ m·∫∑t ng·ªØ nghƒ©a.

---

# 3. Tokenization: C∆° s·ªü x√°c su·∫•t

M√¥ h√¨nh ng√¥n ng·ªØ t·ªëi ∆∞u x√°c su·∫•t:

[
P(X) = \prod_{t=1}^{T} P(x_t \mid x_{<t})
]

Tuy nhi√™n, n·∫øu l√†m vi·ªác ·ªü m·ª©c k√Ω t·ª±:

[
T = n
]

S·ªë b∆∞·ªõc d·ª± ƒëo√°n l·ªõn ‚Üí chi ph√≠ cao.

Gi·∫£i ph√°p:

Chia th√†nh token:

[
X = (w_1, w_2, \dots, w_m), \quad m < n
]

Gi·∫£m ƒë·ªô d√†i chu·ªói v√† tƒÉng t√≠nh bi·ªÉu di·ªÖn.

---

# 4. Byte Pair Encoding (BPE)

BPE ƒë∆∞·ª£c gi·ªõi thi·ªáu cho NLP b·ªüi Sennrich et al. (2016).

## 4.1 Thu·∫≠t to√°n

Ban ƒë·∫ßu:

[
V_0 = { \text{t·∫≠p k√Ω t·ª± ƒë∆°n} }
]

L·∫∑p:

1. T√¨m c·∫∑p k√Ω t·ª± xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
2. G·ªôp th√†nh token m·ªõi
3. C·∫≠p nh·∫≠t t·ª´ v·ª±ng

Gi·∫£ s·ª≠ t·∫ßn su·∫•t c·∫∑p ((a,b)):

[
f(a,b) = \sum_{i} \mathbb{I}[(a,b) \in X_i]
]

Ch·ªçn:

[
(a^*, b^*) = \arg\max_{a,b} f(a,b)
]

C·∫≠p nh·∫≠t:

[
V_{k+1} = V_k \cup {ab}
]

---

# 5. Kh√¥ng gian r·ªùi r·∫°c v√† √°nh x·∫° embedding

Sau tokenization:

[
w_i \rightarrow id_i \in {1, \dots, |V|}
]

Embedding matrix:

[
E \in \mathbb{R}^{|V| \times d}
]

√Ånh x·∫°:

[
e_i = E[id_i]
]

To√†n b·ªô chu·ªói:

[
X \rightarrow (e_1, e_2, \dots, e_m)
]

---

# 6. Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p

N·∫øu:

* (N) l√† s·ªë k√Ω t·ª±
* (V) l√† k√≠ch th∆∞·ªõc t·ª´ v·ª±ng

Chi ph√≠ x√¢y d·ª±ng BPE:

[
\mathcal{O}(N \log V)
]

Chi ph√≠ suy lu·∫≠n tokenization:

[
\mathcal{O}(m)
]

---

# 7. V·∫•n ƒë·ªÅ Out-of-Vocabulary (OOV)

Kh√¥ng nh∆∞ Word2Vec truy·ªÅn th·ªëng, BPE ƒë·∫£m b·∫£o:

[
\forall x \in \Sigma^*, \exists \text{ decomposition into subwords}
]

V√≠ d·ª•:

```
tokenization ‚Üí token + ization
```

ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o:

[
P(x) > 0
]

cho m·ªçi chu·ªói h·ª£p l·ªá.

---

# 8. So s√°nh v·ªõi c√°c ph∆∞∆°ng ph√°p kh√°c

| Ph∆∞∆°ng ph√°p     | ƒê·∫∑c ƒëi·ªÉm        | H·∫°n ch·∫ø           |
| --------------- | --------------- | ----------------- |
| Word-level      | Ng·∫Øn, d·ªÖ hi·ªÉu   | OOV cao           |
| Character-level | Kh√¥ng OOV       | Chu·ªói d√†i         |
| BPE             | C√¢n b·∫±ng        | Ph·ª• thu·ªôc d·ªØ li·ªáu |
| Unigram LM      | X√°c su·∫•t t·ªëi ∆∞u | T√≠nh to√°n cao     |

Unigram Language Model t·ªëi ∆∞u:

[
\max_{\theta} \prod_i \sum_{z \in \mathcal{Z}(x_i)} P(z|\theta)
]

---

# 9. T√°c ƒë·ªông ƒë·∫øn Attention

ƒê·ªô d√†i chu·ªói ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn chi ph√≠ self-attention:

[
\text{Complexity} = \mathcal{O}(T^2 d)
]

N·∫øu tokenization k√©m ‚Üí (T) l·ªõn ‚Üí chi ph√≠ tƒÉng.

Do ƒë√≥, tokenizer t·ªëi ∆∞u gi√∫p:

* Gi·∫£m memory footprint
* TƒÉng t·ªëc inference
* C·∫£i thi·ªán ch·∫•t l∆∞·ª£ng ng·ªØ nghƒ©a

---

# 10. Li√™n h·ªá th·ª±c t·∫ø trong GPT

C√°c m√¥ h√¨nh GPT s·ª≠ d·ª•ng bi·∫øn th·ªÉ c·ªßa BPE ho·∫∑c byte-level BPE.

X√°c su·∫•t sinh token:

[
P(w_t | w_{<t}) =
\frac{\exp(z_t W_{out})}
{\sum_j \exp(z_j W_{out})}
]

Ch·∫•t l∆∞·ª£ng tokenization ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn ph√¢n ph·ªëi logits.

---

# 11. Th·∫£o lu·∫≠n

Chu·∫©n b·ªã vƒÉn b·∫£n kh√¥ng ch·ªâ l√† b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω k·ªπ thu·∫≠t m√† c√≤n l√†:

* B√†i to√°n t·ªëi ∆∞u th√¥ng tin
* B√†i to√°n m√£ h√≥a ngu·ªìn (source coding)
* B√†i to√°n n√©n d·ªØ li·ªáu

Theo ƒë·ªãnh l√Ω Shannon:

[
H(X) = - \sum_x P(x) \log P(x)
]

Tokenizer t·ªët gi√∫p:

[
\text{Length}(X_{tokens}) \approx \frac{H(X)}{\log |V|}
]

---

# 12. K·∫øt lu·∫≠n

Quy tr√¨nh chu·∫©n b·ªã vƒÉn b·∫£n cho tokenization bao g·ªìm:

1. Chu·∫©n h√≥a Unicode
2. L√†m s·∫°ch d·ªØ li·ªáu
3. √Åp d·ª•ng BPE ho·∫∑c Unigram LM
4. √Ånh x·∫° sang embedding

To√°n h·ªçc cho th·∫•y tokenization l√† qu√° tr√¨nh:

[
\Sigma^* \rightarrow V^*
]

gi√∫p t·ªëi ∆∞u:

* ƒê·ªô d√†i chu·ªói
* ƒê·ªô ph·ª©c t·∫°p t√≠nh to√°n
* Bi·ªÉu di·ªÖn ng·ªØ nghƒ©a

---

# T√†i li·ªáu tham kh·∫£o

1. Attention Is All You Need
2. Sennrich, R., Haddow, B., Birch, A. (2016). *Neural Machine Translation of Rare Words with Subword Units*.
3. Kudo, T. (2018). *Subword Regularization: Improving Neural Network Translation Models*.
4. Shannon, C. (1948). *A Mathematical Theory of Communication*.
5. Jurafsky, D., Martin, J. (2023). *Speech and Language Processing*.
<!-- Aero-Footer-Start -->
---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

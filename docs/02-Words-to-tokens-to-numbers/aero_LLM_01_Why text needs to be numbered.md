
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [02 Words to tokens to numbers](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?

## PhÃ¢n tÃ­ch khoa há»c vá» cáº¥u trÃºc, tÃ­nh toÃ¡n vÃ  tá»‘i Æ°u hÃ³a xá»­ lÃ½ ngÃ´n ngá»¯

---

## TÃ³m táº¯t

Trong cÃ¡c há»‡ thá»‘ng xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP) hiá»‡n Ä‘áº¡i, viá»‡c **Ä‘Ã¡nh sá»‘ vÄƒn báº£n (token numbering / positional indexing)** Ä‘Ã³ng vai trÃ² ná»n táº£ng trong biá»ƒu diá»…n chuá»—i, tÃ­nh toÃ¡n attention vÃ  tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh Transformer. BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch vai trÃ² cá»§a Ä‘Ã¡nh sá»‘ vÄƒn báº£n dÆ°á»›i gÃ³c Ä‘á»™ toÃ¡n há»c, khoa há»c nháº­n thá»©c vÃ  kiáº¿n trÃºc mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n. PhÃ¢n tÃ­ch dá»±a trÃªn kiáº¿n trÃºc Transformer cá»§a Vaswani et al. (2017) vÃ  cÃ¡c mÃ´ hÃ¬nh GPT do OpenAI phÃ¡t triá»ƒn.

---

# 1. Giá»›i thiá»‡u

Con ngÆ°á»i hiá»ƒu vÄƒn báº£n theo trÃ¬nh tá»± tuyáº¿n tÃ­nh. MÃ¡y tÃ­nh cÅ©ng cáº§n má»™t cÆ¡ cháº¿ tÆ°Æ¡ng tá»± Ä‘á»ƒ:

* PhÃ¢n biá»‡t vá»‹ trÃ­ token
* XÃ¡c Ä‘á»‹nh quan há»‡ phá»¥ thuá»™c
* TÃ­nh toÃ¡n attention

Náº¿u khÃ´ng Ä‘Ã¡nh sá»‘ hoáº·c mÃ£ hÃ³a vá»‹ trÃ­, chuá»—i:

> â€œTÃ´i yÃªu AIâ€

vÃ 

> â€œAI yÃªu tÃ´iâ€

sáº½ cÃ³ cÃ¹ng táº­p token nhÆ°ng Ã½ nghÄ©a hoÃ n toÃ n khÃ¡c.

Váº¥n Ä‘á» nÃ y dáº«n Ä‘áº¿n nhu cáº§u **positional encoding** trong cÃ¡c mÃ´ hÃ¬nh Transformer.

---

# 2. Biá»ƒu diá»…n chuá»—i dÆ°á»›i dáº¡ng toÃ¡n há»c

Giáº£ sá»­ má»™t cÃ¢u gá»“m ( T ) token:

[
x = (x_1, x_2, ..., x_T)
]

Má»—i token Ä‘Æ°á»£c Ã¡nh xáº¡ thÃ nh vector embedding:

[
e_i = E(x_i)
]

Náº¿u khÃ´ng cÃ³ Ä‘Ã¡nh sá»‘ vá»‹ trÃ­, ta chá»‰ cÃ³:

[
X = (e_1, e_2, ..., e_T)
]

NhÆ°ng self-attention thuáº§n tÃºy lÃ  **báº¥t biáº¿n hoÃ¡n vá»‹ (permutation invariant)**.

Äiá»u nÃ y cÃ³ nghÄ©a:

[
\text{Attention}(X) = \text{Attention}(PX)
]

vá»›i ( P ) lÃ  ma tráº­n hoÃ¡n vá»‹.

Do Ä‘Ã³, mÃ´ hÃ¬nh khÃ´ng phÃ¢n biá»‡t thá»© tá»±.

---

# 3. Positional Encoding

## 3.1. MÃ£ hÃ³a vá»‹ trÃ­ sin-cos

Transformer nguyÃªn báº£n sá»­ dá»¥ng:

[
PE(pos, 2i) = \sin \left( \frac{pos}{10000^{2i/d}} \right)
]

[
PE(pos, 2i+1) = \cos \left( \frac{pos}{10000^{2i/d}} \right)
]

Trong Ä‘Ã³:

* (pos): vá»‹ trÃ­ token
* (i): chá»‰ sá»‘ chiá»u embedding
* (d): kÃ­ch thÆ°á»›c embedding

Vector Ä‘áº§u vÃ o:

[
z_i = e_i + PE(i)
]

---

## 3.2. Positional Embedding há»c Ä‘Æ°á»£c

Trong GPT:

[
z_i = e_i + p_i
]

vá»›i (p_i) lÃ  tham sá»‘ há»c Ä‘Æ°á»£c.

Äiá»u nÃ y cho phÃ©p mÃ´ hÃ¬nh tá»‘i Æ°u trá»±c tiáº¿p biá»ƒu diá»…n vá»‹ trÃ­.

---

# 4. Vai trÃ² cá»§a Ä‘Ã¡nh sá»‘ trong Self-Attention

Attention Ä‘Æ°á»£c tÃ­nh:

[
\text{Attention}(Q,K,V)
=======================

\text{softmax}
\left(
\frac{QK^T}{\sqrt{d_k}}
\right)V
]

Trong Ä‘Ã³:

[
Q = ZW_Q, \quad
K = ZW_K
]

Náº¿u (Z) khÃ´ng chá»©a thÃ´ng tin vá»‹ trÃ­:

[
QK^T
]

chá»‰ pháº£n Ã¡nh ná»™i dung, khÃ´ng pháº£n Ã¡nh thá»© tá»±.

Khi cÃ³ positional encoding:

[
Z = E + P
]

attention cÃ³ thá»ƒ há»c:

* Quan há»‡ xa
* Phá»¥ thuá»™c cÃº phÃ¡p
* Quan há»‡ nguyÃªn nhÃ¢n â€“ káº¿t quáº£

---

# 5. ÄÃ¡nh sá»‘ vÄƒn báº£n trong huáº¥n luyá»‡n mÃ´ hÃ¬nh ngÃ´n ngá»¯

MÃ´ hÃ¬nh GPT tá»‘i Æ°u:

[
P(x) = \prod_{t=1}^{T} P(x_t | x_{<t})
]

Äiá»u kiá»‡n (x_{<t}) phá»¥ thuá»™c trá»±c tiáº¿p vÃ o thá»© tá»±.

Causal masking:

[
M_{ij} =
\begin{cases}
0 & j \le i \
-\infty & j > i
\end{cases}
]

Ma tráº­n attention thá»±c táº¿:

[
\text{softmax}
\left(
\frac{QK^T}{\sqrt{d_k}} + M
\right)
]

ÄÃ¡nh sá»‘ vá»‹ trÃ­ cho phÃ©p xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c token nÃ o thuá»™c (x_{<t}).

---

# 6. ÄÃ¡nh sá»‘ vÃ  tá»‘i Æ°u hÃ³a tÃ­nh toÃ¡n

Self-attention cÃ³ Ä‘á»™ phá»©c táº¡p:

[
\mathcal{O}(T^2 d)
]

Khi tÄƒng chiá»u dÃ i vÄƒn báº£n (T):

[
\text{Compute} \propto T^2
]

Viá»‡c Ä‘Ã¡nh sá»‘ giÃºp:

* Quáº£n lÃ½ cá»­a sá»• ngá»¯ cáº£nh
* Chia chunk
* Triá»ƒn khai sliding window

---

# 7. áº¢nh hÆ°á»Ÿng trong Reinforcement Learning from Human Feedback

Trong RLHF, chuá»—i gá»“m:

[
x = [\text{Prompt}; \text{Response}]
]

ÄÃ¡nh sá»‘ cho phÃ©p:

* PhÃ¢n biá»‡t pháº§n prompt vÃ  response
* Mask loss chÃ­nh xÃ¡c

Loss:

[
\mathcal{L} = -\sum_{t \in R} \log P(x_t | x_{<t})
]

Náº¿u khÃ´ng Ä‘Ã¡nh sá»‘ rÃµ rÃ ng, mÃ´ hÃ¬nh khÃ´ng biáº¿t Ä‘Ã¢u lÃ  pháº§n cáº§n tá»‘i Æ°u.

---

# 8. GÃ³c nhÃ¬n lÃ½ thuyáº¿t thÃ´ng tin

Entropy cá»§a chuá»—i:

[
H(X) = - \sum_x P(x)\log P(x)
]

Thá»© tá»± áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n entropy.

VÃ­ dá»¥:

* Chuá»—i cÃ³ cáº¥u trÃºc â†’ entropy tháº¥p
* Chuá»—i ngáº«u nhiÃªn â†’ entropy cao

ÄÃ¡nh sá»‘ giÃºp mÃ´ hÃ¬nh Æ°á»›c lÆ°á»£ng xÃ¡c suáº¥t chÃ­nh xÃ¡c hÆ¡n.

---

# 9. So sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p encoding vá»‹ trÃ­

| PhÆ°Æ¡ng phÃ¡p   | CÃ´ng thá»©c       | Æ¯u Ä‘iá»ƒm           | NhÆ°á»£c Ä‘iá»ƒm         |
| ------------- | --------------- | ----------------- | ------------------ |
| Sin-Cos       | HÃ m lÆ°á»£ng giÃ¡c  | KhÃ´ng cáº§n há»c     | Cá»©ng               |
| Learned       | Vector há»c Ä‘Æ°á»£c | Linh hoáº¡t         | Giá»›i háº¡n chiá»u dÃ i |
| Rotary (RoPE) | PhÃ©p quay phá»©c  | Tá»•ng quÃ¡t hÃ³a tá»‘t | Phá»©c táº¡p           |
| ALiBi         | Bias tuyáº¿n tÃ­nh | DÃ i ngá»¯ cáº£nh tá»‘t  | Giáº£m linh hoáº¡t     |

---

# 10. Tháº£o luáº­n

ÄÃ¡nh sá»‘ vÄƒn báº£n khÃ´ng chá»‰ lÃ  váº¥n Ä‘á» ká»¹ thuáº­t mÃ  lÃ :

* Äiá»u kiá»‡n cáº§n cho mÃ´ hÃ¬nh hiá»ƒu ngá»¯ nghÄ©a
* CÆ¡ sá»Ÿ cho attention hoáº¡t Ä‘á»™ng
* Yáº¿u tá»‘ then chá»‘t trong huáº¥n luyá»‡n LLM

Náº¿u bá» positional encoding:

[
\text{Transformer} \to \text{Bag-of-Words Model}
]

---

# 11. Káº¿t luáº­n

Viá»‡c Ä‘Ã¡nh sá»‘ vÄƒn báº£n lÃ  ná»n táº£ng cá»§a:

1. MÃ´ hÃ¬nh hÃ³a chuá»—i
2. Self-attention
3. Causal masking
4. Huáº¥n luyá»‡n autoregressive

Vá» máº·t toÃ¡n há»c, positional encoding Ä‘Æ°a thÃªm thÃ´ng tin vá»‹ trÃ­ vÃ o khÃ´ng gian embedding, phÃ¡ vá»¡ tÃ­nh báº¥t biáº¿n hoÃ¡n vá»‹ vÃ  cho phÃ©p mÃ´ hÃ¬nh há»c cáº¥u trÃºc ngÃ´n ngá»¯.

---

# TÃ i liá»‡u tham kháº£o

1. Vaswani, A. et al. (2017). *Attention Is All You Need*.
2. Radford, A. et al. (2019). *Language Models are Unsupervised Multitask Learners*.
3. Su, J. et al. (2021). *RoFormer: Enhanced Transformer with Rotary Position Embedding*.
4. Press, O. et al. (2021). *Train Short, Test Long: Attention with Linear Biases*.

---
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| ğŸ“Œ **[Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_LLM_01_Why text needs to be numbered.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Why text needs to be numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_LLM_02_Parsing text to numbered tokens.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Parsing text to numbered tokens.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_LLM_03_CodeChallenge Create and visualize tokens (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Create and visualize tokens (part 1).md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_LLM_04_CodeChallenge Create and visualize tokens (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Create and visualize tokens (part 2).md) |
| [Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_LLM_05_Preparing text for tokenization.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Preparing text for tokenization.md) |
| [PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_LLM_06_CodeChallenge Tokenizing The Time Machine.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_CodeChallenge Tokenizing The Time Machine.md) |
| [So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_LLM_07_Tokenizing characters vs. subwords vs. words.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Tokenizing characters vs. subwords vs. words.md) |
| [aero_LLM_08_Byte-pair encoding algorithm.md](aero_LLM_08_Byte-pair encoding algorithm.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_Byte-pair encoding algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_LLM_09_CodeChallenge Byte-pair encoding to a desired vocab size.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Byte-pair encoding to a desired vocab size.md) |
| [aero_LLM_10_Exploring ChatGPT4's tokenizer.md](aero_LLM_10_Exploring ChatGPT4's tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_Exploring ChatGPT4's tokenizer.md) |
| [aero_LLM_11_CodeChallenge Token count by subword length (part 1).md](aero_LLM_11_CodeChallenge Token count by subword length (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Token count by subword length (part 1).md) |
| [aero_LLM_12_CodeChallenge Token count by subword length (part 2).md](aero_LLM_12_CodeChallenge Token count by subword length (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_CodeChallenge Token count by subword length (part 2).md) |
| [aero_LLM_13_How many rs in strawberry.md](aero_LLM_13_How many rs in strawberry.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_How many rs in strawberry.md) |
| [aero_LLM_14_CodeChallenge Create your algorithmic rapper name ).md](aero_LLM_14_CodeChallenge Create your algorithmic rapper name ).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_CodeChallenge Create your algorithmic rapper name ).md) |
| [aero_LLM_15_Tokenization in BERT.md](aero_LLM_15_Tokenization in BERT.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_Tokenization in BERT.md) |
| [aero_LLM_16_CodeChallenge Character counts in BERT tokens.md](aero_LLM_16_CodeChallenge Character counts in BERT tokens.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_CodeChallenge Character counts in BERT tokens.md) |
| [aero_LLM_17_Translating between tokenizers.md](aero_LLM_17_Translating between tokenizers.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_Translating between tokenizers.md) |
| [aero_LLM_18_CodeChallenge More on token translation.md](aero_LLM_18_CodeChallenge More on token translation.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_CodeChallenge More on token translation.md) |
| [aero_LLM_19_CodeChallenge Tokenization compression ratios.md](aero_LLM_19_CodeChallenge Tokenization compression ratios.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_19_CodeChallenge Tokenization compression ratios.md) |
| [aero_LLM_20_Tokenization in different languages.md](aero_LLM_20_Tokenization in different languages.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_20_Tokenization in different languages.md) |
| [aero_LLM_21_CodeChallenge Zipf's law in characters and tokens.md](aero_LLM_21_CodeChallenge Zipf's law in characters and tokens.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_21_CodeChallenge Zipf's law in characters and tokens.md) |
| [aero_LLM_22_Word variations in Claude tokenizer.md](aero_LLM_22_Word variations in Claude tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_22_Word variations in Claude tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[üè† Home](../../index.md) > [02 Words to tokens to numbers](../index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../index.md)
- [üìö Module 01: LLM Course](../../01-LLM_Course/index.md)
- [üî¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../04-buildGPT/index.md)
- [üéØ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [üîç Module 19: AI Safety](../../19-AI-safety/index.md)
- [üêç Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thu·∫≠t to√°n Byte Pair Encoding (BPE) v√† B√†i to√°n T·ªëi ∆∞u K√≠ch th∆∞·ªõc T·ª´ v·ª±ng trong M√¥ h√¨nh Ng√¥n ng·ªØ

## T√≥m t·∫Øt

Trong c√°c m√¥ h√¨nh ng√¥n ng·ªØ hi·ªán ƒë·∫°i, ƒë·∫∑c bi·ªát l√† c√°c h·ªá th·ªëng d·ª±a tr√™n ki·∫øn tr√∫c Transformer, vi·ªác x√¢y d·ª±ng tokenizer ƒë√≥ng vai tr√≤ n·ªÅn t·∫£ng quy·∫øt ƒë·ªãnh hi·ªáu su·∫•t v√† chi ph√≠ t√≠nh to√°n. T√†i li·ªáu ƒë√≠nh k√®m tr√¨nh b√†y m·ªôt b√†i to√°n th·ª±c h√†nh: **tri·ªÉn khai thu·∫≠t to√°n Byte Pair Encoding (BPE) ƒë·ªÉ ƒë·∫°t k√≠ch th∆∞·ªõc t·ª´ v·ª±ng mong mu·ªën**. B√†i vi·∫øt n√†y ph√¢n t√≠ch c∆° s·ªü l√Ω thuy·∫øt c·ªßa BPE, m√¥ h√¨nh h√≥a to√°n h·ªçc qu√° tr√¨nh g·ªôp c·∫∑p, b√†i to√°n t·ªëi ∆∞u k√≠ch th∆∞·ªõc t·ª´ v·ª±ng, ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n v√† m·ªëi li√™n h·ªá v·ªõi hu·∫•n luy·ªán m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM).

---

## 1. Gi·ªõi thi·ªáu

C√°c m√¥ h√¨nh h·ªçc s√¢u x·ª≠ l√Ω vƒÉn b·∫£n th√¥ng qua √°nh x·∫°:

\[
\text{text} \rightarrow \text{tokens} \rightarrow \text{embedding vectors}
\]

M·ªôt tokenizer hi·ªáu qu·∫£ c·∫ßn:

- Gi·∫£m k√≠ch th∆∞·ªõc t·ª´ v·ª±ng \(V\)
- H·∫°n ch·∫ø token ngo√†i t·∫≠p hu·∫•n luy·ªán (OOV)
- Gi·ªØ ƒë·ªô d√†i chu·ªói \(L\) ·ªü m·ª©c h·ª£p l√Ω

Byte Pair Encoding (BPE) ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t ban ƒë·∫ßu cho n√©n d·ªØ li·ªáu (Gage, 1994) v√† ƒë∆∞·ª£c √°p d·ª•ng cho NLP b·ªüi Sennrich et al. (2016). Hi·ªán nay, nhi·ªÅu m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn s·ª≠ d·ª•ng bi·∫øn th·ªÉ c·ªßa BPE.

---

## 2. M√¥ h√¨nh h√≥a b√†i to√°n BPE

### 2.1 Bi·ªÉu di·ªÖn d·ªØ li·ªáu

Gi·∫£ s·ª≠ t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán:

\[
\mathcal{D} = \{w_1, w_2, \dots, w_N\}
\]

M·ªói t·ª´ ƒë∆∞·ª£c bi·ªÉu di·ªÖn th√†nh chu·ªói k√Ω t·ª±:

\[
w_i = (c_1, c_2, \dots, c_m)
\]

T·∫≠p token ban ƒë·∫ßu:

\[
V_0 = \{ \text{t·∫•t c·∫£ k√Ω t·ª± xu·∫•t hi·ªán} \}
\]

---

### 2.2 H√†m ƒë·∫øm t·∫ßn su·∫•t c·∫∑p token

T·∫°i b∆∞·ªõc \(k\), t·∫≠p token l√† \(V_k\).

T·∫≠p c√°c c·∫∑p token li·ªÅn k·ªÅ:

\[
P_k = \{(t_i, t_{i+1})\}
\]

H√†m t·∫ßn su·∫•t:

\[
f_k(p) = \sum_{w \in \mathcal{D}} \text{count}(p, w)
\]

Ch·ªçn c·∫∑p t·ªëi ∆∞u:

\[
p_k^* = \arg\max_{p \in P_k} f_k(p)
\]

Sau ƒë√≥ c·∫≠p nh·∫≠t:

\[
V_{k+1} = V_k \cup \{ t_{new} \}
\]

Qu√° tr√¨nh d·ª´ng khi:

\[
|V_k| = V_{target}
\]

---

## 3. B√†i to√°n ƒë·∫°t k√≠ch th∆∞·ªõc t·ª´ v·ª±ng mong mu·ªën

Gi·∫£ s·ª≠:

- T·ª´ v·ª±ng ban ƒë·∫ßu: \( |V_0| = C \)
- S·ªë v√≤ng g·ªôp: \( M \)

Khi ƒë√≥:

\[
|V_M| = C + M
\]

N·∫øu mu·ªën:

\[
|V_M| = V_{target}
\]

Ta c·∫ßn:

\[
M = V_{target} - C
\]

Nh∆∞ v·∫≠y, b√†i to√°n tr·ªü th√†nh:

> Th·ª±c hi·ªán ch√≠nh x√°c \( M \) ph√©p g·ªôp c√≥ t·∫ßn su·∫•t cao nh·∫•t.

---

## 4. Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n

### 4.1 M·ªói v√≤ng l·∫∑p

- ƒê·∫øm t·∫ßn su·∫•t t·∫•t c·∫£ c·∫∑p:  
  \[
  \mathcal{O}(T)
  \]
  v·ªõi \(T\) l√† t·ªïng s·ªë token trong t·∫≠p d·ªØ li·ªáu.

- Ch·ªçn c·∫∑p l·ªõn nh·∫•t:  
  \[
  \mathcal{O}(|P_k|)
  \]

### 4.2 T·ªïng th·ªÉ

V·ªõi \(M\) v√≤ng l·∫∑p:

\[
\mathcal{O}(M \cdot T)
\]

Trong th·ª±c t·∫ø:

\[
T \approx 10^9 - 10^{12}
\]

Do ƒë√≥ c·∫ßn:
- C·∫•u tr√∫c heap
- C·∫≠p nh·∫≠t t·∫ßn su·∫•t c·ª•c b·ªô
- Ph√¢n m·∫£nh d·ªØ li·ªáu (sharding)

---

## 5. ·∫¢nh h∆∞·ªüng ƒë·∫øn M√¥ h√¨nh Ng√¥n ng·ªØ

### 5.1 S·ªë tham s·ªë embedding

Ma tr·∫≠n embedding:

\[
E \in \mathbb{R}^{V \times d}
\]

S·ªë tham s·ªë:

\[
\text{Params} = V \times d
\]

V√≠ d·ª•:

- \(V = 50,000\)
- \(d = 4096\)

\[
\text{Params} = 204,800,000
\]

N·∫øu tƒÉng \(V\) l√™n 100,000:

\[
\text{Params} = 409,600,000
\]

Chi ph√≠ tƒÉng g·∫•p ƒë√¥i.

---

### 5.2 ·∫¢nh h∆∞·ªüng ƒë·∫øn Attention

Attention c√≥ ƒë·ªô ph·ª©c t·∫°p:

\[
\mathcal{O}(L^2 \cdot d)
\]

Trong ƒë√≥:
- \(L\) l√† chi·ªÅu d√†i chu·ªói token.

N·∫øu token qu√° nh·ªè (character-level):

\[
L \uparrow \Rightarrow \text{Chi ph√≠ tƒÉng}
\]

N·∫øu token qu√° l·ªõn (word-level):

- OOV tƒÉng
- M·∫•t kh·∫£ nƒÉng ph√¢n t√≠ch h√¨nh th√°i

BPE c√¢n b·∫±ng hai y·∫øu t·ªë n√†y.

---

## 6. So s√°nh BPE v·ªõi WordPiece v√† Unigram LM

| Thu·∫≠t to√°n | Ti√™u ch√≠ t·ªëi ∆∞u | C∆° ch·∫ø |
|------------|----------------|---------|
| BPE | T·∫ßn su·∫•t c·∫∑p | G·ªôp l·∫∑p |
| WordPiece | Likelihood | Ch·ªçn c·∫∑p t·ªëi ƒëa h√≥a x√°c su·∫•t |
| Unigram LM | X√°c su·∫•t m√¥ h√¨nh | Lo·∫°i b·ªè token k√©m |

BPE l√† ph∆∞∆°ng ph√°p tham lam (greedy):

\[
\max f_k(p)
\]

Trong khi WordPiece t·ªëi ∆∞u:

\[
\max \log P(\mathcal{D} | V_k)
\]

---

## 7. M·ªëi li√™n h·ªá v·ªõi Hu·∫•n luy·ªán LLM

Gi·∫£ s·ª≠:

- T·ªïng token hu·∫•n luy·ªán: \(T\)
- K√≠ch th∆∞·ªõc m√¥ h√¨nh: \(d\)
- S·ªë l·ªõp: \(L\)

Chi ph√≠ hu·∫•n luy·ªán x·∫•p x·ªâ:

\[
\mathcal{O}(T \cdot L \cdot d^2)
\]

Vi·ªác ch·ªçn tokenizer ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn:

- \(T\) (s·ªë token sau ph√¢n t√°ch)
- Hi·ªáu qu·∫£ t·ªïng qu√°t h√≥a
- Kh·∫£ nƒÉng bi·ªÉu di·ªÖn t·ª´ hi·∫øm

---

## 8. H·∫°n ch·∫ø

- Kh√¥ng x√©t ng·ªØ nghƒ©a khi g·ªôp
- Ph·ª• thu·ªôc d·ªØ li·ªáu hu·∫•n luy·ªán ban ƒë·∫ßu
- C√≥ th·ªÉ t·∫°o token kh√¥ng tr·ª±c quan

---

## 9. K·∫øt lu·∫≠n

Thu·∫≠t to√°n Byte Pair Encoding cung c·∫•p m·ªôt c∆° ch·∫ø ph√¢n t√°ch t·ª´ hi·ªáu qu·∫£, ƒë·∫∑c bi·ªát trong b·ªëi c·∫£nh m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn. B√†i to√°n ƒë·∫°t k√≠ch th∆∞·ªõc t·ª´ v·ª±ng mong mu·ªën c√≥ th·ªÉ ƒë∆∞·ª£c m√¥ h√¨nh h√≥a th√†nh vi·ªác th·ª±c hi·ªán ch√≠nh x√°c s·ªë v√≤ng g·ªôp c·∫ßn thi·∫øt:

\[
M = V_{target} - |V_0|
\]

Vi·ªác t·ªëi ∆∞u h√≥a BPE kh√¥ng ch·ªâ l√† b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω, m√† c√≤n ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn:

- B·ªô nh·ªõ
- Th·ªùi gian hu·∫•n luy·ªán
- Ch·∫•t l∆∞·ª£ng m√¥ h√¨nh

Trong t∆∞∆°ng lai, c√°c ph∆∞∆°ng ph√°p tokenizer th√≠ch nghi ƒë·ªông (adaptive tokenization) c√≥ th·ªÉ thay th·∫ø BPE truy·ªÅn th·ªëng nh·∫±m t·ªëi ∆∞u h√≥a t·ªët h∆°n theo m·ª•c ti√™u hu·∫•n luy·ªán.

---

## T√†i li·ªáu tham kh·∫£o

1. Gage, P. (1994). *A New Algorithm for Data Compression.*
2. Sennrich, R., Haddow, B., & Birch, A. (2016). *Neural Machine Translation of Rare Words with Subword Units.*
3. Vaswani, A. et al. (2017). *Attention Is All You Need.*
4. Kudo, T. (2018). *Subword Regularization.*
5. Devlin, J. et al. (2018). *BERT: Pre-training of Deep Bidirectional Transformers.*

---
<!-- Aero-Footer-Start -->
---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

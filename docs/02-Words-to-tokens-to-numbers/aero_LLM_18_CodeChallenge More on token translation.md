
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [02 Words to tokens to numbers](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Má»Ÿ rá»™ng BÃ i toÃ¡n Chuyá»ƒn Ä‘á»•i Token:

PhÃ¢n tÃ­ch HÃ¬nh thá»©c, Äá»‹nh lÆ°á»£ng Sai sá»‘ vÃ  áº¢nh hÆ°á»Ÿng Ä‘áº¿n Biá»ƒu diá»…n Ngá»¯ nghÄ©a

â¸»

TÃ³m táº¯t

Dá»±a trÃªn tÃ i liá»‡u Ä‘Ã­nh kÃ¨m â€œMore on Token Translationâ€, bÃ i viáº¿t nÃ y má»Ÿ rá»™ng phÃ¢n tÃ­ch bÃ i toÃ¡n chuyá»ƒn Ä‘á»•i giá»¯a cÃ¡c há»‡ tokenizer trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs). ChÃºng tÃ´i xÃ¢y dá»±ng má»™t khung toÃ¡n há»c cho Ã¡nh xáº¡ giá»¯a hai khÃ´ng gian token rá»i ráº¡c, phÃ¢n tÃ­ch sai sá»‘ tÃ­ch lÅ©y khi chuyá»ƒn Ä‘á»•i nhiá»u bÆ°á»›c, Ä‘á» xuáº¥t mÃ´ hÃ¬nh ma tráº­n xÃ¡c suáº¥t chuyá»ƒn Ä‘á»•i, vÃ  Ä‘Ã¡nh giÃ¡ áº£nh hÆ°á»Ÿng Ä‘áº¿n embedding vÃ  attention trong kiáº¿n trÃºc Transformer. CÃ¡c vÃ­ dá»¥ Ä‘Æ°á»£c minh há»a vá»›i tokenizer cá»§a BERT vÃ  GPT-2.

â¸»

1. Giá»›i thiá»‡u

Tokenization Ä‘á»‹nh nghÄ©a má»™t phÃ©p mÃ£ hÃ³a:

\mathcal{T}: \Sigma^* \rightarrow V^*

vá»›i:
	â€¢	\Sigma: báº£ng chá»¯ cÃ¡i kÃ½ tá»±
	â€¢	V: táº­p token
	â€¢	V^*: chuá»—i token

Khi tá»“n táº¡i hai tokenizer \mathcal{T}_A vÃ  \mathcal{T}_B, bÃ i toÃ¡n Ä‘áº·t ra lÃ  xÃ¢y dá»±ng Ã¡nh xáº¡:

\Phi_{A \to B}: V_A^* \rightarrow V_B^*

sao cho báº£o toÃ n ná»™i dung ngá»¯ nghÄ©a vÃ  háº¡n cháº¿ sai sá»‘ thÃ´ng tin.

â¸»

2. PhÃ¢n rÃ£ Hai BÆ°á»›c: Decode vÃ  Re-tokenize

CÃ¡ch tá»± nhiÃªn nháº¥t:

\Phi_{A \to B} = \mathcal{T}_B \circ \mathcal{D}_A

Trong Ä‘Ã³:
	â€¢	\mathcal{D}_A: V_A^* \rightarrow \Sigma^* lÃ  hÃ m giáº£i mÃ£

Khi tokenizer kháº£ nghá»‹ch:

\mathcal{D}_A(\mathcal{T}_A(x)) = x

Tuy nhiÃªn, trong thá»±c táº¿ cÃ³ thá»ƒ xuáº¥t hiá»‡n chuáº©n hÃ³a Unicode hoáº·c xá»­ lÃ½ khoáº£ng tráº¯ng gÃ¢y sai sá»‘.

â¸»

3. Sai sá»‘ TÃ­ch lÅ©y khi Chuyá»ƒn Ä‘á»•i Nhiá»u Láº§n

Giáº£ sá»­ thá»±c hiá»‡n chuá»—i chuyá»ƒn Ä‘á»•i:

A \to B \to C

Sai sá»‘ tá»•ng:

\epsilon_{A \to C} \le \epsilon_{A \to B} + \epsilon_{B \to C}

ÄÃ¢y lÃ  há»‡ quáº£ cá»§a báº¥t Ä‘áº³ng thá»©c tam giÃ¡c Ä‘á»‘i vá»›i khoáº£ng cÃ¡ch Levenshtein:

d(x,z) \le d(x,y) + d(y,z)

Náº¿u má»—i bÆ°á»›c cÃ³ sai sá»‘ nhá» nhÆ°ng láº·p nhiá»u láº§n, sai sá»‘ tÃ­ch lÅ©y cÃ³ thá»ƒ tÄƒng tuyáº¿n tÃ­nh theo sá»‘ bÆ°á»›c:

\epsilon_k \le k \epsilon

â¸»

4. MÃ´ hÃ¬nh XÃ¡c suáº¥t cho Chuyá»ƒn Ä‘á»•i Token

Thay vÃ¬ Ã¡nh xáº¡ xÃ¡c Ä‘á»‹nh, ta Ä‘á»‹nh nghÄ©a phÃ¢n bá»‘ xÃ¡c suáº¥t:

P(b_j \mid a_i)

Táº¡o thÃ nh ma tráº­n:

M \in \mathbb{R}^{|V_A| \times |V_B|}

vá»›i:

\sum_{j} M_{ij} = 1

Khi Ä‘Ã³ embedding cÃ³ thá»ƒ chuyá»ƒn Ä‘á»•i tuyáº¿n tÃ­nh:

E_B = M^\top E_A

Trong Ä‘Ã³:
	â€¢	E_A \in \mathbb{R}^{|V_A| \times d}
	â€¢	E_B \in \mathbb{R}^{|V_B| \times d}

â¸»

5. PhÃ¢n tÃ­ch Sai sá»‘ Ngá»¯ nghÄ©a

Giáº£ sá»­ embedding cá»§a token:

e(a_i), \quad e(b_j)

Sai sá»‘ chuyá»ƒn Ä‘á»•i:

\delta_i = \| e(a_i) - \sum_j M_{ij} e(b_j) \|_2

Sai sá»‘ trung bÃ¬nh:

\mathbb{E}[\delta] = \frac{1}{|V_A|} \sum_i \delta_i

Náº¿u embedding hai mÃ´ hÃ¬nh náº±m trong cÃ¹ng khÃ´ng gian ngá»¯ nghÄ©a, ta cÃ³ thá»ƒ tá»‘i Æ°u:

\min_M \sum_i \delta_i^2

â¸»

6. áº¢nh hÆ°á»Ÿng Ä‘áº¿n Self-Attention

Cho vÄƒn báº£n Ä‘á»™ dÃ i n kÃ½ tá»±:

m_A = \frac{n}{\mathbb{E}[L_A]}

m_B = \frac{n}{\mathbb{E}[L_B]}

Chi phÃ­ attention:

C_A = O(m_A^2)

C_B = O(m_B^2)

Tá»· lá»‡:

\frac{C_A}{C_B} = \left(\frac{\mathbb{E}[L_B]}{\mathbb{E}[L_A]}\right)^2

Tokenizer táº¡o token dÃ i hÆ¡n giÃºp giáº£m chi phÃ­ tÃ­nh toÃ¡n.

â¸»

7. CÄƒn chá»‰nh Span KÃ½ tá»±

Má»—i token tÆ°Æ¡ng á»©ng má»™t Ä‘oáº¡n kÃ½ tá»±:

a_i \leftrightarrow [s_i, e_i)

b_j \leftrightarrow [u_j, v_j)

BÃ i toÃ¡n cÄƒn chá»‰nh trá»Ÿ thÃ nh:

\text{match}(a_i, b_j) \iff [s_i, e_i) \cap [u_j, v_j) \neq \emptyset

CÃ³ thá»ƒ xÃ¢y dá»±ng Ã¡nh xáº¡ nhiá»u-nhiá»u.

â¸»

8. Äá»™ phá»©c táº¡p Thuáº­t toÃ¡n

Náº¿u:
	â€¢	Chuá»—i cÃ³ m token á»Ÿ A
	â€¢	k token á»Ÿ B

Thuáº­t toÃ¡n cÄƒn chá»‰nh span cÃ³ thá»ƒ thá»±c hiá»‡n trong:

O(m + k)

vÃ¬ chá»‰ cáº§n quÃ©t hai con trá».

Tuy nhiÃªn náº¿u so khá»›p embedding:

O(mk)

â¸»

9. LiÃªn há»‡ Ä‘áº¿n LÃ½ thuyáº¿t ThÃ´ng tin

Entropy cá»§a phÃ¢n bá»‘ token:

H(V) = - \sum_{t \in V} p(t)\log p(t)

Chuyá»ƒn tokenizer lÃ m thay Ä‘á»•i phÃ¢n bá»‘:

\Delta H = |H(V_A) - H(V_B)|

Theo Claude Shannon (1948), entropy Ä‘o lÆ°á»£ng thÃ´ng tin trung bÃ¬nh trÃªn má»—i token.

â¸»

10. Tháº£o luáº­n

Má»Ÿ rá»™ng tá»« tÃ i liá»‡u Ä‘Ã­nh kÃ¨m, cÃ³ thá»ƒ tháº¥y:
	1.	Token translation khÃ´ng chá»‰ lÃ  thao tÃ¡c chuá»—i
	2.	LÃ  bÃ i toÃ¡n Ã¡nh xáº¡ giá»¯a hai há»‡ mÃ£ hÃ³a rá»i ráº¡c
	3.	CÃ³ thá»ƒ xem nhÆ° biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh trong khÃ´ng gian embedding
	4.	Sai sá»‘ cÃ³ thá»ƒ tÃ­ch lÅ©y náº¿u chuyá»ƒn Ä‘á»•i nhiá»u bÆ°á»›c

Trong thá»±c táº¿, cÃ¡c há»‡ nhÆ° OpenAI hay Google thiáº¿t káº¿ tokenizer gáº¯n cháº·t vá»›i kiáº¿n trÃºc mÃ´ hÃ¬nh, do Ä‘Ã³ viá»‡c chuyá»ƒn Ä‘á»•i Ä‘Ã²i há»i phÃ¢n tÃ­ch cáº©n trá»ng.

â¸»

11. Káº¿t luáº­n

BÃ i toÃ¡n chuyá»ƒn Ä‘á»•i tokenizer cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ hÃ¬nh hÃ³a:

\Phi_{A \to B} = \mathcal{T}_B \circ \mathcal{D}_A

Sai sá»‘ tÃ­ch lÅ©y:

\epsilon_k \le k \epsilon

Embedding cÃ³ thá»ƒ chuyá»ƒn Ä‘á»•i báº±ng:

E_B = M^\top E_A

ÄÃ¢y lÃ  má»™t bÃ i toÃ¡n káº¿t há»£p giá»¯a:
	â€¢	LÃ½ thuyáº¿t mÃ£ hÃ³a
	â€¢	LÃ½ thuyáº¿t thÃ´ng tin
	â€¢	Tá»‘i Æ°u hÃ³a tuyáº¿n tÃ­nh
	â€¢	Kiáº¿n trÃºc Transformer

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
	2.	Radford et al. (2019). GPT-2: Language Models are Unsupervised Multitask Learners.
	3.	Vaswani et al. (2017). Attention Is All You Need.
	4.	Shannon, C. (1948). A Mathematical Theory of Communication.
	5.	Kudo & Richardson (2018). SentencePiece: A simple and language independent subword tokenizer.
	6.	Sennrich et al. (2016). Neural Machine Translation of Rare Words with Subword Units.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_LLM_01_Why text needs to be numbered.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Why text needs to be numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_LLM_02_Parsing text to numbered tokens.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Parsing text to numbered tokens.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_LLM_03_CodeChallenge Create and visualize tokens (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Create and visualize tokens (part 1).md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_LLM_04_CodeChallenge Create and visualize tokens (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Create and visualize tokens (part 2).md) |
| [Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_LLM_05_Preparing text for tokenization.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Preparing text for tokenization.md) |
| [PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_LLM_06_CodeChallenge Tokenizing The Time Machine.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_CodeChallenge Tokenizing The Time Machine.md) |
| [So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_LLM_07_Tokenizing characters vs. subwords vs. words.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Tokenizing characters vs. subwords vs. words.md) |
| [aero_LLM_08_Byte-pair encoding algorithm.md](aero_LLM_08_Byte-pair encoding algorithm.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_Byte-pair encoding algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_LLM_09_CodeChallenge Byte-pair encoding to a desired vocab size.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Byte-pair encoding to a desired vocab size.md) |
| [aero_LLM_10_Exploring ChatGPT4's tokenizer.md](aero_LLM_10_Exploring ChatGPT4's tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_Exploring ChatGPT4's tokenizer.md) |
| [aero_LLM_11_CodeChallenge Token count by subword length (part 1).md](aero_LLM_11_CodeChallenge Token count by subword length (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Token count by subword length (part 1).md) |
| [aero_LLM_12_CodeChallenge Token count by subword length (part 2).md](aero_LLM_12_CodeChallenge Token count by subword length (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_CodeChallenge Token count by subword length (part 2).md) |
| [aero_LLM_13_How many rs in strawberry.md](aero_LLM_13_How many rs in strawberry.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_How many rs in strawberry.md) |
| [aero_LLM_14_CodeChallenge Create your algorithmic rapper name ).md](aero_LLM_14_CodeChallenge Create your algorithmic rapper name ).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_CodeChallenge Create your algorithmic rapper name ).md) |
| [aero_LLM_15_Tokenization in BERT.md](aero_LLM_15_Tokenization in BERT.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_Tokenization in BERT.md) |
| [aero_LLM_16_CodeChallenge Character counts in BERT tokens.md](aero_LLM_16_CodeChallenge Character counts in BERT tokens.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_CodeChallenge Character counts in BERT tokens.md) |
| [aero_LLM_17_Translating between tokenizers.md](aero_LLM_17_Translating between tokenizers.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_Translating between tokenizers.md) |
| ğŸ“Œ **[aero_LLM_18_CodeChallenge More on token translation.md](aero_LLM_18_CodeChallenge More on token translation.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_18_CodeChallenge More on token translation.md) |
| [aero_LLM_19_CodeChallenge Tokenization compression ratios.md](aero_LLM_19_CodeChallenge Tokenization compression ratios.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_19_CodeChallenge Tokenization compression ratios.md) |
| [aero_LLM_20_Tokenization in different languages.md](aero_LLM_20_Tokenization in different languages.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_20_Tokenization in different languages.md) |
| [aero_LLM_21_CodeChallenge Zipf's law in characters and tokens.md](aero_LLM_21_CodeChallenge Zipf's law in characters and tokens.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_21_CodeChallenge Zipf's law in characters and tokens.md) |
| [aero_LLM_22_Word variations in Claude tokenizer.md](aero_LLM_22_Word variations in Claude tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_22_Word variations in Claude tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

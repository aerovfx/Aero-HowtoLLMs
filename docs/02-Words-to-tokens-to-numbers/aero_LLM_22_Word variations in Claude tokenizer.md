
<!-- Aero-Navigation-Start -->
[üè† Home](../../index.md) > [02 Words to tokens to numbers](../index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../index.md)
- [üìö Module 01: LLM Course](../../01-LLM_Course/index.md)
- [üî¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../04-buildGPT/index.md)
- [üéØ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [üîç Module 19: AI Safety](../../19-AI-safety/index.md)
- [üêç Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Bi·∫øn th·ªÉ T·ª´ v·ª±ng trong Tokenizer c·ªßa Claude:

Ph√¢n t√≠ch H√¨nh th·ª©c, Ph√¢n b·ªë X√°c su·∫•t v√† ·∫¢nh h∆∞·ªüng ƒë·∫øn Bi·ªÉu di·ªÖn Ng·ªØ nghƒ©a

‚∏ª

T√≥m t·∫Øt

D·ª±a tr√™n t√†i li·ªáu ƒë√≠nh k√®m ‚ÄúWord Variations in Claude Tokenizer‚Äù, b√†i vi·∫øt n√†y ph√¢n t√≠ch c√°ch tokenizer c·ªßa m√¥ h√¨nh Claude x·ª≠ l√Ω c√°c bi·∫øn th·ªÉ t·ª´ v·ª±ng (word variations) nh∆∞ ti·ªÅn t·ªë, h·∫≠u t·ªë, ch·ªØ hoa‚Äìth∆∞·ªùng v√† h√¨nh th√°i h·ªçc. Ch√∫ng t√¥i x√¢y d·ª±ng m√¥ h√¨nh to√°n h·ªçc cho ph√¢n r√£ subword, ph√¢n t√≠ch ph√¢n b·ªë x√°c su·∫•t token, v√† ƒë√°nh gi√° ·∫£nh h∆∞·ªüng ƒë·∫øn entropy, t·ª∑ l·ªá n√©n v√† chi ph√≠ self-attention trong Transformer. B√†i vi·∫øt c≈©ng so s√°nh v·ªõi tokenizer c·ªßa BERT v√† c√°c ph∆∞∆°ng ph√°p d·ª±a tr√™n BPE.

‚∏ª

1. Gi·ªõi thi·ªáu

Tokenizer l√† h√†m √°nh x·∫°:

\mathcal{T}: \Sigma^* \rightarrow V^*

Trong ƒë√≥:
	‚Ä¢	\Sigma: b·∫£ng k√Ω t·ª±
	‚Ä¢	V: t·∫≠p token
	‚Ä¢	V^*: chu·ªói token

M·ªôt t·ª´ c√≥ nhi·ªÅu bi·∫øn th·ªÉ h√¨nh th√°i:

w_k = r + s_k

v·ªõi:
	‚Ä¢	r: g·ªëc t·ª´ (root)
	‚Ä¢	s_k: h·∫≠u t·ªë (suffix)

Tokenizer subword s·∫Ω ph√¢n r√£:

\mathcal{T}(w_k) = (r, s_k)

Thay v√¨ xem m·ªói bi·∫øn th·ªÉ l√† m·ªôt token ƒë·ªôc l·∫≠p.

‚∏ª

2. M√¥ h√¨nh To√°n h·ªçc c·ªßa Bi·∫øn th·ªÉ T·ª´

Gi·∫£ s·ª≠ m·ªôt t·∫≠p bi·∫øn th·ªÉ:

W = \{w_1, w_2, \dots, w_K\}

Trong ƒë√≥:

w_k = r + s_k

N·∫øu x√°c su·∫•t xu·∫•t hi·ªán:

P(w_k)

th√¨ x√°c su·∫•t c·ªßa root:

P(r) = \sum_{k=1}^{K} P(w_k)

Tokenizer hi·ªáu qu·∫£ s·∫Ω h·ªçc:

P(r) \gg P(w_k)

‚∏ª

3. Entropy Tr∆∞·ªõc v√† Sau Ph√¢n r√£

3.1 Entropy ·ªü m·ª©c t·ª´

H_W = -\sum_{k=1}^{K} P(w_k)\log P(w_k)

‚∏ª

3.2 Entropy ·ªü m·ª©c subword

Gi·∫£ s·ª≠ t√°ch th√†nh root v√† suffix:

H_{sub} = -P(r)\log P(r) - \sum_{k} P(s_k)\log P(s_k)

V√¨:

P(r) = \sum_k P(w_k)

n√™n:

H_{sub} \le H_W

(gi·∫£m entropy nh·ªù gom t·∫ßn su·∫•t v·ªÅ root chung).

‚∏ª

4. Compression Ratio v√† ƒê·ªô d√†i Chu·ªói

Gi·∫£ s·ª≠:
	‚Ä¢	VƒÉn b·∫£n c√≥ n k√Ω t·ª±
	‚Ä¢	Sau tokenization c√≥ m token

Compression ratio:

R = \frac{n}{m}

N·∫øu tokenizer t√°i s·ª≠ d·ª•ng root cho nhi·ªÅu bi·∫øn th·ªÉ:

m \downarrow \Rightarrow R \uparrow

Chi ph√≠ attention:

O(m^2)

Thay:

O\left(\frac{n^2}{R^2}\right)

‚∏ª

5. Ph√¢n b·ªë Zipf trong Bi·∫øn th·ªÉ T·ª´

Theo George Kingsley Zipf:

f(r) \propto \frac{1}{r^\alpha}

Root th∆∞·ªùng c√≥ th·ª© h·∫°ng th·∫•p (t·∫ßn su·∫•t cao).
Suffix c√≥ ph√¢n b·ªë ƒëu√¥i d√†i.

Ph√¢n r√£ subword l√†m thay ƒë·ªïi h·ªá s·ªë:

\alpha_{sub} \neq \alpha_{word}

‚∏ª

6. M√¥ h√¨nh X√°c su·∫•t H√¨nh th√°i

Gi·∫£ s·ª≠ x√°c su·∫•t sinh t·ª´:

P(w_k) = P(r)P(s_k \mid r)

Log-likelihood:

\log P(w_k) = \log P(r) + \log P(s_k \mid r)

Tokenizer subword x·∫•p x·ªâ ph√¢n t√≠ch h√¨nh th√°i n√†y.

‚∏ª

7. So s√°nh v·ªõi Tokenizer c·ªßa BERT

Tokenizer WordPiece trong BERT t·ªëi ∆∞u:

\arg\max_{s_1,\dots,s_m} \prod_i P(s_i)

Trong khi c√°c tokenizer hi·ªán ƒë·∫°i (nh∆∞ Claude) t·ªëi ∆∞u theo t·∫ßn su·∫•t byte ho·∫∑c subword linh ho·∫°t h∆°n.

‚∏ª

8. ·∫¢nh h∆∞·ªüng ƒë·∫øn Embedding

Embedding:

E: V \rightarrow \mathbb{R}^d

N·∫øu c√°c bi·∫øn th·ªÉ chia s·∫ª root:

e(w_k) \approx e(r) + e(s_k)

Sai s·ªë:

\delta_k = \| e(w_k) - (e(r)+e(s_k)) \|_2

T·ªëi ∆∞u h√≥a:

\min \sum_k \delta_k^2

ƒêi·ªÅu n√†y c·∫£i thi·ªán kh·∫£ nƒÉng t·ªïng qu√°t h√≥a.

‚∏ª

9. ·∫¢nh h∆∞·ªüng ƒë·∫øn Hu·∫•n luy·ªán

Gradient c·ªßa token hi·∫øm:

\nabla L(w_k)

N·∫øu chia th√†nh root v√† suffix:

\nabla L(r) = \sum_k \nabla L(w_k)

‚Üí TƒÉng ·ªïn ƒë·ªãnh gradient.

‚∏ª

10. Ph√¢n t√≠ch ƒêa ng√¥n ng·ªØ

Trong ng√¥n ng·ªØ ch·∫Øp d√≠nh:

|s_k| \uparrow

Tokenizer ph·∫£i c√¢n b·∫±ng gi·ªØa:
	‚Ä¢	Gi·ªØ nguy√™n to√†n b·ªô t·ª´
	‚Ä¢	Chia th√†nh nhi·ªÅu subword

T·ªëi ∆∞u h√≥a ƒëa m·ª•c ti√™u:

\min \left( \frac{n^2}{R^2} + \lambda |V| \right)

‚∏ª

11. Th·∫£o lu·∫≠n

Bi·∫øn th·ªÉ t·ª´ v·ª±ng t·∫°o ra:
	‚Ä¢	ƒêu√¥i d√†i trong ph√¢n b·ªë token
	‚Ä¢	TƒÉng entropy n·∫øu kh√¥ng t√°ch

Tokenizer subword hi·ªáu qu·∫£:
	1.	Gom t·∫ßn su·∫•t v√†o root
	2.	Gi·∫£m entropy
	3.	TƒÉng compression ratio
	4.	·ªîn ƒë·ªãnh hu·∫•n luy·ªán

C√°c h·ªá do Anthropic, OpenAI v√† Google ph√°t tri·ªÉn ƒë·ªÅu √°p d·ª•ng nguy√™n t·∫Øc n√†y.

‚∏ª

12. K·∫øt lu·∫≠n

Ph√¢n r√£ bi·∫øn th·ªÉ t·ª´ c√≥ th·ªÉ ƒë∆∞·ª£c m√¥ h√¨nh h√≥a:

P(w_k) = P(r)P(s_k \mid r)

Entropy gi·∫£m khi:

H_{sub} \le H_W

Compression ratio:

R = \frac{n}{m}

Chi ph√≠ attention:

O\left(\frac{n^2}{R^2}\right)

Tokenizer hi·ªán ƒë·∫°i t·∫≠n d·ª•ng c·∫•u tr√∫c h√¨nh th√°i ƒë·ªÉ:
	‚Ä¢	N√©n th√¥ng tin
	‚Ä¢	Gi·∫£m ƒë·ªô d√†i chu·ªói
	‚Ä¢	TƒÉng t√≠nh t·ªïng qu√°t h√≥a

‚∏ª

T√†i li·ªáu tham kh·∫£o
	1.	Zipf, G. K. (1935). The Psycho-Biology of Language.
	2.	Shannon, C. (1948). A Mathematical Theory of Communication.
	3.	Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
	4.	Sennrich et al. (2016). Neural Machine Translation of Rare Words with Subword Units.
	5.	Vaswani et al. (2017). Attention Is All You Need.
	6.	Kudo & Richardson (2018). SentencePiece.
<!-- Aero-Footer-Start -->
---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

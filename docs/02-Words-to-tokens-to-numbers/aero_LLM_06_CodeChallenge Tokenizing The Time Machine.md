
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [02 Words to tokens to numbers](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c

---

## TÃ³m táº¯t

BÃ i viáº¿t nÃ y trÃ¬nh bÃ y má»™t phÃ¢n tÃ­ch khoa há»c vá» quy trÃ¬nh tokenization thÃ´ng qua vÃ­ dá»¥ vÄƒn báº£n *The Time Machine* cá»§a H. G. Wells. Ná»™i dung táº­p trung vÃ o cÃ¡ch tiá»n xá»­ lÃ½ vÄƒn báº£n, xÃ¢y dá»±ng tá»« vá»±ng, Ã¡nh xáº¡ token sang chá»‰ sá»‘ vÃ  chuyá»ƒn Ä‘á»•i sang khÃ´ng gian vector phá»¥c vá»¥ huáº¥n luyá»‡n mÃ´ hÃ¬nh Transformer. BÃ i viáº¿t bá»• sung cÃ¡c ná»n táº£ng lÃ½ thuyáº¿t tá»« kiáº¿n trÃºc Attention Is All You Need vÃ  cÃ¡c mÃ´ hÃ¬nh GPT do OpenAI phÃ¡t triá»ƒn, kÃ¨m theo cÃ¡c cÃ´ng thá»©c toÃ¡n há»c minh há»a cho quÃ¡ trÃ¬nh rá»i ráº¡c hÃ³a vÃ  biá»ƒu diá»…n liÃªn tá»¥c.

---

# 1. Giá»›i thiá»‡u

Trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡n Ä‘áº¡i, dá»¯ liá»‡u vÄƒn báº£n pháº£i Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i tá»« dáº¡ng kÃ½ tá»± sang dáº¡ng token trÆ°á»›c khi Ä‘Æ°a vÃ o máº¡ng nÆ¡-ron.

Cho vÄƒn báº£n Ä‘áº§u vÃ o:

[
X = (c_1, c_2, \dots, c_n), \quad c_i \in \Sigma
]

Tokenization thá»±c hiá»‡n Ã¡nh xáº¡:

[
\tau : \Sigma^* \rightarrow V^*
]

trong Ä‘Ã³:

* (\Sigma): táº­p kÃ½ tá»±
* (V): tá»« vá»±ng token
* (V^*): chuá»—i token

VÃ­ dá»¥ vá»›i *The Time Machine*:

```
The Time Machine by H. G. Wells
```

Sau xá»­ lÃ½ cÃ³ thá»ƒ thÃ nh:

[
[\text{"the"}, \text{"time"}, \text{"machine"}, \text{"by"}, \text{"h"}, \text{"g"}, \text{"wells"}]
]

---

# 2. Tiá»n xá»­ lÃ½ vÄƒn báº£n

## 2.1 Chuáº©n hÃ³a chá»¯ thÆ°á»ng

[
f_{lower}(x) = \text{lower}(x)
]

GiÃºp giáº£m kÃ­ch thÆ°á»›c tá»« vá»±ng:

[
|V_{raw}| > |V_{normalized}|
]

---

## 2.2 Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t

HÃ m lá»c:

[
f_{clean}(x) = x \setminus { \text{punctuation} }
]

Má»¥c tiÃªu:

* Giáº£m nhiá»…u
* Chuáº©n hÃ³a cáº¥u trÃºc

---

# 3. Tokenization má»©c tá»« (Word-level Tokenization)

Sau khi tÃ¡ch theo khoáº£ng tráº¯ng:

[
X = (w_1, w_2, \dots, w_T)
]

Sá»‘ lÆ°á»£ng token:

[
T \leq n
]

Táº§n suáº¥t xuáº¥t hiá»‡n cá»§a tá»« (w):

[
f(w) = \sum_{i=1}^{T} \mathbf{1}(w_i = w)
]

---

# 4. XÃ¢y dá»±ng tá»« vá»±ng (Vocabulary Construction)

Táº­p tá»« vá»±ng:

[
V = { w \mid f(w) \geq \delta }
]

vá»›i (\delta) lÃ  ngÆ°á»¡ng tá»‘i thiá»ƒu.

KÃ­ch thÆ°á»›c tá»« vá»±ng:

[
|V| = M
]

Ãnh xáº¡:

[
w \rightarrow id(w) \in {0, 1, \dots, M-1}
]

---

# 5. Biá»ƒu diá»…n One-Hot

Token (w_i) Ä‘Æ°á»£c biá»ƒu diá»…n:

[
x_i \in \mathbb{R}^{M}
]

vá»›i:

[
x_{ij} =
\begin{cases}
1 & \text{náº¿u } j = id(w_i) \
0 & \text{ngÆ°á»£c láº¡i}
\end{cases}
]

NhÆ°á»£c Ä‘iá»ƒm:

* KÃ­ch thÆ°á»›c lá»›n
* KhÃ´ng pháº£n Ã¡nh ngá»¯ nghÄ©a

---

# 6. Embedding Vector

Embedding matrix:

[
E \in \mathbb{R}^{M \times d}
]

Vector embedding:

[
e_i = E^T x_i
]

Do Ä‘Ã³:

[
e_i \in \mathbb{R}^{d}
]

Khoáº£ng cÃ¡ch cosine:

[
\cos(e_i, e_j) =
\frac{e_i \cdot e_j}{|e_i||e_j|}
]

GiÃºp Ä‘o má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a.

---

# 7. MÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t ngÃ´n ngá»¯

Theo mÃ´ hÃ¬nh tá»± há»“i quy:

[
P(X) = \prod_{t=1}^{T} P(w_t \mid w_{<t})
]

Máº¡ng Transformer tÃ­nh:

[
Z = \text{Transformer}(e_1, \dots, e_T)
]

Logits:

[
z_t = W_{out} h_t
]

Softmax:

[
P(w_t = j \mid w_{<t}) =
\frac{\exp(z_{tj})}
{\sum_{k=1}^{M} \exp(z_{tk})}
]

---

# 8. Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n

Self-attention:

[
\mathcal{O}(T^2 d)
]

Náº¿u vÄƒn báº£n dÃ i nhÆ° *The Time Machine* (~30,000 tá»«), chi phÃ­ tÄƒng theo bÃ¬nh phÆ°Æ¡ng Ä‘á»™ dÃ i chuá»—i.

Do Ä‘Ã³, tokenization tá»‘i Æ°u giÃºp:

* Giáº£m (T)
* Giáº£m bá»™ nhá»›
* TÄƒng tá»‘c huáº¥n luyá»‡n

---

# 9. PhÃ¢n tÃ­ch thá»‘ng kÃª vÄƒn báº£n

Entropy cá»§a táº­p tá»«:

[
H(W) = - \sum_{w \in V} P(w) \log P(w)
]

Vá»›i:

[
P(w) = \frac{f(w)}{T}
]

Náº¿u phÃ¢n bá»‘ Zipf:

[
f(w_r) \propto \frac{1}{r}
]

trong Ä‘Ã³ (r) lÃ  thá»© háº¡ng táº§n suáº¥t.

Äiá»u nÃ y cho tháº¥y:

* Sá»‘ Ã­t tá»« xuáº¥t hiá»‡n ráº¥t nhiá»u
* Nhiá»u tá»« hiáº¿m xuáº¥t hiá»‡n

---

# 10. So sÃ¡nh vá»›i Subword Tokenization

Word-level tokenization cÃ³ nhÆ°á»£c Ä‘iá»ƒm:

[
P(\text{OOV}) > 0
]

Giáº£i phÃ¡p: Byte Pair Encoding (BPE).

Táº­p há»£p phÃ¢n rÃ£:

[
w = s_1 s_2 \dots s_k
]

vá»›i (s_i \in V_{subword})

Äáº£m báº£o:

[
\forall w, \exists \text{ decomposition}
]

---

# 11. Tháº£o luáº­n

Tokenization lÃ  quÃ¡ trÃ¬nh:

[
\text{Text} \rightarrow \text{Discrete Representation} \rightarrow \text{Continuous Geometry}
]

Vá» máº·t toÃ¡n há»c:

* LÃ  Ã¡nh xáº¡ tá»« chuá»—i kÃ½ tá»± sang khÃ´ng gian vector
* LÃ  bÆ°á»›c nÃ©n thÃ´ng tin
* áº¢nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n phÃ¢n phá»‘i xÃ¡c suáº¥t

---

# 12. Káº¿t luáº­n

ThÃ´ng qua vÃ­ dá»¥ *The Time Machine*, ta tháº¥y:

1. Tokenization quyáº¿t Ä‘á»‹nh cáº¥u trÃºc dá»¯ liá»‡u Ä‘áº§u vÃ o
2. Vocabulary áº£nh hÆ°á»Ÿng Ä‘áº¿n kÃ­ch thÆ°á»›c embedding
3. Biá»ƒu diá»…n vector quyáº¿t Ä‘á»‹nh kháº£ nÄƒng há»c ngá»¯ nghÄ©a
4. Äá»™ dÃ i chuá»—i áº£nh hÆ°á»Ÿng Ä‘áº¿n Ä‘á»™ phá»©c táº¡p Transformer

ToÃ n bá»™ quÃ¡ trÃ¬nh cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ hÃ¬nh hÃ³a:

[
\Sigma^* \xrightarrow{\tau} V^* \xrightarrow{E} \mathbb{R}^{T \times d}
]

Ä‘Ã³ng vai trÃ² ná»n táº£ng cho má»i mÃ´ hÃ¬nh Transformer hiá»‡n Ä‘áº¡i.

---

# TÃ i liá»‡u tham kháº£o

1. Attention Is All You Need
2. Sennrich, R. et al. (2016). *Neural Machine Translation of Rare Words with Subword Units*.
3. Shannon, C. (1948). *A Mathematical Theory of Communication*.
4. Jurafsky, D., Martin, J. (2023). *Speech and Language Processing*.
5. Manning, C., SchÃ¼tze, H. (1999). *Foundations of Statistical Natural Language Processing*.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Táº¡i sao vÄƒn báº£n cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘?](aero_LLM_01_Why text needs to be numbered.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Why text needs to be numbered.md) |
| [PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_LLM_02_Parsing text to numbered tokens.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Parsing text to numbered tokens.md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  phÃ¢n tÃ­ch biá»ƒu diá»…n](aero_LLM_03_CodeChallenge Create and visualize tokens (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Create and visualize tokens (part 1).md) |
| [Táº¡o vÃ  trá»±c quan hÃ³a Token (Pháº§n 2): PhÃ¢n tÃ­ch hÃ¬nh há»c khÃ´ng gian embedding vÃ  Attention Map trong mÃ´ hÃ¬nh Transformer](aero_LLM_04_CodeChallenge Create and visualize tokens (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Create and visualize tokens (part 2).md) |
| [Chuáº©n bá»‹ vÄƒn báº£n cho Tokenization trong mÃ´ hÃ¬nh Transformer: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  phÃ¢n tÃ­ch toÃ¡n há»c](aero_LLM_05_Preparing text for tokenization.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Preparing text for tokenization.md) |
| ğŸ“Œ **[PhÃ¢n tÃ­ch quy trÃ¬nh Tokenization qua vÃ­ dá»¥ *The Time Machine*: CÆ¡ sá»Ÿ thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a toÃ¡n há»c](aero_LLM_06_CodeChallenge Tokenizing The Time Machine.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_06_CodeChallenge Tokenizing The Time Machine.md) |
| [So sÃ¡nh Tokenization má»©c kÃ½ tá»±, tá»« vÃ  subword: PhÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  mÃ´ hÃ¬nh toÃ¡n há»c](aero_LLM_07_Tokenizing characters vs. subwords vs. words.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Tokenizing characters vs. subwords vs. words.md) |
| [aero_LLM_08_Byte-pair encoding algorithm.md](aero_LLM_08_Byte-pair encoding algorithm.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_Byte-pair encoding algorithm.md) |
| [Thuáº­t toÃ¡n Byte Pair Encoding (BPE) vÃ  BÃ i toÃ¡n Tá»‘i Æ°u KÃ­ch thÆ°á»›c Tá»« vá»±ng trong MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_LLM_09_CodeChallenge Byte-pair encoding to a desired vocab size.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Byte-pair encoding to a desired vocab size.md) |
| [aero_LLM_10_Exploring ChatGPT4's tokenizer.md](aero_LLM_10_Exploring ChatGPT4's tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_Exploring ChatGPT4's tokenizer.md) |
| [aero_LLM_11_CodeChallenge Token count by subword length (part 1).md](aero_LLM_11_CodeChallenge Token count by subword length (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Token count by subword length (part 1).md) |
| [aero_LLM_12_CodeChallenge Token count by subword length (part 2).md](aero_LLM_12_CodeChallenge Token count by subword length (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_CodeChallenge Token count by subword length (part 2).md) |
| [aero_LLM_13_How many rs in strawberry.md](aero_LLM_13_How many rs in strawberry.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_How many rs in strawberry.md) |
| [aero_LLM_14_CodeChallenge Create your algorithmic rapper name ).md](aero_LLM_14_CodeChallenge Create your algorithmic rapper name ).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_CodeChallenge Create your algorithmic rapper name ).md) |
| [aero_LLM_15_Tokenization in BERT.md](aero_LLM_15_Tokenization in BERT.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_Tokenization in BERT.md) |
| [aero_LLM_16_CodeChallenge Character counts in BERT tokens.md](aero_LLM_16_CodeChallenge Character counts in BERT tokens.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_CodeChallenge Character counts in BERT tokens.md) |
| [aero_LLM_17_Translating between tokenizers.md](aero_LLM_17_Translating between tokenizers.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_Translating between tokenizers.md) |
| [aero_LLM_18_CodeChallenge More on token translation.md](aero_LLM_18_CodeChallenge More on token translation.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_CodeChallenge More on token translation.md) |
| [aero_LLM_19_CodeChallenge Tokenization compression ratios.md](aero_LLM_19_CodeChallenge Tokenization compression ratios.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_19_CodeChallenge Tokenization compression ratios.md) |
| [aero_LLM_20_Tokenization in different languages.md](aero_LLM_20_Tokenization in different languages.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_20_Tokenization in different languages.md) |
| [aero_LLM_21_CodeChallenge Zipf's law in characters and tokens.md](aero_LLM_21_CodeChallenge Zipf's law in characters and tokens.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_21_CodeChallenge Zipf's law in characters and tokens.md) |
| [aero_LLM_22_Word variations in Claude tokenizer.md](aero_LLM_22_Word variations in Claude tokenizer.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_22_Word variations in Claude tokenizer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

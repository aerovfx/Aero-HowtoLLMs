
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [02 Words to tokens to numbers](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# PhÃ¢n tÃ­ch vÃ  chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘: CÆ¡ sá»Ÿ toÃ¡n há»c vÃ  á»©ng dá»¥ng trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n

---

## TÃ³m táº¯t

QuÃ¡ trÃ¬nh phÃ¢n tÃ­ch (parsing) vÄƒn báº£n thÃ nh cÃ¡c token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘ lÃ  bÆ°á»›c ná»n táº£ng trong huáº¥n luyá»‡n vÃ  suy luáº­n cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs). BÃ i viáº¿t nÃ y trÃ¬nh bÃ y cÆ¡ sá»Ÿ lÃ½ thuyáº¿t cá»§a tokenization, Ä‘Ã¡nh sá»‘ vá»‹ trÃ­ (positional indexing), vÃ  vai trÃ² cá»§a chÃºng trong kiáº¿n trÃºc Transformer. PhÃ¢n tÃ­ch dá»±a trÃªn cÃ¡c cÃ´ng trÃ¬nh ná»n táº£ng nhÆ° Transformer cá»§a Vaswani et al. (2017) vÃ  cÃ¡c mÃ´ hÃ¬nh GPT do OpenAI phÃ¡t triá»ƒn. CÃ¡c cÃ´ng thá»©c toÃ¡n há»c minh há»a quÃ¡ trÃ¬nh Ã¡nh xáº¡ vÄƒn báº£n sang khÃ´ng gian vector vÃ  cÃ¡ch mÃ´ hÃ¬nh xá»­ lÃ½ chuá»—i cÃ³ thá»© tá»±.

---

# 1. Giá»›i thiá»‡u

MÃ¡y tÃ­nh khÃ´ng xá»­ lÃ½ trá»±c tiáº¿p â€œtá»«â€ hay â€œcÃ¢uâ€ nhÆ° con ngÆ°á»i, mÃ  xá»­ lÃ½ **chuá»—i sá»‘**.

Do Ä‘Ã³, vÄƒn báº£n pháº£i Ä‘Æ°á»£c:

1. TÃ¡ch thÃ nh token
2. Ãnh xáº¡ thÃ nh chá»‰ sá»‘ (ID)
3. Chuyá»ƒn thÃ nh vector embedding
4. ÄÃ¡nh sá»‘ theo vá»‹ trÃ­ trong chuá»—i

VÃ­ dá»¥:

> "AI is powerful"

Sau tokenization cÃ³ thá»ƒ trá»Ÿ thÃ nh:

[
["AI", " is", " powerful"]
]

VÃ  Ä‘Æ°á»£c Ã¡nh xáº¡ thÃ nh:

[
[50256, 318, 3665]
]

---

# 2. Tokenization: CÆ¡ sá»Ÿ toÃ¡n há»c

Giáº£ sá»­ táº­p tá»« vá»±ng (V) cÃ³ kÃ­ch thÆ°á»›c:

[
|V| = N
]

HÃ m tokenization:

[
T: \mathcal{X} \to V^T
]

vá»›i:

* ( \mathcal{X} ): khÃ´ng gian vÄƒn báº£n
* (V^T): chuá»—i cÃ¡c token ID

Náº¿u chuá»—i vÄƒn báº£n lÃ  (x), ta cÃ³:

[
T(x) = (t_1, t_2, ..., t_T)
]

Má»—i (t_i \in {1,2,...,N})

---

# 3. Byte Pair Encoding (BPE)

GPT sá»­ dá»¥ng BPE Ä‘á»ƒ xá»­ lÃ½ tá»« hiáº¿m.

Giáº£ sá»­ ban Ä‘áº§u ta cÃ³ táº­p kÃ½ tá»± (C).
Thuáº­t toÃ¡n láº·p:

1. TÃ¬m cáº·p kÃ½ tá»± xuáº¥t hiá»‡n nhiá»u nháº¥t
2. Gá»™p thÃ nh token má»›i
3. ThÃªm vÃ o tá»« vá»±ng

QuÃ¡ trÃ¬nh tá»‘i Æ°u hÃ³a nháº±m giáº£m entropy:

[
H(X) = -\sum_x P(x)\log P(x)
]

BPE giÃºp:

* Giáº£m Ä‘á»™ dÃ i chuá»—i (T)
* TÄƒng hiá»‡u quáº£ tÃ­nh toÃ¡n

---

# 4. ÄÃ¡nh sá»‘ token (Positional Indexing)

Sau tokenization:

[
(t_1, t_2, ..., t_T)
]

Ta cáº§n biá»ƒu diá»…n thá»© tá»±:

[
i = 1,2,...,T
]

Náº¿u khÃ´ng cÃ³ chá»‰ sá»‘ vá»‹ trÃ­, mÃ´ hÃ¬nh Transformer sáº½ báº¥t biáº¿n hoÃ¡n vá»‹.

---

## 4.1. Biá»ƒu diá»…n embedding

Má»—i token ID Ä‘Æ°á»£c Ã¡nh xáº¡:

[
e_i = E(t_i)
]

Vector Ä‘áº§u vÃ o cuá»‘i cÃ¹ng:

[
z_i = e_i + p_i
]

Trong Ä‘Ã³:

* (p_i): vector vá»‹ trÃ­

---

# 5. Self-Attention vÃ  vai trÃ² cá»§a thá»© tá»±

Attention Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a:

[
\text{Attention}(Q,K,V)
=======================

\text{softmax}
\left(
\frac{QK^T}{\sqrt{d_k}}
\right)V
]

Náº¿u khÃ´ng cÃ³ positional encoding:

[
\text{Attention}(PX) = P\text{Attention}(X)
]

â†’ KhÃ´ng phÃ¢n biá»‡t thá»© tá»±.

Khi thÃªm (p_i):

[
Z = E + P
]

ma tráº­n attention pháº£n Ã¡nh quan há»‡ phá»¥ thuá»™c cÃ³ hÆ°á»›ng.

---

# 6. Causal Masking

Trong mÃ´ hÃ¬nh tá»± há»“i quy (GPT):

[
P(x) = \prod_{t=1}^{T} P(x_t | x_{<t})
]

Mask:

[
M_{ij} =
\begin{cases}
0 & j \le i \
-\infty & j > i
\end{cases}
]

Ma tráº­n attention thá»±c táº¿:

[
A = \text{softmax}
\left(
\frac{QK^T}{\sqrt{d_k}} + M
\right)
]

ÄÃ¡nh sá»‘ token cho phÃ©p xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c vá»‹ trÃ­ (i).

---

# 7. Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n

Self-attention:

[
\mathcal{O}(T^2 d)
]

Náº¿u chiá»u dÃ i chuá»—i tÄƒng gáº¥p Ä‘Ã´i:

[
\text{Compute} \approx 4\times
]

Do Ä‘Ã³ viá»‡c tokenization hiá»‡u quáº£ giÃºp:

* Giáº£m (T)
* Giáº£m chi phÃ­ huáº¥n luyá»‡n

---

# 8. VÃ­ dá»¥ minh há»a

Giáº£ sá»­ cÃ¢u:

> "Machine learning is amazing"

Tokenization:

[
[1543, 4673, 318, 4996]
]

Embedding:

[
E \in \mathbb{R}^{|V| \times d}
]

Äáº§u vÃ o:

[
Z \in \mathbb{R}^{T \times d}
]

Qua attention:

[
Z' = \text{Transformer}(Z)
]

---

# 9. LiÃªn há»‡ vá»›i Reinforcement Learning from Human Feedback

Trong RLHF:

[
x = [\text{Prompt}; \text{Response}]
]

ÄÃ¡nh sá»‘ cho phÃ©p:

* PhÃ¢n biá»‡t Ä‘oáº¡n cáº§n tá»‘i Æ°u
* Mask loss chÃ­nh xÃ¡c

Loss:

[
\mathcal{L} = - \sum_{t \in R} \log P(x_t | x_{<t})
]

---

# 10. Tháº£o luáº­n

QuÃ¡ trÃ¬nh parsing text to numbered tokens lÃ :

* BÆ°á»›c Ä‘áº§u tiÃªn cá»§a NLP pipeline
* Äiá»u kiá»‡n cáº§n cho Transformer hoáº¡t Ä‘á»™ng
* Yáº¿u tá»‘ quyáº¿t Ä‘á»‹nh hiá»‡u suáº¥t tÃ­nh toÃ¡n

Náº¿u bá» bÆ°á»›c nÃ y:

[
\text{Model} \to \text{KhÃ´ng thá»ƒ huáº¥n luyá»‡n}
]

---

# 11. Káº¿t luáº­n

Chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chuá»—i token Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘ lÃ :

1. Ná»n táº£ng cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯
2. CÆ¡ sá»Ÿ cho self-attention
3. Äiá»u kiá»‡n Ä‘á»ƒ thá»±c hiá»‡n causal modeling

ToÃ¡n há»c cho tháº¥y thá»© tá»± lÃ  thÃ nh pháº§n thiáº¿t yáº¿u trong biá»ƒu diá»…n ngÃ´n ngá»¯.

---

# TÃ i liá»‡u tham kháº£o

1. Vaswani, A. et al. (2017). *Attention Is All You Need*.
2. Radford, A. et al. (2019). *Language Models are Unsupervised Multitask Learners*.
3. Sennrich, R. et al. (2016). *Neural Machine Translation of Rare Words with Subword Units*.
4. Goodfellow, I., Bengio, Y., Courville, A. (2016). *Deep Learning*. MIT Press.

---
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

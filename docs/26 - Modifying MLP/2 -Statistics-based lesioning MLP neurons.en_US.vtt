WEBVTT

00:02.080 --> 00:08.440
There are some conceptual similarities here in this demo compared to the previous video.

00:09.040 --> 00:17.200
In particular, the idea of using some kind of statistical procedure to select subsets of neurons in

00:17.200 --> 00:24.720
the MLP expansion layer as a way to deal with the large dimensionality of the model internals.

00:25.200 --> 00:33.400
But here in this video, I will take a more statistically sophisticated approach by using a t test on

00:33.400 --> 00:39.280
one data set to select neurons to manipulate in a separate data set.

00:40.080 --> 00:43.360
There's a lot of steps in this demo, a lot of moving parts.

00:43.360 --> 00:48.680
So I'm going to organize this demo into three sections or three parts.

00:49.160 --> 00:52.120
And I will give you the overview of each part.

00:52.160 --> 00:56.920
Then switch to code and then come back to the slides to introduce the next part.

00:57.120 --> 01:03.390
It's slightly different from how I typically organize the Python video demos, but I think you will

01:03.390 --> 01:04.990
agree that it's a good approach.

01:05.550 --> 01:15.270
So the major goal of part one is to get activations of MLP neurons for him versus her tokens in different

01:15.270 --> 01:23.390
sentences, and then run a batch of T tests to identify neurons that have significantly different activations

01:23.390 --> 01:25.310
between those two pronouns.

01:25.910 --> 01:29.510
So to begin, I will import the Bert model.

01:29.870 --> 01:37.110
As you know, there are naming convention differences between the Bert model as opposed to the OpenAI

01:37.230 --> 01:38.550
GPT models.

01:39.070 --> 01:43.270
So this is the layer that we want to hook into.

01:43.670 --> 01:47.270
It's called the dense layer in the intermediate group.

01:47.750 --> 01:55.830
You can see that it has the number of output features of 3072 that corresponds to the expansion layer

01:55.830 --> 01:57.230
of the MLP.

01:58.150 --> 02:00.060
So the hook function looks like this.

02:00.420 --> 02:01.180
Very simple.

02:01.220 --> 02:07.540
All I'm doing is extracting the activation values and storing them in a dictionary, so I can later

02:07.580 --> 02:09.660
access and analyze them.

02:10.500 --> 02:17.220
And the data that I will use to identify neurons that have a statistically significant difference in

02:17.220 --> 02:24.780
activation levels between him and her are the same him and her sentences that you've now seen many times

02:24.780 --> 02:26.460
before in this course.

02:27.420 --> 02:32.420
These are the histograms of all the activations from the target indices.

02:32.740 --> 02:37.500
The blue line is the activations that the hook functions get out from the model.

02:38.580 --> 02:44.700
And I have applied the Jlu activation function to show you what it would look like that you see in the

02:44.700 --> 02:45.540
orange line.

02:45.980 --> 02:49.140
Now that's something that I just calculated manually.

02:49.140 --> 02:51.700
It's not what I pulled out of the model.

02:52.340 --> 03:00.800
Anyway, I then ran a t test on each of the 3000 neurons comparing the 54 sentences with him versus

03:00.800 --> 03:01.120
her.

03:01.880 --> 03:05.640
And here you see a scatter plot of those T values.

03:06.000 --> 03:13.920
Turns out that over two thirds of the neurons showed a significant T value for one or the other category.

03:14.440 --> 03:17.760
That is quite a large number of significant effects.

03:17.920 --> 03:26.280
But here I used the correction for multiple comparisons, which actually rewards lots of small p values.

03:26.720 --> 03:28.920
You could also use Bonferroni correction.

03:28.920 --> 03:32.520
It's more stringent than it probably needs to be here.

03:32.680 --> 03:38.800
You'll get a lot fewer significant neurons, but that won't actually change the pattern of findings

03:38.960 --> 03:43.280
that you get from the experiments that I will show you later on.

03:43.920 --> 03:46.400
Okay, so that is the end of part one.

03:46.440 --> 03:52.760
The idea is now I have a T value for each of the neurons in one layer of the model.

03:53.760 --> 03:54.160
All right.

03:54.200 --> 03:58.510
So now I will switch to code and show you how all this stuff gets implemented.

03:59.670 --> 04:04.310
Quite a few libraries that we will use here for the rest of this code demo.

04:04.510 --> 04:06.990
And here I'm loading in the Bert model.

04:07.150 --> 04:08.870
And yeah here you see the layers.

04:08.870 --> 04:12.430
So we are looking for uh these are the transformer blocks.

04:12.430 --> 04:17.430
Here we have the attention block and here is the MLP block.

04:17.430 --> 04:19.710
All of this stuff here is the MLP block.

04:20.070 --> 04:21.630
So here this is called dense.

04:21.670 --> 04:24.070
This is the input to the MLP layer.

04:24.230 --> 04:25.470
This is intermediate.

04:25.470 --> 04:35.670
This is where the MLP layer expands from a dimensionality of 768 to a much larger dimensionality, fanning

04:35.670 --> 04:39.150
out to four x giving us 3072.

04:39.790 --> 04:42.910
And then here in the output layer we have also dense.

04:42.910 --> 04:47.550
And that goes from 3000 back down to 768.

04:47.870 --> 04:49.750
So this is what we want to hook into.

04:49.790 --> 04:56.860
And we are going to get these activation values before we apply the non-linear nonlinear jlu activation.

04:57.380 --> 05:01.300
All right, so, uh, yeah, here's just some convenience variables.

05:01.340 --> 05:02.620
The number of neurons.

05:02.820 --> 05:04.780
And, uh, this just shows you the size.

05:04.780 --> 05:06.860
So 3072 neurons.

05:07.100 --> 05:09.980
And this is the layer that we will hook into.

05:10.460 --> 05:10.700
Okay.

05:10.740 --> 05:11.940
So now we get to part one.

05:11.940 --> 05:13.260
Here is the hook function.

05:13.260 --> 05:14.980
Again super duper simple.

05:15.020 --> 05:20.220
All I'm doing is hooking this into intermediate dense.

05:20.460 --> 05:24.140
And yeah you can see this is just a standard forward hook.

05:24.460 --> 05:31.340
And then all that happens inside this hook is that we grab the output, detach it from the computational

05:31.340 --> 05:33.820
graph, transform it into numpy.

05:33.980 --> 05:40.820
That's just to make things a little bit smoother later on when I'm calling uh numpy and scipy stats

05:40.820 --> 05:41.580
functions.

05:41.700 --> 05:44.100
And then I store it in this dictionary here.

05:44.500 --> 05:45.780
Okay, so run that.

05:45.780 --> 05:51.700
Actually we don't even really need to have this be dynamically named here because I'm only implanting

05:51.700 --> 05:52.700
this into one layer.

05:52.700 --> 05:57.370
But anyway, this will be useful for the code challenge in the next video.

05:57.970 --> 05:59.810
Okay, so now the forward pass.

05:59.810 --> 06:03.610
You've seen these sentences more than once before in this course.

06:03.610 --> 06:05.290
Let me scroll down to the bottom.

06:05.530 --> 06:05.810
Okay.

06:05.850 --> 06:10.690
So all of that stuff is not anything particularly new or here.

06:10.730 --> 06:11.050
Yeah.

06:11.090 --> 06:14.490
So yeah I won't even talk about that okay.

06:14.530 --> 06:15.650
So forward pass.

06:15.770 --> 06:21.970
Uh, here I'm doing the forward pass and then removing the handle immediately because I'm going to create

06:22.010 --> 06:26.330
a new handle in a later part of this code demo.

06:26.610 --> 06:33.650
And then here I'm just printing out the shape so we can confirm the size of this, uh, matrix that

06:33.650 --> 06:37.010
we have exported from the MLP activation values.

06:37.050 --> 06:38.490
Don't worry about this warning.

06:38.490 --> 06:41.770
Uh, if that becomes a problem, then we'll deal with it in the future.

06:42.290 --> 06:42.650
Okay.

06:42.690 --> 06:52.170
So this data set has 108 sequences, uh, of course, 54 for the hymn sentences and 54 identical sentences,

06:52.290 --> 06:53.960
but with her instead of him.

06:54.520 --> 06:57.760
Uh, there are 11 tokens in total, and.

06:57.800 --> 06:59.280
Yeah, 3000 neurons.

06:59.800 --> 06:59.960
Okay.

07:00.000 --> 07:02.200
And then here I'm creating the histograms.

07:02.200 --> 07:05.040
So here's the histogram of those values.

07:05.040 --> 07:06.720
And I just flatten them all out.

07:06.920 --> 07:11.280
So I take all of these values the gigantic number of numbers.

07:11.520 --> 07:16.560
And then I flatten them out into a vector and create a histogram.

07:16.960 --> 07:24.800
By the way when I do this it means I'm also including in this distribution the masked tokens.

07:24.800 --> 07:29.640
So the tokens that correspond to a zero in the attention mask that's here.

07:29.640 --> 07:36.000
So that is where we don't actually want the model to generate any predictions, because the sentences

07:36.000 --> 07:38.040
just had no text after them.

07:38.080 --> 07:45.120
So in theory it would be a little bit cleaner to go through this sequence by sequence and only extract

07:45.120 --> 07:48.160
the activations from valid tokens.

07:48.160 --> 07:49.120
But anyway, it doesn't matter.

07:49.120 --> 07:51.320
It's a relatively small amount of data.

07:51.520 --> 07:53.460
Okay, So I get those data.

07:53.460 --> 07:55.980
And then here is exactly the same data.

07:56.180 --> 08:00.860
But I am running them through the nonlinear activation function.

08:00.860 --> 08:05.420
And that's just to show you the impact of the nonlinear activation function.

08:05.580 --> 08:13.140
And also so you can see that what we've hooked into actually is the pre activation values.

08:13.140 --> 08:18.980
They are normally distributed which is great because we are going to run a bunch of t tests.

08:19.220 --> 08:26.740
And if you're running a parametric statistical test like a t test then uh, it's pretty handy if the

08:26.740 --> 08:28.620
data are normally distributed.

08:29.100 --> 08:29.500
Okay.

08:29.540 --> 08:37.700
Here I am also showing the exact same histograms, but in log scale that just compresses all of the

08:37.700 --> 08:38.620
large values.

08:38.620 --> 08:40.740
It expands the small values.

08:40.940 --> 08:47.100
And that allows you to better appreciate what the jlu activation really does.

08:47.100 --> 08:52.850
So you can see that all of the negative values down here get completely obliterated.

08:53.170 --> 09:00.970
Of course, there are some negative values that pass through the Jlu function because it leaks a little

09:01.010 --> 09:03.650
bit of negative results through.

09:03.850 --> 09:07.010
And up here, when we get to the positive values these are unchanged.

09:07.050 --> 09:08.850
The data are exactly the same.

09:09.450 --> 09:12.850
For relatively large positive values.

09:12.850 --> 09:15.930
So all of these values have basically been squashed out.

09:15.970 --> 09:16.890
They've been clipped.

09:17.250 --> 09:25.610
And this is interesting to observe because this is one of the mechanisms by which the MLP layer imposes

09:25.610 --> 09:26.490
sparsity.

09:26.970 --> 09:29.170
So you have lots of negative values.

09:29.170 --> 09:36.050
And then when they move through this activation a lot of this distribution just gets obliterated.

09:36.050 --> 09:38.810
It gets squashed down to zero.

09:39.050 --> 09:46.930
And that means that only a relatively small number of activations in the MLP layers actually pass through

09:47.170 --> 09:53.880
to be incorporated into the embeddings vector at the end of the transformer block.

09:54.240 --> 09:56.800
Okay, so that is enough for that.

09:56.960 --> 09:57.240
Okay.

09:57.280 --> 10:04.600
So here what I'm doing is looping through all of the sentences, finding one of the target locations.

10:04.600 --> 10:11.760
So I'm looking for either the token for him or the token index for her in this sentence.

10:11.960 --> 10:19.400
And then I'm extracting the first one and then getting the values from this particular sentence only

10:19.400 --> 10:20.600
for that target.

10:21.480 --> 10:27.920
Now, I have explained this code here in a lot of detail earlier in the course, but I just thought

10:27.920 --> 10:33.840
it would be useful to put a little reminder in here, in case you've forgotten that this particular

10:33.840 --> 10:41.680
code happens to work, because I have already guaranteed that each of these sentences has exactly one

10:41.680 --> 10:44.640
appearance of one of the two tokens.

10:44.920 --> 10:51.310
If you have a different experimental setup, different data set where, for example, the word him could

10:51.310 --> 10:56.550
appear multiple times in the sentence, or maybe both him and her can appear in the sentence.

10:56.870 --> 10:59.510
Then you would have to adapt this code.

10:59.950 --> 11:00.430
Okay.

11:00.790 --> 11:03.510
Anyway, so let me run this code here.

11:03.630 --> 11:05.550
And now here I'm doing a t test.

11:05.550 --> 11:13.430
So one sample t test on the activations, comparing the first 54 sentences to the last 54 sentences.

11:13.550 --> 11:19.910
And I'm subtracting the two and testing against a difference value of zero.

11:20.230 --> 11:24.390
So now we can look for positive t values and negative t values.

11:25.230 --> 11:29.510
And we want them to be significant at an FDR correction.

11:30.030 --> 11:37.230
By the way I do not actually specify the significance threshold here in this call to the function.

11:37.230 --> 11:43.870
But the default alpha value, the default threshold is 0.05 corrected for multiple comparisons.

11:43.870 --> 11:44.830
So that's fine.

11:45.150 --> 11:53.140
FDR correction is appropriate for situations where the tests are related to each other, which is certainly

11:53.140 --> 11:53.940
the case here.

11:53.940 --> 11:56.660
All of these data are coming from the same model.

11:56.980 --> 12:01.900
If you would like to be a little bit more stringent, then that's also fine.

12:01.940 --> 12:10.660
You could do something like t dot p value less than 0.05 divided by the number of neurons.

12:10.940 --> 12:11.980
Something like this.

12:12.340 --> 12:12.700
Sorry.

12:12.740 --> 12:13.460
Like this.

12:13.460 --> 12:16.380
That would correspond to a Bonferroni correction.

12:16.860 --> 12:18.100
This is more stringent.

12:18.100 --> 12:24.380
It assumes independence which is actually not appropriate in this case because yeah, we do not have

12:24.380 --> 12:27.780
independent tests here, but it is more stringent.

12:27.780 --> 12:34.860
You're going to find fewer neurons having significant T values, but the end result is going to be basically

12:34.860 --> 12:36.460
the same qualitatively the same.

12:37.540 --> 12:37.900
Okay.

12:37.940 --> 12:41.460
So here I'm looking for I'm calling this variable him neurons.

12:41.460 --> 12:45.100
And her neurons I'm a little bit loose with the terminology here.

12:45.340 --> 12:51.760
I mean first of all I'm being very loose and generous with this terminology because these little computational

12:51.760 --> 12:55.080
units have nothing whatsoever to do with biological neurons.

12:55.080 --> 12:56.320
But that's a separate issue.

12:56.680 --> 13:02.120
Anyway, so the idea is that if the neuron is statistically significant.

13:02.120 --> 13:10.520
So if the p value is small and the t statistic is greater than zero, then I'm calling that him neurons.

13:10.520 --> 13:16.000
So this is going to be a boolean vector I can show you this him neurons.

13:16.000 --> 13:17.040
This is Boolean vector.

13:17.040 --> 13:18.560
It's all trues and falses.

13:18.760 --> 13:21.600
It is true where it is.

13:21.640 --> 13:30.120
The t test was significant and I was also positive meaning more activation for the Him tokens compared

13:30.120 --> 13:31.440
to the Her tokens.

13:31.960 --> 13:32.200
Okay.

13:32.240 --> 13:34.480
And then vice versa for the Her tokens.

13:34.600 --> 13:36.280
And then yeah that's what you see here.

13:36.280 --> 13:39.480
So the red X's are non-significant T values.

13:39.480 --> 13:40.560
And you see that.

13:40.720 --> 13:47.950
Yeah most of the neurons showed some significant Ghent differentiation in their activation values between

13:47.950 --> 13:50.030
the hymn tokens and the her tokens.

13:51.550 --> 13:59.110
The goal of part two of this demo is to have the model make predictions about him or her words in a

13:59.110 --> 14:02.310
completely different set of sentences.

14:03.070 --> 14:05.910
There's also no manipulations of the model in this part.

14:05.950 --> 14:08.350
We're just setting up the experiment.

14:08.470 --> 14:11.750
For that, we will do in part three.

14:12.310 --> 14:13.870
Here are the sentences.

14:13.870 --> 14:16.550
Or it's really one key sentence that we will use.

14:16.550 --> 14:23.670
But I'm going to have the model process this sentence three times with different words masked out.

14:24.390 --> 14:30.910
So the sentence is Robert helped Lucy with her project and she thanked him for his hard work.

14:31.390 --> 14:38.950
Now in the second sentence, we want to know whether the model will predict that the mask token should

14:38.950 --> 14:40.790
be replaced with the word her.

14:41.390 --> 14:47.660
And in the third sentence, we want to know whether the model will predict that the missing token here

14:47.660 --> 14:49.100
should be the word him.

14:49.780 --> 14:52.300
Now, perhaps you can already see where this is going.

14:52.340 --> 15:00.140
In part three of this demo, I'm going to use the t test results from part one to determine which neurons

15:00.140 --> 15:03.220
I should zero out in a causal experiment.

15:03.620 --> 15:05.500
But that's something for the future.

15:05.620 --> 15:11.940
Here what we're going to do is develop the task and the analyses so we know what to look for when we

15:11.940 --> 15:13.220
do the manipulation.

15:13.740 --> 15:23.020
So what I want to do now is find the token logits for him and her for the mask position in all three

15:23.020 --> 15:24.620
of these sentences.

15:25.620 --> 15:28.260
And here you see the visualization of all of those results.

15:28.260 --> 15:29.900
So there's a lot going on here.

15:30.060 --> 15:33.860
Let me take a moment to explain the organization of this graph.

15:34.460 --> 15:40.740
These four bars here on the left are all data from the first sentence, which I am here calling the

15:40.740 --> 15:43.530
clean sentence because there's no mask.

15:44.170 --> 15:50.410
And then this middle group is for the second sentence where the token her was masked out.

15:50.890 --> 15:54.970
And then the third sentence was where him was masked out.

15:55.290 --> 15:55.650
Okay.

15:55.690 --> 16:03.010
And then within each of these sentences, we are interested in two token positions corresponding to

16:03.050 --> 16:04.570
him and her.

16:05.290 --> 16:10.250
And then we have two vocab indices for her and for him.

16:10.690 --> 16:15.130
So that explains why there are three rows in the x axis labels.

16:15.290 --> 16:19.970
The first row is for the vocab tokens, where we get the logits from.

16:20.570 --> 16:27.770
The second row is for the token positions in the text, and the third row indicates which sentence we're

16:27.810 --> 16:29.290
getting the data from.

16:29.610 --> 16:29.930
Okay.

16:29.970 --> 16:36.450
So now let me focus your attention on interpreting these results here from the clean model with no mask.

16:37.370 --> 16:44.560
So we expect the her vocab to have the highest logit when the word in the sentence was her.

16:44.960 --> 16:52.760
And we expect the highest logit for the him vocab term when the token was actually him.

16:53.480 --> 16:59.680
Now there was no mask in this sentence, so the model literally saw exactly those words her and him.

17:00.080 --> 17:05.400
So, you know, this pattern of results is nice to see, but it's kind of trivial.

17:05.400 --> 17:08.360
It's not really surprising or terribly interesting.

17:08.800 --> 17:15.880
In fact, we can use results like this as a sanity check to make sure that our code is correct.

17:16.120 --> 17:22.480
Because if you did not see such a clear pattern or, you know, for example, if the logit pattern was

17:22.480 --> 17:26.360
reversed, then we would suspect that something was wrong with the code.

17:26.360 --> 17:30.600
Maybe in the analysis, maybe in the visualization somewhere.

17:30.960 --> 17:38.200
Anyway, so then when we look at the masked sentences, we see exactly the same pattern of results.

17:38.440 --> 17:48.020
So in particular the her logit is greater than the him logit for the her token, and when the her was

17:48.020 --> 17:54.140
masked out and vice versa for the him token in the him position when that was masked out.

17:54.460 --> 17:56.740
So here when him was masked out.

17:56.780 --> 17:57.740
Pretty confusing.

17:57.740 --> 18:01.260
Overall, this pattern of findings is nice to see.

18:01.540 --> 18:06.420
It shows that the Bert model has a good grasp of grammar and so on.

18:06.620 --> 18:10.820
But anyway, this is the end of the part two of the code demo.

18:10.980 --> 18:18.820
So again, the idea is that we are going to see how this pattern of results changes when we manipulate

18:18.820 --> 18:22.620
the MLP neurons in the experiment in part three.

18:23.380 --> 18:29.940
But before getting to that, I will now switch to Python and show you how to implement this analysis.

18:31.340 --> 18:33.300
So here are the three sentences.

18:33.340 --> 18:33.700
Again.

18:33.700 --> 18:41.490
They're all identical except for the addition of one mask replacing the word her over here and replacing

18:41.490 --> 18:43.930
the word him over here and here.

18:43.930 --> 18:50.930
I tokenize this now, normally if you are tokenizing multiple sentences like this and it's going to

18:50.970 --> 18:59.130
form a batch, you would want to have something like padding equals true because the different sequences

18:59.130 --> 19:01.090
in the batch might have different lengths.

19:01.170 --> 19:06.330
However, we don't have to worry about that here because it's all the same number of words.

19:06.330 --> 19:07.810
It's the same number of tokens.

19:07.810 --> 19:11.530
So we don't actually need to worry about padding in this special case.

19:12.010 --> 19:18.530
Okay, here I am finding the indices of the mask in each of these sentences.

19:18.650 --> 19:24.690
And basically that's just telling me that index five up here is the mask.

19:24.690 --> 19:28.770
So let's count this 01234.

19:29.290 --> 19:29.730
Uh oh.

19:30.130 --> 19:31.770
Is something wrong with the code here.

19:31.770 --> 19:34.850
Why does this say index five?

19:34.970 --> 19:39.120
But here when we literally just counted it we got zero, one, two, 01234.

19:39.600 --> 19:40.680
What's going on here?

19:40.920 --> 19:48.960
Remember that with the Bert tokenizer, the token sequences always begin with the CLS token.

19:48.960 --> 19:49.840
That's 101.

19:49.840 --> 19:50.800
That's what you see here.

19:51.160 --> 19:54.800
So there's actually another token in here for CLS.

19:54.840 --> 19:56.280
So that is zero.

19:56.760 --> 19:59.640
And then we have 12345.

19:59.760 --> 20:03.280
So I just wanted to make sure that was clear.

20:03.640 --> 20:03.960
Okay.

20:04.000 --> 20:07.440
Then I do the forward pass and I get the logits.

20:07.760 --> 20:09.960
Here is where I'm extracting all of the data.

20:09.960 --> 20:13.600
So I'm looping over the three sentences.

20:13.760 --> 20:17.480
And I'm going to extract the logits into this variable here.

20:18.080 --> 20:21.440
So for each sentence I have zero and one.

20:21.920 --> 20:24.000
So this is for the three sentences.

20:24.160 --> 20:31.440
This corresponds to the mask index for her and the mask index for him that you see here.

20:31.920 --> 20:35.360
And then here I have two values that I'm going to extract.

20:35.360 --> 20:38.230
And that corresponds to the output logits.

20:38.230 --> 20:41.590
So the final output logits from the model for the.

20:41.630 --> 20:45.870
Her vocab item and the Him vocab item.

20:46.230 --> 20:48.910
Okay, so I hope that all makes sense.

20:49.110 --> 20:50.790
And these are the key results.

20:50.830 --> 20:54.390
Now when you just look at these numbers it's kind of hard to make sense of them.

20:54.630 --> 20:58.910
So therefore I'm going to visualize them all in a bar plot.

20:59.190 --> 21:05.830
And that is what you see here again as a reminder because it's kind of a complicated organization of

21:05.830 --> 21:06.470
results.

21:06.910 --> 21:10.230
Three sentences are organized into the three groups here.

21:10.510 --> 21:16.070
And then this first row here indicates the token in the model.

21:16.070 --> 21:20.910
So for the vocab logit that we want to extract for her and him.

21:21.110 --> 21:27.550
And this corresponds to the token the missing token the masked token in these sentences.

21:27.870 --> 21:35.910
And the key result is that when we mask out the word her the model has a higher logit for her compared

21:35.910 --> 21:36.690
to him.

21:37.010 --> 21:44.770
And when we mask out the token for him, so the word him does not actually appear anywhere in the sentence

21:44.770 --> 21:46.210
in this sentence here.

21:46.530 --> 21:51.730
And yet the model still has a higher logit for him compared to her.

21:51.770 --> 21:54.370
So that is nice set of findings.

21:54.370 --> 21:57.370
And then the goal of experiment part three.

21:57.490 --> 22:05.330
The experiment in part three is to see whether we can change this bar and this bar by disrupting the

22:05.330 --> 22:08.290
activations of the MLP neurons.

22:10.170 --> 22:17.570
This is the experiment where we are going to implant a hook into the intermediate layer.

22:18.010 --> 22:22.450
Now this hook is not just for getting data like in part one.

22:22.930 --> 22:28.730
We are actually going to manipulate data and put it back into the model for the rest of the forward

22:28.730 --> 22:29.250
pass.

22:29.730 --> 22:33.970
I'm doing two manipulations in these two lines of code.

22:34.650 --> 22:42.600
Now remember that this output tensor is sequence in the batch by tokens by neurons.

22:42.920 --> 22:47.160
So in this first line I'm setting the second sentence.

22:47.160 --> 22:54.280
So index one and the token index corresponding to the word her that is actually masked out.

22:54.760 --> 23:00.400
And all of the neurons that showed a significant t test from part one.

23:00.400 --> 23:06.640
So all of those neurons just for this token, just for this sentence, I'm setting their value to be

23:06.680 --> 23:10.120
zero and similar for the second line.

23:10.280 --> 23:17.720
So these are the neurons that were significantly more active for him versus her from part one in the

23:17.720 --> 23:25.320
token position in the sequence for the masked word him and the third sentence and those are set to zero.

23:25.920 --> 23:28.520
And here is the line where I implant this.

23:28.520 --> 23:36.590
Now notice here I'm just implanting this into the intermediate block and not into intermediate dense.

23:36.990 --> 23:39.390
So here is the question for you.

23:39.670 --> 23:45.590
Why is it the case that in part one I got data from intermediate dense.

23:45.910 --> 23:51.110
But here I'm implanting the hook just into the intermediate without the dense.

23:51.670 --> 23:53.870
Is that a problem or does it matter.

23:53.990 --> 23:55.630
And what would be the difference?

23:56.190 --> 24:01.790
Of course I will answer this question when I switch to code, but I would like you to think about this

24:01.790 --> 24:03.350
before I get there.

24:04.230 --> 24:06.790
And now we can have a look at the results.

24:06.950 --> 24:13.150
So I extracted exactly the same data in exactly the same way as in part two.

24:13.710 --> 24:17.150
Except this is for the manipulated model.

24:17.630 --> 24:24.790
And then I will just subtract those two sets of numbers and show them in a graph that looks like this.

24:25.310 --> 24:29.510
So the x axis here is exactly the same as in part two.

24:30.030 --> 24:37.500
And the y axis is now the change in the logits here in this experiment with the manipulations.

24:37.700 --> 24:40.300
Versus the clean run from part two.

24:40.900 --> 24:43.860
So the first thing to notice is that all four.

24:43.900 --> 24:48.180
Of these bars for the first sentence are exactly zero.

24:49.100 --> 24:49.660
That's good.

24:49.660 --> 24:53.580
That provides yet another sanity check and manipulation control.

24:53.620 --> 24:58.780
Because I did not change any of the activation values from the first sentence.

24:58.940 --> 25:05.020
So if these numbers were anything other than zero, then something would be weird and we'd have to figure

25:05.020 --> 25:06.300
out where's the problem.

25:07.020 --> 25:14.020
But the important results that we are looking for here is the change in the her logit for the token

25:14.140 --> 25:22.700
corresponding that to the her position that was masked out and the him logit in the him token that was

25:22.700 --> 25:24.660
masked out in the third sentence.

25:25.100 --> 25:30.380
So that's this pink bar over here and this blue bar over here.

25:30.860 --> 25:38.290
And the fact that these are both positive indicates that the logits were higher in the clean version

25:38.290 --> 25:43.610
of the model, with no manipulations compared to when we manipulated those neurons.

25:44.130 --> 25:50.650
And over here on the right, you see a scatter plot of the clean logits versus the ablated final output

25:50.650 --> 25:53.770
logits across all of these results.

25:54.090 --> 26:00.090
And they're quite strongly correlated, which just indicates that this manipulation was pretty subtle.

26:00.370 --> 26:02.330
Now, that's not so surprising.

26:02.490 --> 26:08.210
We only manipulated a subset of MLP neurons from one transformer block.

26:08.570 --> 26:13.050
So we wouldn't really expect a large change at the final output.

26:13.050 --> 26:13.770
Logit.

26:14.610 --> 26:14.930
Okay.

26:14.970 --> 26:19.170
So now I will switch to code and go through the details of this experiment.

26:21.050 --> 26:22.890
Here is the hook function.

26:22.890 --> 26:31.310
As a reminder, I'm manipulating the output of the intermediate layer here and only for the second sentence

26:31.310 --> 26:38.670
for the mask position, for where her was masked out, and only for the neurons that had a significant

26:38.710 --> 26:46.350
t test value from, uh, the part one where we tested the data in the independent data set with the

26:46.390 --> 26:48.110
him and her sentences.

26:48.390 --> 26:55.510
And then the corresponding, uh, masking for sentence three, where I got rid of the Him token and

26:55.510 --> 26:57.030
replaced that with the mask.

26:57.430 --> 26:58.950
Okay, so here's the question.

26:58.950 --> 27:02.270
Why am I using intermediate here?

27:02.430 --> 27:06.830
When up above, let me scroll back up to part one.

27:07.470 --> 27:09.150
Uh, where is that hook function?

27:09.590 --> 27:10.150
That's here.

27:10.150 --> 27:10.590
Here.

27:10.590 --> 27:15.950
I implanted this hook to grab the data from intermediate dense.

27:16.150 --> 27:17.470
And why did I do that?

27:17.470 --> 27:18.550
And does it matter?

27:18.910 --> 27:23.510
So the question, the answer to the second question is it does not matter.

27:23.830 --> 27:28.350
And the answer to the first question is I did that basically, literally.

27:28.350 --> 27:30.420
So I could have this short discussion here.

27:30.780 --> 27:31.100
Okay.

27:31.140 --> 27:34.940
So what's the difference between intermediate and intermediate dense.

27:35.060 --> 27:37.060
So intermediate dense.

27:37.100 --> 27:39.620
Let me go back to this description here.

27:40.900 --> 27:42.380
Intermediate dense.

27:42.420 --> 27:47.420
This is the pre uh nonlinear activation functions.

27:47.540 --> 27:53.660
And if you just hook into this entire layer then you're going to get the output of this layer, which

27:53.660 --> 28:01.140
is uh, the same thing that dense calculates but then passed through the jlu activation.

28:01.380 --> 28:04.100
And why is that important.

28:04.100 --> 28:05.340
Why is that relevant?

28:05.820 --> 28:11.420
As I hinted at here with these distributions we're doing t tests on these data.

28:11.540 --> 28:18.660
And so when you are doing a t test to compare two different conditions, it's nice if you can have a

28:18.820 --> 28:22.340
normal distribution that the data are drawn from.

28:22.780 --> 28:28.100
And for the pre non-linear activations those are drawn from a normal distribution.

28:28.330 --> 28:32.170
so it's more amenable to statistical parametric testing.

28:33.090 --> 28:37.210
Whereas here with the Jlu these are certainly not normally distributed.

28:37.370 --> 28:43.730
So technically you could still do a T test, but it would be more appropriate to do some kind of nonparametric

28:43.730 --> 28:47.850
t test like based on medians or based on permutation testing.

28:48.450 --> 28:48.770
Okay.

28:48.810 --> 28:52.770
So that answers the question of why I use these values.

28:52.770 --> 28:56.370
The non-linear activation values for the t test.

28:56.770 --> 29:02.810
The question is why do I claim that it doesn't matter whether we zero out one or the other?

29:03.210 --> 29:08.770
And the answer is that there's no mixing involved here in the difference between.

29:08.810 --> 29:10.250
Let me scroll back up here.

29:10.770 --> 29:17.330
The difference between the output of this intermediate layer and the output of this dense layer.

29:17.610 --> 29:20.210
There's no mixing of the activations.

29:20.210 --> 29:22.410
The neurons don't interact with each other.

29:22.450 --> 29:26.690
Every neuron individually goes through this activation function.

29:27.440 --> 29:33.520
And so if you zero out a neuron here, it's also going to be zeroed out here.

29:33.840 --> 29:40.280
Likewise if you zero out a neuron here then it doesn't matter what value it had here because here you're

29:40.280 --> 29:41.160
zeroing it out.

29:41.520 --> 29:41.920
Okay.

29:41.960 --> 29:43.360
So I hope that makes sense.

29:43.800 --> 29:46.200
Uh, the answer is it does not matter.

29:46.200 --> 29:51.480
We could put this into intermediate or intermediate dense.

29:51.520 --> 29:53.200
We're multiplying by zero.

29:53.200 --> 29:55.120
So it literally has no impact.

29:55.240 --> 29:55.600
Okay.

29:55.640 --> 30:01.000
But I hope that helps you gain some more insight into the workings of these models.

30:01.680 --> 30:06.640
Okay, so, uh, implants that hook, and then I do another forward pass.

30:06.880 --> 30:13.160
And here I am extracting all of the logits exactly as I did before.

30:13.200 --> 30:20.080
I'm calling this variable logits zero just as a way to differentiate them from the clean version.

30:20.200 --> 30:22.320
And here I call this target logits.

30:22.320 --> 30:25.400
Z also stands for zero okay.

30:25.440 --> 30:27.510
Again So these are the key data.

30:27.510 --> 30:30.990
But we want to visualize them to know how to interpret them.

30:31.590 --> 30:37.830
All of the visualization code here is the same as what I showed before from part two.

30:38.150 --> 30:44.750
Except I'm showing the data for this variable delta logits, which is literally just the subtraction

30:44.750 --> 30:47.790
of the non manipulated model.

30:47.790 --> 30:52.830
So the clean model minus the manipulated model over here.

30:53.910 --> 30:56.390
And again we see these are all zeros.

30:56.390 --> 30:56.870
That's good.

30:56.870 --> 30:58.630
That's a confirmation check.

30:58.790 --> 31:04.470
And here what we are particularly interested in is this pink bar and this blue bar.

31:04.790 --> 31:11.350
This shows that when we manipulated the model for the her neurons, or at least the neurons that showed

31:11.350 --> 31:19.270
a significant t test from the independent sample in the third position in the her mask sentence, then

31:19.270 --> 31:25.650
those logits increased here, which means they were more positive in the clean version, which means

31:25.650 --> 31:34.890
that zeroing out those MLP neurons actually had a negative impact on the logits at the output of the

31:34.890 --> 31:35.450
model.

31:35.610 --> 31:36.730
And same thing for him.

31:36.730 --> 31:42.890
There's other stuff going on in here as well, which I'm not going to make any feeble attempt to interpret

31:42.890 --> 31:48.690
because I don't have like hypotheses about those particular sets of findings.

31:49.130 --> 31:55.810
And yeah, as I also mentioned in the slides, we are manipulating a very, very small part of the model.

31:55.810 --> 32:03.130
So it's not surprising that the final model logits at the very end of the model have been perturbed

32:03.170 --> 32:08.210
only a tiny bit by this really targeted focal manipulation.

32:09.290 --> 32:14.250
I hope you found this code demo to be interesting and also inspiring.

32:14.570 --> 32:21.850
There are so many ways that you can do great research and set up really elegant experiments to uncover

32:21.970 --> 32:29.560
subtle patterns of where in the model different kinds of language information is processed and passed

32:29.600 --> 32:31.400
on to subsequent layers.

32:31.920 --> 32:37.800
You're going to follow up on some of these ideas in the code challenge in the next video.

32:38.280 --> 32:41.200
And one small point about coding here.

32:41.400 --> 32:47.760
Hook functions are nothing special in the sense that anything you can do with Python code, you can

32:47.760 --> 32:49.400
put into a hook function.

32:50.080 --> 32:54.480
They can be long and complicated and implement lots of operations.

32:54.960 --> 32:59.920
So that can be useful, but it can also really slow down the processing of the model.

33:00.400 --> 33:05.760
Now, I didn't actually use any complicated hook functions here in this video, but this point will

33:05.760 --> 33:12.840
be relevant for the next video, where you're basically going to take all of the t test code and put

33:12.840 --> 33:20.200
it inside a hook function, so that the language model itself is actually running statistical analyses

33:20.320 --> 33:21.840
during the forward pass.

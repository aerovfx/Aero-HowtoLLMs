WEBVTT

00:02.080 --> 00:07.920
Welcome to this section of the course on modifying the MLP activations.

00:08.640 --> 00:15.960
In some sense, this section is not any different from the previous few sections, in that we are going

00:15.960 --> 00:22.200
to write hook functions to modify the MLP activations, sometimes just in one layer.

00:22.320 --> 00:29.680
Sometimes we will loop over all the layers and we will measure the impact of those modifications on

00:29.720 --> 00:33.440
embeddings, vectors and on model output logits.

00:34.160 --> 00:41.240
But on the other hand, we do have some unique challenges when working with MLP layers.

00:41.360 --> 00:43.560
In particular, the expansion layers.

00:44.040 --> 00:50.040
And these are challenges that we didn't really have to worry about in the previous sections, at least

00:50.040 --> 00:50.920
not as much.

00:51.560 --> 00:59.520
For example, when we are zeroing out attention heads, there's only one or a few dozen attention heads,

00:59.520 --> 01:06.130
depending on the model size, so we can actually just go through and cut out each one in turn and see

01:06.130 --> 01:07.050
what happens.

01:07.610 --> 01:14.290
But that kind of strategy just doesn't really work for the MLP layer, especially not the expansion

01:14.290 --> 01:14.810
layer.

01:15.250 --> 01:22.810
There are thousands of neurons in that layer, and so each one is going to have a relatively small impact.

01:23.050 --> 01:29.850
And it also just becomes completely computationally infeasible to loop through every single one of those

01:29.850 --> 01:30.570
neurons.

01:31.250 --> 01:38.450
So basically the goals and the unique features of this section are to think of some ways that we can

01:38.450 --> 01:41.810
deal with that huge dimensionality expansion.

01:42.770 --> 01:49.090
The different approaches that I will introduce you to in this section are all focused on using some

01:49.090 --> 01:56.890
kind of statistical criterion, perhaps a descriptive statistic, or an inferential statistical test

01:57.170 --> 02:04.580
to determine how we can slice up the MLP Space so that we are doing causal experiments without just

02:04.580 --> 02:09.900
completely losing our minds over the sheer number of neurons in those layers.

02:11.180 --> 02:11.500
Okay.

02:11.540 --> 02:17.460
And with that, as an introduction to this section, let me now tell you about this video.

02:18.020 --> 02:26.380
This is a code demo where I will show you how to replace percentages of neurons with the median value.

02:27.020 --> 02:35.620
So for example, we can find the top 10% of the MLP neurons in some layer that are most active, maybe

02:35.660 --> 02:40.380
for one token or maybe for all of the tokens in the sequence.

02:40.820 --> 02:48.660
And then we just replace those activations from those neurons with the median value from the population.

02:49.300 --> 02:57.180
And that means that all of the other neurons have their activation values preserved so unaffected in

02:57.180 --> 02:59.740
the little experiment that I will set up here.

02:59.860 --> 03:08.960
I will successively increase the rate of replacement from 10% to 90%, and you'll see a pretty remarkable

03:08.960 --> 03:18.520
observation that there isn't much difference between 10% and 90% of the replacements on the final model

03:18.520 --> 03:19.320
logits.

03:19.760 --> 03:23.280
And of course, we will discuss how to interpret that finding.

03:24.520 --> 03:24.840
Okay.

03:24.880 --> 03:27.560
So here's how it's going to go down in the demo.

03:27.720 --> 03:30.560
We're going to work with GPT two large.

03:30.840 --> 03:39.560
This is a nice model to work with here because it has over 5000 neurons in the expansion layer of MLP.

03:40.240 --> 03:47.120
So there is definitely no way that we are going to have a for loop over all 5000 of those individual

03:47.120 --> 03:52.560
neurons to explore the impact of manipulating that layer.

03:52.560 --> 03:56.600
I'm going to use this famous, really cliche text.

03:56.600 --> 03:59.760
It was a dark and stormy night.

04:00.280 --> 04:03.690
You can see the word night is missing here from the text.

04:03.690 --> 04:11.930
And then we are going to manipulate and then look at the output logits for the word stormy to see whether

04:12.210 --> 04:14.610
GPT predicts the word night.

04:15.370 --> 04:15.930
Okay.

04:15.970 --> 04:23.130
And I also have this other text here as a suggestion for you to continue exploring and see whether the

04:23.130 --> 04:31.410
findings that we observe, whether those are unique to this cheesy prose, or whether it seems to be

04:31.410 --> 04:35.970
a more general phenomenon that replicates in other text sequences.

04:36.730 --> 04:44.090
Now, when I run the forward pass on this text, I'm going to implant a hook function that simply extracts

04:44.090 --> 04:47.530
all of the MLP activations from one layer.

04:48.530 --> 04:52.050
And then we can make a histogram of those activations.

04:52.290 --> 04:53.890
This is from layer nine.

04:54.570 --> 05:01.260
Now the point here is just to explore the algorithm for replacing data with the median value.

05:01.900 --> 05:05.100
I'm not actually going to manipulate the data just yet.

05:05.660 --> 05:13.020
And yet, as you've seen earlier in the course, this distribution is shifted towards negative and kind

05:13.060 --> 05:16.620
of looks like there's two distributions superimposed.

05:16.820 --> 05:23.900
One is this Gaussian here and the other is maybe also Gaussian or maybe like a uniform distribution.

05:24.300 --> 05:29.580
Anyway, I'm not really going to investigate that possibility of multiple distributions here.

05:29.820 --> 05:38.980
But what I am going to do in this video is take 10% of the top most active neurons here, and replace

05:38.980 --> 05:43.740
their activation value with the median of the entire distribution.

05:44.340 --> 05:45.860
And that's what you see here.

05:45.860 --> 05:55.260
So the black squares show the histogram of the MLP activations from one token before replacing with

05:55.260 --> 05:56.020
the median.

05:56.140 --> 06:03.390
And then the blue distribution shows the histogram after I replaced the top 10% of the values with the

06:03.390 --> 06:03.950
median.

06:04.590 --> 06:10.790
So there's this sharp peak here, and that is the median of the original distribution.

06:11.150 --> 06:15.670
And then you can see this like deflation of the distribution over here.

06:16.150 --> 06:20.710
That corresponds to where the top 10% of the values were sitting.

06:20.990 --> 06:27.230
And now of course there's no more data here because all of these neurons had their activation values

06:27.230 --> 06:30.070
replaced with the median value up here.

06:30.710 --> 06:30.990
Okay.

06:31.030 --> 06:33.750
I still haven't done any manipulations yet.

06:33.790 --> 06:40.630
This is just exploration and demonstration of what this method is doing and how it works.

06:41.310 --> 06:48.550
Next, I want to see whether the clean model really does think that the token after the sequence ends

06:48.550 --> 06:50.430
should be the word night.

06:51.070 --> 06:53.550
And that is indeed what we see here.

06:53.550 --> 06:57.270
These are the final output logits from the clean model.

06:57.510 --> 07:00.730
And the largest token is for the word night.

07:01.090 --> 07:08.730
And you can also see that the log softmax value is very close to zero, which means the softmax probability

07:08.730 --> 07:10.650
will be very close to one.

07:11.890 --> 07:14.330
Okay, so now we are ready for the experiment.

07:14.330 --> 07:17.450
Here you see the code that runs the experiment.

07:17.850 --> 07:21.570
So I have this for loop here over all of the layers.

07:22.010 --> 07:30.570
And then within each layer I define this hook function that I will implant and then immediately remove.

07:30.890 --> 07:37.850
And inside this hook I grab all of the activations from the MLP expansion layer.

07:37.850 --> 07:41.170
You can see I'm implanting this hook into CFC.

07:41.210 --> 07:46.170
So the fully connected layer just from the final token.

07:46.170 --> 07:52.290
So here I'm only modifying the final token that would correspond to the word stormy.

07:52.930 --> 07:56.690
It could also be interesting to try manipulating the other tokens.

07:56.810 --> 08:03.180
Maybe the token for the word dark, but I'm going to stick with just the final token for this code demo.

08:03.780 --> 08:07.740
So then I calculate the median of all of those values.

08:07.940 --> 08:10.180
Find the top p percent.

08:10.420 --> 08:12.980
So that's this index variable here.

08:13.140 --> 08:14.020
Ripple rates.

08:14.020 --> 08:18.180
You can see that's this for loop indexing variable here.

08:18.700 --> 08:25.780
So this variable ripple rate or replacement rate varies between 1 and 9 inclusive.

08:26.220 --> 08:32.860
And then this little expression down here basically says that I am dividing the ripple rate by ten and

08:32.860 --> 08:36.580
multiplying it by the number of MLP neurons.

08:37.060 --> 08:44.020
And that gives me the percentage of the neurons corresponding to a given value of ripple rate.

08:44.340 --> 08:53.780
So all this means is that at different iterations of this inner for loop here I will find the indices

08:54.100 --> 09:03.670
for the top 10%, 20%, 30%, and so on up to 90% of the neurons, and then I replace them with the

09:03.670 --> 09:10.310
median value from the whole population of neurons, again only for the final token.

09:10.830 --> 09:17.190
And then I return that manipulated output so that it continues to be processed in the rest of the forward

09:17.190 --> 09:18.270
pass in the model.

09:18.950 --> 09:21.950
So here I do the actual forward pass.

09:22.230 --> 09:28.870
And then yeah, here I've removed the hook so that we can use a fresh instance of the hook in the next

09:28.870 --> 09:31.150
iteration in this for loop.

09:31.310 --> 09:37.870
And then down here I calculate the log softmax of the output logits for the final token.

09:38.190 --> 09:42.790
And then I grab the one index that corresponds to the target index.

09:42.790 --> 09:45.750
This was the word night okay.

09:45.790 --> 09:47.950
So that is the experiment code.

09:48.070 --> 09:54.230
Now these results are two dimensional because I've manipulated every layer.

09:55.230 --> 10:02.400
And within every layer I have nine different values corresponding to varying amounts of replacement.

10:03.000 --> 10:05.080
So here you see the results.

10:05.080 --> 10:08.280
The x axis is the transformer block.

10:08.400 --> 10:13.320
Remember there are 36 transformers in GPT two large.

10:13.960 --> 10:21.360
And the y axis here is the final output logit difference from the clean version of the model, where

10:21.360 --> 10:27.920
there was no manipulations versus the manipulated version of the model that you get from this for loop

10:27.920 --> 10:28.360
here.

10:28.800 --> 10:33.840
So a value of zero would mean absolutely no effect whatsoever.

10:34.240 --> 10:41.400
Negative values would mean that when I replaced the MLP neurons with their median value, then the final

10:41.400 --> 10:44.920
model prediction for the word night actually increased.

10:44.920 --> 10:52.440
So the model was more likely to say the word night and positive values here means that disrupting the

10:52.440 --> 10:57.800
MLP layer had a negative impact on the model's prediction for the word night.

10:58.760 --> 11:03.980
In other words, the words, the word the logit for the word night is higher in the clean version than

11:03.980 --> 11:05.660
in the manipulated version.

11:06.260 --> 11:10.020
Okay, now that is certainly an expected pattern of results.

11:10.380 --> 11:16.980
And then, yeah, I did nine different manipulations replacing between 10 and 90% of the neurons.

11:17.100 --> 11:21.740
And so therefore there are nine different circles within each layer.

11:22.620 --> 11:22.980
Okay.

11:23.020 --> 11:26.860
So there are several interesting observations here in this graph.

11:27.100 --> 11:31.020
And I'll have more to say about this when I produce the graph in code.

11:31.100 --> 11:38.220
But one of the striking findings that I want to draw your attention to before looking at the code is

11:38.220 --> 11:43.500
that there's very little variability with the different manipulations within each layer.

11:44.100 --> 11:48.820
In other words, each layer has some impact on the final logits.

11:48.820 --> 11:55.780
So some layers have a stronger manipulation impact, some less, some even going in the opposite direction.

11:55.980 --> 12:05.870
But within a layer, it doesn't seem to matter that much if I replace 10% or 90% of the neurons in that

12:05.870 --> 12:06.310
layer.

12:06.830 --> 12:08.070
That's quite surprising.

12:08.990 --> 12:15.910
I would like you to think about what that might mean and how you would interpret that finding as I work

12:15.910 --> 12:17.030
through the code file.

12:17.430 --> 12:22.070
Of course, I will have a discussion about this towards the end of the video, but I want you to come

12:22.070 --> 12:25.590
up with your own interpretation before you hear mine.

12:26.270 --> 12:27.470
Anyway, very nice.

12:27.470 --> 12:31.750
Let's now switch to code some libraries to import.

12:31.750 --> 12:37.150
Here I'm importing GPT two large and I'm putting it on the GPU.

12:37.830 --> 12:39.470
Okay, so let's see.

12:39.470 --> 12:44.510
Here are some convenience variables the number of layers and the number of neurons.

12:44.510 --> 12:50.270
This is like pretty arbitrary that I'm just picking uh transformer block uh three.

12:50.310 --> 12:51.230
Index three.

12:51.270 --> 12:52.990
Of course they're all exactly the same.

12:53.590 --> 12:53.950
Okay.

12:53.990 --> 12:58.910
So here I'm creating a hook that I will implant into the model.

12:58.910 --> 13:04.200
It goes into the fully connected layer of layer nine.

13:04.440 --> 13:06.600
And this is just arbitrary by the way.

13:06.600 --> 13:08.800
You can play around with this if you want later.

13:09.160 --> 13:16.520
And all I'm doing in this hook is grabbing the output from the first piece of text in this sequence,

13:16.520 --> 13:23.040
or the first sequence in this batch, and then detaching it and bringing it back from the GPU to the

13:23.040 --> 13:23.640
CPU.

13:24.080 --> 13:30.000
This is just for convenience here, because we're only working with one sentence here with one text

13:30.040 --> 13:30.920
sequence.

13:30.920 --> 13:35.360
So therefore yeah, this just makes the indexing a little bit easier later on.

13:35.840 --> 13:36.160
Okay.

13:36.200 --> 13:37.560
So I'll run this code.

13:37.800 --> 13:40.720
It was a dark and stormy night.

13:41.000 --> 13:41.480
Okay.

13:41.840 --> 13:44.600
Kind of cheesy but that's the famous thing.

13:45.000 --> 13:45.280
Okay.

13:45.320 --> 13:50.240
So now I'm running the tokens through the model and getting all the output data.

13:50.400 --> 13:51.520
Here I am.

13:51.560 --> 13:57.040
Actually, I'm not even using the hidden states here, so if we can leave that in or out, doesn't matter.

13:57.920 --> 14:02.250
Here I'm calculating the log softmax for the final token.

14:02.450 --> 14:10.450
And again the idea is that I'm going to analyze these log softmax values to see what the model thinks

14:10.450 --> 14:12.490
the next token should be.

14:12.770 --> 14:16.050
And hopefully it thinks that the next token should be night.

14:16.610 --> 14:16.970
Okay.

14:17.010 --> 14:18.810
Seven tokens in total.

14:18.930 --> 14:26.970
And there's 5120 neurons in the expansion part of the MLP layer okay.

14:27.010 --> 14:29.210
And then here is that distribution.

14:29.330 --> 14:35.130
And I said in the slides that I wasn't going to investigate this quirky thing that much.

14:35.170 --> 14:37.130
But you see there's like two peaks here.

14:37.250 --> 14:46.730
And so while I was discussing that, I had a suspicion that this is due to the first token in this sequence.

14:46.730 --> 14:49.290
As you know, the first token always behaves weird.

14:49.450 --> 14:53.970
So let's recreate this histogram excluding the first token.

14:54.130 --> 14:56.370
And we get an error message.

14:56.410 --> 14:59.710
Of course the indexing isn't with flatten, it's with MLP.

15:00.350 --> 15:05.310
And then, lo and behold, you see that this second piece disappears.

15:05.310 --> 15:10.510
So this weird thing over here is actually just due to the very first token.

15:11.750 --> 15:12.110
Okay.

15:12.150 --> 15:15.670
So let's explore median based replacement.

15:16.150 --> 15:19.550
So here what I'm doing is just grabbing the final token.

15:19.710 --> 15:21.310
All of the neurons.

15:21.430 --> 15:27.270
So all the activations just from the final token calling this variable T for token or temp or something

15:27.310 --> 15:27.830
I don't know.

15:28.310 --> 15:31.510
Uh, finding the median of those values.

15:31.710 --> 15:35.430
And then here I'm getting 0.1 times the number of neurons.

15:35.430 --> 15:38.190
So this is 10% of the neurons.

15:38.190 --> 15:44.150
And then I'm just converting it to an integer just to make sure that this is really a whole number and

15:44.150 --> 15:45.470
not a floating point number.

15:46.070 --> 15:49.110
Uh, torch top-k from these neurons.

15:49.110 --> 15:53.630
And then I get these indices and then I just replace them.

15:53.670 --> 16:02.000
So that means that the 10% of the most active neurons will have their activation values replaced with

16:02.000 --> 16:04.040
the median of the whole distribution.

16:04.400 --> 16:11.880
And then I'm just making histograms on that shows exactly what you saw in the slides with the little

16:11.880 --> 16:13.080
x axis shift here.

16:13.080 --> 16:20.120
So the values the x axis values are actually exactly the same for these two distributions I just add

16:20.120 --> 16:25.840
and subtract a tiny tiny x axis offset just to make it a little bit more visible.

16:26.760 --> 16:27.040
Okay.

16:27.080 --> 16:34.280
So again the idea is that I am taking the large activation values and manipulating them to replace them

16:34.280 --> 16:37.000
with the median of the distribution.

16:37.600 --> 16:38.120
Okay.

16:38.160 --> 16:43.800
So this code here just generates the output logits that I also showed.

16:43.800 --> 16:47.240
This probably should go earlier in the code but it doesn't matter okay.

16:47.280 --> 16:51.760
So this is for the token corresponding to the target which is night.

16:51.760 --> 16:53.560
You can see it's really high up there.

16:53.680 --> 17:02.250
It has a log softmax value close to zero which means when we exponentiate this, then uh, e to the

17:02.250 --> 17:03.650
zero is one.

17:03.650 --> 17:08.970
So the probability of the model picking this token is almost 100%.

17:09.010 --> 17:12.210
It's going to be extremely close to 100% okay.

17:12.250 --> 17:15.770
And that is indeed what the model thinks the next token should be.

17:16.250 --> 17:16.570
Okay.

17:16.610 --> 17:18.970
So now we are ready for the experiment.

17:19.330 --> 17:21.370
A little bit of hard coding in here.

17:21.370 --> 17:24.290
I suppose this could be better uh soft coded.

17:24.290 --> 17:26.290
But anyway, this is how I coded it up.

17:27.250 --> 17:32.250
Loop over all the layers, and then I'm looping over the, uh, ripple rate.

17:32.250 --> 17:40.530
So the replacement rate and the idea is that I am going to take this integer, divide it by ten, and

17:40.530 --> 17:45.650
then use that same multiplication and division that I showed earlier.

17:45.890 --> 17:48.690
So this code is what you saw above.

17:48.690 --> 17:55.250
And I'm just putting it into this hook function which I then implant into MLP.

17:55.530 --> 18:00.300
Fully connected this particular layer Run a forward pass.

18:00.340 --> 18:07.860
Delete that hook function from the model and then softmax the output logits again, only working with

18:07.860 --> 18:09.460
the final token here as well.

18:09.500 --> 18:11.380
I'm only working with the final token.

18:11.740 --> 18:20.140
And then all the way out here I get the log softmax value for the target index for the word night.

18:20.500 --> 18:20.820
Okay.

18:20.860 --> 18:22.380
So we can run through that.

18:22.420 --> 18:26.940
This is pretty fast to run in GPT or on the GPU.

18:27.660 --> 18:28.060
Okay.

18:28.100 --> 18:30.540
So well I guess it still takes several seconds.

18:30.540 --> 18:32.580
And here's the code where I'm generating the figure.

18:32.580 --> 18:35.260
So I loop over all of the different layers.

18:35.420 --> 18:44.700
And for each layer I just make a plot of all of those log softmax values here for this layer, all nine

18:44.740 --> 18:46.460
of the manipulations.

18:46.700 --> 18:53.060
And then I'm subtracting that from the clean version of the logit for the same target.

18:53.060 --> 19:00.080
And this is basically our baseline comparison value so that we can interpret the data more meaningfully.

19:00.480 --> 19:04.000
Okay, now it looks like on layer zero you only see one dot.

19:04.000 --> 19:06.920
So something interesting is happening there.

19:07.240 --> 19:09.840
The full plot actually looks like this.

19:10.040 --> 19:18.920
So this manipulation has a gigantic effect when it is implanted into the very, very, very first layer,

19:18.960 --> 19:21.000
the very first transformer block.

19:21.040 --> 19:22.440
It's really catastrophic.

19:23.240 --> 19:25.440
So here there's something happening.

19:25.480 --> 19:31.640
And right in the very beginning, when the transformer sequence first takes the embeddings from the

19:31.640 --> 19:37.120
embeddings layer, and there's just something really important and the model just gets really confused

19:37.120 --> 19:37.440
here.

19:37.440 --> 19:44.320
So would be interesting to look into this in a little more detail and see, for example, what is the

19:44.320 --> 19:49.240
actual token that the model wants to get for this run here.

19:49.800 --> 19:51.560
But just for uh, yeah.

19:51.600 --> 19:59.130
Just in the interest of focusing the analyses on the broader patterns, I'm going to fix the y axis

19:59.130 --> 20:03.170
limit, so that we're actually just ignoring a lot of those data up there.

20:03.770 --> 20:04.170
Okay.

20:04.210 --> 20:05.930
So a couple of things you notice.

20:05.930 --> 20:13.690
One is that, uh, across all of the layers, more often than not the values are positive, which means

20:13.690 --> 20:21.930
that, uh, replacing the MLP neuron activation values with the median has a negative impact.

20:21.930 --> 20:29.970
So it's lowering the log, uh, softmax value for the token that we are looking for the word night,

20:30.010 --> 20:31.770
but not every time.

20:31.770 --> 20:39.450
It also happens that there are layers where interfering with the MLP actually boosts the, uh, target

20:39.730 --> 20:40.850
token logit.

20:41.130 --> 20:45.410
Now, this sort of effect, I would be reluctant to overinterpret it.

20:45.410 --> 20:49.770
I would want to see that reproduced in more, uh, text.

20:49.770 --> 20:56.690
So, for example, is there something special about, uh, layer 32 that it's always producing a negative

20:56.690 --> 20:59.980
value, or do you see that across different texts.

20:59.980 --> 21:02.820
Sometimes it's negative and sometimes it's positive and so on.

21:03.260 --> 21:11.980
But I find it really striking that there's very little difference between knocking out 10% and knocking

21:11.980 --> 21:15.900
out 90% of the transformer blocks.

21:15.940 --> 21:21.340
There's very little variability across these different manipulations within a given layer.

21:21.700 --> 21:26.140
And so to investigate that a little bit more I'm going to do is add another zero here.

21:26.500 --> 21:33.780
So that means that now instead of varying how much how many neurons we uh, we replace with the median

21:33.780 --> 21:37.980
value between 10% and 90% of the population.

21:38.180 --> 21:43.180
Instead, I'm going to replace between 1% and 9%.

21:43.420 --> 21:50.900
So now when I rerun this experiment, every single manipulation is smaller than the smallest manipulation

21:51.060 --> 21:52.340
that we see here.

21:52.460 --> 21:57.380
So only manipulating between 1% and 9%.

21:57.780 --> 22:01.190
And now we actually do start to see more variability.

22:01.190 --> 22:03.070
And you could, you know, keep running this.

22:03.110 --> 22:05.990
You could make this be like 500%.

22:06.110 --> 22:07.070
Let's just try that.

22:07.710 --> 22:09.030
Sorry not 500%.

22:09.070 --> 22:10.830
Dividing by five hundredths.

22:10.830 --> 22:13.950
So now it's varying between 0.2%.

22:13.950 --> 22:19.390
And whatever nine divided by 500 is 4.5% I guess.

22:19.430 --> 22:22.310
And now again we are seeing more variability.

22:22.310 --> 22:23.510
And what does this tell us?

22:23.510 --> 22:30.790
This tells us that the more we focus in on a very, very, very small number of neurons, the more we

22:30.830 --> 22:34.590
find that we can get more variability in these results.

22:35.430 --> 22:36.510
And what does that mean?

22:36.590 --> 22:45.350
That means that that most of the information that is projected by the MLP layer going forwards to adjust

22:45.350 --> 22:51.990
the embeddings vector is carried by a really tiny number, just a handful of neurons, maybe somewhere

22:51.990 --> 22:58.150
around 1% or 2 or 3% of the neurons are really carrying most of the information.

22:58.530 --> 23:02.050
All the rest of the neurons are just not doing a whole lot.

23:02.090 --> 23:09.690
They're not making a really major contribution that we can actually measure at the final end of the

23:09.690 --> 23:10.290
model.

23:10.730 --> 23:16.770
Of course, that conclusion is just based on this one sample text that I used.

23:16.930 --> 23:22.970
So you could certainly try reproducing this pattern of results with more text.

23:22.970 --> 23:30.250
But in general, this is a fairly typical finding that for a given, uh, token of relatively small

23:30.250 --> 23:40.250
number of MLP neurons carry a lot of information, the dimensionality explosion problem is present basically

23:40.250 --> 23:44.770
everywhere in mechanistic interpretability of large language models.

23:45.170 --> 23:52.330
But it's really the worst in the MLP expansion layer because there's just so many neurons in there.

23:52.690 --> 23:57.530
So you really need some way to select neurons for manipulation.

23:58.050 --> 24:05.700
What I showed you in this video is a descriptive, characteristic based approach, and in the next video

24:05.700 --> 24:10.540
for contrast, I will show you an inferential statistical approach.

24:11.460 --> 24:20.180
Also quite striking that we get basically the same effect for 10% versus 90% of neuron ablations.

24:20.420 --> 24:27.100
And we really have to get down to the single digit percents before we start seeing some variability.

24:27.660 --> 24:34.940
So that is consistent with the idea that a very small number of MLP neurons are carrying most of the

24:34.940 --> 24:37.260
information for a given token.

24:37.540 --> 24:43.220
Of course, you have to keep in mind that these neurons are not processing the actual embeddings vectors

24:43.220 --> 24:44.020
themselves.

24:44.500 --> 24:51.460
They are processing the adjustments to the embeddings vectors that will be added back onto the embeddings

24:51.620 --> 24:57.580
after this layer gets contracted, and then summed back onto the residual stream.

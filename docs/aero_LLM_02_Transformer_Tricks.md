# Lecture 2: Transformer Tricks & BERT ๐๏ธ

> **Tรณm tแบฏt tแปซ khรณa hแปc Stanford CME 295: Transformers & Large Language Models.**
> Bรi giแบฃng nรy ฤi sรขu vรo cรกc cแบฃi tiแบฟn kแปน thuแบญt giรบp Transformer hoแบกt ฤแปng tแปt hฦกn vร sแปฑ ra ฤแปi cแปงa cรกc mรด hรฌnh Encoder-only nhฦฐ BERT.

---

## ๐ Mแปฅc Lแปฅc
1. [Cแบฃi tiแบฟn Positional Embeddings (RoPE, ALiBi)](#1-cแบฃi-tiแบฟn-positional-embeddings)
2. [Cแบฃi tiแบฟn Normalization (LayerNorm vs RMSNorm)](#2-cแบฃi-tiแบฟn-normalization)
3. [Tแปi ฦฐu Attention (Sliding Window, GQA)](#3-tแปi-ฦฐu-attention)
4. [Cรกc hแป mรด hรฌnh Transformer](#4-cรกc-hแป-mรด-hรฌnh-transformer)
5. [BERT & Encoder-only Models](#5-bert--encoder-only-models)

---

## 1. Cแบฃi tiแบฟn Positional Embeddings
Trong bรi bรกo gแปc, Positional Encoding ฤฦฐแปฃc cแปng trแปฑc tiแบฟp vรo Input Embedding. Tuy nhiรชn, cรกc mรด hรฌnh hiแปn ฤแบกi sแปญ dแปฅng cรกc phฦฐฦกng phรกp tiรชn tiแบฟn hฦกn ฤแป xแปญ lรฝ tแปt hฦกn ฤแป dรi chuแปi thay ฤแปi.

### Learned Positional Embedding
*   Hแปc mแปt vector riรชng cho mแปi vแป trรญ.
*   **Hแบกn chแบฟ:** Khรดng thแป mแป rแปng (extrapolate) cho cรกc chuแปi dรi hฦกn ฤแป dรi ฤรฃ thแบฅy trong khi huแบฅn luyแปn.

### Rotary Positional Embedding (RoPE) ๐
*   **Hiแปn ฤแบกi nhแบฅt:** ฤฦฐแปฃc sแปญ dแปฅng trong Llama, Mistral, PaLM.
*   **Cฦก chแบฟ:** Thay vรฌ cแปng vector vแป trรญ, RoPE **xoay** vector Query vร Key mแปt gรณc phแปฅ thuแปc vรo vแป trรญ cแปงa chรบng.
*   **ฦฏu ฤiแปm:**
    *   Mรด hรฌnh hแปc ฤฦฐแปฃc **khoแบฃng cรกch tฦฐฦกng ฤแปi** (relative distance) giแปฏa cรกc tแปซ mแปt cรกch tแปฑ nhiรชn thรดng qua tรญch vรด hฦฐแปng (dot product).
    *   Khแบฃ nฤng mแป rแปng (extrapolation) tแปt hฦกn cho cรกc chuแปi dรi.

### ALiBi (Attention with Linear Biases)
*   Thรชm mแปt bias tฤฉnh vรo ma trแบญn Attention score dแปฑa trรชn khoแบฃng cรกch giแปฏa hai token.
*   ฤฦกn giแบฃn, khรดng cแบงn hแปc tham sแป, nhฦฐng RoPE hiแปn nay phแป biแบฟn hฦกn.

---

## 2. Cแบฃi tiแบฟn Normalization
Chuแบฉn hรณa (Normalization) giรบp mรด hรฌnh hแปi tแปฅ nhanh vร แปn ฤแปnh hฦกn.

*   **Post-Norm (Gแปc):** Norm sau khi cแปng nhรกnh dฦฐ (Residual).
*   **Pre-Norm (Hiแปn ฤแบกi):** Norm **trฦฐแปc** khi vรo Attention/FFN. Giรบp huแบฅn luyแปn แปn ฤแปnh hฦกn vแปi cรกc mรด hรฌnh rแบฅt sรขu.
*   **RMSNorm (Root Mean Square Norm):** Mแปt biแบฟn thแป cแปงa LayerNorm, bแป qua viแปc trแปซ giรก trแป trung bรฌnh (mean), chแป chia cho cฤn bแบญc hai trung bรฌnh bรฌnh phฦฐฦกng.
    *   *ฦฏu ฤiแปm:* Tรญnh toรกn nhanh hฦกn, hiแปu quแบฃ tฦฐฦกng ฤฦฐฦกng. ฤฦฐแปฃc dรนng trong Llama, Gopher.

---

## 3. Tแปi ฦฐu Attention
Self-Attention cรณ ฤแป phแปฉc tแบกp $O(N^2)$ (vแปi N lร ฤแป dรi chuแปi), rแบฅt tแปn kรฉm khi chuแปi dรi.

*   **Sliding Window Attention (Cแปญa sแป trฦฐแปฃt):** Mแปi token chแป nhรฌn thแบฅy cรกc token lรขn cแบญn trong mแปt cแปญa sแป nhแบฅt ฤแปnh (vรญ dแปฅ: Mistral). Giแบฃm chi phรญ tรญnh toรกn nhฦฐng vแบซn giแปฏ ฤฦฐแปฃc khแบฃ nฤng hiแปu ngแปฏ cแบฃnh nhแป cรกc lแปp chแปng lรชn nhau (tฦฐฦกng tแปฑ Receptive field trong CNN).
*   **Grouped Query Attention (GQA):**
    *   *Multi-Head Attention (MHA):* Mแปi Head cรณ Q, K, V riรชng. (Tแปn bแป nhแป KV Cache).
    *   *Multi-Query Attention (MQA):* Tแบฅt cแบฃ Heads chia sแบป chung 1 bแป K, V. (Tiแบฟt kiแปm nhแป, giแบฃm chแบฅt lฦฐแปฃng).
    *   *GQA:* Trung hรฒa. Chia Heads thรnh cรกc nhรณm, mแปi nhรณm chia sแบป chung K, V. (Cรขn bแบฑng tแปt nhแบฅt giแปฏa tแปc ฤแป vร chแบฅt lฦฐแปฃng, dรนng trong Llama-2-70b, Llama-3).

---

## 4. Cรกc hแป mรด hรฌnh Transformer
Dแปฑa trรชn kiแบฟn trรบc, cรณ 3 nhรกnh phรกt triแปn chรญnh:

1.  **Encoder-Decoder (T5, BART):** Giแปi cรกc tรกc vแปฅ "Text-to-Text" nhฦฐ dแปch thuแบญt, tรณm tแบฏt.
2.  **Encoder-only (BERT, RoBERTa):** Chแป dรนng phแบงn Encoder. Giแปi cรกc tรกc vแปฅ "Hiแปu ngรดn ngแปฏ" (NLU) nhฦฐ phรขn loแบกi, tรฌm kiแบฟm, NER.
3.  **Decoder-only (GPT, Llama):** Chแป dรนng phแบงn Decoder. Giแปi cรกc tรกc vแปฅ "Sinh ngรดn ngแปฏ" (Generative). ฤรขy lร nhรกnh thแปng trแป hiแปn nay cแปงa LLMs.

---

## 5. BERT & Encoder-only Models
**BERT (Bidirectional Encoder Representations from Transformers)** lร tฦฐแปฃng ฤรi cแปงa dรฒng Encoder-only.

### ฤแบทc ฤiแปm:
*   **Bidirectional (Hai chiแปu):** Mแปi tแปซ nhรฌn thแบฅy toรn bแป cรขu (trรกi vร phแบฃi) cรนng lรบc.
*   **Tokens ฤแบทc biแปt:** `[CLS]` (ฤแบกi diแปn cho toรn cรขu, dรนng ฤแป phรขn loแบกi), `[SEP]` (ngฤn cรกch cรขu).

### Quรก trรฌnh Huแบฅn luyแปn (Pre-training)
BERT ฤฦฐแปฃc huแบฅn luyแปn vแปi 2 tรกc vแปฅ tแปฑ giรกm sรกt (Self-supervised):
1.  **Masked Language Modeling (MLM):** แบจn ฤi 15% sแป tแปซ trong cรขu, yรชu cแบงu mรด hรฌnh ฤiแปn vรo chแป trแปng. (Giรบp mรด hรฌnh hแปc ngแปฏ cแบฃnh hai chiแปu).
2.  **Next Sentence Prediction (NSP):** Cho 2 cรขu A vร B, hแปi B cรณ phแบฃi lร cรขu tiแบฟp theo cแปงa A khรดng? (Giรบp mรด hรฌnh hiแปu mแปi quan hแป giแปฏa cรกc cรขu).

### Fine-tuning (Tinh chแปnh)
Sau khi Pre-training, BERT tแบกo ra cรกc vector embedding rแบฅt tแปt. Ta chแป cแบงn gแบฏn thรชm mแปt lแปp Linear nhแป phรญa sau ฤแป giแบฃi quyแบฟt cรกc bรi toรกn cแปฅ thแป (Sentiment Analysis, Question Answering) vแปi rแบฅt รญt dแปฏ liแปu.

### Biแบฟn thแป
*   **DistilBERT:** Dรนng kแปน thuแบญt *Distillation* (Chฦฐng cแบฅt) ฤแป tแบกo mรด hรฌnh nhแป hฦกn, nhanh hฦกn nhฦฐng giแปฏ ฤฦฐแปฃc 97% hiแปu nฤng cแปงa BERT.
*   **RoBERTa:** Tแปi ฦฐu hรณa BERT (bแป NSP, train lรขu hฦกn, dแปฏ liแปu nhiแปu hฦกn) -> Hiแปu nฤng tแปt hฦกn.

---
*Biรชn soแบกn bแปi Pixiboss - Dแปฑa trรชn Stanford CME 295.*

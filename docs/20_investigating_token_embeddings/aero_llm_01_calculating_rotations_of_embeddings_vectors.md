
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [20 investigating token embeddings](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# PhÃ¢n Kháº£o Token Embeddings: Äo LÆ°á»ng GÃ³c Quay Cá»§a Vector Biá»ƒu Diá»…n

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y Ä‘Ã¡nh dáº¥u sá»± dá»‹ch chuyá»ƒn trá»ng tÃ¢m nghiÃªn cá»©u tá»« cÆ¡ cháº¿ phÃ¢n bá»• tÃ­n hiá»‡u toÃ n táº§ng (layer-wise) sang viá»‡c theo dÃµi hÃ nh trÃ¬nh tiáº¿n hoÃ¡ cá»§a tá»«ng cÃ¡ thá»ƒ vector nhÃºng (embeddings vectors). Báº±ng há»‡ thá»‘ng toÃ¡n há»c biáº¿n Ä‘á»•i tá»« *Äá»™ tÆ°Æ¡ng Ä‘á»“ng Cosine* (Cosine Similarity) sang *GÃ³c quay* (Angle of rotation) Ä‘o báº±ng hÃ m lÆ°á»£ng giÃ¡c chuáº©n nghá»‹ch Ä‘áº£o $\arccos$, nghiÃªn cá»©u Ä‘á»‹nh lÆ°á»£ng má»©c Ä‘á»™ Ä‘iá»u chá»‰nh ngá»¯ cáº£nh (context modulation) mÃ  cÃ¡c khá»‘i Transformer Ã¡p Ä‘áº·t lÃªn cÃ¡c Token qua má»—i cháº·ng. Thá»±c nghiá»‡m trÃªn GPT-2 XL váº¡ch ra má»™t Ä‘á»“ thá»‹ biÃªn Ä‘á»™ gÃ³c sáº¯c nÃ©t: Khá»Ÿi Ä‘á»™ng vá»›i cÃ¡c vÃ²ng xoay khá»•ng lá»“ (lÃªn tá»›i $90^\circ$ táº¡i Transformer Ä‘áº§u tiÃªn), rá»“i nguá»™i dáº§n á»•n Ä‘á»‹nh vá»›i nhá»¯ng bÆ°á»›c xoay nhá» á»Ÿ cÃ¡c táº§ng trung gian, vÃ  bÃ¹ng ná»• trá»Ÿ láº¡i á»Ÿ khÃ¢u giáº£i mÃ£ sÃ¡t vÃ¡ch lá»‘i ra.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Trong má»™t khÃ´ng gian áº©n Ä‘a chiá»u (High-dimensional space), má»—i tá»« vá»±ng khá»Ÿi thá»§y (vÃ­ dá»¥: "cat" hay "her") Ä‘Æ°á»£c cáº¥p phÃ¡t má»™t vector tÄ©nh tá»« Ma tráº­n Token Embeddings. Theo lÃ½ thuyáº¿t ná»n táº£ng cá»§a Transformer, lÃºc chÆ°a Ä‘Æ°á»£c náº¡p bá»‘i cáº£nh, cÃ¡c vector nÃ y hoÃ n toÃ n cÃ´ láº­p. 
Tuy nhiÃªn, khi mÅ©i tÃªn vector ráº½ nÆ°á»›c Ä‘i sÃ¢u vÃ o cÃ¡c lá»›p mÃ´ hÃ¬nh (Transformer Blocks), cÆ¡ cháº¿ bá»™ chÃº Ã½ (Attention) vÃ  máº¡ng lan truyá»n tiáº¿n (MLP) "lÃ´i kÃ©o" vector nÃ y lá»‡ch khá»i trá»¥c hoÃ nh tháº³ng Ä‘á»©ng ban Ä‘áº§u. QuÃ¡ trÃ¬nh báº» gÃ³c thay Ä‘á»•i phÆ°Æ¡ng hÆ°á»›ng Ä‘Æ°á»£c Ä‘iá»u phá»‘i hoÃ n toÃ n dá»±a trÃªn sá»± hiá»‡n diá»‡n cá»§a cÃ¡c token Ä‘á»©ng cáº¡nh nhau (Surrounding Context). BÃ i bÃ¡o cÃ¡o nÃ y táº­p trung lÆ°á»£ng hÃ³a nhá»¯ng "cÃº quay xe" nÃ y tá»« khá»‘i kiáº¿n trÃºc hiá»‡n táº¡i so vá»›i khá»‘i kiáº¿n trÃºc á»Ÿ táº§ng ngay trÆ°á»›c nÃ³.

---

## 2. Ná»n Táº£ng ToÃ¡n Há»c & Tiá»n Xá»­ LÃ½ (Methodology)

### 2.1. PhÆ°Æ¡ng TrÃ¬nh Chuyá»ƒn Trá»¥c (Rotation Formula)
Há»‡ sá»‘ Ä‘á»™ Ä‘o khoáº£ng cÃ¡ch quen thuá»™c lÃ  Äá»™ tÆ°Æ¡ng Ä‘á»“ng Cosine (Cosine Similarity). Tuy nhiÃªn, Ä‘á»ƒ tÃ­nh Ä‘Æ°á»£c cÆ°á»ng Ä‘á»™ chá»‡ch phÆ°Æ¡ng báº±ng hÃ¬nh há»c rÃµ rÃ ng, khÃ¡i niá»‡m *GÃ³c (Angle)* Ä‘Æ°á»£c giá»›i thiá»‡u.
Sá»­ dá»¥ng cÃ´ng thá»©c tÃ­ch vÃ´ hÆ°á»›ng vÃ  Ä‘á»™ dÃ i Norm vector:

$$

$$

\cos(\theta) = \frac{\langle x, y \rangle}{\|x\| \cdot \|y\|}

$$

$$

Ta cÃ´ láº­p há»‡ sá»‘ gÃ³c $\theta$ báº±ng hÃ m lÆ°á»£ng giÃ¡c ngÆ°á»£c (Arc Cosine):

$$

$$

\theta = \arccos\left(\frac{\langle x, y \rangle}{\|x\| \cdot \|y\|}\right)

$$

$$

Káº¿t quáº£ ra Ä‘Æ¡n vá»‹ Radians, Ä‘Æ°á»£c nhÃ¢n vá»›i tá»· sá»‘ $180 / \pi$ Ä‘á»ƒ tráº£ vá» Ä‘Æ¡n vá»‹ Ä‘á»™ (Degrees). Äiá»ƒm Æ°u viá»‡t cá»§a gÃ³c quay lÃ  loáº¡i bá» nhiá»…u Ä‘áº¿n tá»« Ä‘á»™ giÃ£n ná»Ÿ (Vector Lengths) cá»§a cÃ¡c Activation Norms.

### 2.2. Dá»¯ Liá»‡u Input (Targeted Setup)
Trá»ng sá»‘ há»c táº­p: `gpt2-xl` (MÃ´ hÃ¬nh tá»· Ä‘Ã´ vá»›i 48 Transformer blocks).
Nguá»“n má»“i (Prompt corpus): Sá»­ dá»¥ng bá»™ sÆ°u táº­p 54 cÃ¢u (cung cáº¥p bá»Ÿi Claude AI), táº¥t cáº£ Ä‘á»u chá»©a chung má»™t tá»« khÃ³a Ä‘áº¡i tá»« nhÃ¢n xÆ°ng lÃ  `"her"`.
Bá»Ÿi vÃ¬ chiá»u dÃ i cÃ¡c cÃ¢u vÄƒn (lengths) lá»‡ch nhau, Padding ID (chÃ­nh lÃ  EOS token) Ä‘Æ°á»£c Ã¡p dá»¥ng táº¡i khÃ¢u Batch Tokenization Ä‘á»ƒ Ä‘iá»n vÃ o nhá»¯ng khoáº£ng trá»‘ng dÆ° thá»«a, táº¡o thÃ nh há»‡ tensor mÆ°á»£t mÃ  cÃ³ sá»©c chá»©a dÃ i báº±ng cÃ¢u dÃ i nháº¥t. 

CÃ¡c phÃ©p Ä‘o chÃ©o Ä‘Æ°á»£c tiáº¿n hÃ nh dá»±a trÃªn:
1. **Target Word:** Vector Ä‘áº¡i diá»‡n cho chá»¯ `"her"`.
2. **Non-Target Word:** Vector Ä‘áº¡i diá»‡n cho tá»« Ä‘á»©ng ngay Ä‘áº±ng trÆ°á»›c chá»¯ `"her"` (vÃ­ dá»¥: "introduced", "married").
3. **Random Control:** Vector xÃ¡o trá»™n báº¯t cáº·p ngáº«u nhiÃªn giá»¯a báº¥t cá»© vá»‹ trÃ­ vÃ  báº¥t cá»© Layer tÃ¹y Ã½ nÃ o.

---

## 3. KhÃ¡m PhÃ¡ Trá»±c Quan (Analysis & Visualizations)

### 3.1. PhÃ¢n TÃ­ch Äá»™ Lá»‡ch Ngáº«u NhiÃªn (Randomly Shuffled Angles)
Vá»›i cÃ¡c vector khÃ´ng cÃ³ Ä‘á»‹nh hÆ°á»›ng liÃªn káº¿t, má»©c chÃªnh lá»‡ch gÃ³c dao Ä‘á»™ng xoay quanh $70^\circ \to 80^\circ$. ÄÃ¡ng láº½ nÃ³ pháº£i sÃ¡t má»‘c cá»± tuyá»‡t $90^\circ$ (Orthogonal - vuÃ´ng gÃ³c hoÃ n toÃ n). Má»©c há»¥t nÃ y chá»©ng minh phÃ¡t hiá»‡n á»Ÿ pháº§n Ä‘áº§u khoÃ¡ há»c: Tokenizer GPT luÃ´n bá»‹ káº¹p má»™t biÃªn Ä‘á»™ thiÃªn kiáº¿n Ä‘á»™ chá»‡ch (Bias) cho phÃ©p chá»«ng má»±c cÃ¡c Cá»±c lÃ¢n cáº­n tá»« vá»±ng mang chá»‰ sá»‘ dÆ°Æ¡ng vá»›i nhau.

### 3.2. CÃº "Quay Xe" Lá»‹ch Sá»­ Táº¡i Táº§ng NÃ´ng Thiáº¿t Láº­p (Early Blocks)
VÃ o thá»i Ä‘iá»ƒm náº¡p vector gá»‘c thoÃ¡t khá»i Embedding Matrix tiáº¿n vÃ o Khá»‘i Transformer Sá»‘ 1, vector cá»§a "her" vÃ  má»i tá»« khÃ¡c nháº­n má»™t cÃº ráº½ cá»±c máº¡nh, gáº§n nhÆ° quay ngáº¯t gÃ³c $90^\circ$ hoÃ n chá»‰nh. 
**Giáº£i mÃ£ cÆ¡ há»c:** Khá»‘i Transformer vÃ²ng 1 pháº£i gá»“ng gÃ¡nh luá»“ng dung náº¡p vÄƒn cáº£nh nguyÃªn sinh khá»•ng lá»“, trá»™n láº«n Positional Encoding vÃ o Ã½ nghÄ©a vÄƒn pháº¡m thÃ´ (Zero context modulation).

### 3.3. DÃ²ng Cháº£y Thay Äá»•i Tiá»‡m Cáº­n Khá»‘i Äáº¡i Tá»« (Pronoun Binding Constraints)
Tiáº¿n hÃ nh ná»™i suy t-Test cho má»©c xoay cá»§a Ä‘áº¡i tá»« `"her"` so vá»›i cÃ¡c Äá»™ng tá»« Non-Target liá»n ká», ta tháº¥y tá»« Khá»‘i giá»¯a Ä‘áº¿n Khá»‘i cuá»‘i: 
- Tá»« bÃ¬nh thÆ°á»ng chá»¯ng láº¡i, chá»‰ quay lá»‡ch táº§m $10^\circ$. 
- Äi sÃ¢u xuá»‘ng Layer 30+, má»©c xoay cá»§a "her" (Target - $\approx 15^\circ$) trá»™i hÆ¡n háº³n so vá»›i khá»‘i Non-Target (Äá»™ng tá»«). LÃ½ giáº£i: CÃ¡c chá»¯ biá»ƒu thá»‹ ngÃ´i xÆ°ng ("her") bá»‹ lá»‡ thuá»™c náº·ng ná» vÃ o cÃ¡c cáº¥u trÃºc liÃªn Ä‘á»›i vÄ© mÃ´ (Co-reference bindings) á»Ÿ má»‡nh Ä‘á» phÃ­a sau. LLM báº¯t buá»™c pháº£i sá»­a vector cá»§a Ä‘áº¡i tá»« liÃªn tá»¥c Ä‘á»ƒ náº¯n khá»›p ngá»¯ cáº£nh phá»©c táº¡p cá»§a cÃ¡c nhÃ¢n váº­t thay vÃ¬ giá»¯ nÃ³ cá»‘ Ä‘á»‹nh.

---

## 4. Káº¿t Luáº­n
Viá»‡c á»©ng dá»¥ng toÃ¡n há»c hÃ¬nh chiáº¿u (Arc Cosine) biáº¿n viá»‡c theo dÃµi tÃ­nh trá»«u tÆ°á»£ng cá»§a Transformers thÃ nh viá»‡c cÃ¢n chá»‰nh vÃ²ng xoay kim la bÃ n ráº¥t dá»… diá»…n giáº£i. ThÃ´ng qua Ä‘á»™ xoay, ta báº¯t quáº£ tang LLM chá»‰ tá»‘n duy nháº¥t 1 Layer Ä‘áº§u Ä‘á»ƒ nhá»“i nhÃ©t khÃ¡i niá»‡m cÃº phÃ¡p cá»¥c bá»™, vÃ  dÃ nh Ä‘áº¿n hÃ ng chá»¥c Layers cuá»‘i Ä‘á»ƒ Ä‘iá»u hÆ°á»›ng vÃ  tinh chá»‰nh cÃ¡c khÃ¡i niá»‡m Ä‘a tham chiáº¿u nhÆ° danh Ä‘áº¡i tá»«. 

---

## TÃ i Liá»‡u Tham Kháº£o (Citations)
1. ThÃ­ nghiá»‡m Ä‘o Ä‘áº¡c tá»« thÆ° viá»‡n lá»‡nh ngáº§m táº¡i `aero_LLM_01_Calculating rotations of embeddings vectors.md` (Triá»ƒn khai cÃ´ng thá»©c toÃ¡n Arc Cosine cho Embeddings vÃ  thá»‘ng kÃª Independent T-test).
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| ğŸ“Œ **[PhÃ¢n Kháº£o Token Embeddings: Äo LÆ°á»ng GÃ³c Quay Cá»§a Vector Biá»ƒu Diá»…n](aero_llm_01_calculating_rotations_of_embeddings_vectors.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_01_calculating_rotations_of_embeddings_vectors.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): Tiáº¿n HÃ³a Äa Táº§ng Cá»§a CÃ¡c Äiá»u Chá»‰nh GÃ³c Quay Tuáº§n Tá»±](aero_llm_02_codechallenge_laminar_evolution_of_sequential_angular_adjustments.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_codechallenge_laminar_evolution_of_sequential_angular_adjustments.md) |
| [Äo LÆ°á»ng Äá»™ DÃ i ÄÆ°á»ng Dáº«n (Path Length) Sá»± TÆ°Æ¡ng Quan Vá»›i Dá»± ÄoÃ¡n Token](aero_llm_03_path_length_and_logit_token_prediction.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_path_length_and_logit_token_prediction.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: PhÃ¢n RÃ£ Äá»™ DÃ i ÄÆ°á»ng Dáº«n Luá»“ng Sá»‘ DÆ° (Pháº§n 1)](aero_llm_04_codechallenge_residual_stream_path_length_decomposition_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_residual_stream_path_length_decomposition_part_1_.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: PhÃ¢n RÃ£ Äá»™ DÃ i ÄÆ°á»ng Dáº«n Luá»“ng Sá»‘ DÆ° (Pháº§n 2)](aero_llm_05_codechallenge_residual_stream_path_length_decomposition_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_codechallenge_residual_stream_path_length_decomposition_part_2_.md) |
| [Quá»¹ Äáº¡o KhÃ´ng Gian Tráº¡ng ThÃ¡i (State-Space Trajectories) Cá»§a Há»‡ Vector NgÃ´n Ngá»¯](aero_llm_06_state_space_trajectories_through_embedding_space.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_state_space_trajectories_through_embedding_space.md) |
| [PhÃ¢n Loáº¡i Tá»« Loáº¡i Báº±ng ThÆ° Viá»‡n SpaCy Trong PhÃ¢n TÃ­ch Mechanistic Interpretability](aero_llm_07_parts_of_speech_with_spacy_library.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_parts_of_speech_with_spacy_library.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: So SÃ¡nh Äá»™ DÃ i Quá»¹ Äáº¡o Cá»§a Danh Tá»« VÃ  TÃ­nh Tá»« (Pháº§n 1)](aero_llm_08_codechallenge_do_nouns_or_adjectives_have_longer_trajectories_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_codechallenge_do_nouns_or_adjectives_have_longer_trajectories_part_1_.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: So SÃ¡nh Äá»™ DÃ i Quá»¹ Äáº¡o Cá»§a Danh Tá»« VÃ  TÃ­nh Tá»« (Pháº§n 2)](aero_llm_09_codechallenge_do_nouns_or_adjectives_have_longer_trajectories_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_do_nouns_or_adjectives_have_longer_trajectories_part_2_.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [20 investigating token embeddings](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# PhÃ¢n Loáº¡i Tá»« Loáº¡i Báº±ng ThÆ° Viá»‡n SpaCy Trong PhÃ¢n TÃ­ch Mechanistic Interpretability

## TÃ³m táº¯t (Abstract)
Tá»« loáº¡i (Parts of speech - POS) nhÆ° danh tá»«, Ä‘á»™ng tá»«, tÃ­nh tá»« lÃ  cá»™t sá»‘ng cá»§a cáº¥u trÃºc ngÃ´n ngá»¯. Viá»‡c má»Ÿ rá»™ng phÃ¢n tÃ­ch Mechanistic Interpretability lÃªn cÃ¡c bá»™ dá»¯ liá»‡u quy mÃ´ lá»›n Ä‘Ã²i há»i pháº£i tá»± Ä‘á»™ng hÃ³a quÃ¡ trÃ¬nh nháº­n diá»‡n tá»« loáº¡i thay vÃ¬ dÃ¡n nhÃ£n thá»§ cÃ´ng tÄ©nh. BÃ¡o cÃ¡o nÃ y giá»›i thiá»‡u viá»‡c á»©ng dá»¥ng thÆ° viá»‡n Xá»­ lÃ½ NgÃ´n ngá»¯ Tá»± nhiÃªn `spaCy` Ä‘á»ƒ dÃ¡n nhÃ£n POS. Tuy nhiÃªn, má»™t xung Ä‘á»™t ká»¹ thuáº­t lá»›n náº£y sinh giá»¯a thuáº­t toÃ¡n mÃ£ hÃ³a tá»« phá»¥ (Sub-word Byte-Pair Encoding) cá»§a Tokenizer trong LLMs (nhÆ° GPT-2) vÃ  bá»™ Tokenizer tiÃªu chuáº©n cá»§a `spaCy`, dáº«n Ä‘áº¿n viá»‡c nháº­n diá»‡n sai khÃ´ng gian tráº¯ng (spaces) vÃ  chia cáº¯t tá»«. TÃ i liá»‡u nÃ y tháº£o luáº­n nguyÃªn nhÃ¢n lÃµi vÃ  bÆ°á»›c Ä‘á»‡m xá»­ lÃ½ cÆ¡ báº£n trÆ°á»›c khi Ä‘i vÃ o giáº£i phÃ¡p Ä‘á»‹nh lÆ°á»£ng sÃ¢u hÆ¡n.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Trong cÃ¡c kháº£o sÃ¡t trÆ°á»›c Ä‘Ã¢y (nhÆ° phÃ¢n tÃ­ch tá»« `"her"`, `"him"`, `"round"`), ta chá»§ yáº¿u xÃ¢y dá»±ng bá»™ dá»¯ liá»‡u nhá» vÃ  nháº·t tay tá»«ng vá»‹ trÃ­ Token. Äá»ƒ thá»±c sá»± kháº£o nghiá»‡m cÃ¡ch má»™t LLM nhÆ° GPT-2 pháº£n á»©ng vá»›i danh tá»« so vá»›i tÃ­nh tá»« trÃªn hÃ ng ngÃ n cá»¥m vÄƒn báº£n biá»ƒu Ä‘áº¡t tá»± do, ta cáº§n má»™t bá»™ mÃ¡y gÃ¡n nhÃ£n tá»± Ä‘á»™ng.
`spaCy` lÃ  má»™t thÆ° viá»‡n Python máº¡nh máº½, tá»‘i Æ°u hÃ³a cho cÃ´ng viá»‡c bÃ³c tÃ¡ch thÃ´ng tin POS. Tuy nhiÃªn, vÃ¬ kiáº¿n trÃºc cáº¯t chá»¯ cá»§a LLMs khÃ´ng tuÃ¢n theo quy táº¯c ngÃ´n ngá»¯ há»c truyá»n thá»‘ng, viá»‡c Ã©p chÃ©o thÆ° viá»‡n POS dÃ¡n nhÃ£n lÃªn cÃ¡c Model Tokens thÃ´ phÃ¡c sinh ra nhiá»u há»‡ lá»¥y ká»¹ thuáº­t cáº£n trá»Ÿ phÃ¢n tÃ­ch.

---

## 2. Tiá»n Xá»­ LÃ½: á»¨ng Dá»¥ng spaCy CÆ¡ Báº£n (Methodology)

### 2.1. Ngá»¯ Cáº£nh Quyáº¿t Äá»‹nh Tá»« Loáº¡i (Context-Dependent POS)
Má»™t lá»—i sÆ¡ Ä‘áº³ng khi á»©ng dá»¥ng `spaCy` lÃ  chia nhá» cÃ¢u thÃ nh cÃ¡c tá»« rá»i ráº¡c (`split()`) vÃ  cháº¡y gÃ¡n nhÃ£n Ä‘Æ¡n láº» Ä‘á»™c láº­p.
VÃ­ dá»¥: Tá»« `explore` náº¿u Ä‘á»©ng má»™t mÃ¬nh cÃ³ thá»ƒ bá»‹ `spaCy` nÃ©m vÃ o nhÃ£n Danh tá»« (Noun). NhÆ°ng khi Ä‘áº·t nguyÃªn váº¹n trong cÃ¢u *"a sentence that I will use to **explore**"*, bá»™ mÃ¡y phÃ¢n tÃ­ch cÃº phÃ¡p cá»§a `spaCy` sáº½ nháº­n thá»©c Ä‘Æ°á»£c cáº¥u trÃºc Ä‘á»™ng tá»« nguyÃªn thá»ƒ cÃ³ `to` vÃ  gÃ¡n chuáº©n xÃ¡c Ä‘Ã¢y lÃ  Äá»™ng tá»« (Verb). Do Ä‘Ã³, nguyÃªn táº¯c tá»‘i thÆ°á»£ng lÃ : **Pháº£i Ä‘Æ°a toÃ n bá»™ nguyÃªn cÃ¢u (Full Context) vÃ o bá»™ pháº­n NLP Object cá»§a `spaCy` phÃ¢n tÃ­ch má»™t láº§n.**

### 2.2. Nghá»‹ch LÃ½ Giá»¯a LLM Tokenizer vÃ  rÃ o cáº£n Tá»« Phá»¥ (Subwords)
Sá»± káº¿t ná»‘i giá»¯a LLMs vÃ  `spaCy` tháº¥t báº¡i khi Ä‘á»‘i diá»‡n thuáº­t toÃ¡n BPE (Byte-Pair Encoding) cá»§a GPT.
- **Váº¥n Ä‘á» dáº¥u cÃ¡ch:** GPT-2 ghim cháº·t khoáº£ng tráº¯ng (space character, thÆ°á»ng kÃ½ hiá»‡u lÃ  Ä‘áº±ng trÆ°á»›c chá»¯ cÃ¡i) vÃ o trong Token. Khi nÃ©m tháº³ng token nhÆ° `[ Ä sentence]` vÃ o `spaCy`, há»‡ thá»‘ng POS láº­p tá»©c Ä‘Ã¡nh giÃ¡ kÃ½ tá»± Ä‘áº§u lÃ  dáº¥u ngáº¯t khoáº£ng tráº¯ng vÃ  tráº£ vá» káº¿t quáº£ rÃ¡c (Space) thay vÃ¬ (Noun). Giáº£i phÃ¡p sÆ¡ cá»©u cá»¥c bá»™ lÃ  dÃ¹ng chuá»—i hÃ m `.strip()` gá»t bá» khoáº£ng tráº¯ng thá»«a.
- **Váº¥n Ä‘á» phÃ¢n máº£nh tá»« vá»±ng:** GPT-2 cáº¯t nhá»¯ng chá»¯ láº¡ thÃ nh nhiá»u token con. VÃ­ dá»¥: tá»« `spacy` biáº¿n thÃ nh 2 tokens: `spa` vÃ  `cy`. Khi cáº¯t Ä‘á»©t nhÆ° váº­y, `spaCy` máº¥t hoÃ n toÃ n khÃ¡i niá»‡m tá»« gá»‘c Ä‘á»ƒ dÃ¡n nhÃ£n.

---

## 3. Kháº£o SÃ¡t ÄÃ¡nh GiÃ¡: Thá»­ Nghiá»‡m Káº¿t TrÃ­ch VÄƒn Báº£n (Analysis) 
Äá»ƒ trÃ¬nh diá»…n kháº£ nÄƒng á»©ng dá»¥ng, thuáº­t toÃ¡n sá»­ dá»¥ng bá»™ khá»Ÿi táº¡o GPT-2 Small sinh ra 400 Token ngáº«u nhiÃªn tiáº¿p ná»‘i cÃ¢u má»“i: *"I think the world could be better if"*.
Káº¿t quáº£ Ä‘Æ°á»£c cháº¡y qua vÃ²ng láº·p quÃ©t cá»§a `spaCy` Tokenizer:
```python

if token.pos_ == 'NOUN': count_noun += 1

$$
if token.pos_ == 'VERB': count_verb += 1
$$



<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [10 identifying circuits](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Ráº¡ch RÃ²i Giá»›i TÃ­nh (Him vs Her) Báº±ng Generalized Eigendecomposition Trong MLP

## TÃ³m táº¯t (Abstract)
ThÃ¡ch thá»©c lá»›n nháº¥t khi Ã¡p dá»¥ng Generalized Eigendecomposition (GED) trÃªn Máº¡ng ngÃ´n ngá»¯ Lá»›n (LLMs) náº±m á»Ÿ Váº¥n Ä‘á» Khá»§ng hoáº£ng KhÃ´ng gian Äa chiá»u: QuÃ¡ nhiá»u Biáº¿n sá»‘ (Neurons) nhÆ°ng Cáº¥p báº­c thá»© háº¡ng dá»¯ liá»‡u (Rank) láº¡i quÃ¡ tháº¥p, khiáº¿n hÃ m ma tráº­n khÃ´ng thá»ƒ tá»± nghá»‹ch Ä‘áº£o. BÃ¡o cÃ¡o nÃ y Ä‘Æ°a ra phÆ°Æ¡ng thá»©c "NÃ©n rá»“i TÃ¡ch" (Two-stage Compression-Separation Procedure) báº±ng cÃ¡ch Ã©p pháº³ng KhÃ´ng gian $3072$ chiá»u thÃ nh $63$ chiá»u thÃ´ng qua PCA, sau Ä‘Ã³ má»›i dÃ¹ng thuáº­t GED cÃ³ Shrinkage Regularization. Káº¿t quáº£ trÃªn bá»™ test-set CÃ¢u Äiá»u kiá»‡n Äáº¡i Tá»« (Pronouns dataset) cho tháº¥y thuáº­t toÃ¡n tÃ¡ch Ä‘Ã´i vÃ  khoanh vÃ¹ng Ä‘á»™c láº­p thÃ nh cÃ´ng LÆ°á»›i kÃ­ch hoáº¡t dÃ nh riÃªng cho tá»« 'Him' so diá»‡n vá»›i KhÃ´ng gian dÃ nh riÃªng cho 'Her' ngay táº¡i MLP Expansion. 

---

## 1. Má»Ÿ Äáº§u (Introduction)
Tá»« Demo mÃ´ phá»ng á»Ÿ chÆ°Æ¡ng trÆ°á»›c, chÃºng ta Ä‘Ã£ náº¯m Ä‘Æ°á»£c GED hoáº¡t Ä‘á»™ng báº±ng cÃ¡ch lÃ m PhÃ©p chia Tá»‰ Lá»‡ (SNR) giá»¯a Khá»‘i Cáº§u TÃ­n hiá»‡u vÃ  Khá»‘i Cáº§u Äá»‘i Chiáº¿u. Khi Ã¡p dá»¥ng tháº³ng vÃ o KhÃ´ng gian Thá»±c cá»§a MÃ´ hÃ¬nh Transformers, tá»©c $\sim 3072$ chiá»u NÆ¡-ron trÃªn hÃ ng ngÃ n Lá»›p Token, mÃ¡y tÃ­nh sáº½ bÃ¡o lá»—i VÃ´ Hiá»‡u Lá»‡nh (Rank Deficient) do $R^{-1}$ tiáº¿n tá»›i vÃ´ cá»±c. 
Nhiá»‡m vá»¥ cá»§a bÃ i thÃ­ nghiá»‡m lÃ  pháº£i Cáº¯t Bá» Má»¡ Thá»«a (Data Compression) cho bá»™ Data trÆ°á»›c khi Ä‘Æ°a vÃ o MÃ¡y váº¯t GED: Thay vÃ¬ thao tÃ¡c trÃªn 3072 tham sá»‘ rá»—ng, ta thu bÃ© nÃ³ láº¡i vá» má»™t NhÃ³m Nhá» Äáº¡i Diá»‡n nhÆ°ng váº«n chá»©a $99\%$ nÄƒng lÆ°á»£ng biáº¿n thiÃªn cá»§a dá»¯ liá»‡u. 

---

## 2. Tiáº¿t Thiáº¿t Láº­p Cáº¥u TrÃºc Khá»‘i NÃ©n KÃ©p (Methodology)

### 2.1. Giáº£i Thuáº­t Hai Giai Äoáº¡n (Two-stage Separation Procedure)
Khi $\text{Rank} \ll \text{Size}$, PhÃ©p TÃ­nh Eigendecomposition trá»Ÿ nÃªn báº¥t á»•n tá»™t Ä‘á»™. Ta thi hÃ nh "PCA lá»c Ná»n":
1. TrÃ­ch xuáº¥t Activations kÃ­ch thÆ°á»›c `[N_Máº«u_cÃ¢u, 3072_Neurons]`. 
2. Cháº¡y **PCA** trÃªn Ma Tráº­n Trung BÃ¬nh (Average Covariance Matrix) cá»§a cáº£ Hai Dá»¯ Kho (Cáº£ HIM vÃ  HER gá»™p chung). Táº¡i sao? Äá»ƒ PCA Ä‘i lÃ¹ng sá»¥c **"ToÃ n bá»™ vÃ¹ng khÃ´ng gian chung mÃ  Cáº£ hai Ä‘á»‘i tÆ°á»£ng nÃ y cÃ¹ng kÃ­ch hoáº¡t"**, lá»c láº¥y cÃ¡c PhÃ¢n máº£nh ChÃ­nh mang tÃ­nh sá»‘ng cÃ²n.
3. Cáº¯t LÃ¡t (Scree Plot Cut-off): Chá»‰ giá»¯ láº¡i cÃ¡c PC gá»™p Ä‘á»§ $99\%$ lÆ°á»£ng Variance (Lá»‡ch chuáº©n) cÃ¹a toÃ n Ä‘á»“ thá»‹. VÃ­ dá»¥ á»Ÿ Ä‘Ã¢y ta thu vá» NhÃ³m Tinh TÃºy $63$ Máº¡ch $PC$.
4. **Chiáº¿u RÃºt Chiá»u:** PhÃ³ng (Project) khá»‘i Dá»¯ liá»‡u Gá»‘c lÃªn khÃ´ng gian 63 chiá»u má»›i nÃ y Ä‘á»ƒ "XÃ³a sá»• 3000 chiá»u RÃ¡c".

### 2.2. Trá»±c KhÃ¡n Vá»›i Shrinkage (Shrinkage Regularized GED)

$$
Tuyá»ƒn 63-Dimension Matrix má»›i cÃ³ váº» bÃ©, nhÆ°ng báº£n thÃ¢n nÃ³ váº«n bá»‹ VÆ°á»›ng Rank Zero! NghÄ©a lÃ  \text{Rank}(Cov) = 52 < 63.
$$

$$
Ãp dá»¥ng cÆ¡ cháº¿ Covariance Shrinking 1\% (\gamma = 0.01):
$$

$$
\tilde{\mathbf{R}} = (1 - 0.01)\mathbf{R} + 0.01 \alpha \mathbf{I}
$$

$$
PhÃ©p toÃ¡n nÃ y biáº¿n hÃ³a Rank 52 \xrightarrow{Inflate} 63 (Full Rank). LÃºc nÃ y hÃ m vi phÃ¢n cá»§a SciPy (`scipy.linalg.eigh`) cÃ³ thá»ƒ tiÃªu hÃ³a ma tráº­n R_{her\_shrunk}^{-1} \cdot S_{him} hoÃ n toÃ n trÆ¡n tru. --- ## 3. Kháº£o SÃ¡t TÃ¡ch Máº¡ch CÄƒn Giá»›i (Analysis) ### 3.1. Sá»± Trá»—i Dáº­y Cá»§a ThÃ nh Pháº§n PhÃ¢n Cá»±c Tuyá»‡t Äá»‘i (Top Eigenvector) Khi GED hoÃ n táº¥t, há»‡ sá»‘ Trá»‹ RiÃªng (Eigenvalues) Ä‘Æ°á»£c sáº¯p xáº¿p tá»« cao xuá»‘ng tháº¥p. Top 1 Eigenvalue cho tháº¥y cÃ³ má»™t VectÆ¡ Ä‘áº·c biá»‡t (Eigenvector) mÃ  khi dá»¯ liá»‡u chiáº¿u vÃ o: - NÃ³ TrÃ n Äáº§y NÄƒng lÆ°á»£ng (Táº¡o Max Variance) khi dá»¯ liá»‡u mang chá»¯ HIM. - NÃ³ Triá»‡t TiÃªu NÄƒng lÆ°á»£ng (ChÃ¬m nghá»‰m thÃ nh Zero Variance) khi dá»¯ liá»‡u mang chá»¯ HER.
$$

$$
(VÃ  khi Ä‘áº£o \mathbf{S=Her}, \mathbf{R=Him}, ta láº¡i tháº¥y Ä‘iá»u ngÆ°á»£c láº¡i hoáº¡t Ä‘á»™ng song song).
$$

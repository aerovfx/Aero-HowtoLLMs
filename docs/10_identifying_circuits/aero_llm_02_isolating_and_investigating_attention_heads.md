
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [10 identifying circuits](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# CÃ´ Láº­p VÃ  ThÄƒm DÃ² Khá»‘i ChÃº Ã (Attention Heads)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o trÃ¬nh bÃ y phÆ°Æ¡ng phÃ¡p giáº£i pháº«u má»™t trong nhá»¯ng linh há»“n cá»‘t lÃµi cá»§a kiáº¿n trÃºc Transformer: CÆ¡ cháº¿ Äa chÃº Ã½ (Multi-head Attention). Báº±ng viá»‡c theo dÃµi cÃ¡ch biáº¿n Ä‘á»•i vÃ  phÃ¢n máº£nh ma tráº­n TÆ°Æ¡ng tÃ¡c $Q, K, V$ dá»c theo há»‡ chiá»u nhÃºng thÃ nh cÃ¡c module Ä‘áº§u Ä‘á»™c láº­p (Heads), ta cÃ³ thá»ƒ nháº­n thá»©c Ä‘Æ°á»£c sá»± chuyÃªn biá»‡t hÃ³a luá»“ng thÃ´ng tin cá»§a LLM. Má»™t phÃ¡t hiá»‡n Ä‘Ã¡ng chÃº Ã½ lÃ  sá»± thiÃªn vá»‹ Ä‘iá»ƒm Ã¢m (Negative shift) cá»§a cÃ¡c TÃ­ch vÃ´ hÆ°á»›ng gá»‘c (Raw Attention Scores), Ä‘iá»u nÃ y giáº£i thÃ­ch toÃ¡n há»c cho cÆ¡ cháº¿ "Sparsity" - triá»‡t tiÃªu sá»± nhiá»…u loáº¡n tá»« token khÃ´ng liÃªn quan. BÃ¡o cÃ¡o cÅ©ng Ä‘á» xuáº¥t phÆ°Æ¡ng phÃ¡p mÃ´ hÃ¬nh hÃ³a máº­t Ä‘á»™ háº¡t nhÃ¢n Kernel Density Estimation (KDE) thay cho Scatter plots ná»™i suy, giÃºp trá»±c quan hÃ³a phÃ¢n bá»• xÃ¡c suáº¥t má»™t cÃ¡ch khoa há»c.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Trong Máº¡ng Máº¡ch cá»§a Deep Learning, viá»‡c quy tá»¥ hÃ ng khá»‘i Head láº¡i vá»›i nhau thÃ´ng qua ma tráº­n trá»™n tuyáº¿n tÃ­nh $W_o$ (Linear mix matrix) lÃ  chÃ¬a khÃ³a tá»•ng há»£p kiáº¿n thá»©c ngÃ´n ngá»¯. Tuy nhiÃªn, náº¿u chÃºng ta cÃ³ thá»ƒ cháº» nhá» vÃ  truy cáº­p vÃ o tá»«ng "NÃ£o bá»™ phá»¥" (Head) riÃªng láº» Ä‘ang phÃ¢n tÃ­ch gÃ¬, ta sáº½ hiá»ƒu Ä‘Æ°á»£c cÆ¡ cháº¿ hoáº¡t Ä‘á»™ng vi mÃ´ (Mechanistic Interpretability). CÃ´ng viá»‡c nÃ y Ä‘Ã²i há»i káº¿t há»£p phÆ°Æ¡ng trÃ¬nh Attention cá»‘t lÃµi: $Softmax(\frac{QK^T}{\sqrt{d_k}})V$ káº¿t há»£p thao tÃ¡c ma tráº­n tinh vi.

---

## 2. Tiáº¿t Thiáº¿t Láº­p (Methodology)

### 2.1. Nháº¯c Láº¡i Thuáº­t ToÃ¡n Attention vÃ  Máº·t Náº¡ Causal Label (Masking)
$Q$ (Query) Ä‘áº¡i diá»‡n cho "Token hiá»‡n táº¡i Ä‘ang tÃ¬m kiáº¿m gÃ¬?", cÃ²n $K$ (Keys) Ä‘áº¡i diá»‡n cho "CÃ¡c token cÅ© giá»¯ thÃ´ng tin gÃ¬ Ä‘Ã¡ng giÃ¡?". TÃ­ch vÃ´ hÆ°á»›ng $QK^T$ Ä‘o lÆ°á»ng sá»± tÆ°Æ¡ng thÃ­ch. 
Tuy nhiÃªn, Transformer lÃ  bá»™ dá»± bÃ¡o chuá»—i theo thá»i gian (Autoregressive), nÃ³ báº¯t buá»™c khÃ´ng Ä‘Æ°á»£c "NhÃ¬n trá»™m" tÆ°Æ¡ng lai. Lá»›p Máº·t Náº¡ $M$ (Masking matrix) Ä‘Æ°á»£c phá»§ lÃªn $QK^T$: CÃ¡c tá»a Ä‘á»™ á»Ÿ tam giÃ¡c dÆ°á»›i (QuÃ¡ khá»©) nháº­n má»©c $1$, tá»a Ä‘á»™ á»Ÿ tam giÃ¡c trÃªn (TÆ°Æ¡ng lai) nháº­n $-$\infty$$. Khi qua hÃ m phi tuyáº¿n kÃ­ch hoáº¡t $Softmax$, hÃ m sá»‘ $e^{-$\infty$}$ biáº¿n máº¥t thÃ nh Ä‘iá»ƒm $0$ tuyá»‡t Ä‘á»‘i. 
*Há»‡ quáº£ dá»‹ biá»‡t:* Máº£nh Token Ä‘áº§u tiÃªn cá»§a chuá»—i khÃ´ng cÃ³ quÃ¡ khá»©, nÃªn toÃ n bá»™ thÃ´ng sá»‘ liÃªn káº¿t ngÆ°á»£c bá»‹ xÃ³a sá»• $\to$ tá»± gÃ¡n $100\%$ lá»±c chÃº Ã½ vÃ o chÃ­nh báº£n thÃ¢n nÃ³ (Outlier error).

### 2.2. TrÃ­ch Xuáº¥t Attention Äáº§u Phá»¥ (Heads Isolation)
TrÃªn GPT-2 Small, ma tráº­n sau khi hook láº¥y cáº¯p tá»« `hook_h.attn.c_attn` sáº½ lÃ  má»™t khá»‘i $768 \times 2304$. Do $2304 = 768 \times 3$, nÃ³ Ä‘ang gá»™p chung tá»‡p $Q, K, V$. 
1. Cáº¯t láº¥y $1/3$ Ä‘áº§u tiÃªn ta Ä‘Æ°á»£c Ma tráº­n thuáº§n Query $Q$ (KÃ­ch thÆ°á»›c: $SequenceLength \times 768$).
2. Tiáº¿p tá»¥c dÃ¹ng hÃ m `torch.split` chiáº¿u dá»c theo chiá»u Dimensions (768), vÄƒm thÃ nh $12$ khÃºc. Káº¿t quáº£: $12$ Attention Heads, má»—i Head thu Ä‘Æ°á»£c ma tráº­n $SequenceLength \times 64$.
3. Táº¡i Ä‘iá»ƒm nÃ y, ta tÃ­nh TÃ­ch vÃ´ hÆ°á»›ng (Dot products) ná»™i bá»™ cho tá»«ng Head riÃªng biá»‡t Ä‘á»ƒ láº¥y Raw Attention Scores.

---

## 3. Kháº£o SÃ¡t & Giáº£i Pháº«u MÃ´ HÃ¬nh (Analysis)

### 3.1. Sá»± ThiÃªn Vá»‹ Ã‚m TÃ­nh VÃ´ HÆ°á»›ng (Negative Raw Attention Shift)
Theo lÃ½ thuyáº¿t xÃ¡c suáº¥t, khi láº¥y máº«u ma tráº­n Ä‘iá»ƒm nhÃ¢n vá»›i nhau, phÃ¢n bá»• Ä‘á»“ thá»‹ phÃ¢n tÃ¡n (Scatter plots) cá»§a $QK^T$ (Raw attention scores) thÆ°á»ng nÃªn náº±m á»Ÿ dáº¡ng Ä‘á»‘i xá»©ng ngay quanh má»‘c zero. Tuy nhiÃªn, GPT-2 Ä‘iá»u hÆ°á»›ng trá»ng sá»‘ lá»‡ch máº¡nh máº½ vá» khu vá»±c cá»±c Ã¢m (Negative numbers). 
ÄÃ¢y khÃ´ng pháº£i lÃ  lá»—i. NÃ³ lÃ  má»™t thá»§ thuáº­t Tá»‘i Æ°u ThÆ°a (Sparsity mechanism). Khi sá»‘ gá»‘c mang giÃ¡ trá»‹ Ã¢m sÃ¢u, hÃ m kÃ­ch hoáº¡t $Softmax$ sáº½ dáº­p toÃ n bá»™ táº­p xÃ¡c suáº¥t nÃ y xáº¥p xá»‰ má»©c $0$. Viá»‡c LLM Ä‘áº©y háº§u háº¿t Ä‘iá»ƒm tÆ°Æ¡ng tÃ¡c xuá»‘ng má»©c Ã¢m giÃºp triá»‡t tiÃªu hoÃ n toÃ n cÃ¡c má»‘i quan há»‡ Token dÆ° thá»«a tá»« quÃ¡ khá»© (suppression), qua Ä‘Ã³ Ä‘á»ƒ nhÆ°á»ng chá»—, vinh danh cho má»™t sá»‘ ráº¥t nhá» cÃ¡c káº¿t ná»‘i ngá»¯ phÃ¡p thá»±c sá»± Ã½ nghÄ©a (VÃ­ dá»¥: tÃ­nh tá»« liÃªn káº¿t danh tá»«).

### 3.2. Ná»™i Suy PhÃ¢n Bá»• Máº­t Äá»™ KDE (Kernel Density Estimation)
PhÆ°Æ¡ng thá»©c biá»ƒu diá»…n báº±ng cÃ¡c cháº¥m phÃ¢n tÃ¡n Scatter plots trá»Ÿ nÃªn vÃ´ dá»¥ng náº¿u dá»¯ liá»‡u lá»›n cá»“ng ká»nh qua hÃ ng chá»¥c Layers. PhÆ°Æ¡ng thá»©c thay tháº¿: KDE (MÃ´ hÃ¬nh hÃ³a máº­t Ä‘á»™ háº¡t nhÃ¢n).
KDE coi má»™t Ä‘iá»ƒm phÃ¢n tÃ¡n lÃ  má»™t tÃ¢m thu hÃºt phÃ¢n phá»‘i vi mÃ´ (Gaussian blur). Báº±ng cÃ¡ch convolve láº·p vÃ  cá»™ng dá»“n toÃ n bá»™ cÃ¡c mÃ ng sÆ°Æ¡ng Gaussian cÃ³ Ä‘á»™ bÄƒng thÃ´ng nháº¥t Ä‘á»‹nh (Bandwidth parameter), ta biáº¿n cÃ¡c sá»‘ thÃ´ (Discrete values) thÃ nh Ä‘Æ°á»ng cong phá»• phÃ¢n bá»• mÆ°á»£t mÃ  (Probability distribution curve). 

---

## 4. Káº¿t Luáº­n
Viá»‡c tÃ¡ch láº» cÃ¡c Head giáº£i pháº«u quÃ¡ trÃ¬nh tÃ­nh tÆ°Æ¡ng pháº£n Query-Key Ä‘Æ°a láº¡i lá»i giáº£i Ä‘Ã¡p vÃ¬ sao $Softmax$ cÃ³ nÄƒng lá»±c xá»­ lÃ½ ngÃ´n ngá»¯ sáº¡ch sáº½ vÃ  sáº¯c bÃ©n: Nhá» mÃ´ hÃ¬nh tá»± Ä‘á»™ng "DÃ¬m" phá»• TÃ­ch vÃ´ hÆ°á»›ng gá»‘c vá» cÃ¡c chá»‰ sá»‘ siÃªu nhá» Ä‘á»ƒ loáº¡i bá» nhiá»…u. PhÆ°Æ¡ng phÃ¡p tÃ¡ch cháº» ma tráº­n trá»±c tiáº¿p vÃ  Ã¡p dá»¥ng há»‡ tÃ­nh toÃ¡n máº­t Ä‘á»™ háº¡t nhÃ¢n (KDE) lÃ  báº­c thang dá»¯ liá»‡u hoÃ n háº£o trÆ°á»›c khi Ä‘i sÃ¢u váº½ dáº£i viá»n (Laminar Profiles) Attention head, bÆ°á»›c cÆ¡ báº£n Ä‘á»ƒ khÃ¡m phÃ¡ "Máº¡ng máº¡ch" á»Ÿ mÃ´-Ä‘un káº¿ tiáº¿p bÃ i thá»­ thÃ¡ch.

---

## TÃ i Liá»‡u Tham Kháº£o (Citations)
1. CÆ¡ cháº¿ cáº¯t máº£nh ma tráº­n vÃ  phÃ¢n chia Tensor trong `aero_LLM_02_Isolating and investigating attention heads.md`. ThÃ­ nghiá»‡m váº½ KDE thÃ´ng qua thÆ° viá»‡n `scipy.stats.gaussian_kde` vÃ  minh há»a dá»‹ch chuyá»ƒn Ã¢m TÃ­ch vÃ´ hÆ°á»›ng.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Máº¡ng Máº¡ch Thuáº­t ToÃ¡n (Circuits) Trong MÃ´ HÃ¬nh Há»c SÃ¢u](aero_llm_01_what_is_a_circuit_in_a_dl_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_what_is_a_circuit_in_a_dl_model.md) |
| ğŸ“Œ **[CÃ´ Láº­p VÃ  ThÄƒm DÃ² Khá»‘i ChÃº Ã (Attention Heads)](aero_llm_02_isolating_and_investigating_attention_heads.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_02_isolating_and_investigating_attention_heads.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: Biá»ƒu Diá»…n PhÃ¢n Bá»‘ Nhiá»‡t Laminar Cá»§a Trá»ng Sá»‘ ChÃº Ã](aero_llm_03_codechallenge_laminar_profile_of_attention_head_weights.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_laminar_profile_of_attention_head_weights.md) |
| [Kháº£o SÃ¡t TÆ°Æ¡ng Quan Cá»¥m (Clustering) Vi Máº¡ch (Circuits) Trong KhÃ´ng Gian Giáº£m Chiá»u](aero_llm_04_are_circuits_clustered_in_low_dimensional_space.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_are_circuits_clustered_in_low_dimensional_space.md) |
| [LÃ½ Thuyáº¿t VÃ  á»¨ng Dá»¥ng Cá»§a Ká»¹ Thuáº­t DÃ² ThÆ°a (Sparse Probing)](aero_llm_05_sparse_probing_theory_and_code.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_sparse_probing_theory_and_code.md) |
| [ThÃ¡ch Thá»©c Cá»§a TÃ­n Hiá»‡u ThÆ°a Trong Dá»¯ Liá»‡u Táº­p Lá»›n (Statistical Suppression)](aero_llm_06_challenges_with_sparse_logistic_regression_in_large_datasets.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_challenges_with_sparse_logistic_regression_in_large_datasets.md) |
| [Biáº¿n Tiá»m áº¨n (Latent) VÃ  Biáº¿n Hiá»ƒn NgÃ´n (Manifest) Trong Giáº£i Diá»…n AI](aero_llm_07_latent_vs_manifest_variables.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_latent_vs_manifest_variables.md) |
| [MÃ´ HÃ¬nh Sparse Autoencoders (SAEs): LÃ½ Thuyáº¿t VÃ  Kiáº¿n TrÃºc KhÃ´i Phá»¥c Vi Máº¡ch Tiá»m áº¨n](aero_llm_08_sparse_autoencoders_theory_and_code.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_sparse_autoencoders_theory_and_code.md) |
| [Huáº¥n Luyá»‡n Sparse Autoencoder TrÃ­ch Xuáº¥t KhÃ¡i Niá»‡m Ngá»¯ Cáº£nh Palinka TrÃªn GPT-2](aero_llm_09_sae_in_gpt2_learns_about_hungarian_palinka.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_sae_in_gpt2_learns_about_hungarian_palinka.md) |
| [Kháº£o SÃ¡t PhÃ¢n Táº§ng KÃ­ch Hoáº¡t (Laminar Profile) Qua Sparse Autoencoder](aero_llm_10_codechallenge_laminar_profile_of_autoencoder_sparsity.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_codechallenge_laminar_profile_of_autoencoder_sparsity.md) |
| [Nháº­n Diá»‡n KhÃ¡i Niá»‡m XuyÃªn TÃ¢m Vá»›i PhÃ¢n RÃ£ GiÃ¡ Trá»‹ RiÃªng Suy Rá»™ng (Generalized Eigendecomposition - GED)](aero_llm_11_non_orthogonal_latent_components_via_eigendecomposition_theory_and_demo_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_non_orthogonal_latent_components_via_eigendecomposition_theory_and_demo_.md) |
| [Ráº¡ch RÃ²i Giá»›i TÃ­nh (Him vs Her) Báº±ng Generalized Eigendecomposition Trong MLP](aero_llm_12_generalized_eigendecomposition_separates_him_from_her_in_mlp.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_generalized_eigendecomposition_separates_him_from_her_in_mlp.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): TÃ¡ch NhÃ³m GED Äa Táº§ng (Pháº§n 1)](aero_llm_13_codechallenge_ged_for_category_isolation_across_layers_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_codechallenge_ged_for_category_isolation_across_layers_part_1_.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): TÃ¡ch NhÃ³m GED Äa Táº§ng (Pháº§n 2) & Kiá»ƒm Chá»©ng ChÃ©o](aero_llm_14_codechallenge_ged_for_category_isolation_across_layers_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_ged_for_category_isolation_across_layers_part_2_.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

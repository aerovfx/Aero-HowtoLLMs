
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [10 identifying circuits](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Nháº­n Diá»‡n KhÃ¡i Niá»‡m XuyÃªn TÃ¢m Vá»›i PhÃ¢n RÃ£ GiÃ¡ Trá»‹ RiÃªng Suy Rá»™ng (Generalized Eigendecomposition - GED)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y chuyá»ƒn Ä‘á»•i phÆ°Æ¡ng phÃ¡p luáº­n Giáº£i Diá»…n CÆ¡ Há»c cá»§a Máº¡ng nÆ¡-ron (Mechanistic Interpretability) tá»« mÃ´ hÃ¬nh Autoencoders tuyáº¿n tÃ­nh Ä‘a táº§ng sang PhÆ°Æ¡ng phÃ¡p Äáº¡i sá»‘ Tuyáº¿n TÃ­nh Giáº£i TÃ­ch - Cá»¥ thá»ƒ lÃ  PhÃ¢n rÃ£ Trá»‹ RiÃªng Suy Rá»™ng (Generalized Eigen-Decomposition, GED). KhÃ¡c biá»‡t sÃ¢u sáº¯c Ä‘á»‘i vá»›i PhÃ¢n tÃ­ch ThÃ nh pháº§n ChÃ­nh (PCA - vá»‘n bá»‹ gÃ² bÃ³ bá»Ÿi tÃ­nh Trá»±c Giao - Orthogonality), **GED cho phÃ©p cÃ¡c vectors thÃ nh pháº§n phi-trá»±c giao Ä‘an chÃ©o**, nháº±m má»¥c Ä‘Ã­ch tÃ¬m ra má»™t Trá»ng sá»‘ (Weight Matrix) phÃ¢n ly tá»‘i Ä‘a tá»‰ sá»‘ PhÆ°Æ¡ng Sai (Variance Ratio) giá»¯a Quáº§n thá»ƒ Dá»¯ liá»‡u TÃ­n hiá»‡u (Signal) so vá»›i Dá»¯ liá»‡u Táº¡p Ã‚m $Reference/Noise$. ThÃ´ng qua thao tÃ¡c thu háº¹p há»‡ sá»‘ Covariance Matrix (Shrinkage Regularization), GED loáº¡i bá» thÃ nh cÃ´ng cÃ¡c áº£o giÃ¡c Nhiá»…u TÆ°Æ¡ng Quan, má»Ÿ ra tiá»m nÄƒng lá»c KhÃ¡i Niá»‡m Rá»i ráº¡c (VD: DÃ² tÃ¬m Máº¡ch vi phÃ¢n tÃ¡ch Nouns vs. Verbs).

---

## 1. Má»Ÿ Äáº§u (Introduction)
Trong KhÃ´ng gian Há»‡ sá»‘ (Weight Space) khá»•ng lá»“ cá»§a má»™t Language Model, cÃ¡c KhÃ¡i Niá»‡m Tiá»m áº¨n (Latent constructs) nhÆ° Ngá»¯ NghÄ©a Váº¡n Váº­t khÃ´ng náº±m Ä‘á»™c láº­p táº¡i má»™t gÃ³c tá»a Ä‘á»™ mÃ  bá»‹ Phá»‘i Trá»™n (Mixed together).
BÃ¬nh thÆ°á»ng, phÆ°Æ¡ng phÃ¡p tiáº¿p cáº­n kinh Ä‘iá»ƒn nháº¥t cá»§a Khoa há»c Dá»¯ liá»‡u lÃ  **PCA (Principal Component Analysis)**. Tuy nhiÃªn PCA báº£n cháº¥t chá»‰ lÃ  PhÃ©p Cáº¯t vÃ  Xoay Trá»¥c Tá»a Ä‘á»™ Trá»±c giao: NÃ³ tá»‘i Ä‘a hÃ³a PhÆ°Æ¡ng Sai ToÃ n Khá»‘i (Variance) nhÆ°ng KhÃ´ng cÃ³ khÃ¡i niá»‡m vá» "Sá»± khÃ¡c biá»‡t NhÃ£n Loáº¡i". NÃ³ sáº½ cho báº¡n biáº¿t Trá»¥c nÃ o dá»¯ liá»‡u táº£n máº¡n nháº¥t, nhÆ°ng KhÃ´ng há» cÃ³ kháº£ nÄƒng cáº¯t Ä‘Ã´i Khá»‘i Dá»¯ liá»‡u MÃ u Xanh (DÃ nh cho danh tá»«) ra khá»i Khá»‘i Dá»¯ Liá»‡u MÃ u TÃ­m (DÃ nh cho Ä‘á»™ng tá»«), vÃ¬ nÃ³ khÃ´ng Ä‘Æ°á»£c láº­p trÃ¬nh Ä‘á»ƒ "Tá»‘i Äa HÃ³a Khoáº£ng CÃ¡ch PhÃ¢n Ly TÃ­n Hiá»‡u".
GED xuáº¥t hiá»‡n nhÆ° má»™t "Cá»— mÃ¡y siÃªu viá»‡t" Ä‘á»ƒ trÃ³c lá»›p hai khÃ¡i niá»‡m Ä‘á»“ng vá»‹.

---

## 2. Tiáº¿t Láº­p TrÃ¬nh ToÃ¡n Há»c (Methodology)

### 2.1. Äáº¡o HÃ m Tá»‰ Sá»‘ TÃ­n Hiá»‡u - Nhiá»…u (Signal-to-Noise Ratio Optimization)
Ta Ä‘áº·t 2 Ma tráº­n Hiá»‡p PhÆ°Æ¡ng Sai (Covariance Matrix): 
- Cáº¥u trÃºc Máº¡ch TÃ­n Hiá»‡u cáº§n lÃ m ná»•i báº­t $\mathbf{S}$. (VÃ­ dá»¥: Tá»« vá»±ng xe Ã” tÃ´)
- Cáº¥u trÃºc Máº¡ch Äá»‘i Chiáº¿u cáº§n bá»‹ triá»‡t tiÃªu $\mathbf{R}$. (VÃ­ dá»¥: Tá»« vá»±ng xe Táº£i) 
Má»¥c Ä‘Ã­ch lÃ  Ä‘i tÃ¬m má»™t vector trá»ng sá»‘ $W$ sao cho nÃ³ khuáº¿ch Ä‘áº¡i tá»‘i Ä‘a Ma tráº­n $S$ vÃ  nÃ©n nhá» tá»‘i Ä‘a Ma tráº­n $R$. Hay Ä‘á»‹nh nghÄ©a báº±ng cÃ´ng thá»©c Tá»‰ sá»‘ Rayleigh Quotient:

$$

\Lambda = \frac{W^T \mathbf{S} W}{W^T \mathbf{R} W}

$$


Khi ta cáº§n tÃ¬m Äáº¡o hÃ m vi phÃ¢n Lagrange (Báº±ng cÃ¡ch trÃ³i Buá»™c $W^T \mathbf{R} W = 1$), toÃ n bá»™ Biá»ƒu thá»©c ToÃ¡n há»c kinh Ä‘iá»ƒn nÃ y hÃ³a giáº£i dÆ°á»›i dáº¡ng Biá»ƒu thá»©c Eigendecomposition trÃªn TÃ­ch hiá»‡p:

$$

\mathbf{R}^{-1} \mathbf{S} \ W = \Lambda W

$$


Tuy nhiÃªn, nghá»‹ch lÃ½ lÃ  á»Ÿ Ä‘Ã¢y: Trong khi $\mathbf{R}^{-1}$ vÃ  $\mathbf{S}$ Ä‘á»u lÃ  ma tráº­n Äá»‘i Xá»©ng Pháº³ng (Symmetric), khi chÃºng cáº¥u thÃ nh PhÃ©p nhÃ¢n $\mathbf{R}^{-1} \mathbf{S}$, nÃ³ táº¡o thÃ nh Thá»ƒ Äa HÃ¬nh (Non-Symmetric). Há»‡ quáº£ cá»±c quan trá»ng cá»§a lÃ½ thuyáº¿t Tuyáº¿n tÃ­nh: **Eigenvectors ($W$) tÃ¬m Ä‘Æ°á»£c sáº½ máº¥t tÃ­nh Trá»±c Giao (Orthogonal).** Thay vÃ¬ cÃ¡c Vector xoay gÃ³c 90 Ä‘á»™ VuÃ´ng vá»©c, nÃ³ cÃ³ thá»ƒ nhá»n hÆ¡n, xÃ²e hÆ¡n, tá»± Ä‘iá»u chá»‰nh linh Ä‘á»™ng Ä‘á»ƒ men theo Dáº£i PhÃ¢n TÃ¡ch dá»¯ liá»‡u thá»±c thá»¥.

### 2.2. Pháº«u Thuáº­t Covariance Matrix Vá»›i Äiá»u HÃ²a Thu Háº¹p (Shrinkage Regularization)
VÃ¬ cáº¥u trÃºc Language Model sá»Ÿ há»¯u Feature Khá»•ng Lá»“, Ma tráº­n $R$ sáº½ dá»… bá»‹ rÆ¡i vÃ o dáº¡ng Dáº¹t Pháº³ng SiÃªu HÃ¬nh (Flattened ellipse with Zero-Rank determinant) - tá»©c Determinant $=0$, dá»“n Ã©p phÃ©p Ká»‹ch Äáº£o $\mathbf{R}^{-1}$ thÃ nh vÃ´ cá»±c.
PhÆ°Æ¡ng phÃ¡p "Shrinkage Regularized" Ã©p phá»“ng khá»‘i Ellipse xáº¹p lÃ©p nÃ y báº±ng cÃ¡ch Ä‘á»™n lÃªn má»™t chÃºt nÄƒng lÆ°á»£ng vÃ o ÄÆ°á»ng ChÃ©o (Identity Matrix), mÃ´ phá»ng báº±ng cÃ´ng thá»©c:

$$

\tilde{\mathbf{R}} = (1 - \gamma)\mathbf{R} + \gamma \alpha \mathbf{I}

$$


**(Trong Ä‘Ã³ $\alpha$ lÃ  Trung bÃ¬nh dÃ£y GiÃ¡ trá»‹ riÃªng Eigenvalues).**
Náº¿u $\gamma \to 0$, KhÃ´ng cÃ³ Ä‘iá»u hÃ²a Ã¡p dá»¥ng. Náº¿u $\gamma \to 1$, PhÃ¢n rÃ£ GED tan rÃ£ trá»Ÿ vá» láº¡i hÃ¬nh thÃ¡i cháº¯p vÃ¡ PCA thÃ´ ban Ä‘áº§u. Viá»‡c chá»n biáº¿n sá»‘ Gamma (ThÆ°á»ng $\approx 0.01$) Ä‘Ã³ng vai trÃ² then chá»‘t cho sá»± sinh tá»“n cá»§a mÃ´ hÃ¬nh GED.

---

## 3. Kháº£o SÃ¡t Thá»­ Nghiá»‡m PhÃ¢n Phá»‘i (Analysis)

Sá»­ dá»¥ng thÆ° viá»‡n `scipy.linalg.eigh` (Äá»‹nh má»©c ToÃ¡n HÃ m Hermitian) trÃªn bá»™ Dá»¯ liá»‡u 2-Stream MÃ´ phá»ng:
1. **Qua PCA:** Trá»¥c tá»a Ä‘á»™ Component quay vuÃ´ng gÃ³c, gá»™p chung hai dÃ²ng Dá»¯ liá»‡u (Xanh-TÃ­m) lÃ m má»™t. Kháº£ nÄƒng tÃ¬m Vi Máº¡ch Äá»™c Láº­p $\to 0$.
2. **Qua Thuáº§n GED:** Äá»“ thá»‹ PhÃ©p Chiáº¿u (Projected Space) tráº£ vá» 2 Luá»“ng Vector PhÃ¢n láº­p Äá»™c láº­p rÃµ rá»‡t KhÃ´ng ÄÃ¨ Nhau, dÃ¹ cÃ¡c hÆ°á»›ng gÃ³c Eigenvectors tá»± do xiÃªn xáº¹o phi-trá»±c giao. NÃ³ bÃ³c tráº§n Ä‘Ãºng nghÄ©a hai dÃ²ng dá»¯ liá»‡u ra hai há»‡ tham chiáº¿u khÃ¡c nhau.
3. **Hiá»‡u á»¨ng Shrinkage:** 
  Khi ta cá»‘ tÃ¬nh cáº¥p phÃ©p thá»­ $\gamma = 0.4 \to 0.9$ ráº¥t lá»›n. Cáº·p Eigenvectors KhÃ´ng trá»±c giao bá»‹ báº» dÃ£n gÃ³c lá»“i dáº§n, bá»‹ lá»±c Ã©p cÆ°á»¡ng cháº¿ chuyá»ƒn hÃ³a dáº§n thÃ nh Äá»“ thá»‹ vuÃ´ng gÃ³c PCA nhÆ° cÅ©. NÃ³ tÃ¡i kháº³ng Ä‘á»‹nh láº¡i nguyÃªn lÃ½ ToÃ¡n: Viá»‡c BÆ¡m quÃ¡ tay Ma tráº­n ÄÆ°á»ng chÃ©o Ä‘á»ƒ vÃ¡ lá»—i Rank Null cÃ³ thá»ƒ Ä‘Ã¡nh Ä‘á»•i báº±ng sá»± mÃ¹ lÃ²a cá»§a ToÃ¡n LÃ½ PhÃ¢n Ly.

---

## 4. Káº¿t Luáº­n
Viá»‡c Ã¡p dá»¥ng PhÃ¢n tÃ­ch Generalized Eigen-Decomposition (HÃ m Tá»‰ Sá»‘ Cáº¡nh Tranh Giá»¯a 2 Covariances) lÃ  phÃ©p tá»‹nh tiáº¿n thay tháº¿ Æ°u viá»‡t cho thuáº­t giáº£i ráº­p khuÃ´n cá»§a PCA khi lÃ m viá»‡c vá»›i Multi-variates Source. Báº±ng viá»‡c cá»Ÿi trÃ³i KhÃ´ng Gian Trá»±c Giao Ä‘á»ƒ tá»± do luá»“n qua cÃ¡c Vector phÃ¢n máº£nh, vÃ  dÃ¹ng pháº«u thuáº­t Cáº¥y ÄÆ°á»ng chÃ©o Shrinkage Ä‘á»ƒ vÆ°á»£t rÃ o Cáº¥m Zero-Inverse, GED khÃ´ng chá»‰ á»©ng dá»¥ng cá»±c thá»‹nh trong Tháº§n kinh há»c mÃ  cÃ²n má»Ÿ toang cÃ¡nh cá»­a Ä‘á»ƒ Cáº¯t Máº¡ch (Circuit Cutting) má»™t cÃ¡ch tÆ°á»ng minh cho nhá»¯ng KhÃ¡i Äiá»ƒm áº¨n (Gender, Logic) giáº¥u kÃ­n bÃªn trong LLMs. 

---

## TÃ i Liá»‡u Tham Kháº£o (Citations)
1. ThÃ­ nghiá»‡m á»©ng dá»¥ng Khoa há»c Giáº£i TÃ­ch Trá»±c Giao táº¡i `aero_LLM_11_Non-orthogonal latent components via eigendecomposition (theory and demo).md`. Minh Ä‘á»‹nh hÃ³a cÃ´ng thá»©c Khá»‘i Co GiÃ£n Há»‡ Sá»‘ Covariance $\tilde{\mathbf{R}}$ vÃ  sá»± suy thoÃ¡i PCA theo bÆ°á»›c cá»§a biáº¿n $\gamma$.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Máº¡ng Máº¡ch Thuáº­t ToÃ¡n (Circuits) Trong MÃ´ HÃ¬nh Há»c SÃ¢u](aero_llm_01_what_is_a_circuit_in_a_dl_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_what_is_a_circuit_in_a_dl_model.md) |
| [CÃ´ Láº­p VÃ  ThÄƒm DÃ² Khá»‘i ChÃº Ã (Attention Heads)](aero_llm_02_isolating_and_investigating_attention_heads.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_isolating_and_investigating_attention_heads.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: Biá»ƒu Diá»…n PhÃ¢n Bá»‘ Nhiá»‡t Laminar Cá»§a Trá»ng Sá»‘ ChÃº Ã](aero_llm_03_codechallenge_laminar_profile_of_attention_head_weights.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_laminar_profile_of_attention_head_weights.md) |
| [Kháº£o SÃ¡t TÆ°Æ¡ng Quan Cá»¥m (Clustering) Vi Máº¡ch (Circuits) Trong KhÃ´ng Gian Giáº£m Chiá»u](aero_llm_04_are_circuits_clustered_in_low_dimensional_space.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_are_circuits_clustered_in_low_dimensional_space.md) |
| [LÃ½ Thuyáº¿t VÃ  á»¨ng Dá»¥ng Cá»§a Ká»¹ Thuáº­t DÃ² ThÆ°a (Sparse Probing)](aero_llm_05_sparse_probing_theory_and_code.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_sparse_probing_theory_and_code.md) |
| [ThÃ¡ch Thá»©c Cá»§a TÃ­n Hiá»‡u ThÆ°a Trong Dá»¯ Liá»‡u Táº­p Lá»›n (Statistical Suppression)](aero_llm_06_challenges_with_sparse_logistic_regression_in_large_datasets.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_challenges_with_sparse_logistic_regression_in_large_datasets.md) |
| [Biáº¿n Tiá»m áº¨n (Latent) VÃ  Biáº¿n Hiá»ƒn NgÃ´n (Manifest) Trong Giáº£i Diá»…n AI](aero_llm_07_latent_vs_manifest_variables.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_latent_vs_manifest_variables.md) |
| [MÃ´ HÃ¬nh Sparse Autoencoders (SAEs): LÃ½ Thuyáº¿t VÃ  Kiáº¿n TrÃºc KhÃ´i Phá»¥c Vi Máº¡ch Tiá»m áº¨n](aero_llm_08_sparse_autoencoders_theory_and_code.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_sparse_autoencoders_theory_and_code.md) |
| [Huáº¥n Luyá»‡n Sparse Autoencoder TrÃ­ch Xuáº¥t KhÃ¡i Niá»‡m Ngá»¯ Cáº£nh Palinka TrÃªn GPT-2](aero_llm_09_sae_in_gpt2_learns_about_hungarian_palinka.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_sae_in_gpt2_learns_about_hungarian_palinka.md) |
| [Kháº£o SÃ¡t PhÃ¢n Táº§ng KÃ­ch Hoáº¡t (Laminar Profile) Qua Sparse Autoencoder](aero_llm_10_codechallenge_laminar_profile_of_autoencoder_sparsity.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_codechallenge_laminar_profile_of_autoencoder_sparsity.md) |
| ğŸ“Œ **[Nháº­n Diá»‡n KhÃ¡i Niá»‡m XuyÃªn TÃ¢m Vá»›i PhÃ¢n RÃ£ GiÃ¡ Trá»‹ RiÃªng Suy Rá»™ng (Generalized Eigendecomposition - GED)](aero_llm_11_non_orthogonal_latent_components_via_eigendecomposition_theory_and_demo_.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_11_non_orthogonal_latent_components_via_eigendecomposition_theory_and_demo_.md) |
| [Ráº¡ch RÃ²i Giá»›i TÃ­nh (Him vs Her) Báº±ng Generalized Eigendecomposition Trong MLP](aero_llm_12_generalized_eigendecomposition_separates_him_from_her_in_mlp.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_generalized_eigendecomposition_separates_him_from_her_in_mlp.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): TÃ¡ch NhÃ³m GED Äa Táº§ng (Pháº§n 1)](aero_llm_13_codechallenge_ged_for_category_isolation_across_layers_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_codechallenge_ged_for_category_isolation_across_layers_part_1_.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): TÃ¡ch NhÃ³m GED Äa Táº§ng (Pháº§n 2) & Kiá»ƒm Chá»©ng ChÃ©o](aero_llm_14_codechallenge_ged_for_category_isolation_across_layers_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_ged_for_category_isolation_across_layers_part_2_.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

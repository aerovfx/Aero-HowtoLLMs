
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [10 identifying circuits](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Huáº¥n Luyá»‡n Sparse Autoencoder TrÃ­ch Xuáº¥t KhÃ¡i Niá»‡m Ngá»¯ Cáº£nh Palinka TrÃªn GPT-2

## TÃ³m táº¯t (Abstract)
Tháº¿ giá»›i ToÃ¡n há»c mÃ´ phá»ng dÃ¹ ráº¥t hoÃ n má»¹ nhÆ°ng khÃ´ng thá»ƒ sÃ¡nh Ä‘Æ°á»£c vá»›i sá»± há»—n loáº¡n cá»§a vÄƒn báº£n NgÃ´n ngá»¯ Tá»± nhiÃªn. á» bÃ i thá»±c hÃ nh nÃ y, ta nÃ¢ng cáº¥p mÃ´ hÃ¬nh Sparse Autoencoder (SAE) Ä‘á»ƒ khai thÃ¡c vÃ  trá»±c tiáº¿p nuá»‘t trá»n ma tráº­n $MLP\ Activations$ thu Ä‘Æ°á»£c tá»« mÃ´ hÃ¬nh GPT-2 Small, dÆ°á»›i sá»± kÃ­ch thÃ­ch cá»§a Ä‘oáº¡n vÄƒn báº£n tra cá»©u Wikipedia vá» "Palinka" (Má»™t loáº¡i Quá»‘c tá»­u ÄÃ´ng Ã‚u). Báº±ng viá»‡c dÃ¹ng cÆ¡ cháº¿ Top-K Ã©p ThÆ°a (Top-K Sparsity) thay vÃ¬ L1 loss thÃ´ng thÆ°á»ng, ta kháº£o sÃ¡t quy trÃ¬nh tÃ¬m kiáº¿m Máº¡ch Vi Ngá»¯ (Circuit) Ä‘áº·c thÃ¹ chá»‹u trÃ¡ch nhiá»‡m cho cÃ¡c Ã½ niá»‡m "Äá»‹a lÃ½". NghiÃªn cá»©u lÃ m lá»™ rÃµ kháº£ nÄƒng phÃ¢n tÃ¡ch TÃ­n hiá»‡u (Denoising / De-mixing) ráº¥t máº¡nh cá»§a SAE, song cÅ©ng chá»‰ ra Ä‘iá»ƒm yáº¿u cháº¿t ngÆ°á»i liÃªn quan tá»›i sá»± nhiá»…u loáº¡n cá»§a "Vung tham sá»‘" (Hyperparameter Sensitivity) khi Ä‘á»‘i diá»‡n vá»›i Real Datasets quy mÃ´ nhá». 

---

## 1. Má»Ÿ Äáº§u (Introduction)
Dá»¯ liá»‡u sinh ra tá»« GPT-2 khÃ´ng pháº£i lÃ  hÃ m $Sine$ gá»n gÃ ng nhÆ° bÃ i trÆ°á»›c. Má»™t tá»« Ä‘Æ¡n nhÆ° "Romania" sáº½ lÃ m chÃ¡y sÃ¡ng hÃ ng trÄƒm NÆ¡-ron Ä‘á»“ng thá»i, vÃ  chÃºng ta khÃ´ng cÃ³ má»™t Há»‡ quy chiáº¿u TiÃªu chuáº©n (Ground Truth Latent Sources) Ä‘á»ƒ cháº¥m Ä‘iá»ƒm Ä‘Ãºng sai.
VÄƒn báº£n Ä‘Æ°a vÃ o GPT-2 dÃ i cá»¡ $220$ Token, bÃ n vá» nguá»“n gá»‘c khu vá»±c Ä‘á»‹a lÃ½ cá»§a rÆ°á»£u Palinka (Tá»« Hungary Ä‘áº¿n Hy Láº¡p). Má»¥c tiÃªu cá»§a SAE á»Ÿ Ä‘Ã¢y lÃ : BÃ³c tÃ¡ch $768\ MLP\ Neurons$ thÃ nh hÆ¡n $6000$ KhÃ¡i niá»‡m Vi Ngá»¯ (Latent Components) trong nÃºt cá»• chai phÃ¬nh to, sau Ä‘Ã³ cá»‘ gáº¯ng dÃ² tÃ¬m xem cÃ³ báº¥t ká»³ Circuit Ä‘Æ¡n láº» nÃ o cháº» riÃªng Ä‘Æ°á»£c khÃ¡i niá»‡m "Quá»‘c gia / LÃ£nh thá»•" ra khá»i bÃ i vÄƒn hay khÃ´ng.

---

## 2. Thiáº¿t Láº­p Thuáº­t ToÃ¡n Má»Ÿ Rá»™ng Nháº­n Diá»‡n (Methodology)

### 2.1. Giáº£i PhÃ³ng Cáº¥u TrÃºc Báº±ng MÃ³c Ná»‘i Tied Weights
Vá»›i sá»‘ lÆ°á»£ng dá»¯ liá»‡u huáº¥n luyá»‡n quÃ¡ má»ng (chá»‰ 220 máº«u x $768$ Feature), mÃ  Táº§ng Latent láº¡i má»Ÿ rá»™ng lÃªn Ä‘áº¿n hÆ¡n $6000$ Feature, bÃ i toÃ¡n sáº½ lÃ¢m vÃ o há»a QuÃ¡ Khá»›p (Overfitting) vÃ¬ lÆ°á»£ng Tham sá»‘ Parameter khá»•ng lá»“. 
Äá»ƒ vÃ¡ lá»—i, Thuáº­t toÃ¡n Ã©p dÃ¹ng ká»¹ thuáº­t Trá»ng sá»‘ TÃ¡i cháº¿ (Tied Weights):
$\text{Decoder\_Weight} = \text{Encoder\_Weight}^T$
Viá»‡c loáº¡i bá» ma tráº­n há»c Decoder riÃªng vÃ  dÃ¹ng Ma tráº­n chuyá»ƒn vá»‹ (Transpose) cá»§a pháº§n Encoder giÃºp giáº£m 50% sá»‘ lÆ°á»£ng biáº¿n sá»‘ tá»± do, thÃºc Ä‘áº©y mÃ´ hÃ¬nh há»c mÆ°á»£t mÃ  hÆ¡n.

### 2.2. Kiá»ƒm SoÃ¡t Sá»± ThÆ°a Thá»›t Báº±ng Ká»¹ Thuáº­t Lá»c Top-K Sparsity
Thay vÃ¬ dÃ¹ng "thuá»‘c kÃ¬m hÃ£m" L1 Regularization truyá»n thá»‘ng, á»Ÿ cáº¥u trÃºc nÃ y hÃ m $ReLU$ Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m Non-linearity kÃ¨m Bá»™ Lá»c Cháº¯n (Thresholding).
Thay vÃ¬ chÃ¨n Ã©p má»i thÃ´ng sá»‘, ta chá»‰ báº£o toÃ n CÃ¡c káº¿t quáº£ $K$ Äiá»ƒm sÃ¡ng máº¡nh nháº¥t. VÃ­ dá»¥, cho biáº¿n $k = 0.5 \times Dimensions$: Trong má»—i Vector kÃ­ch hoáº¡t, SAE sáº½ dáº­p bá» $50\%$ sá»‘ NÆ¡-ron cÃ³ giÃ¡ trá»‹ Ä‘iá»‡n Ã¡p tháº¥p tuyá»‡t Ä‘á»‘i vá» $0$. Ká»¹ thuáº­t "Diá»‡t cá» táº­n gá»‘c" nÃ y Ã©p cho Máº¡ng pháº£i chuyá»ƒn dá»“n sá»©c chá»©a vÃ o má»™t há»‡ sinh thÃ¡i SiÃªu thÆ°a thá»›t (Super Sparsity). 

---

## 3. Kháº£o SÃ¡t & Giáº£i Pháº«u MÃ´ HÃ¬nh (Analysis)

### 3.1. TÃ­nh Tráº¡ng KhÃ´ng Gian Hoáº¡t HÃ³a SiÃªu PhÃ¢n TÃ¡n (Sparsity Distribution)
Háº­u huáº¥n luyá»‡n báº±ng Adam Optimizer ($100\ Epochs, LR=0.0001$), káº¿t quáº£ Density Matrix (Máº­t Ä‘á»™ tÃ­n hiá»‡u) phÆ¡i bÃ y cáº£nh sáº¯c hoang tÃ n:
- Má»©c Ä‘á»™ Máº­t Ä‘á»™ thÆ°a toÃ n khá»‘i (Sparsity Volume) tÃ n lá»¥i cá»±c máº¡nh, khiáº¿n tá»•ng Cáº¥u trÃºc lÆ°á»›i chá»‰ cÃ²n $\sim 6.5\%$ sá»‘ng sÃ³t. 
- Diá»…n giáº£i Ä‘a hÃ¬nh thÃ¡i: Trong sá»‘ hÆ¡n $6000$ Vi máº¡ch tiá»m áº©n (Latent Components): 
  - Pháº§n lá»›n (HÆ¡n phÃ¢n ná»­a) **Tuyá»‡t Ä‘á»‘i khÃ´ng pháº£n á»©ng (Tá»‹t ngÃ²i 100%)** trÆ°á»›c báº¥t cá»© má»™t Token nÃ o trong tá»•ng sá»‘ 220 Tá»« vá»±ng Ä‘áº§u vÃ o.
  - Má»™t thiá»ƒu sá»‘ siÃªu hiáº¿m **Hoáº¡t Ä‘á»™ng liÃªn há»“i** cho $100\%$ cÃ¡c Tokens. 
  - Ráº£i rÃ¡c vÃ i cá»¥c bá»™ cÃ³ chá»©c nÄƒng pháº£n á»©ng chá»n lá»c Ngá»¯ phÃ¡p.

### 3.2. Rá»§i Ro TÃ­nh ToÃ¡n Thá»‘ng KÃª Trong Lá»c Dá»¯ Liá»‡u
Ta láº­p má»™t Trá»ng sá»‘ Äá»‹a lÃ½ (Geography Selectivity Score) $=$ Chia giÃ¡ trá»‹ Activations cá»§a chuá»—i tá»« $\{}$"Romania", "Hungary", "Greece"$\}$ cho kÃ­ch hoáº¡t cá»§a Ä‘Ã¡m tá»« thá»«a thÃ£i nhÆ° "fruit, alcohol". Ráº¥t nhiá»u Component Ä‘Æ°á»£c Ä‘iá»ƒm Cao ngáº¥t ngÆ°á»Ÿng.
Lá»—i diá»…n giáº£i (Interpretability trap) xáº£y ra á»Ÿ Ä‘Ã¢y: 
Viá»‡c chÃ¨n bá»™ lá»c cÆ¡ há»c dá»… khiáº¿n ta nháº§m tÆ°á»Ÿng ta Ä‘Ã£ tÃ¬m ra "Bá»™ NÃ£o Äá»‹a lÃ½" (Äá»‹a danh). Khi váº½ Heatmap, Ä‘Ãºng lÃ  cÃ¡c NÆ¡-ron Latent nÃ y cÃ³ "chÃ¡y sÃ¡ng" táº¡i khu vá»±c "Czech Republic, Union". NhÆ°ng Ä‘á»“ng thá»i nÃ³ láº¡i nhiá»…u loáº¡n chÃ¡y sÃ¡ng bÃ¹ng ná»• vÃ´ cÄƒn cá»© á»Ÿ cÃ¡c cá»¥m tá»« ngá»¯ phÃ¡p vÃ´ nghÄ©a nhÆ° ngoáº·c Ä‘Æ¡n $()$, dáº¥y pháº©y $,$, hay chá»¯ "naught". 
Sá»± tháº¥t báº¡i cá»¥c bá»™ trong viá»‡c dÃ² tÃ¬m ra "Máº¡ch Ã½ nghÄ©a tuyá»‡t Ä‘á»‘i" nÃ y chá»©ng minh quy luáº­t: Dá»¯ liá»‡u quÃ¡ nhá» lÃ m sai lá»‡ch Há»‡ sá»‘ ThÆ°a thá»›t (Sparse Representation), táº¡o ra rÃ¡c Ä‘á»“ng quy (Correlated Redundant Noise) thay vÃ¬ TrÃ­ tuá»‡ trá»«u tÆ°á»£ng sáº¯c nÃ©t.

---

## 4. Káº¿t Luáº­n
Autoencoder lÃ  thiáº¿t cháº¿ dÃ² tÃ¬m SiÃªu KhÃ¡i Niá»‡m cá»±c máº¡nh, nhÆ°ng nÃ³ khÃ´ng pháº£i MÅ©i TÃªn Báº¡c thuáº­t giáº£ kim. Khi thao tÃ¡c trÃªn Datasets thá»±c táº¿ nhÆ°ng thiáº¿u khá»‘i lÆ°á»£ng máº«u, hÃ nh Ä‘á»™ng CÆ°á»¡ng Ã©p TÃ­nh chÃªnh lá»‡ch (Top-K / Tied Weights) cÃ³ thá»ƒ gÃ¢y ra hiá»‡n tÆ°á»£ng PhÃ¢n Máº¡ch Giáº£ (Proxy Circuits) cÃ³ Ä‘iá»ƒm sá»‘ ToÃ¡n há»c cao nhÆ°ng phi Ã½ nghÄ©a Logic (Semantic Invalidity). NÃ³ nháº¥n máº¡nh tiÃªu chuáº©n vÃ ng: Trong Giáº£i diá»…n CÆ¡ há»c, Thá»‘ng KÃª Äiá»ƒm Sá»‘ báº¯t buá»™c pháº£i Ä‘Æ°á»£c Ä‘i Ä‘Ã´i vá»›i QuÃ¡ TrÃ¬nh Soi Äo Trá»±c Quan (Visual Inspection) má»™t cÃ¡ch cháº·t cháº½. á» chÆ°Æ¡ng tiáº¿p theo, ta sáº½ dÃ¹ng SAE Ä‘á»ƒ quÃ©t qua toÃ n bá»™ cáº¥u trÃºc Laminar nhiá»u táº§ng cáº¯t thay vÃ¬ Ä‘Ã¢m trá»¥ má»™t má» Ä‘Æ¡n láº».

---

## TÃ i Liá»‡u Tham Kháº£o (Citations)
1. ThÃ­ nghiá»‡m á»©ng dá»¥ng Sparse Autoencoder báº±ng bá»™ lá»c Top-K thay cho L1 Penalty trÃªn dá»¯ liá»‡u GPT-2 Small Hook tá»« module `aero_LLM_09_SAE in GPT2 learns about Hungarian Palinka.md`. Äiá»ƒm xuyáº¿t thá»§ thuáº­t KhÃ³a cháº·n Kiáº¿n trÃºc Transpose Encoder-Decoder Tied Weights.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Máº¡ng Máº¡ch Thuáº­t ToÃ¡n (Circuits) Trong MÃ´ HÃ¬nh Há»c SÃ¢u](aero_llm_01_what_is_a_circuit_in_a_dl_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_what_is_a_circuit_in_a_dl_model.md) |
| [CÃ´ Láº­p VÃ  ThÄƒm DÃ² Khá»‘i ChÃº Ã (Attention Heads)](aero_llm_02_isolating_and_investigating_attention_heads.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_isolating_and_investigating_attention_heads.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: Biá»ƒu Diá»…n PhÃ¢n Bá»‘ Nhiá»‡t Laminar Cá»§a Trá»ng Sá»‘ ChÃº Ã](aero_llm_03_codechallenge_laminar_profile_of_attention_head_weights.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_laminar_profile_of_attention_head_weights.md) |
| [Kháº£o SÃ¡t TÆ°Æ¡ng Quan Cá»¥m (Clustering) Vi Máº¡ch (Circuits) Trong KhÃ´ng Gian Giáº£m Chiá»u](aero_llm_04_are_circuits_clustered_in_low_dimensional_space.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_are_circuits_clustered_in_low_dimensional_space.md) |
| [LÃ½ Thuyáº¿t VÃ  á»¨ng Dá»¥ng Cá»§a Ká»¹ Thuáº­t DÃ² ThÆ°a (Sparse Probing)](aero_llm_05_sparse_probing_theory_and_code.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_sparse_probing_theory_and_code.md) |
| [ThÃ¡ch Thá»©c Cá»§a TÃ­n Hiá»‡u ThÆ°a Trong Dá»¯ Liá»‡u Táº­p Lá»›n (Statistical Suppression)](aero_llm_06_challenges_with_sparse_logistic_regression_in_large_datasets.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_challenges_with_sparse_logistic_regression_in_large_datasets.md) |
| [Biáº¿n Tiá»m áº¨n (Latent) VÃ  Biáº¿n Hiá»ƒn NgÃ´n (Manifest) Trong Giáº£i Diá»…n AI](aero_llm_07_latent_vs_manifest_variables.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_latent_vs_manifest_variables.md) |
| [MÃ´ HÃ¬nh Sparse Autoencoders (SAEs): LÃ½ Thuyáº¿t VÃ  Kiáº¿n TrÃºc KhÃ´i Phá»¥c Vi Máº¡ch Tiá»m áº¨n](aero_llm_08_sparse_autoencoders_theory_and_code.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_sparse_autoencoders_theory_and_code.md) |
| ğŸ“Œ **[Huáº¥n Luyá»‡n Sparse Autoencoder TrÃ­ch Xuáº¥t KhÃ¡i Niá»‡m Ngá»¯ Cáº£nh Palinka TrÃªn GPT-2](aero_llm_09_sae_in_gpt2_learns_about_hungarian_palinka.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_09_sae_in_gpt2_learns_about_hungarian_palinka.md) |
| [Kháº£o SÃ¡t PhÃ¢n Táº§ng KÃ­ch Hoáº¡t (Laminar Profile) Qua Sparse Autoencoder](aero_llm_10_codechallenge_laminar_profile_of_autoencoder_sparsity.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_codechallenge_laminar_profile_of_autoencoder_sparsity.md) |
| [Nháº­n Diá»‡n KhÃ¡i Niá»‡m XuyÃªn TÃ¢m Vá»›i PhÃ¢n RÃ£ GiÃ¡ Trá»‹ RiÃªng Suy Rá»™ng (Generalized Eigendecomposition - GED)](aero_llm_11_non_orthogonal_latent_components_via_eigendecomposition_theory_and_demo_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_non_orthogonal_latent_components_via_eigendecomposition_theory_and_demo_.md) |
| [Ráº¡ch RÃ²i Giá»›i TÃ­nh (Him vs Her) Báº±ng Generalized Eigendecomposition Trong MLP](aero_llm_12_generalized_eigendecomposition_separates_him_from_her_in_mlp.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_generalized_eigendecomposition_separates_him_from_her_in_mlp.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): TÃ¡ch NhÃ³m GED Äa Táº§ng (Pháº§n 1)](aero_llm_13_codechallenge_ged_for_category_isolation_across_layers_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_codechallenge_ged_for_category_isolation_across_layers_part_1_.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): TÃ¡ch NhÃ³m GED Äa Táº§ng (Pháº§n 2) & Kiá»ƒm Chá»©ng ChÃ©o](aero_llm_14_codechallenge_ged_for_category_isolation_across_layers_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_codechallenge_ged_for_category_isolation_across_layers_part_2_.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

# üìÇ Index ‚Äî 06-pretraining

[‚Üê Quay l·∫°i danh m·ª•c ch√≠nh](../README.md)

## üìÑ T√†i li·ªáu trong m·ª•c n√†y

- [üìò Hu·∫•n Luy·ªán M√¥ H√¨nh Ng√¥n Ng·ªØ V·ªõi Thi√™n L·ªách C√≥ Ch·ªß ƒê√≠ch B·∫±ng KL-Divergence: M·ªôt Nghi√™n C·ª©u Th·ª±c Nghi·ªám](aero_LLM_010_CodeChallenge Train a model to like X.md)
- [üìò C√°c V·∫•n ƒê·ªÅ T·ª∑ L·ªá S·ªë H·ªçc Trong M√¥ H√¨nh H·ªçc S√¢u: Ph√¢n T√≠ch Vai Tr√≤ C·ªßa Scaling v√† Normalization Trong C∆° Ch·∫ø Attention](aero_LLM_011_CodeChallenge Numerical scaling issues in DL models copy 2.md)
- [**Weight Initialization and Numerical Stability in Large Language Models**](aero_LLM_012_Weight initializations.md)
- [**Ph√¢n T√≠ch ·∫¢nh H∆∞·ªüng C·ªßa Kh·ªüi T·∫°o Tr·ªçng S·ªë V√† S·ª± Ti·∫øn H√≥a Ph√¢n Ph·ªëi Tham S·ªë Trong Qu√° Tr√¨nh Hu·∫•n Luy·ªán M√¥ H√¨nh Transformer**](aero_LLM_013_CodeChallenge Train model 5 with weight inits.md)
- [**Dropout as a Regularization Mechanism in Large Language Models: Theory, Implementation, and Practical Implications**](aero_LLM_014_Dropout in theory and in Pytorch.md)
- [**So S√°nh ƒê·∫ßu Ra Logits v√† Log-Softmax Trong M√¥ H√¨nh Ng√¥n Ng·ªØ: T√°c ƒê·ªông ƒê·∫øn Hu·∫•n Luy·ªán v√† Sinh VƒÉn B·∫£n**](aero_LLM_015_Should you output logits or log-softmax(logits).md)
- [aero_LLM_016_The FineWeb dataset.md](aero_LLM_016_The FineWeb dataset.md)
- [**T√≠ch H·ª£p Dropout Trong M√¥ H√¨nh Ng√¥n Ng·ªØ Transformer: Ph√¢n T√≠ch Tr∆∞·ªùng H·ª£p Model 5**](aero_LLM_017_CodeChallenge Fine dropout in model 5 (part 1.md)
- [**Chi·∫øn L∆∞·ª£c Hu·∫•n Luy·ªán D·ª±a Tr√™n Final-Token Loss Trong M√¥ H√¨nh Transformer: Ph√¢n T√≠ch Tr∆∞·ªùng H·ª£p Model 5 V·ªõi Dropout**](aero_LLM_018_CodeChallenge Fine dropout in model 5 (part 2).md)
- [Ph√¢n T√≠ch H√†nh Vi H·ªçc Bi·ªÉu Di·ªÖn Token Trong M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn](aero_LLM_019_CodeChallenge What happens to unused tokens..md)
- [üìò Vai Tr√≤ C·ªßa Pre-training Trong M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn: Ph√¢n T√≠ch Chi Ph√≠, Hi·ªáu Qu·∫£ v√† T√≠nh ·ª®ng D·ª•ng](aero_LLM_01_What is pretraining.md)
- [T·ªëi ∆Øu H√≥a Qu√° Tr√¨nh Ti·ªÅn Hu·∫•n Luy·ªán M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn: Ph√¢n T√≠ch C√°c Chi·∫øn L∆∞·ª£c T√≠nh To√°n v√† H·ªçc T·∫≠p](aero_LLM_020_Optimization options.md)
- [üìò N·ªÅn T·∫£ng Hugging Face Trong H·ªá Sinh Th√°i Tr√≠ Tu·ªá Nh√¢n T·∫°o: Vai Tr√≤, C·∫•u Tr√∫c v√† ·ª®ng D·ª•ng Trong Nghi√™n C·ª©u M√¥ H√¨nh Ng√¥n Ng·ªØ](aero_LLM_02_huggingface.md)
- [üìò Thu·∫≠t To√°n T·ªëi ∆Øu AdamW Trong Hu·∫•n Luy·ªán M√¥ H√¨nh H·ªçc S√¢u: C∆° S·ªü L√Ω Thuy·∫øt, C·∫£i Ti·∫øn v√† ·ª®ng D·ª•ng](aero_LLM_03_The AdamW optimizer.md)
- [üìò So S√°nh SGD, Adam v√† AdamW Trong Hu·∫•n Luy·ªán M√¥ H√¨nh H·ªçc S√¢u: Ph√¢n T√≠ch Th·ª±c Nghi·ªám v√† ·ª®ng D·ª•ng](aero_LLM_04_CodeChallenge SGD vs. Adam vs. AdamW..md)
- [üìò Hu·∫•n Luy·ªán M√¥ H√¨nh Ng√¥n Ng·ªØ ƒê∆°n Gi·∫£n B·∫±ng PyTorch: Ph√¢n T√≠ch Quy Tr√¨nh, ƒê·ªông L·ª±c H·ªçc v√† Hi·ªáu Su·∫•t Th·ª±c Nghi·ªám](aero_LLM_05_Train model.md)
- [üìò Thi·∫øt L·∫≠p T·∫≠p Ki·ªÉm Th·ª≠ Trong Hu·∫•n Luy·ªán M√¥ H√¨nh Ng√¥n Ng·ªØ: Ph√¢n T√≠ch Ph∆∞∆°ng Ph√°p Train‚ÄìTest Split v√† ƒê√°nh Gi√° Hi·ªáu Su·∫•t](aero_LLM_06_CodeChallenge Add a test set.md)
- [üìò Chuy·ªÉn Giao Tr·ªçng S·ªë v√† ƒê√≥ng BƒÉng Tham S·ªë Trong Hu·∫•n Luy·ªán M√¥ H√¨nh Ng√¥n Ng·ªØ: Ph√¢n T√≠ch Th·ª±c Nghi·ªám V·ªõi Embedding GPT-2](aero_LLM_07_CodeChallenge Train model 1 with GPT2's embeddings.md)
- [üìò Ph∆∞∆°ng Ph√°p L·∫•y M·∫´u Ng·∫´u Nhi√™n v√† Hu·∫•n Luy·ªán M√¥ H√¨nh GPT-2 Thu G·ªçn: Ph√¢n T√≠ch Th·ª±c Nghi·ªám V·ªõi D·ªØ Li·ªáu VƒÉn B·∫£n C·ªï ƒêi·ªÉn](aero_LLM_08_CodeChallenge Train model 5 with modifications.md)
- [**Thi·∫øt K·∫ø H√†m M·∫•t M√°t T√πy Bi·∫øn Trong Hu·∫•n Luy·ªán M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn**](aero_LLM_09_Create a custom loss function.md)

---
*T·ª± ƒë·ªông c·∫≠p nh·∫≠t b·ªüi Aero-Indexer*
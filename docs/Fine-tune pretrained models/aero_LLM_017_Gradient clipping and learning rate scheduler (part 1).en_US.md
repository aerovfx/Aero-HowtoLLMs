

# üìò ·ª®ng D·ª•ng Gradient Clipping v√† Learning Rate Scheduler Trong Hu·∫•n Luy·ªán M√¥ H√¨nh H·ªçc S√¢u

## T√≥m t·∫Øt (Abstract)

Trong qu√° tr√¨nh hu·∫•n luy·ªán c√°c m√¥ h√¨nh h·ªçc s√¢u quy m√¥ l·ªõn, hi·ªán t∆∞·ª£ng m·∫•t ·ªïn ƒë·ªãnh s·ªë h·ªçc v√† h·ªôi t·ª• k√©m th∆∞·ªùng xuy√™n x·∫£y ra. Hai k·ªπ thu·∫≠t ph·ªï bi·∫øn nh·∫±m kh·∫Øc ph·ª•c v·∫•n ƒë·ªÅ n√†y l√† Gradient Clipping v√† Learning Rate Scheduler. B√†i vi·∫øt tr√¨nh b√†y nguy√™n l√Ω, c∆° s·ªü to√°n h·ªçc v√† ·ª©ng d·ª•ng th·ª±c nghi·ªám c·ªßa hai ph∆∞∆°ng ph√°p tr√™n, d·ª±a tr√™n t√†i li·ªáu hu·∫•n luy·ªán th·ª±c t·∫ø. K·∫øt qu·∫£ cho th·∫•y vi·ªác √°p d·ª•ng h·ª£p l√Ω c√°c k·ªπ thu·∫≠t n√†y gi√∫p tƒÉng t√≠nh ·ªïn ƒë·ªãnh v√† ƒë·ªô tin c·∫≠y c·ªßa qu√° tr√¨nh t·ªëi ∆∞u.

---

## 1. Gi·ªõi thi·ªáu

Hu·∫•n luy·ªán m·∫°ng n∆°-ron s√¢u th∆∞·ªùng d·ª±a tr√™n ph∆∞∆°ng ph√°p t·ªëi ∆∞u gradient descent. Tuy nhi√™n, v·ªõi c√°c m√¥ h√¨nh l·ªõn, gradient c√≥ th·ªÉ tr·ªü n√™n r·∫•t l·ªõn (gradient explosion), d·∫´n ƒë·∫øn:

* M·∫•t ·ªïn ƒë·ªãnh s·ªë h·ªçc
* Sai l·ªách qu√° tr√¨nh c·∫≠p nh·∫≠t
* M√¥ h√¨nh kh√¥ng h·ªôi t·ª•

Theo t√†i li·ªáu h∆∞·ªõng d·∫´n , hai k·ªπ thu·∫≠t th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ n√†y l√†:

* Gradient Clipping
* Learning Rate Scheduler

M·ª•c ti√™u nghi√™n c·ª©u g·ªìm:

* Ph√¢n t√≠ch c∆° ch·∫ø ho·∫°t ƒë·ªông c·ªßa hai k·ªπ thu·∫≠t
* Tr√¨nh b√†y c√¥ng th·ª©c to√°n h·ªçc li√™n quan
* ƒê√°nh gi√° ·∫£nh h∆∞·ªüng t·ªõi qu√° tr√¨nh h·ªçc
* ƒê·ªÅ xu·∫•t h∆∞·ªõng √°p d·ª•ng th·ª±c t·∫ø

---

## 2. C∆° s·ªü l√Ω thuy·∫øt

### 2.1 Gradient Descent

Qu√° tr√¨nh c·∫≠p nh·∫≠t tham s·ªë trong h·ªçc s√¢u ƒë∆∞·ª£c m√¥ t·∫£ b·ªüi:

[
\theta_{t+1}=\theta_t-\eta \nabla_\theta L(\theta_t)
]

Trong ƒë√≥:

* (\theta_t): tham s·ªë t·∫°i b∆∞·ªõc (t)
* (\eta): learning rate
* (L): h√†m m·∫•t m√°t
* (\nabla_\theta L): gradient

Khi (|\nabla_\theta L|) qu√° l·ªõn, c·∫≠p nh·∫≠t tham s·ªë tr·ªü n√™n kh√¥ng ·ªïn ƒë·ªãnh.

---

### 2.2 Chu·∫©n c·ªßa Gradient

Chu·∫©n Euclid c·ªßa gradient:

[
|\mathbf{g}|*2=\sqrt{\sum*{i=1}^{n}g_i^2}
]

Trong ƒë√≥:

* (\mathbf{g}): vector gradient
* (g_i): ph·∫ßn t·ª≠ th·ª© (i)

Gradient explosion x·∫£y ra khi:

[
|\mathbf{g}|_2 \gg 1
]

---

## 3. Ph∆∞∆°ng ph√°p nghi√™n c·ª©u

### 3.1 Gradient Clipping

#### 3.1.1 Kh√°i ni·ªám

Gradient clipping l√† k·ªπ thu·∫≠t gi·ªõi h·∫°n ƒë·ªô l·ªõn c·ªßa gradient nh·∫±m tr√°nh c·∫≠p nh·∫≠t qu√° m·ª©c.

Theo t√†i li·ªáu , thay v√¨ c·∫Øt t·ª´ng ph·∫ßn t·ª≠ ri√™ng l·∫ª, to√†n b·ªô vector gradient ƒë∆∞·ª£c chu·∫©n h√≥a.

---

#### 3.1.2 C√¥ng th·ª©c to√°n h·ªçc

V·ªõi ng∆∞·ª°ng (c), gradient sau clipping:

[
\mathbf{g}_{clip}=
\begin{cases}
\mathbf{g} & \text{n·∫øu } |\mathbf{g}|\le c\
\frac{c}{|\mathbf{g}|}\mathbf{g} & \text{n·∫øu } |\mathbf{g}|>c
\end{cases}
]

ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o:

[
|\mathbf{g}_{clip}|\le c
]

---

#### 3.1.3 C·∫≠p nh·∫≠t tham s·ªë

Sau clipping:

[
\theta_{t+1}=\theta_t-\eta \mathbf{g}_{clip}
]

Vi·ªác n√†y gi√∫p gi·ªõi h·∫°n b∆∞·ªõc nh·∫£y c·ªßa tham s·ªë.

---

### 3.2 Learning Rate Scheduler

#### 3.2.1 Kh√°i ni·ªám

Learning rate scheduler l√† k·ªπ thu·∫≠t thay ƒë·ªïi learning rate theo th·ªùi gian hu·∫•n luy·ªán.

Theo , vi·ªác duy tr√¨ learning rate c·ªë ƒë·ªãnh c√≥ th·ªÉ l√†m gi·∫£m hi·ªáu qu·∫£ h·ªçc v·ªõi m√¥ h√¨nh l·ªõn.

---

#### 3.2.2 Warm-up

Trong giai ƒëo·∫°n kh·ªüi ƒë·ªông:

[
\eta_t=\eta_{max}\cdot\frac{t}{T_{warm}}
]

Trong ƒë√≥:

* (T_{warm}): s·ªë epoch warm-up
* (\eta_{max}): learning rate c·ª±c ƒë·∫°i

---

#### 3.2.3 Cosine Scheduler

H√†m cosine decay:

[
\eta_t=\eta_{min}+\frac{1}{2}(\eta_{max}-\eta_{min})\left(1+\cos\frac{\pi t}{T}\right)
]

Trong ƒë√≥:

* (T): t·ªïng s·ªë epoch
* (\eta_{min}): learning rate t·ªëi thi·ªÉu

---

#### 3.2.4 Linear Scheduler

Gi·∫£m tuy·∫øn t√≠nh:

[
\eta_t=\eta_{max}\left(1-\frac{t}{T}\right)
]

---

### 3.3 K·∫øt h·ª£p Clipping v√† Scheduler

Quy tr√¨nh hu·∫•n luy·ªán:

1. T√≠nh gradient
2. √Åp d·ª•ng clipping
3. C·∫≠p nh·∫≠t learning rate
4. C·∫≠p nh·∫≠t tham s·ªë

[
\theta_{t+1}=\theta_t-\eta_t\cdot \mathbf{g}_{clip}
]

---

## 4. Th·ª±c nghi·ªám

### 4.1 M√¥ h√¨nh minh h·ªça

Theo m√¥ t·∫£ trong t√†i li·ªáu , m√¥ h√¨nh g·ªìm:

* Hai tham s·ªë tr·ªçng s·ªë
* H√†m m·∫•t m√°t L2
* SGD optimizer

Loss function:

[
L=\sum_{i=1}^{n}w_i^2
]

---

### 4.2 ·∫¢nh h∆∞·ªüng c·ªßa Gradient Clipping

| Tr·∫°ng th√°i     | Chu·∫©n Gradient | T·ªëc ƒë·ªô h·ªçc                |
| -------------- | -------------- | ------------------------- |
| Kh√¥ng clipping | > 10           | Nhanh nh∆∞ng kh√¥ng ·ªïn ƒë·ªãnh |
| C√≥ clipping    | = 1            | Ch·∫≠m, ·ªïn ƒë·ªãnh             |

Clipping gi√∫p gi·∫£m hi·ªán t∆∞·ª£ng gradient explosion nh∆∞ng l√†m ch·∫≠m t·ªëc ƒë·ªô h·ªôi t·ª•.

---

### 4.3 ·∫¢nh h∆∞·ªüng c·ªßa Scheduler

K·∫øt qu·∫£ cho th·∫•y:

* Giai ƒëo·∫°n ƒë·∫ßu: h·ªçc ·ªïn ƒë·ªãnh
* Giai ƒëo·∫°n sau: gi·∫£m dao ƒë·ªông
* Tr√°nh overfitting

Learning curve m∆∞·ª£t h∆°n khi d√πng scheduler.

---

### 4.4 So s√°nh t·ªïng h·ª£p

| Ph∆∞∆°ng ph√°p   | ·ªîn ƒë·ªãnh    | H·ªôi t·ª•     | Hi·ªáu qu·∫£   |
| ------------- | ---------- | ---------- | ---------- |
| Kh√¥ng d√πng    | Th·∫•p       | K√©m        | Trung b√¨nh |
| Ch·ªâ clipping  | Trung b√¨nh | Trung b√¨nh | T·ªët        |
| Ch·ªâ scheduler | T·ªët        | T·ªët        | T·ªët        |
| K·∫øt h·ª£p       | R·∫•t t·ªët    | Cao        | R·∫•t t·ªët    |

---

## 5. Th·∫£o lu·∫≠n

### 5.1 L·ª£i √≠ch c·ªßa Gradient Clipping

Theo ph√¢n t√≠ch t·ª´ :

* NgƒÉn gradient explosion
* ·ªîn ƒë·ªãnh s·ªë h·ªçc
* Ph√π h·ª£p m√¥ h√¨nh l·ªõn

Tuy nhi√™n, l√†m m·∫•t th√¥ng tin v·ªÅ ƒë·ªô l·ªõn gradient.

---

### 5.2 Vai tr√≤ c·ªßa Learning Rate Scheduler

Scheduler gi√∫p:

* Tr√°nh c·∫≠p nh·∫≠t qu√° m·∫°nh ban ƒë·∫ßu
* Tinh ch·ªânh ·ªü giai ƒëo·∫°n cu·ªëi
* C·∫£i thi·ªán kh·∫£ nƒÉng h·ªôi t·ª•

ƒê·∫∑c bi·ªát hi·ªáu qu·∫£ v·ªõi Transformer v√† LLM.

---

### 5.3 H·∫°n ch·∫ø

* C·∫ßn tinh ch·ªânh si√™u tham s·ªë
* Kh√¥ng ph√π h·ª£p m√¥ h√¨nh nh·ªè
* C√≥ th·ªÉ l√†m ch·∫≠m hu·∫•n luy·ªán

Do ƒë√≥ c·∫ßn l·ª±a ch·ªçn ph√π h·ª£p v·ªõi b√†i to√°n.

---

## 6. K·∫øt lu·∫≠n

B√†i vi·∫øt ƒë√£ tr√¨nh b√†y c∆° s·ªü l√Ω thuy·∫øt v√† th·ª±c nghi·ªám c·ªßa Gradient Clipping v√† Learning Rate Scheduler trong hu·∫•n luy·ªán h·ªçc s√¢u.

K·∫øt qu·∫£ cho th·∫•y:

* Gradient Clipping gi√∫p ·ªïn ƒë·ªãnh qu√° tr√¨nh t·ªëi ∆∞u
* Scheduler c·∫£i thi·ªán h·ªôi t·ª•
* K·∫øt h·ª£p hai ph∆∞∆°ng ph√°p cho hi·ªáu qu·∫£ cao nh·∫•t

C√°c k·ªπ thu·∫≠t n√†y ƒë·∫∑c bi·ªát quan tr·ªçng trong hu·∫•n luy·ªán m√¥ h√¨nh l·ªõn v√† h·ªá th·ªëng AI hi·ªán ƒë·∫°i.

---

## T√†i li·ªáu tham kh·∫£o

1. Gradient Clipping and Learning Rate Scheduler Tutorial 
2. Goodfellow, I., Bengio, Y., Courville, A. (2016). Deep Learning. MIT Press.
3. Loshchilov, I., Hutter, F. (2017). SGDR: Stochastic Gradient Descent with Warm Restarts.


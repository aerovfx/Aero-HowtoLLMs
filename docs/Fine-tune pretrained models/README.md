
# ğŸ—‚ Chá»‰ Má»¥c: Fine-tuning Pretrained Models

ChÃ o má»«ng báº¡n Ä‘áº¿n vá»›i má»¥c tÃ i liá»‡u vá» **Fine-tuning cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ tiá»n huáº¥n luyá»‡n**. DÆ°á»›i Ä‘Ã¢y lÃ  danh sÃ¡ch cÃ¡c bÃ i há»c vÃ  thá»­ thÃ¡ch mÃ£ nguá»“n (Code Challenges) Ä‘Æ°á»£c sáº¯p xáº¿p theo trÃ¬nh tá»±.

---

### ğŸ“š Danh SÃ¡ch CÃ¡c BÃ i Há»c

1.  **[ChÆ°Æ¡ng 01: Tinh chá»‰nh (Fine-tuning) cÃ³ nghÄ©a lÃ  gÃ¬?](./aero_LLM_01_What does fine-tuning mean.md)**
    *   Giá»›i thiá»‡u vá» khÃ¡i niá»‡m vÃ  má»¥c Ä‘Ã­ch cá»§a viá»‡c tinh chá»‰nh mÃ´ hÃ¬nh.
2.  **[ChÆ°Æ¡ng 02: Tinh chá»‰nh mÃ´ hÃ¬nh GPT-2 tiá»n huáº¥n luyá»‡n](./aero_LLM_02_Fine-tune a pretrained GPT2.md)**
    *   HÆ°á»›ng dáº«n thá»±c hÃ nh tinh chá»‰nh GPT-2 trÃªn tÃ¡c pháº©m *Gulliver's Travels*.
3.  **[Thá»­ thÃ¡ch 03: Tá»‘c Ä‘á»™ há»c cá»§a Gulliver](./aero_LLM_03CodeChallenge Gulliver's learning rates.md)**
    *   ÄÃ¡nh giÃ¡ áº£nh hÆ°á»Ÿng cá»§a Learning Rate Ä‘áº¿n hiá»‡u suáº¥t mÃ´ hÃ¬nh.
4.  **[ChÆ°Æ¡ng 04: Quy trÃ¬nh sinh vÄƒn báº£n tá»« cÃ¡c mÃ´ hÃ¬nh tiá»n huáº¥n luyá»‡n](./aero_LLM_04_On generating text from pretrained models.md)**
    *   NghiÃªn cá»©u cÃ¡ch mÃ´ hÃ¬nh sinh dá»¯ liá»‡u sau khi Ä‘Æ°á»£c huáº¥n luyá»‡n.
5.  **[Thá»­ thÃ¡ch 05: Tá»‘i Ä‘a hÃ³a yáº¿u tá»‘ X](./aero_LLM_05_CodeChallenge Maximize the X factor..md)**
    *   Sá»­ dá»¥ng KL Divergence Ä‘á»ƒ tá»‘i Æ°u hÃ³a viá»‡c sinh cÃ¡c token cá»¥ thá»ƒ.
6.  **[ChÆ°Æ¡ng 06: Alice vÃ  Edgar Allan Poe (vá»›i GPT-Neo)](./aero_LLM_06_Alice in Wonderland and Edgar Allen Poe (with GPT-neo).md)**
    *   Thá»±c hÃ nh tinh chá»‰nh phong cÃ¡ch vÄƒn há»c vá»›i GPT-Neo.
7.  **[Thá»­ thÃ¡ch 07: Äá»‹nh lÆ°á»£ng sá»± tinh chá»‰nh Alice-Edgar](./aero_LLM_07_CodeChallenge Quantify the AliceEdgar fine-tuning.md)**
    *   Sá»­ dá»¥ng BERT Ä‘á»ƒ Ä‘o lÆ°á»ng má»©c Ä‘á»™ thÃ nh cÃ´ng cá»§a viá»‡c chuyá»ƒn Ä‘á»•i phong cÃ¡ch.
8.  **[Thá»­ thÃ¡ch 08: Cuá»™c trÃ² chuyá»‡n giá»¯a Alice vÃ  Edgar](./aero_LLM_08_CodeChallenge A chat between Alice and Edgar.md)**
    *   MÃ´ phá»ng há»™i thoáº¡i giá»¯a hai mÃ´ hÃ¬nh mang phong cÃ¡ch khÃ¡c nhau.
9.  **[ChÆ°Æ¡ng 09: Tinh chá»‰nh tá»«ng pháº§n báº±ng cÃ¡ch Ä‘Ã³ng bÄƒng trá»ng sá»‘ Attention](./aero_LLM_09_Partial fine-tuning by freezing attention weights.md)**
    *   Chiáº¿n lÆ°á»£c Ä‘Ã³ng bÄƒng (freezing) Ä‘á»ƒ tá»‘i Æ°u hÃ³a tham sá»‘.
10. **[Thá»­ thÃ¡ch 10: Tinh chá»‰nh vÃ  Ä‘Ã³ng bÄƒng cÃ³ má»¥c tiÃªu (Pháº§n 1)](./aero_LLM_010_CodeChallenge Fine-tuning and targeted freezing (part 1).md)**
    *   Thá»±c hÃ nh ká»¹ thuáº­t Ä‘Ã³ng bÄƒng tham sá»‘ trÃªn BERT.
11. **[Thá»­ thÃ¡ch 11: Tinh chá»‰nh vÃ  Ä‘Ã³ng bÄƒng cÃ³ má»¥c tiÃªu (Pháº§n 2)](./aero_LLM_011_CodeChallenge Fine-tuning and targeted freezing (part 2).md)**
    *   Tiáº¿p tá»¥c tá»‘i Æ°u hÃ³a quy trÃ¬nh Ä‘Ã³ng bÄƒng Ä‘á»ƒ Ä‘áº¡t hiá»‡u suáº¥t cao hÆ¡n.
12. **[ChÆ°Æ¡ng 12: Tinh chá»‰nh hiá»‡u quáº£ tham sá»‘ (PEFT)](./aero_LLM_012_Parameter-efficient fine-tuning (PEFT).md)**
    *   Tá»•ng quan vá» cÃ¡c ká»¹ thuáº­t LoRA, Adapter, Prefix Tuning.
13. **[ChÆ°Æ¡ng 13: Sá»­ dá»¥ng CodeGen Ä‘á»ƒ hoÃ n thÃ nh mÃ£ nguá»“n](./aero_LLM_013_CodeGen for code completion.md)**
    *   Kiáº¿n trÃºc vÃ  á»©ng dá»¥ng cá»§a mÃ´ hÃ¬nh CodeGen trong láº­p trÃ¬nh.
14. **[Thá»­ thÃ¡ch 14: Tinh chá»‰nh CodeGen cho toÃ¡n giáº£i tÃ­ch](./aero_LLM_014_CodeChallenge Fine-tune codeGen for calculus.md)**
    *   Huáº¥n luyá»‡n mÃ´ hÃ¬nh sinh mÃ£ Python Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n toÃ¡n há»c.
15. **[ChÆ°Æ¡ng 15: Tinh chá»‰nh BERT cho bÃ i toÃ¡n phÃ¢n loáº¡i](./aero_LLM_015_Fine-tuning BERT for classification.md)**
    *   Cáº¥u trÃºc vÃ  quy trÃ¬nh fine-tuning BERT cho dá»¯ liá»‡u vÄƒn báº£n.
16. **[Thá»­ thÃ¡ch 16: PhÃ¢n tÃ­ch cáº£m xÃºc IMDB báº±ng BERT](./aero_LLM_016_CodeChallenge IMDB sentiment analysis using BERT.en_US.md)**
    *   Thá»±c hÃ nh phÃ¢n loáº¡i cáº£m xÃºc tÃ­ch cá»±c/tiÃªu cá»±c trÃªn dá»¯ liá»‡u Ä‘iá»‡n áº£nh.
17. **[ChÆ°Æ¡ng 17: Cáº¯t gradient vÃ  bá»™ Ä‘iá»u chá»‰nh tá»‘c Ä‘á»™ há»c (Pháº§n 1)](./aero_LLM_017_Gradient clipping and learning rate scheduler (part 1).en_US.md)**
    *   Ká»¹ thuáº­t á»•n Ä‘á»‹nh quÃ¡ trÃ¬nh huáº¥n luyá»‡n báº±ng Gradient Clipping.
18. **[ChÆ°Æ¡ng 18: Cáº¯t gradient vÃ  bá»™ Ä‘iá»u chá»‰nh tá»‘c Ä‘á»™ há»c (Pháº§n 2)](./aero_LLM_018_Gradient clipping and learning rate scheduler (part 2).md)**
    *   TÃ¬m hiá»ƒu sÃ¢u vá» Learning Rate Schedulers (Cosine, Linear).
19. **[Thá»­ thÃ¡ch 19: Cáº¯t, ÄÃ³ng bÄƒng vÃ  Äiá»u chá»‰nh BERT](./aero_LLM_019_CodeChallenge Clip, freeze, and schedule BERT.md)**
    *   Káº¿t há»£p cÃ¡c ká»¹ thuáº­t Ä‘á»ƒ tinh chá»‰nh BERT Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c ~90%.
20. **[ChÆ°Æ¡ng 20: LÆ°u vÃ  táº£i cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n](./aero_LLM_020_Saving and loading trained models.md)**
    *   Quáº£n lÃ½ tham sá»‘ mÃ´ hÃ¬nh trong PyTorch vÃ  Hugging Face.
21. **[ChÆ°Æ¡ng 21: BERT phÃ¢n loáº¡i Alice hay Edgar](./aero_LLM_021_BERT decides Alice or Edgar.md)**
    *   á»¨ng dá»¥ng BERT trong nghiÃªn cá»©u phong cÃ¡ch vÄƒn há»c sá»‘.
22. **[Thá»­ thÃ¡ch 22: Sá»± tiáº¿n hÃ³a cá»§a Alice vÃ  Edgar (Pháº§n 1)](./aero_LLM_022_CodeChallenge Evolution of Alice and Edgar (part 1).md)**
    *   Äá»“ng tiáº¿n hÃ³a há»‡ thá»‘ng sinh vÃ  phÃ¢n loáº¡i vÄƒn báº£n.
23. **[Thá»­ thÃ¡ch 23: Sá»± tiáº¿n hÃ³a cá»§a Alice vÃ  Edgar (Pháº§n 2)](./aero_LLM_023_CodeChallenge Evolution of Alice and Edgar (part 2).md)**
    *   ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng mÃ´ hÃ¬nh sinh thÃ´ng qua mÃ´ hÃ¬nh phÃ¢n loáº¡i.

---
*Ghi chÃº: CÃ¡c tÃ i liá»‡u Ä‘Æ°á»£c biÃªn soáº¡n nháº±m phá»¥c vá»¥ má»¥c tiÃªu nghiÃªn cá»©u vÃ  Ä‘Ã o táº¡o vá» LLM.*


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [09 quantitative evaluations](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Perplexity trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Diá»…n Giáº£i vÃ  Giá»›i Háº¡n

TÃ³m táº¯t

Perplexity (Ä‘á»™ rá»‘i) lÃ  thÆ°á»›c Ä‘o chuáº©n Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh ngÃ´n ngá»¯ xÃ¡c suáº¥t. NÃ³ pháº£n Ã¡nh má»©c Ä‘á»™ â€œbáº¥t ngá»â€ trung bÃ¬nh cá»§a mÃ´ hÃ¬nh khi dá»± Ä‘oÃ¡n má»™t chuá»—i tá»«. BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch ná»n táº£ng toÃ¡n há»c cá»§a perplexity, má»‘i liÃªn há»‡ vá»›i entropy vÃ  cross-entropy, cÅ©ng nhÆ° cÃ¡c háº¡n cháº¿ khi sá»­ dá»¥ng perplexity Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡n Ä‘áº¡i. Ná»™i dung má»Ÿ rá»™ng tá»« cÃ¡c cÃ´ng trÃ¬nh cá»§a Claude Shannon, Christopher D. Manning vÃ  Yoshua Bengio.

â¸»

1. Giá»›i thiá»‡u

MÃ´ hÃ¬nh ngÃ´n ngá»¯ xÃ¡c suáº¥t Æ°á»›c lÆ°á»£ng:

P($w_1$, $w_2$, ..., $w_T$)

Theo quy táº¯c chuá»—i:

P($w_1$^T)

$$
= \prod_{t=1}^{T}
$$

P($w_t$ | $w_1$^{t-1})

Má»¥c tiÃªu lÃ  tá»‘i Ä‘a hÃ³a xÃ¡c suáº¥t chuá»—i vÄƒn báº£n trong táº­p kiá»ƒm tra.

â¸»

2. Entropy vÃ  Cross-Entropy

2.1 Entropy (Shannon, 1948)

Entropy cá»§a phÃ¢n phá»‘i p$x$:

H$p$

$$
=
$$

-
$\sum$_x

$$
px\log px
$$

ÄÆ¡n vá»‹: bits (log base 2) hoáº·c nats (log base e).

â¸»

2.2 Cross-Entropy

Náº¿u mÃ´ hÃ¬nh Æ°á»›c lÆ°á»£ng phÃ¢n phá»‘i q$x$:

H(p, q)

$$
=
$$

-
$\sum$_x

$$
px\log qx
$$

Trong thá»±c nghiá»‡m, ta dÃ¹ng Æ°á»›c lÆ°á»£ng:

\hat{H}

$$
=
$$

-
\frac{1}{T}

$$
\sum_{t=1}^{T}
$$

$\log$ P_\theta($w_t$ | $w_1$^{t-1})

â¸»

3. Äá»‹nh nghÄ©a Perplexity

Perplexity Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ :

$$
PP =
$$

\exp$\hat{H}$

Hoáº·c:

$$
PP =
$$

\exp

$$
\left(
$$

-
\frac{1}{T}

$$
\sum_{t=1}^{T}
$$

$\log$ P($w_t$ | $w_1$^{t-1})
\right)

Náº¿u log base 2:

$$
PP = 2^{H}
$$

â¸»

4. Diá»…n giáº£i Trá»±c quan

Perplexity cÃ³ thá»ƒ hiá»ƒu lÃ :

Sá»‘ lÆ°á»£ng lá»±a chá»n trung bÃ¬nh mÃ  mÃ´ hÃ¬nh â€œphÃ¢n vÃ¢nâ€ táº¡i má»—i bÆ°á»›c.

VÃ­ dá»¥:

$$
â€¢	Náº¿u PP = 10 â†’ mÃ´ hÃ¬nh nhÆ° Ä‘ang chá»n trong 10 tá»« kháº£ dÄ©. â€¢	Náº¿u PP = 1 â†’ dá»± Ä‘oÃ¡n hoÃ n háº£o.
$$

â¸»

5. Má»‘i liÃªn há»‡ vá»›i Likelihood

Log-likelihood trung bÃ¬nh:

$\ell(

$$
=
$$

\frac{1}{T}

)$
$\sum$_{t=1}^{T}

$$
\log P(w_t  \mid  w_1^{t-1}) Khi Ä‘Ã³: PP = e^{-\ell(}
$$

)$

Giáº£m perplexity â‡” tÄƒng log-likelihood.

â¸»

6. VÃ­ dá»¥ Minh há»a

Giáº£ sá»­ mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n xÃ¡c suáº¥t trung bÃ¬nh:

P(w_t  \mid  context) = 0.2

Khi Ä‘Ã³:

\hat{H} = -\log(0.2)

$$
PP = \exp-\log 0.2 = \frac{1}{0.2} = 5 â¸» 7. Perplexity vÃ  MÃ´ hÃ¬nh N-gram Trong mÃ´ hÃ¬nh n-gram: P(w_t  \mid  w_{t-n+1}^{t-1}) Perplexity giáº£m khi: â€¢	n tÄƒng â€¢	dá»¯ liá»‡u huáº¥n luyá»‡n lá»›n hÆ¡n Tuy nhiÃªn: n \rightarrow lá»›n \Rightarrow Data\ sparsity â¸» 8. Perplexity trong MÃ´ hÃ¬nh Neural Vá»›i máº¡ng nÆ¡-ron: z_t = W h_t P(w_t  \mid  context)
$$

=

$$
\text{softmax}z_t Cross-entropy loss: \mathcal{L}
$$

=

$$
- \sum_t \log P(w_t  \mid  context) Perplexity:
$$

PP =

$$
\exp
$$

$\le$ft(

\frac{\mathcal{L}}{T}

$$
\right) â¸» 9. Háº¡n cháº¿ cá»§a Perplexity 9.1 KhÃ´ng pháº£n Ã¡nh cháº¥t lÆ°á»£ng sinh vÄƒn báº£n Perplexity tháº¥p â‰  vÄƒn báº£n tá»± nhiÃªn hÆ¡n. 9.2 Phá»¥ thuá»™c tokenization Náº¿u thay Ä‘á»•i cÃ¡ch tÃ¡ch tá»«: T \text{ thay Ä‘á»•i} \Rightarrow PP \text{ thay Ä‘á»•i} KhÃ´ng thá»ƒ so sÃ¡nh trá»±c tiáº¿p giá»¯a cÃ¡c tokenizer khÃ¡c nhau. â¸» 9.3 KhÃ´ng Ä‘o Ä‘Æ°á»£c hiá»ƒu ngá»¯ nghÄ©a Perplexity chá»‰ Ä‘o: P(data) KhÃ´ng Ä‘o: â€¢	Äá»™ logic â€¢	TÃ­nh Ä‘Ãºng sá»± tháº­t â€¢	SÃ¡ng táº¡o â¸» 10. PhÃ¢n tÃ­ch Giá»›i háº¡n Thá»‘ng kÃª Perplexity thá»±c nghiá»‡m: \hat{PP}
$$

=

$$
\exp\hat{H} Sai sá»‘ chuáº©n cá»§a entropy: SEH
$$

=

$$
\frac{\sigma}{\sqrt{T}} Khi T nhá» â†’ phÆ°Æ¡ng sai cao â†’ PP khÃ´ng á»•n Ä‘á»‹nh. â¸» 11. LiÃªn há»‡ vá»›i KL-Divergence H(p,q)
$$

=

$$
Hp + D_{KL}(p||q) Do Ä‘Ã³: PP
$$

=

$$
\exp(H(p) + D_{KL}(p||q)) Perplexity tá»‘i thiá»ƒu khi: q = p â¸» 12. Perplexity trong LLMs Hiá»‡n Ä‘áº¡i Trong mÃ´ hÃ¬nh lá»›n: â€¢	Zero-shot evaluation â€¢	Few-shot evaluation â€¢	Instruction tuning Perplexity thÆ°á»ng dÃ¹ng Ä‘á»ƒ: â€¢	So sÃ¡nh checkpoint â€¢	PhÃ¡t hiá»‡n overfitting â€¢	ÄÃ¡nh giÃ¡ há»™i tá»¥ Tuy nhiÃªn, vá»›i mÃ´ hÃ¬nh instruction-tuned: Perplexity cÃ³ thá»ƒ tÄƒng nhÆ°ng cháº¥t lÆ°á»£ng há»™i thoáº¡i tá»‘t hÆ¡n. â¸» 13. Káº¿t luáº­n Perplexity lÃ  thÆ°á»›c Ä‘o toÃ¡n há»c cháº·t cháº½ dá»±a trÃªn entropy vÃ  likelihood: PP = e^{H}
$$


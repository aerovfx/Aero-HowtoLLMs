
<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [09 quantitative evaluations](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Nh·∫≠p v√† tri·ªÉn khai m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn b·∫±ng l∆∞·ª£ng t·ª≠ h√≥a 8-bit/4-bit v·ªõi BitsAndBytes

Ph√¢n t√≠ch ki·∫øn tr√∫c, c∆° s·ªü to√°n h·ªçc v√† hi·ªáu nƒÉng th·ª±c nghi·ªám

‚∏ª

T√≥m t·∫Øt

B√†i vi·∫øt n√†y ph√¢n t√≠ch ph∆∞∆°ng ph√°p nh·∫≠p v√† tri·ªÉn khai m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLMs) b·∫±ng k·ªπ thu·∫≠t l∆∞·ª£ng t·ª≠ h√≥a (quantization) s·ª≠ d·ª•ng th∆∞ vi·ªán bitsandbytes. D·ª±a tr√™n n·ªôi dung t√†i li·ªáu ƒë√≠nh k√®m, ch√∫ng t√¥i m·ªü r·ªông v·ªõi c√°c n·ªÅn t·∫£ng l√Ω thuy·∫øt t·ª´ Transformer c·ªßa Ashish Vaswani et al. (2017), nghi√™n c·ª©u v·ªÅ scaling laws c·ªßa OpenAI v√† h·ªá sinh th√°i tri·ªÉn khai c·ªßa Hugging Face.

B√†i vi·∫øt tr√¨nh b√†y:
	‚Ä¢	B√†i to√°n gi·ªõi h·∫°n b·ªô nh·ªõ khi t·∫£i LLM
	‚Ä¢	Nguy√™n l√Ω l∆∞·ª£ng t·ª≠ h√≥a tr·ªçng s·ªë 8-bit v√† 4-bit
	‚Ä¢	C√¥ng th·ª©c sai s·ªë l∆∞·ª£ng t·ª≠ h√≥a
	‚Ä¢	Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p b·ªô nh·ªõ
	‚Ä¢	So s√°nh hi·ªáu nƒÉng tr∆∞·ªõc v√† sau l∆∞·ª£ng t·ª≠ h√≥a

‚∏ª

1. Gi·ªõi thi·ªáu

M√¥ h√¨nh ng√¥n ng·ªØ l·ªõn hi·ªán ƒë·∫°i c√≥ s·ªë tham s·ªë t·ª´:

10^9 \rightarrow 10^{11}

Gi·∫£ s·ª≠:
	‚Ä¢	M√¥ h√¨nh c√≥ N tham s·ªë
	‚Ä¢	M·ªói tham s·ªë ·ªü d·∫°ng FP32 (4 bytes)

Dung l∆∞·ª£ng b·ªô nh·ªõ:

Memory = 4N \text{ bytes}

V√≠ d·ª•:

N = 7 \times 10^9

Memory = 28GB

ƒêi·ªÅu n√†y v∆∞·ª£t qu√° kh·∫£ nƒÉng c·ªßa nhi·ªÅu GPU ph·ªï th√¥ng.

‚∏ª

2. Nguy√™n l√Ω l∆∞·ª£ng t·ª≠ h√≥a (Quantization)

2.1 ƒê·ªãnh nghƒ©a

L∆∞·ª£ng t·ª≠ h√≥a l√† √°nh x·∫°:

w \in \mathbb{R} \rightarrow \hat{w} \in \mathbb{Z}_k

Trong ƒë√≥:
	‚Ä¢	k = 2^b
	‚Ä¢	b l√† s·ªë bit (8-bit, 4-bit,‚Ä¶)

‚∏ª

2.2 L∆∞·ª£ng t·ª≠ h√≥a tuy·∫øn t√≠nh (Linear Quantization)

Cho tr·ªçng s·ªë w n·∫±m trong kho·∫£ng:

$$
w_{min}, w_{max}
$$
H·ªá s·ªë scale:

s = \frac{w_{max} - w_{min}}{2^b - 1}

Gi√° tr·ªã l∆∞·ª£ng t·ª≠ h√≥a:

\hat{w} = \text{round}\left$\frac{w - w_{min}}{s}\right$

Gi·∫£i l∆∞·ª£ng t·ª≠:

w \approx s \hat{w} + w_{min}

‚∏ª

3. Sai s·ªë l∆∞·ª£ng t·ª≠ h√≥a

Sai s·ªë:

\epsilon = w - \hat{w}

Gi·∫£ s·ª≠ ph√¢n ph·ªëi ƒë·ªÅu:

Var$\epsilon$ = \frac{s^2}{12}

Khi gi·∫£m s·ªë bit b:
	‚Ä¢	s tƒÉng
	‚Ä¢	Sai s·ªë tƒÉng
	‚Ä¢	M·∫•t m√°t th√¥ng tin tƒÉng

‚∏ª

4. 8-bit vs 4-bit

4.1 B·ªô nh·ªõ

V·ªõi FP32:

Memory_{32} = 32N \text{ bits}

V·ªõi 8-bit:

Memory_{8} = 8N \text{ bits}

Gi·∫£m:

\frac{Memory_{8}}{Memory_{32}} = \frac{1}{4}

V·ªõi 4-bit:

Memory_{4} = 4N \text{ bits}

Gi·∫£m:

\frac{Memory_{4}}{Memory_{32}} = \frac{1}{8}

‚∏ª

4.2 ·∫¢nh h∆∞·ªüng ƒë·∫øn forward pass

Transformer s·ª≠ d·ª•ng:

Y = XW

Sau l∆∞·ª£ng t·ª≠ h√≥a:

Y = X\hat{W}

Sai s·ªë lan truy·ªÅn:

\Delta Y = X$W - \hat{W}$

N·∫øu:

||W - \hat{W}||_2 \text{ nh·ªè}

‚Üí ·∫¢nh h∆∞·ªüng t·ªõi output nh·ªè.

‚∏ª

5. K·ªπ thu·∫≠t c·ªßa BitsAndBytes

Th∆∞ vi·ªán bitsandbytes tri·ªÉn khai:
	‚Ä¢	L∆∞·ª£ng t·ª≠ h√≥a ƒë·ªông (dynamic quantization)
	‚Ä¢	L∆∞·ª£ng t·ª≠ h√≥a theo block
	‚Ä¢	NF4 (NormalFloat4)

NF4 gi·∫£ ƒë·ªãnh tr·ªçng s·ªë ph√¢n ph·ªëi chu·∫©n:

w \sim \mathcal{N}$0, \sigma^2$

Mapping phi tuy·∫øn gi√∫p gi·∫£m sai s·ªë so v·ªõi l∆∞·ª£ng t·ª≠ h√≥a tuy·∫øn t√≠nh.

‚∏ª

6. T√≠ch h·ª£p v·ªõi Hugging Face Transformers

H·ªá sinh th√°i c·ªßa Hugging Face h·ªó tr·ª£:
	‚Ä¢	load_in_8bit=True
	‚Ä¢	load_in_4bit=True

Gi·∫£m b·ªô nh·ªõ GPU ƒë√°ng k·ªÉ m√† kh√¥ng c·∫ßn hu·∫•n luy·ªán l·∫°i to√†n b·ªô m√¥ h√¨nh.

‚∏ª

7. ·∫¢nh h∆∞·ªüng ƒë·∫øn Perplexity

Perplexity:

PP = \exp\left$- \frac{1}{N} \sum \log P(w_i$\right)

Sau l∆∞·ª£ng t·ª≠ h√≥a:

PP_{quant} = PP_{fp32} + \delta

Trong th·ª±c nghi·ªám:
	‚Ä¢	8-bit: \delta \approx 1\% - 3\%
	‚Ä¢	4-bit: \delta \approx 3\% - 8\%

Ph·ª• thu·ªôc k√≠ch th∆∞·ªõc m√¥ h√¨nh.

‚∏ª

8. Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n

Ph√©p nh√¢n ma tr·∫≠n:

O$n^3$

Nh∆∞ng khi d√πng int8:
	‚Ä¢	Gi·∫£m bƒÉng th√¥ng b·ªô nh·ªõ
	‚Ä¢	TƒÉng throughput
	‚Ä¢	T·ªëi ∆∞u Tensor Core

T·ªëc ƒë·ªô th·ª±c t·∫ø tƒÉng 1.5‚Äì2x tr√™n GPU h·ªó tr·ª£ INT8.

‚∏ª

9. L∆∞·ª£ng t·ª≠ h√≥a v√† Scaling Law

Theo nghi√™n c·ª©u scaling law c·ªßa OpenAI:

Loss$N$ = A N^{-\alpha}

N·∫øu l∆∞·ª£ng t·ª≠ h√≥a l√†m tƒÉng loss m·ªôt l∆∞·ª£ng nh·ªè \delta,
th√¨ c√≥ th·ªÉ b√π b·∫±ng tƒÉng nh·∫π s·ªë tham s·ªë N.

‚∏ª

10. So s√°nh v·ªõi Pruning

K·ªπ thu·∫≠t	Gi·∫£m b·ªô nh·ªõ	Gi·∫£m FLOPs	·∫¢nh h∆∞·ªüng ƒë·ªô ch√≠nh x√°c
Quantization	‚úî	‚úñ	Th·∫•p‚ÄìTrung
Pruning	‚úî	‚úî	Trung
Distillation	‚úî	‚úî	Th·∫•p

Quantization ph√π h·ª£p cho tri·ªÉn khai inference.

‚∏ª

11. H·∫°n ch·∫ø
	‚Ä¢	Gradient kh√¥ng ·ªïn ƒë·ªãnh khi fine-tune tr·ª±c ti·∫øp 4-bit
	‚Ä¢	M·ªôt s·ªë layer nh·∫°y c·∫£m (LayerNorm, Embedding)
	‚Ä¢	C·∫ßn mixed-precision

‚∏ª

12. K·∫øt lu·∫≠n

L∆∞·ª£ng t·ª≠ h√≥a b·∫±ng bitsandbytes:
	‚Ä¢	Gi·∫£m 4‚Äì8 l·∫ßn b·ªô nh·ªõ
	‚Ä¢	Gi·ªØ ch·∫•t l∆∞·ª£ng g·∫ßn t∆∞∆°ng ƒë∆∞∆°ng FP32
	‚Ä¢	Ph√π h·ª£p tri·ªÉn khai LLM tr√™n GPU t·∫ßm trung

Trong t∆∞∆°ng lai:
	‚Ä¢	QLoRA
	‚Ä¢	Post-training quantization n√¢ng cao
	‚Ä¢	Mixed precision adaptive

‚∏ª

T√†i li·ªáu tham kh·∫£o
	1.	Vaswani, A. et al. (2017). Attention is All You Need.
	2.	Dettmers, T. et al. (2022). 8-bit Optimizers via Block-wise Quantization.
	3.	Kaplan et al. (2020). Scaling Laws for Neural Language Models.
	4.	Goodfellow et al. (2016). Deep Learning.
	5.	Hugging Face Transformers Documentation.
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [ƒê√°nh Gi√° H·ªôp ƒêen (Black-box Evaluations) trong M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn](aero_llm_016_black_box_evals.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_016_black_box_evals.md) |
| [Red Teaming: ƒê·ªôi ƒê·ªè v√† Th·ª≠ Nghi·ªám ƒê·ªëi Kh√°ng trong AI Safety](aero_llm_017_red_teaming.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_017_red_teaming.md) |
| [ƒê·ªô Ch√≠nh X√°c, T√≠nh M·∫°ch L·∫°c v√† S·ª± Ph√π H·ª£p trong ƒê√°nh Gi√° M√¥ H√¨nh Ng√¥n Ng·ªØ](aero_llm_018_accuracy_coherence_and_relevance.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_018_accuracy_coherence_and_relevance.md) |
| [Ph√¢n Ph·ªëi C·ªßa C√°c K√≠ch Ho·∫°t Tr·∫°ng Th√°i ·∫®n Trong M√¥ H√¨nh Ng√¥n Ng·ªØ](aero_llm_019_distributions_of_hidden_state_activations.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_019_distributions_of_hidden_state_activations.md) |
| [H·ª©a H·∫πn v√† Th√°ch Th·ª©c c·ªßa ƒê√°nh Gi√° ƒê·ªãnh L∆∞·ª£ng trong M√¥ H√¨nh H·ªçc M√°y](aero_llm_01_promises_and_challenges_of_quantitative_evaluations.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_01_promises_and_challenges_of_quantitative_evaluations.md) |
| [B·∫£n ƒê·ªì Nhi·ªát C·ªßa Token Cho C√¢n Nh·∫Øc ƒê·ªãnh T√≠nh (Text Heatmaps)](aero_llm_020_heatmaps_of_tokens_for_qualitative_inspection.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_020_heatmaps_of_tokens_for_qualitative_inspection.md) |
| [Th·ª≠ Th√°ch L·∫≠p Tr√¨nh: Tr·ª±c Quan H√≥a D·ª± ƒêo√°n ƒê∆°n Token](aero_llm_021_codechallenge_visualize_single_token_predictions.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_021_codechallenge_visualize_single_token_predictions.md) |
| [C√°c V·∫•n ƒê·ªÅ S·ªë H·ªçc trong Logits v√† Softmax: Ph√¢n T√≠ch To√°n H·ªçc v√† Gi·∫£i Ph√°p ·ªîn ƒê·ªãnh](aero_llm_02_numerical_issues_in_logits_and_softmax.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_02_numerical_issues_in_logits_and_softmax.md) |
| [Perplexity trong M√¥ H√¨nh Ng√¥n Ng·ªØ: C∆° S·ªü To√°n H·ªçc, Di·ªÖn Gi·∫£i v√† Gi·ªõi H·∫°n](aero_llm_03_perplexity.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_03_perplexity.md) |
| [aero llm 04 codechallenge perplexing perplexities](aero_llm_04_codechallenge_perplexing_perplexities.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_04_codechallenge_perplexing_perplexities.md) |
| [aero llm 05 masked word prediction accuracy](aero_llm_05_masked_word_prediction_accuracy.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_05_masked_word_prediction_accuracy.md) |
| [aero llm 06 hellaswag](aero_llm_06_hellaswag.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_06_hellaswag.md) |
| üìå **[aero llm 07 import large models using bitsandbytes](aero_llm_07_import_large_models_using_bitsandbytes.md)** | [Xem b√†i vi·∫øt ‚Üí](aero_llm_07_import_large_models_using_bitsandbytes.md) |
| [aero llm 08 codechallenge hellaswag evals in two models part 1](aero_llm_08_codechallenge_hellaswag_evals_in_two_models_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_08_codechallenge_hellaswag_evals_in_two_models_part_1_.md) |
| [aero llm 09 codechallenge hellaswag evals in two models part 2](aero_llm_09_codechallenge_hellaswag_evals_in_two_models_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_09_codechallenge_hellaswag_evals_in_two_models_part_2_.md) |
| [aero llm 10 kl kullback leibler divergence](aero_llm_10_kl_kullback_leibler_divergence.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_10_kl_kullback_leibler_divergence.md) |
| [aero llm 11 mauve](aero_llm_11_mauve.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_11_mauve.md) |
| [aero llm 12 codechallenge large and small mauve explorations](aero_llm_12_codechallenge_large_and_small_mauve_explorations.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_12_codechallenge_large_and_small_mauve_explorations.md) |
| [aero llm 13 superglue and other amalgamations](aero_llm_13_superglue_and_other_amalgamations.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_13_superglue_and_other_amalgamations.md) |
| [aero llm 14 assessing bias and fairness](aero_llm_14_assessing_bias_and_fairness.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_14_assessing_bias_and_fairness.md) |
| [aero llm 15 non technical benchmarks](aero_llm_15_non_technical_benchmarks.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_15_non_technical_benchmarks.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

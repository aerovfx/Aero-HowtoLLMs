
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [09 quantitative evaluations](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# PhÃ¢n Phá»‘i Cá»§a CÃ¡c KÃ­ch Hoáº¡t Tráº¡ng ThÃ¡i áº¨n Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯

## TÃ³m táº¯t

Ngay cáº£ khi cÃ³ quyá»n truy cáº­p vÃ o toÃ n bá»™ cÃ¡c thÃ´ng sá»‘ ná»™i bá»™ cá»§a má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM), báº£n cháº¥t phi tuyáº¿n tÃ­nh vÃ  phá»©c táº¡p cá»§a chÃºng lÃ m cho viá»‡c hiá»ƒu cÃ¡ch mÃ´ hÃ¬nh nháº­n thá»©c vÃ  xá»­ lÃ½ thÃ´ng tin trá»Ÿ nÃªn ráº¥t khÃ³ khÄƒn. BÃ i viáº¿t nÃ y khÃ¡m phÃ¡ cÃ¡c phÆ°Æ¡ng phÃ¡p trÃ­ch xuáº¥t máº«u kÃ­ch hoáº¡t ná»™i táº¡i tá»« cÃ¡c lá»›p `transformer` vÃ  trá»±c quan hÃ³a phÃ¢n phá»‘i cá»§a chÃºng thÃ´ng qua biá»ƒu Ä‘á»“ phÃ¢n tÃ¡n (scatter plots), ma tráº­n hiá»‡p phÆ°Æ¡ng sai (covariance matrix), vÃ  biá»ƒu Ä‘á»“ táº§n suáº¥t (histograms).

---

## 1. CÆ¡ sá»Ÿ vá» Tráº¡ng ThÃ¡i áº¨n (Hidden States)

Trong má»™t LLM nhÆ° GPT-2, vÄƒn báº£n Ä‘áº§u vÃ o Ä‘Æ°á»£c mÃ£ hÃ³a thÃ nh cÃ¡c chá»‰ sá»‘ token, sau Ä‘Ã³ Ä‘Æ°á»£c Ã¡nh xáº¡ thÃ nh cÃ¡c **vectors nhÃºng** (embedding vectors). Táº¡i má»—i khá»‘i `transformer`, cÃ¡c vector nÃ y láº¡i Ä‘Æ°á»£c biáº¿n Ä‘á»•i, quay, co giÃ£n, Ä‘á»ƒ rá»“i hÃ¬nh thÃ nh nÃªn biá»ƒu diá»…n cuá»‘i cÃ¹ng cho viá»‡c dá»± Ä‘oÃ¡n token.

Báº±ng cÃ¡ch cháº¡y má»™t lÆ°á»£t lan truyá»n xuÃ´i (forward pass), ta cÃ³ thá»ƒ kÃ­ch hoáº¡t tÃ¹y chá»n xuáº¥t tráº¡ng thÃ¡i áº©n:

`output_hidden_states = True`

Trong GPT-2 nhá», tÃ­nh toÃ¡n nÃ y sáº½ tráº£ ra 13 ten-xÆ¡ (tensors), bao gá»“m:
1 Ä‘áº§u ra tá»« Lá»›p NhÃºng (Embeddings layer).
12 Ä‘áº§u ra tÆ°Æ¡ng á»©ng tá»« 12 khá»‘i transformer.
Má»—i máº¡ng lÆ°á»›i cÃ³ cáº¥u hÃ¬nh kÃ­ch thÆ°á»›c dáº¡ng `[Batch Size, Sequences, Embedding Dimension]`. Trong GPT, thiáº¿t láº­p nÃ y thÆ°á»ng lÃ  `[1, 62, 768]`.

---

## 2. CÃ¡c CÃ´ng Cá»¥ Trá»±c Quan HÃ³a 

### 2.1 Biá»ƒu Äá»“ PhÃ¢n TÃ¡n (Scatter Plots)

Vá»›i biá»ƒu Ä‘á»“ phÃ¢n tÃ¡n, ta Ä‘á»‘i chiáº¿u cÃ¡c chá»‰ sá»‘ token vÃ  chiá»u biá»ƒu diá»…n (embedding dimensions) vá»›i giÃ¡ trá»‹ kÃ­ch hoáº¡t.

Äiá»ƒm quan trá»ng rÃºt ra lÃ  **yáº¿u tá»‘ nhiá»…u cá»§a token Ä‘áº§u tiÃªn**. Trong tá»± nhiÃªn, viá»‡c xá»­ lÃ½ token Ä‘áº§u tiÃªn lÃ  phi chuáº©n vÃ¬ khÃ´ng cÃ³ context (ngá»¯ cáº£nh) Ä‘á»©ng trÆ°á»›c nÃ³. Äá»ƒ viá»‡c quan sÃ¡t khÃ´ng bá»‹ sai lá»‡ch, thÃ´ng thÆ°á»ng token nÃ y cáº§n bá»‹ loáº¡i trá»« (sá»­ dá»¥ng token cÃ³ chá»‰ sá»‘ 1 trá»Ÿ lÃªn).

### 2.2 Ma Tráº­n Hiá»‡p PhÆ°Æ¡ng Sai vÃ  $R^2$ (Covariance & $R^2$ Matrix)

Äá»ƒ hiá»ƒu Ä‘Æ°á»£c cÃ¡c phÃ©p tÃ­nh áº©n liÃªn Ä‘á»›i nhÆ° tháº¿ nÃ o qua tá»«ng lá»›p, ta sá»­ dá»¥ng ma tráº­n **Hiá»‡p phÆ°Æ¡ng sai** (Covariance) vÃ  ma tráº­n tÆ°Æ¡ng quan Ä‘Æ°á»£c bÃ¬nh phÆ°Æ¡ng ($R^2$, giáº£i thÃ­ch lÆ°á»£ng phÆ°Æ¡ng sai Ä‘Æ°á»£c chia sáº»).

R^2 = \text{Corr}(X, Y)^2

Hai Ä‘áº¡i lÆ°á»£ng X vÃ  Y hoÃ n toÃ n khÃ´ng tÆ°Æ¡ng quan sáº½ cÃ³ R^2 \approx 0. NgÆ°á»£c láº¡i, náº¿u chÃºng giá»‘ng há»‡t, káº¿t quáº£ tráº£ vá» 1 (hoáº·c 100%).
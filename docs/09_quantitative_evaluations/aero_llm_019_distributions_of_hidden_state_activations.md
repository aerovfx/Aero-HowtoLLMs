
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [09 quantitative evaluations](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# PhÃ¢n Phá»‘i Cá»§a CÃ¡c KÃ­ch Hoáº¡t Tráº¡ng ThÃ¡i áº¨n Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯

## TÃ³m táº¯t

Ngay cáº£ khi cÃ³ quyá»n truy cáº­p vÃ o toÃ n bá»™ cÃ¡c thÃ´ng sá»‘ ná»™i bá»™ cá»§a má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM), báº£n cháº¥t phi tuyáº¿n tÃ­nh vÃ  phá»©c táº¡p cá»§a chÃºng lÃ m cho viá»‡c hiá»ƒu cÃ¡ch mÃ´ hÃ¬nh nháº­n thá»©c vÃ  xá»­ lÃ½ thÃ´ng tin trá»Ÿ nÃªn ráº¥t khÃ³ khÄƒn. BÃ i viáº¿t nÃ y khÃ¡m phÃ¡ cÃ¡c phÆ°Æ¡ng phÃ¡p trÃ­ch xuáº¥t máº«u kÃ­ch hoáº¡t ná»™i táº¡i tá»« cÃ¡c lá»›p `transformer` vÃ  trá»±c quan hÃ³a phÃ¢n phá»‘i cá»§a chÃºng thÃ´ng qua biá»ƒu Ä‘á»“ phÃ¢n tÃ¡n (scatter plots), ma tráº­n hiá»‡p phÆ°Æ¡ng sai (covariance matrix), vÃ  biá»ƒu Ä‘á»“ táº§n suáº¥t (histograms).

---

## 1. CÆ¡ sá»Ÿ vá» Tráº¡ng ThÃ¡i áº¨n (Hidden States)

Trong má»™t LLM nhÆ° GPT-2, vÄƒn báº£n Ä‘áº§u vÃ o Ä‘Æ°á»£c mÃ£ hÃ³a thÃ nh cÃ¡c chá»‰ sá»‘ token, sau Ä‘Ã³ Ä‘Æ°á»£c Ã¡nh xáº¡ thÃ nh cÃ¡c **vectors nhÃºng** (embedding vectors). Táº¡i má»—i khá»‘i `transformer`, cÃ¡c vector nÃ y láº¡i Ä‘Æ°á»£c biáº¿n Ä‘á»•i, quay, co giÃ£n, Ä‘á»ƒ rá»“i hÃ¬nh thÃ nh nÃªn biá»ƒu diá»…n cuá»‘i cÃ¹ng cho viá»‡c dá»± Ä‘oÃ¡n token.

Báº±ng cÃ¡ch cháº¡y má»™t lÆ°á»£t lan truyá»n xuÃ´i (forward pass), ta cÃ³ thá»ƒ kÃ­ch hoáº¡t tÃ¹y chá»n xuáº¥t tráº¡ng thÃ¡i áº©n:

`output_hidden_states = True`

Trong GPT-2 nhá», tÃ­nh toÃ¡n nÃ y sáº½ tráº£ ra 13 ten-xÆ¡ (tensors), bao gá»“m:
1 Ä‘áº§u ra tá»« Lá»›p NhÃºng (Embeddings layer).
12 Ä‘áº§u ra tÆ°Æ¡ng á»©ng tá»« 12 khá»‘i transformer.
Má»—i máº¡ng lÆ°á»›i cÃ³ cáº¥u hÃ¬nh kÃ­ch thÆ°á»›c dáº¡ng `[Batch Size, Sequences, Embedding Dimension]`. Trong GPT, thiáº¿t láº­p nÃ y thÆ°á»ng lÃ  `[1, 62, 768]`.

---

## 2. CÃ¡c CÃ´ng Cá»¥ Trá»±c Quan HÃ³a 

### 2.1 Biá»ƒu Äá»“ PhÃ¢n TÃ¡n (Scatter Plots)

Vá»›i biá»ƒu Ä‘á»“ phÃ¢n tÃ¡n, ta Ä‘á»‘i chiáº¿u cÃ¡c chá»‰ sá»‘ token vÃ  chiá»u biá»ƒu diá»…n (embedding dimensions) vá»›i giÃ¡ trá»‹ kÃ­ch hoáº¡t.

Äiá»ƒm quan trá»ng rÃºt ra lÃ  **yáº¿u tá»‘ nhiá»…u cá»§a token Ä‘áº§u tiÃªn**. Trong tá»± nhiÃªn, viá»‡c xá»­ lÃ½ token Ä‘áº§u tiÃªn lÃ  phi chuáº©n vÃ¬ khÃ´ng cÃ³ context (ngá»¯ cáº£nh) Ä‘á»©ng trÆ°á»›c nÃ³. Äá»ƒ viá»‡c quan sÃ¡t khÃ´ng bá»‹ sai lá»‡ch, thÃ´ng thÆ°á»ng token nÃ y cáº§n bá»‹ loáº¡i trá»« (sá»­ dá»¥ng token cÃ³ chá»‰ sá»‘ 1 trá»Ÿ lÃªn).

### 2.2 Ma Tráº­n Hiá»‡p PhÆ°Æ¡ng Sai vÃ  $R^2$ (Covariance & $R^2$ Matrix)

Äá»ƒ hiá»ƒu Ä‘Æ°á»£c cÃ¡c phÃ©p tÃ­nh áº©n liÃªn Ä‘á»›i nhÆ° tháº¿ nÃ o qua tá»«ng lá»›p, ta sá»­ dá»¥ng ma tráº­n **Hiá»‡p phÆ°Æ¡ng sai** (Covariance) vÃ  ma tráº­n tÆ°Æ¡ng quan Ä‘Æ°á»£c bÃ¬nh phÆ°Æ¡ng ($R^2$, giáº£i thÃ­ch lÆ°á»£ng phÆ°Æ¡ng sai Ä‘Æ°á»£c chia sáº»). 

$$ R^2 = \text{Corr}(X, Y)^2 $$

Hai Ä‘áº¡i lÆ°á»£ng $X$ vÃ  $Y$ hoÃ n toÃ n khÃ´ng tÆ°Æ¡ng quan sáº½ cÃ³ $R^2 \approx 0$. NgÆ°á»£c láº¡i, náº¿u chÃºng giá»‘ng há»‡t, káº¿t quáº£ tráº£ vá» 1 (hoáº·c 100%).

- Lá»›p nhÃºng (Embeddings) vÃ  Khá»‘i transformer káº¿t quáº£ (Layer cuá»‘i) cÃ³ ráº¥t Ã­t sá»± hiá»‡p biáº¿n so vá»›i cÃ¡c blocks khÃ¡c. 
- Giá»¯a khá»‘i Transformer trung gian, cÃ³ sá»± chia sáº» phÆ°Æ¡ng sai khÃ¡ máº­t thiáº¿t. VectÆ¡ embedding thay Ä‘á»•i qua má»—i block, nhÆ°ng khÃ´ng Ä‘á»™t biáº¿n. CÃ³ sá»± tinh chá»‰nh tá»« tá»«, lÃ m cáº¥u trÃºc giá»‘ng nhÆ° chiáº¿c "cÃ¡nh chim" ná»Ÿ rá»™ng ra dáº§n.

### 2.3 Biá»ƒu Äá»“ Táº§n Suáº¥t (Histograms)

Sá»­ dá»¥ng phÃ¢n Scale logarith cá»§a trá»¥c $y$ cho biá»ƒu Ä‘á»“ Histogram giÃºp biá»ƒu thá»‹ cÃ¡c giÃ¡ trá»‹ lá»‡ch (side lobes) cá»§a phÃ¢n phá»‘i theo cÃ¡ch nháº¡y bÃ©n nháº¥t.

- Táº¡i lá»›p nhÃºng ban Ä‘áº§u, phÃ¢n phá»‘i khÃ¡ háº¹p do thiáº¿u ngá»¯ cáº£nh.
- CÃ ng Ä‘i sÃ¢u, phÃ¢n phá»‘i táº£n máº¡n hÆ¡n vá»›i nhiá»u Ä‘iá»ƒm cÃ³ cÃ¡c Activation Values mang tÃ­nh thÃ¡i quÃ¡ (extreme activation). Má»Ÿ rá»™ng sá»± tham kháº£o tri thá»©c trÃªn pre-trained weights.
- á» block Transformer cuá»‘i cÃ¹ng, Ä‘Æ°á»ng cong phÃ¢n phá»‘i láº¡i háº¹p dáº§n. Äiá»u nÃ y mang Ã½ nghÄ©a sá»± khÃ´ng cháº¯c cháº¯n (Uncertainty) bá»‹ triá»‡t tiÃªu, vÃ  model táº­p trung Æ°u tiÃªn bá» phiáº¿u cho cÃ¡c token máº¡nh nháº¥t Ä‘á»ƒ sinh chuá»—i kÃ½ tá»±.

---

## 3. Ã NghÄ©a

Máº·c dÃ¹ cÃ³ Ä‘Æ°á»£c sá»‘ liá»‡u, viá»‡c hiá»ƒu Ä‘Æ°á»£c thá»±c sá»± tá»«ng chuá»—i kÃ­ch hoáº¡t á»©ng vá»›i pattern nÃ o cá»§a thÃ´ng tin ngÃ´n ngá»¯ váº«n ráº¥t thá»­ thÃ¡ch. PhÆ°Æ¡ng phÃ¡p trá»±c quan hÃ³a sá»‘ vÄ© mÃ´ nhÆ° Scatter hay Covariances giÃºp ta "cháº¡m" gáº§n hÆ¡n vÃ o kiáº¿n trÃºc cá»§a **Kháº£ nÄƒng BÃ¡o tÃ­ch CÆ¡ há»c (Mechanistic Interpretability)**.

---

## TÃ i liá»‡u tham kháº£o

1. **Radford, A. et al. (2019).** *Language Models are Unsupervised Multitask Learners.*
2. **Elhage, N. et al. (2021).** *A Mathematical Framework for Transformer Circuits.* Anthropic.
3. **Clark, K. et al. (2019).** *What Does BERT Look At? An Analysis of BERT's Attention.*
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [ÄÃ¡nh GiÃ¡ Há»™p Äen (Black-box Evaluations) trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n](aero_llm_016_black_box_evals.md) | [Xem bÃ i viáº¿t â†’](aero_llm_016_black_box_evals.md) |
| [Red Teaming: Äá»™i Äá» vÃ  Thá»­ Nghiá»‡m Äá»‘i KhÃ¡ng trong AI Safety](aero_llm_017_red_teaming.md) | [Xem bÃ i viáº¿t â†’](aero_llm_017_red_teaming.md) |
| [Äá»™ ChÃ­nh XÃ¡c, TÃ­nh Máº¡ch Láº¡c vÃ  Sá»± PhÃ¹ Há»£p trong ÄÃ¡nh GiÃ¡ MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_018_accuracy_coherence_and_relevance.md) | [Xem bÃ i viáº¿t â†’](aero_llm_018_accuracy_coherence_and_relevance.md) |
| ğŸ“Œ **[PhÃ¢n Phá»‘i Cá»§a CÃ¡c KÃ­ch Hoáº¡t Tráº¡ng ThÃ¡i áº¨n Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_019_distributions_of_hidden_state_activations.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_019_distributions_of_hidden_state_activations.md) |
| [Há»©a Háº¹n vÃ  ThÃ¡ch Thá»©c cá»§a ÄÃ¡nh GiÃ¡ Äá»‹nh LÆ°á»£ng trong MÃ´ HÃ¬nh Há»c MÃ¡y](aero_llm_01_promises_and_challenges_of_quantitative_evaluations.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_promises_and_challenges_of_quantitative_evaluations.md) |
| [Báº£n Äá»“ Nhiá»‡t Cá»§a Token Cho CÃ¢n Nháº¯c Äá»‹nh TÃ­nh (Text Heatmaps)](aero_llm_020_heatmaps_of_tokens_for_qualitative_inspection.md) | [Xem bÃ i viáº¿t â†’](aero_llm_020_heatmaps_of_tokens_for_qualitative_inspection.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh: Trá»±c Quan HÃ³a Dá»± ÄoÃ¡n ÄÆ¡n Token](aero_llm_021_codechallenge_visualize_single_token_predictions.md) | [Xem bÃ i viáº¿t â†’](aero_llm_021_codechallenge_visualize_single_token_predictions.md) |
| [CÃ¡c Váº¥n Äá» Sá»‘ Há»c trong Logits vÃ  Softmax: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Giáº£i PhÃ¡p á»”n Äá»‹nh](aero_llm_02_numerical_issues_in_logits_and_softmax.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_numerical_issues_in_logits_and_softmax.md) |
| [Perplexity trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯: CÆ¡ Sá»Ÿ ToÃ¡n Há»c, Diá»…n Giáº£i vÃ  Giá»›i Háº¡n](aero_llm_03_perplexity.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_perplexity.md) |
| [aero llm 04 codechallenge perplexing perplexities](aero_llm_04_codechallenge_perplexing_perplexities.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_codechallenge_perplexing_perplexities.md) |
| [aero llm 05 masked word prediction accuracy](aero_llm_05_masked_word_prediction_accuracy.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_masked_word_prediction_accuracy.md) |
| [aero llm 06 hellaswag](aero_llm_06_hellaswag.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_hellaswag.md) |
| [aero llm 07 import large models using bitsandbytes](aero_llm_07_import_large_models_using_bitsandbytes.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_import_large_models_using_bitsandbytes.md) |
| [aero llm 08 codechallenge hellaswag evals in two models part 1](aero_llm_08_codechallenge_hellaswag_evals_in_two_models_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_codechallenge_hellaswag_evals_in_two_models_part_1_.md) |
| [aero llm 09 codechallenge hellaswag evals in two models part 2](aero_llm_09_codechallenge_hellaswag_evals_in_two_models_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_codechallenge_hellaswag_evals_in_two_models_part_2_.md) |
| [aero llm 10 kl kullback leibler divergence](aero_llm_10_kl_kullback_leibler_divergence.md) | [Xem bÃ i viáº¿t â†’](aero_llm_10_kl_kullback_leibler_divergence.md) |
| [aero llm 11 mauve](aero_llm_11_mauve.md) | [Xem bÃ i viáº¿t â†’](aero_llm_11_mauve.md) |
| [aero llm 12 codechallenge large and small mauve explorations](aero_llm_12_codechallenge_large_and_small_mauve_explorations.md) | [Xem bÃ i viáº¿t â†’](aero_llm_12_codechallenge_large_and_small_mauve_explorations.md) |
| [aero llm 13 superglue and other amalgamations](aero_llm_13_superglue_and_other_amalgamations.md) | [Xem bÃ i viáº¿t â†’](aero_llm_13_superglue_and_other_amalgamations.md) |
| [aero llm 14 assessing bias and fairness](aero_llm_14_assessing_bias_and_fairness.md) | [Xem bÃ i viáº¿t â†’](aero_llm_14_assessing_bias_and_fairness.md) |
| [aero llm 15 non technical benchmarks](aero_llm_15_non_technical_benchmarks.md) | [Xem bÃ i viáº¿t â†’](aero_llm_15_non_technical_benchmarks.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

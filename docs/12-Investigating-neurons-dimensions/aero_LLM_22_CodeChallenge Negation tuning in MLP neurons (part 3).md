
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [12 Investigating neurons dimensions](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 3)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y tá»•ng káº¿t thá»­ thÃ¡ch nghiÃªn cá»©u vá» cÆ¡ cháº¿ phá»§ Ä‘á»‹nh trong GPT-2 Large báº±ng cÃ¡ch phÃ¢n tÃ­ch sá»± biáº¿n Ä‘á»•i cá»§a nÆ¡-ron xuyÃªn suá»‘t toÃ n bá»™ chiá»u sÃ¢u cá»§a máº¡ng Transformer. ChÃºng ta thá»±c hiá»‡n má»™t phÃ¢n tÃ­ch há»‡ thá»‘ng trÃªn 36 táº§ng, Ä‘á»‹nh lÆ°á»£ng máº­t Ä‘á»™ vÃ  hiá»‡u nÄƒng cá»§a cÃ¡c "nÆ¡-ron phá»§ Ä‘á»‹nh". Káº¿t quáº£ thá»±c nghiá»‡m cho tháº¥y má»™t xu hÆ°á»›ng suy giáº£m rÃµ rá»‡t vá» cáº£ sá»‘ lÆ°á»£ng nÆ¡-ron chuyÃªn biá»‡t hÃ³a láº«n Ä‘á»™ chÃ­nh xÃ¡c phÃ¢n loáº¡i khi Ä‘i vá» phÃ­a cÃ¡c táº§ng cuá»‘i. KhÃ¡m phÃ¡ nÃ y á»§ng há»™ giáº£ thuyáº¿t vá» sá»± chuyá»ƒn dá»‹ch chá»©c nÄƒng cá»§a mÃ´ hÃ¬nh: tá»« viá»‡c giáº£i mÃ£ Ä‘áº·c tÃ­nh cá»§a token hiá»‡n táº¡i sang viá»‡c tÃ­ch há»£p ngá»¯ cáº£nh Ä‘á»ƒ dá»± bÃ¡o tÆ°Æ¡ng lai.

---

## 1. PhÃ¢n tÃ­ch chá»©c nÄƒng má»Ÿ rá»™ng (Exercise 5)

### 1.1. Sá»± chá»n lá»c ngá»¯ nghÄ©a (Semantic Selectivity)
ThÃ´ng qua báº£n Ä‘á»“ nhiá»‡t hoáº¡t hÃ³a, chÃºng ta quan sÃ¡t tháº¥y nÆ¡-ron cÃ³ Beta cao nháº¥t khÃ´ng chá»‰ pháº£n á»©ng vá»›i cÃ¡c tá»« phá»§ Ä‘á»‹nh thuáº§n tÃºy (*not, neither, won't*) mÃ  cÃ²n kÃ­ch hoáº¡t máº¡nh vá»›i cÃ¡c tá»« mang sáº¯c thÃ¡i tiÃªu cá»±c hoáº·c suy tÃ n (vÃ­ dá»¥: *rusty, dead, corroded*). 
- **Giáº£i thÃ­ch:** Äiá»u nÃ y chá»©ng tá» nÆ¡-ron Ä‘Ã£ há»c Ä‘Æ°á»£c má»™t "tÃ­nh nÄƒng" (feature) trá»«u tÆ°á»£ng hÆ¡n lÃ  chá»‰ má»™t danh sÃ¡ch tá»« vá»±ng. NÃ³ mÃ£ hÃ³a má»™t khÃ¡i niá»‡m logic/ngá»¯ nghÄ©a rá»™ng hÆ¡n vá» sá»± phá»§ Ä‘á»‹nh hoáº·c sá»± thiáº¿u váº¯ng.

---

## 2. Xu hÆ°á»›ng xuyÃªn Táº§ng (Exercise 6)

### 2.1. Máº­t Ä‘á»™ NÆ¡-ron cÃ³ Ã nghÄ©a Thá»‘ng kÃª
ChÃºng ta cháº¡y há»“i quy trÃªn toÃ n bá»™ 36 táº§ng vÃ  Ã¡p dá»¥ng hiá»‡u chá»‰nh Bonferroni kháº¯t khe ($p < 0.05 / 5120$).
- **Táº§ng Ä‘áº§u (Early Layers):** LÃªn Ä‘áº¿n 70% sá»‘ nÆ¡-ron trong táº§ng bá»™c lá»™ tÃ­nh chá»n lá»c phá»§ Ä‘á»‹nh rÃµ rá»‡t.
- **Táº§ng cuá»‘i (Late Layers):** Tá»· lá»‡ nÃ y giáº£m xuá»‘ng chá»‰ cÃ²n khoáº£ng 25%. Máº·c dÃ¹ giáº£m máº¡nh, nhÆ°ng váº«n cÃ²n khoáº£ng 1000 nÆ¡-ron duy trÃ¬ Ä‘Æ°á»£c tÃ­n hiá»‡u, cho tháº¥y cÆ¡ cháº¿ nÃ y váº«n Ä‘Æ°á»£c báº£o tá»“n má»™t pháº§n á»Ÿ giai Ä‘oáº¡n cuá»‘i.

### 2.2. Hiá»‡u nÄƒng Dá»± bÃ¡o (Accuracy)
Äá»™ chÃ­nh xÃ¡c trung bÃ¬nh cá»§a cÃ¡c nÆ¡-ron Ä‘Æ°á»£c chá»n lá»c (Significant Positive Beta) cÅ©ng bá»™c lá»™ xu hÆ°á»›ng tÆ°Æ¡ng tá»±:
- Äáº¡t Ä‘á»‰nh khoáº£ng 75-80% táº¡i cÃ¡c táº§ng tháº¥p.
- Giáº£m xuá»‘ng má»©c 65% á»Ÿ cÃ¡c táº§ng cao nháº¥t (vÆ°á»£t trÃªn má»©c ngáº«u nhiÃªn 50%).

---

## 3. Kiáº¿n giáº£i vá» Kiáº¿n trÃºc Transformer

### 3.1. Chuyá»ƒn dá»‹ch tá»« Hiá»‡n táº¡i sang TÆ°Æ¡ng lai
Sá»± suy giáº£m cá»§a cÃ¡c "nÆ¡-ron phá»§ Ä‘á»‹nh" á»Ÿ cÃ¡c táº§ng sÃ¢u cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i thÃ­ch báº±ng nhiá»‡m vá»¥ chÃ­nh cá»§a mÃ´ hÃ¬nh: **Dá»± bÃ¡o Token tiáº¿p theo (Next Token Prediction)**.
- **Giai Ä‘oáº¡n Ä‘áº§u:** Residual stream chá»©a thÃ´ng tin Ä‘áº­m Ä‘áº·c vá» thuá»™c tÃ­nh cá»§a chÃ­nh token Ä‘Ã³. CÃ¡c nÆ¡-ron MLP táº­p trung giáº£i mÃ£ logic cá»§a token hiá»‡n táº¡i.
- **Giai Ä‘oáº¡n cuá»‘i:** MÃ´ hÃ¬nh Æ°u tiÃªn viá»‡c chuáº©n bá»‹ Logits cho tá»« tiáº¿p theo. Do Ä‘Ã³, cÃ¡c biá»ƒu diá»…n vá» "hiá»‡n táº¡i" (nhÆ° phá»§ Ä‘á»‹nh) bá»‹ má» nháº¡t dáº§n Ä‘á»ƒ nhÆ°á»ng chá»— cho cÃ¡c dá»± bÃ¡o vá» "tÆ°Æ¡ng lai".

---

## 4. Ká»¹ thuáº­t Láº­p trÃ¬nh: Ma tráº­n Máº·t náº¡ (Masked Arrays)
Äá»ƒ tÃ­nh toÃ¡n trung bÃ¬nh trÃªn cÃ¡c táº­p há»£p nÆ¡-ron thá»a mÃ£n Ä‘á»“ng thá»i hai Ä‘iá»u kiá»‡n ($\beta > 0$ vÃ  $p < \alpha$), chÃºng ta sá»­ dá»¥ng `np.ma.masked_array`.
- **LÆ°u Ã½ quan trá»ng:** Trong NumPy, mask cÃ³ giÃ¡ trá»‹ `True` nghÄ©a lÃ  Ä‘iá»ƒm dá»¯ liá»‡u bá»‹ che khuáº¥t (khÃ´ng tÃ­nh). VÃ¬ váº­y, Ä‘á»ƒ láº¥y cÃ¡c nÆ¡-ron Ã½ nghÄ©a, chÃºng ta pháº£i sá»­ dá»¥ng toÃ¡n tá»­ nghá»‹ch Ä‘áº£o (`~`) trÃªn mask Ä‘iá»u kiá»‡n.

---

## 5. Káº¿t Luáº­n Chung
Thá»­ thÃ¡ch nÃ y lÃ m ná»•i báº­t tÃ­nh phá»©c táº¡p vÃ  thÃº vá»‹ cá»§a Mechanistic Interpretability. ChÃºng ta Ä‘Ã£ biáº¿n má»™t cÃ¢u há»i ngÃ´n ngá»¯ há»c trá»«u tÆ°á»£ng ("MÃ´ hÃ¬nh xá»­ lÃ½ sá»± phá»§ Ä‘á»‹nh nhÆ° tháº¿ nÃ o?") thÃ nh má»™t bÃ i toÃ¡n Ä‘á»‹nh lÆ°á»£ng vá»›i dá»¯ liá»‡u thá»±c nghiá»‡m. Viá»‡c hiá»ƒu rÃµ báº£n cháº¥t thá»‘ng kÃª vÃ  xu hÆ°á»›ng cá»§a nÆ¡-ron theo táº§ng lÃ  bÆ°á»›c chuáº©n bá»‹ quan trá»ng Ä‘á»ƒ khÃ¡m phÃ¡ cÃ¡c thÃ nh pháº§n khÃ¡c nhÆ° cÃ¡c Ä‘áº§u Attention (Attention Heads) trong cÃ¡c nghiÃªn cá»©u tiáº¿p theo.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. Tá»•ng káº¿t Negation tuning trÃªn GPT-2 Large dá»±a trÃªn `aero_LLM_22_CodeChallenge Negation tuning in MLP neurons (part 3).md`. PhÃ¢n tÃ­ch xu hÆ°á»›ng Accuracy theo táº§ng vÃ  giáº£ thuyáº¿t chuyá»ƒn dá»‹ch chá»©c nÄƒng.
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

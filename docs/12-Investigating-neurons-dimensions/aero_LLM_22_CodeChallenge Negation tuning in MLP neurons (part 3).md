
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [12 Investigating neurons dimensions](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 3)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y tá»•ng káº¿t thá»­ thÃ¡ch nghiÃªn cá»©u vá» cÆ¡ cháº¿ phá»§ Ä‘á»‹nh trong GPT-2 Large báº±ng cÃ¡ch phÃ¢n tÃ­ch sá»± biáº¿n Ä‘á»•i cá»§a nÆ¡-ron xuyÃªn suá»‘t toÃ n bá»™ chiá»u sÃ¢u cá»§a máº¡ng Transformer. ChÃºng ta thá»±c hiá»‡n má»™t phÃ¢n tÃ­ch há»‡ thá»‘ng trÃªn 36 táº§ng, Ä‘á»‹nh lÆ°á»£ng máº­t Ä‘á»™ vÃ  hiá»‡u nÄƒng cá»§a cÃ¡c "nÆ¡-ron phá»§ Ä‘á»‹nh". Káº¿t quáº£ thá»±c nghiá»‡m cho tháº¥y má»™t xu hÆ°á»›ng suy giáº£m rÃµ rá»‡t vá» cáº£ sá»‘ lÆ°á»£ng nÆ¡-ron chuyÃªn biá»‡t hÃ³a láº«n Ä‘á»™ chÃ­nh xÃ¡c phÃ¢n loáº¡i khi Ä‘i vá» phÃ­a cÃ¡c táº§ng cuá»‘i. KhÃ¡m phÃ¡ nÃ y á»§ng há»™ giáº£ thuyáº¿t vá» sá»± chuyá»ƒn dá»‹ch chá»©c nÄƒng cá»§a mÃ´ hÃ¬nh: tá»« viá»‡c giáº£i mÃ£ Ä‘áº·c tÃ­nh cá»§a token hiá»‡n táº¡i sang viá»‡c tÃ­ch há»£p ngá»¯ cáº£nh Ä‘á»ƒ dá»± bÃ¡o tÆ°Æ¡ng lai.

---

## 1. PhÃ¢n tÃ­ch chá»©c nÄƒng má»Ÿ rá»™ng (Exercise 5)

### 1.1. Sá»± chá»n lá»c ngá»¯ nghÄ©a (Semantic Selectivity)
ThÃ´ng qua báº£n Ä‘á»“ nhiá»‡t hoáº¡t hÃ³a, chÃºng ta quan sÃ¡t tháº¥y nÆ¡-ron cÃ³ Beta cao nháº¥t khÃ´ng chá»‰ pháº£n á»©ng vá»›i cÃ¡c tá»« phá»§ Ä‘á»‹nh thuáº§n tÃºy (*not, neither, won't*) mÃ  cÃ²n kÃ­ch hoáº¡t máº¡nh vá»›i cÃ¡c tá»« mang sáº¯c thÃ¡i tiÃªu cá»±c hoáº·c suy tÃ n (vÃ­ dá»¥: *rusty, dead, corroded*). 
- **Giáº£i thÃ­ch:** Äiá»u nÃ y chá»©ng tá» nÆ¡-ron Ä‘Ã£ há»c Ä‘Æ°á»£c má»™t "tÃ­nh nÄƒng" (feature) trá»«u tÆ°á»£ng hÆ¡n lÃ  chá»‰ má»™t danh sÃ¡ch tá»« vá»±ng. NÃ³ mÃ£ hÃ³a má»™t khÃ¡i niá»‡m logic/ngá»¯ nghÄ©a rá»™ng hÆ¡n vá» sá»± phá»§ Ä‘á»‹nh hoáº·c sá»± thiáº¿u váº¯ng.

---

## 2. Xu hÆ°á»›ng xuyÃªn Táº§ng (Exercise 6)

### 2.1. Máº­t Ä‘á»™ NÆ¡-ron cÃ³ Ã nghÄ©a Thá»‘ng kÃª
ChÃºng ta cháº¡y há»“i quy trÃªn toÃ n bá»™ 36 táº§ng vÃ  Ã¡p dá»¥ng hiá»‡u chá»‰nh Bonferroni kháº¯t khe ($p < 0.05 / 5120$).
- **Táº§ng Ä‘áº§u (Early Layers):** LÃªn Ä‘áº¿n 70% sá»‘ nÆ¡-ron trong táº§ng bá»™c lá»™ tÃ­nh chá»n lá»c phá»§ Ä‘á»‹nh rÃµ rá»‡t.
- **Táº§ng cuá»‘i (Late Layers):** Tá»· lá»‡ nÃ y giáº£m xuá»‘ng chá»‰ cÃ²n khoáº£ng 25%. Máº·c dÃ¹ giáº£m máº¡nh, nhÆ°ng váº«n cÃ²n khoáº£ng 1000 nÆ¡-ron duy trÃ¬ Ä‘Æ°á»£c tÃ­n hiá»‡u, cho tháº¥y cÆ¡ cháº¿ nÃ y váº«n Ä‘Æ°á»£c báº£o tá»“n má»™t pháº§n á»Ÿ giai Ä‘oáº¡n cuá»‘i.

### 2.2. Hiá»‡u nÄƒng Dá»± bÃ¡o (Accuracy)
Äá»™ chÃ­nh xÃ¡c trung bÃ¬nh cá»§a cÃ¡c nÆ¡-ron Ä‘Æ°á»£c chá»n lá»c (Significant Positive Beta) cÅ©ng bá»™c lá»™ xu hÆ°á»›ng tÆ°Æ¡ng tá»±:
- Äáº¡t Ä‘á»‰nh khoáº£ng 75-80% táº¡i cÃ¡c táº§ng tháº¥p.
- Giáº£m xuá»‘ng má»©c 65% á»Ÿ cÃ¡c táº§ng cao nháº¥t (vÆ°á»£t trÃªn má»©c ngáº«u nhiÃªn 50%).

---

## 3. Kiáº¿n giáº£i vá» Kiáº¿n trÃºc Transformer

### 3.1. Chuyá»ƒn dá»‹ch tá»« Hiá»‡n táº¡i sang TÆ°Æ¡ng lai
Sá»± suy giáº£m cá»§a cÃ¡c "nÆ¡-ron phá»§ Ä‘á»‹nh" á»Ÿ cÃ¡c táº§ng sÃ¢u cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i thÃ­ch báº±ng nhiá»‡m vá»¥ chÃ­nh cá»§a mÃ´ hÃ¬nh: **Dá»± bÃ¡o Token tiáº¿p theo (Next Token Prediction)**.
- **Giai Ä‘oáº¡n Ä‘áº§u:** Residual stream chá»©a thÃ´ng tin Ä‘áº­m Ä‘áº·c vá» thuá»™c tÃ­nh cá»§a chÃ­nh token Ä‘Ã³. CÃ¡c nÆ¡-ron MLP táº­p trung giáº£i mÃ£ logic cá»§a token hiá»‡n táº¡i.
- **Giai Ä‘oáº¡n cuá»‘i:** MÃ´ hÃ¬nh Æ°u tiÃªn viá»‡c chuáº©n bá»‹ Logits cho tá»« tiáº¿p theo. Do Ä‘Ã³, cÃ¡c biá»ƒu diá»…n vá» "hiá»‡n táº¡i" (nhÆ° phá»§ Ä‘á»‹nh) bá»‹ má» nháº¡t dáº§n Ä‘á»ƒ nhÆ°á»ng chá»— cho cÃ¡c dá»± bÃ¡o vá» "tÆ°Æ¡ng lai".

---

## 4. Ká»¹ thuáº­t Láº­p trÃ¬nh: Ma tráº­n Máº·t náº¡ (Masked Arrays)
Äá»ƒ tÃ­nh toÃ¡n trung bÃ¬nh trÃªn cÃ¡c táº­p há»£p nÆ¡-ron thá»a mÃ£n Ä‘á»“ng thá»i hai Ä‘iá»u kiá»‡n ($\beta > 0$ vÃ  $p < \alpha$), chÃºng ta sá»­ dá»¥ng `np.ma.masked_array`.
- **LÆ°u Ã½ quan trá»ng:** Trong NumPy, mask cÃ³ giÃ¡ trá»‹ `True` nghÄ©a lÃ  Ä‘iá»ƒm dá»¯ liá»‡u bá»‹ che khuáº¥t (khÃ´ng tÃ­nh). VÃ¬ váº­y, Ä‘á»ƒ láº¥y cÃ¡c nÆ¡-ron Ã½ nghÄ©a, chÃºng ta pháº£i sá»­ dá»¥ng toÃ¡n tá»­ nghá»‹ch Ä‘áº£o (`~`) trÃªn mask Ä‘iá»u kiá»‡n.

---

## 5. Káº¿t Luáº­n Chung
Thá»­ thÃ¡ch nÃ y lÃ m ná»•i báº­t tÃ­nh phá»©c táº¡p vÃ  thÃº vá»‹ cá»§a Mechanistic Interpretability. ChÃºng ta Ä‘Ã£ biáº¿n má»™t cÃ¢u há»i ngÃ´n ngá»¯ há»c trá»«u tÆ°á»£ng ("MÃ´ hÃ¬nh xá»­ lÃ½ sá»± phá»§ Ä‘á»‹nh nhÆ° tháº¿ nÃ o?") thÃ nh má»™t bÃ i toÃ¡n Ä‘á»‹nh lÆ°á»£ng vá»›i dá»¯ liá»‡u thá»±c nghiá»‡m. Viá»‡c hiá»ƒu rÃµ báº£n cháº¥t thá»‘ng kÃª vÃ  xu hÆ°á»›ng cá»§a nÆ¡-ron theo táº§ng lÃ  bÆ°á»›c chuáº©n bá»‹ quan trá»ng Ä‘á»ƒ khÃ¡m phÃ¡ cÃ¡c thÃ nh pháº§n khÃ¡c nhÆ° cÃ¡c Ä‘áº§u Attention (Attention Heads) trong cÃ¡c nghiÃªn cá»©u tiáº¿p theo.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. Tá»•ng káº¿t Negation tuning trÃªn GPT-2 Large dá»±a trÃªn `aero_LLM_22_CodeChallenge Negation tuning in MLP neurons (part 3).md`. PhÃ¢n tÃ­ch xu hÆ°á»›ng Accuracy theo táº§ng vÃ  giáº£ thuyáº¿t chuyá»ƒn dá»‹ch chá»©c nÄƒng.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [ğŸ“‚ Module: 12-Investigating-neurons-dimensions](README.md) | [Xem bÃ i viáº¿t â†’](README.md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Activation Maximization): CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Nhá»¯ng thÃ¡ch thá»©c trong LLM](aero_LLM_01_Activation maximization via gradient ascent (theory).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Activation maximization via gradient ascent (theory).md) |
| [Triá»ƒn khai Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a: Tá»« Gradient Ascent Ä‘áº¿n Giáº£i mÃ£ Token (Activation Maximization Implementation)](aero_LLM_02_Activation maximization (code).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Activation maximization (code).md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a qua Láº¥y máº«u Dá»¯ liá»‡u (Activation Maximization via Data Sampling)](aero_LLM_03_Activation maximization via data sampling.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_Activation maximization via data sampling.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Kiá»ƒm chá»©ng TÃ­nh láº·p láº¡i cá»§a Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Reproducibility of Activation Maximization)](aero_LLM_04_CodeChallenge Reproducibility of activation maximization.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Reproducibility of activation maximization.md) |
| [Giáº£i pháº«u Ná»™i táº¡i MÃ´ hÃ¬nh báº±ng Hooks: Ká»¹ thuáº­t TrÃ­ch xuáº¥t Hoáº¡t hÃ³a (Extracting Activations via Hooks)](aero_LLM_05_Extracting activations using hooks.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Extracting activations using hooks.md) |
| [Má»‘i tÆ°Æ¡ng quan giá»¯a Hooks vÃ  Hidden States: Giáº£i cáº¥u trÃºc Khá»‘i Transformer (Reconstructing Transformer Blocks)](aero_LLM_06_Relation between hooks and output.hidden_states.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_Relation between hooks and output.hidden_states.md) |
| [LÃ m rÃµ vá» Hidden States Táº§ng cuá»‘i: Vai trÃ² cá»§a LayerNorm (Clarification of Final Hidden States)](aero_LLM_07_Clarification of final hidden_states output.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Clarification of final hidden_states output.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 1)](aero_LLM_08_CodeChallenge Grammar tuning in MLP neurons (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge Grammar tuning in MLP neurons (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 2)](aero_LLM_09_CodeChallenge Grammar tuning in MLP neurons (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Grammar tuning in MLP neurons (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Sá»± Äiá»u cháº¿ Ngá»¯ cáº£nh trong Hoáº¡t hÃ³a MLP (Context-modulated Activation)](aero_LLM_10_CodeChallenge Context-modulated activation in MLP.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_CodeChallenge Context-modulated activation in MLP.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 1)](aero_LLM_11_CodeChallenge Activation histograms by token length (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Activation histograms by token length (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 2)](aero_LLM_12_CodeChallenge Activation histograms by token length (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_CodeChallenge Activation histograms by token length (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 3)](aero_LLM_13_CodeChallenge Activation histograms by token length (part 3).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_CodeChallenge Activation histograms by token length (part 3).md) |
| [Xá»­ lÃ½ Biá»ƒu diá»…n NÆ¡-ron cho cÃ¡c Tá»« Ä‘a Token (Multi-token Words)](aero_LLM_14_Dealing with multitoken word embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_Dealing with multitoken word embeddings.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 1)](aero_LLM_15_CodeChallenge Category-tuned MLP projections (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_CodeChallenge Category-tuned MLP projections (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 2)](aero_LLM_16_CodeChallenge Category-tuned MLP projections (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_CodeChallenge Category-tuned MLP projections (part 2).md) |
| [Há»“i quy Logistic: LÃ½ thuyáº¿t vÃ  Triá»ƒn khai PhÃ¢n loáº¡i NÆ¡-ron](aero_LLM_17_Classification via logistic regression theory and code.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_Classification via logistic regression theory and code.md) |
| [Äá»‘i chiáº¿u Há»“i quy Logistic vÃ  Kiá»ƒm Ä‘á»‹nh T-test: Giáº£ Ä‘á»‹nh vÃ  á»¨ng dá»¥ng](aero_LLM_18_Logistic regression vs. t-test assumptions and applications.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_Logistic regression vs. t-test assumptions and applications.md) |
| [Äiá»u chá»‰nh Danh tá»« riÃªng trong GPT-2 Medium](aero_LLM_19_Proper noun tuning in GPT2-medium.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_19_Proper noun tuning in GPT2-medium.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 1)](aero_LLM_20_CodeChallenge Negation tuning in MLP neurons (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_20_CodeChallenge Negation tuning in MLP neurons (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 2)](aero_LLM_21_CodeChallenge Negation tuning in MLP neurons (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_21_CodeChallenge Negation tuning in MLP neurons (part 2).md) |
| ğŸ“Œ **[Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 3)](aero_LLM_22_CodeChallenge Negation tuning in MLP neurons (part 3).md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_22_CodeChallenge Negation tuning in MLP neurons (part 3).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron QVK (Attention)](aero_LLM_23_CodeChallenge Negation tuning in QVK neurons.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_23_CodeChallenge Negation tuning in QVK neurons.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

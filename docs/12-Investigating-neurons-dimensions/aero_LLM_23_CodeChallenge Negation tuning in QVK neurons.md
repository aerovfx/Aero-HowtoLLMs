
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [12 Investigating neurons dimensions](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron QVK (Attention)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y má»Ÿ rá»™ng nghiÃªn cá»©u vá» cÆ¡ cháº¿ phá»§ Ä‘á»‹nh tá»« cÃ¡c nÆ¡-ron MLP sang cÃ¡c Ä‘Æ¡n vá»‹ trong lá»›p Attention (Query, Key, Value - QVK). Sá»­ dá»¥ng cÃ¹ng má»™t bá»™ dá»¯ liá»‡u tá»« Philip K. Dick vÃ  phÆ°Æ¡ng phÃ¡p Há»“i quy Logistic, chÃºng ta so sÃ¡nh hÃ nh vi cá»§a cÃ¡c thÃ nh pháº§n Attention vá»›i cÃ¡c phÃ¡t hiá»‡n trÆ°á»›c Ä‘Ã³ vá» MLP. NghiÃªn cá»©u triá»ƒn khai ká»¹ thuáº­t tÃ¡ch ma tráº­n (tensor splitting) Ä‘á»ƒ phÃ¢n tÃ­ch Ä‘á»™c láº­p ba kÃªnh Q, K, V. Káº¿t quáº£ xÃ¡c nháº­n tÃ­nh phÃ¢n tÃ¡n (distributed) cá»§a biá»ƒu diá»…n logic phá»§ Ä‘á»‹nh trong toÃ n bá»™ kiáº¿n trÃºc mÃ´ hÃ¬nh, Ä‘á»“ng thá»i cá»§ng cá»‘ quan sÃ¡t vá» sá»± suy giáº£m tÃ­n hiá»‡u "hiá»‡n táº¡i" táº¡i cÃ¡c táº§ng sÃ¢u cá»§a máº¡ng Transformer.

---

## 1. Thiáº¿t láº­p Thá»±c nghiá»‡m vÃ  Hooking Attention

### 1.1. Chuyá»ƒn Ä‘á»•i Äá»‘i tÆ°á»£ng NghiÃªn cá»©u
Thay vÃ¬ táº­p trung vÃ o lá»›p má»Ÿ rá»™ng cá»§a MLP (`mlp.c_fc`), nghiÃªn cá»©u chuyá»ƒn hÆ°á»›ng sang lá»›p tuyáº¿n tÃ­nh cá»§a attention (`attn.c_attn`). Trong kiáº¿n trÃºc GPT-2 cá»§a OpenAI, cÃ¡c vector Query, Key vÃ  Value Ä‘Æ°á»£c táº¡o ra Ä‘á»“ng thá»i vÃ  ná»‘i tiáº¿p nhau trong má»™t ma tráº­n rá»™ng.
- **KÃ­ch thÆ°á»›c:** Trong GPT-2 Large ($d=1280$), lá»›p nÃ y cÃ³ kÃ­ch thÆ°á»›c $3 \times 1280 = 3840$ Ä‘Æ¡n vá»‹.
- **Vá»‹ trÃ­ trÃ­ch xuáº¥t:** ChÃºng ta thu tháº­p hoáº¡t hÃ³a **trÆ°á»›c** khi chÃºng Ä‘Æ°á»£c Ä‘Æ°a vÃ o phÆ°Æ¡ng trÃ¬nh tÃ­nh toÃ¡n Attention Score (Pre-attention activations).

### 1.2. TÃ¡i sá»­ dá»¥ng TÃ i nguyÃªn Dá»¯ liá»‡u
ToÃ n bá»™ quy trÃ¬nh lá»c token phá»§ Ä‘á»‹nh (*not, won't*) vÃ  kháº³ng Ä‘á»‹nh (*can, may*) tá»« cÃ¡c bÃ i thá»±c hÃ nh trÆ°á»›c Ä‘Æ°á»£c giá»¯ nguyÃªn. Äiá»u nÃ y Ä‘áº£m báº£o tÃ­nh khÃ¡ch quan khi so sÃ¡nh hiá»‡u nÄƒng giá»¯a khá»‘i MLP vÃ  khá»‘i Attention trÃªn cÃ¹ng má»™t ngá»¯ cáº£nh ngÃ´n ngá»¯.

---

## 2. Ká»¹ thuáº­t PhÃ¢n tÃ­ch Äa kÃªnh (Exercise 2)

### 2.1. Há»“i quy Logistic trÃªn Ma tráº­n Há»£p nháº¥t
Há»“i quy Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn toÃ n bá»™ 3840 Ä‘Æ¡n vá»‹ QVK cÃ¹ng má»™t lÃºc. Äiá»u nÃ y giÃºp tá»‘i Æ°u hÃ³a khá»‘i lÆ°á»£ng tÃ­nh toÃ¡n trÆ°á»›c khi Ä‘i sÃ¢u vÃ o chi tiáº¿t tá»«ng loáº¡i vector.

### 2.2. Ká»¹ thuáº­t TÃ¡ch Tensor (Tensor Splitting)
Äá»ƒ trá»±c quan hÃ³a sá»± khÃ¡c biá»‡t giá»¯a Query, Key vÃ  Value, chÃºng ta cáº§n tÃ¡ch ma tráº­n káº¿t quáº£:
- **ThÃ¡ch thá»©c:** CÃ¡c thÆ° viá»‡n nhÆ° NumPy khÃ´ng há»— trá»£ trá»±c tiáº¿p hÃ m tÃ¡ch theo kÃ­ch thÆ°á»›c linh hoáº¡t nhÆ° PyTorch.
- **Giáº£i phÃ¡p:** Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u (bao gá»“m cáº£ cÃ¡c Masked Arrays) sang `torch.tensor` vÃ  sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `.split(n_embed, dim=1)`. Ká»¹ thuáº­t nÃ y cho phÃ©p chÃºng ta cÃ´ láº­p cÃ¡c chá»‰ sá»‘ thá»‘ng kÃª ($\beta, p$, Accuracy) cho riÃªng tá»«ng thÃ nh pháº§n Q, K, V má»™t cÃ¡ch chÃ­nh xÃ¡c.

---

## 3. Káº¿t Quáº£ Quan SÃ¡t vÃ  Äá»‘i chiáº¿u

### 3.1. Sá»± tÆ°Æ¡ng Ä‘á»“ng vá»›i MLP
Xu hÆ°á»›ng xuyÃªn táº§ng cá»§a cÃ¡c Ä‘Æ¡n vá»‹ QVK pháº£n chiáº¿u gáº§n nhÆ° hoÃ n háº£o nhá»¯ng gÃ¬ Ä‘Ã£ quan sÃ¡t á»Ÿ MLP:
- **Tá»· lá»‡ nÆ¡-ron "nháº¡y cáº£m":** Giáº£m tá»« ~70% á»Ÿ cÃ¡c táº§ng Ä‘áº§u xuá»‘ng dÆ°á»›i 30% á»Ÿ cÃ¡c táº§ng cuá»‘i.
- **Äá»™ chÃ­nh xÃ¡c:** Giáº£m tá»« má»©c 75% xuá»‘ng gáº§n má»©c ngáº«u nhiÃªn (50-60%) khi tiáº¿n vá» phÃ­a Output layer.

### 3.2. Hiá»‡u nÄƒng cá»§a cÃ¡c Ä‘Æ¡n vá»‹ Q, K, V
Thá»±c nghiá»‡m cho tháº¥y cáº£ ba loáº¡i vector (Q, K, V) Ä‘á»u tham gia vÃ o viá»‡c mÃ£ hÃ³a sá»± phá»§ Ä‘á»‹nh, nhÆ°ng vá»›i cÃ¡c quy mÃ´ hiá»‡u á»©ng khÃ¡c nhau. Viá»‡c Ä‘á»™ chÃ­nh xÃ¡c giáº£m máº¡nh á»Ÿ cÃ¡c táº§ng cuá»‘i trong Attention sublayer cÃ ng cá»§ng cá»‘ giáº£ thuyáº¿t ráº±ng mÃ´ hÃ¬nh Ä‘ang Æ°u tiÃªn tÃ­ch há»£p thÃ´ng tin liÃªn ngá»¯ cáº£nh (inter-token) Ä‘á»ƒ dá»± Ä‘oÃ¡n tá»« tiáº¿p theo hÆ¡n lÃ  duy trÃ¬ Ä‘áº·c tÃ­nh ngá»¯ phÃ¡p cá»§a tá»« hiá»‡n táº¡i.

---

## 4. Tháº£o luáº­n vÃ  Káº¿t luáº­n

### 4.1. Báº£n cháº¥t PhÃ¢n tÃ¡n cá»§a LLM
Má»™t káº¿t luáº­n quan trá»ng rÃºt ra tá»« chuá»—i thá»­ thÃ¡ch nÃ y lÃ : CÃ¡c Ä‘áº·c tÃ­nh chá»©c nÄƒng (nhÆ° nháº­n diá»‡n phá»§ Ä‘á»‹nh) Ä‘Æ°á»£c phÃ¢n bá»• má»™t cÃ¡ch Ä‘á»‹nh lÆ°á»£ng (quantitative) thay vÃ¬ Ä‘á»‹nh tÃ­nh (qualitative). KhÃ´ng cÃ³ sá»± phÃ¢n chia module tuyá»‡t Ä‘á»‘i; thay vÃ o Ä‘Ã³, thÃ´ng tin vá» phá»§ Ä‘á»‹nh "tháº¥m" qua cáº£ MLP vÃ  Attention, giáº£m dáº§n theo chiá»u sÃ¢u nhÆ°ng khÃ´ng bao giá» biáº¿n máº¥t hoÃ n toÃ n.

### 4.2. BÃ i há»c vá» Láº­p trÃ¬nh Khoa há»c
Viá»‡c dÃ nh thá»i gian Ä‘á»ƒ viáº¿t vÃ  hiá»ƒu tá»«ng dÃ²ng mÃ£ nguá»“n, thay vÃ¬ phá»¥ thuá»™c vÃ o cÃ¡c cÃ´ng cá»¥ AI táº¡o mÃ£, lÃ  khoáº£n Ä‘áº§u tÆ° cáº§n thiáº¿t Ä‘á»ƒ náº¯m báº¯t Ä‘Æ°á»£c cÃ¡c sáº¯c thÃ¡i tinh táº¿ trong dá»¯ liá»‡u. Sá»± hiá»ƒu biáº¿t vá» kiá»ƒu dá»¯ liá»‡u (Data types) vÃ  cÃ¡c phÃ©p biáº¿n Ä‘á»•i Tensor lÃ  ná»n táº£ng Ä‘á»ƒ tin tÆ°á»Ÿng vÃ o káº¿t quáº£ nghiÃªn cá»©u trong tÆ°Æ¡ng lai.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. Thá»­ thÃ¡ch Negation tuning trÃªn nÆ¡-ron QVK dá»±a trÃªn `aero_LLM_23_CodeChallenge Negation tuning in QVK neurons.md`. Sá»­ dá»¥ng `torch.split` Ä‘á»ƒ phÃ¢n tÃ­ch Ä‘a kÃªnh vÃ  so sÃ¡nh tÃ­nh phÃ¢n tÃ¡n vá»›i MLP.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [ğŸ“‚ Module: 12-Investigating-neurons-dimensions](README.md) | [Xem bÃ i viáº¿t â†’](README.md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Activation Maximization): CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Nhá»¯ng thÃ¡ch thá»©c trong LLM](aero_LLM_01_Activation maximization via gradient ascent (theory).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Activation maximization via gradient ascent (theory).md) |
| [Triá»ƒn khai Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a: Tá»« Gradient Ascent Ä‘áº¿n Giáº£i mÃ£ Token (Activation Maximization Implementation)](aero_LLM_02_Activation maximization (code).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Activation maximization (code).md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a qua Láº¥y máº«u Dá»¯ liá»‡u (Activation Maximization via Data Sampling)](aero_LLM_03_Activation maximization via data sampling.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_Activation maximization via data sampling.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Kiá»ƒm chá»©ng TÃ­nh láº·p láº¡i cá»§a Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Reproducibility of Activation Maximization)](aero_LLM_04_CodeChallenge Reproducibility of activation maximization.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Reproducibility of activation maximization.md) |
| [Giáº£i pháº«u Ná»™i táº¡i MÃ´ hÃ¬nh báº±ng Hooks: Ká»¹ thuáº­t TrÃ­ch xuáº¥t Hoáº¡t hÃ³a (Extracting Activations via Hooks)](aero_LLM_05_Extracting activations using hooks.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Extracting activations using hooks.md) |
| [Má»‘i tÆ°Æ¡ng quan giá»¯a Hooks vÃ  Hidden States: Giáº£i cáº¥u trÃºc Khá»‘i Transformer (Reconstructing Transformer Blocks)](aero_LLM_06_Relation between hooks and output.hidden_states.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_Relation between hooks and output.hidden_states.md) |
| [LÃ m rÃµ vá» Hidden States Táº§ng cuá»‘i: Vai trÃ² cá»§a LayerNorm (Clarification of Final Hidden States)](aero_LLM_07_Clarification of final hidden_states output.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Clarification of final hidden_states output.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 1)](aero_LLM_08_CodeChallenge Grammar tuning in MLP neurons (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge Grammar tuning in MLP neurons (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 2)](aero_LLM_09_CodeChallenge Grammar tuning in MLP neurons (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Grammar tuning in MLP neurons (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Sá»± Äiá»u cháº¿ Ngá»¯ cáº£nh trong Hoáº¡t hÃ³a MLP (Context-modulated Activation)](aero_LLM_10_CodeChallenge Context-modulated activation in MLP.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_CodeChallenge Context-modulated activation in MLP.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 1)](aero_LLM_11_CodeChallenge Activation histograms by token length (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Activation histograms by token length (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 2)](aero_LLM_12_CodeChallenge Activation histograms by token length (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_CodeChallenge Activation histograms by token length (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 3)](aero_LLM_13_CodeChallenge Activation histograms by token length (part 3).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_CodeChallenge Activation histograms by token length (part 3).md) |
| [Xá»­ lÃ½ Biá»ƒu diá»…n NÆ¡-ron cho cÃ¡c Tá»« Ä‘a Token (Multi-token Words)](aero_LLM_14_Dealing with multitoken word embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_Dealing with multitoken word embeddings.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 1)](aero_LLM_15_CodeChallenge Category-tuned MLP projections (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_CodeChallenge Category-tuned MLP projections (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 2)](aero_LLM_16_CodeChallenge Category-tuned MLP projections (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_CodeChallenge Category-tuned MLP projections (part 2).md) |
| [Há»“i quy Logistic: LÃ½ thuyáº¿t vÃ  Triá»ƒn khai PhÃ¢n loáº¡i NÆ¡-ron](aero_LLM_17_Classification via logistic regression theory and code.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_Classification via logistic regression theory and code.md) |
| [Äá»‘i chiáº¿u Há»“i quy Logistic vÃ  Kiá»ƒm Ä‘á»‹nh T-test: Giáº£ Ä‘á»‹nh vÃ  á»¨ng dá»¥ng](aero_LLM_18_Logistic regression vs. t-test assumptions and applications.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_Logistic regression vs. t-test assumptions and applications.md) |
| [Äiá»u chá»‰nh Danh tá»« riÃªng trong GPT-2 Medium](aero_LLM_19_Proper noun tuning in GPT2-medium.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_19_Proper noun tuning in GPT2-medium.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 1)](aero_LLM_20_CodeChallenge Negation tuning in MLP neurons (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_20_CodeChallenge Negation tuning in MLP neurons (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 2)](aero_LLM_21_CodeChallenge Negation tuning in MLP neurons (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_21_CodeChallenge Negation tuning in MLP neurons (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 3)](aero_LLM_22_CodeChallenge Negation tuning in MLP neurons (part 3).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_22_CodeChallenge Negation tuning in MLP neurons (part 3).md) |
| ğŸ“Œ **[Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron QVK (Attention)](aero_LLM_23_CodeChallenge Negation tuning in QVK neurons.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_23_CodeChallenge Negation tuning in QVK neurons.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

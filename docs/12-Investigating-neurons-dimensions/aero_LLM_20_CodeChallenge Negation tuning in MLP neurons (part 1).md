
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [12 Investigating neurons dimensions](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 1)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y khá»Ÿi Ä‘áº§u má»™t thá»­ thÃ¡ch láº­p trÃ¬nh chuyÃªn sÃ¢u nháº±m xÃ¡c Ä‘á»‹nh cÃ¡c nÆ¡-ron MLP chuyÃªn biá»‡t hÃ³a cho cÃ¡c khÃ¡i niá»‡m logic nhá»‹ phÃ¢n: Phá»§ Ä‘á»‹nh (Negation) vÃ  Kháº³ng Ä‘á»‹nh (Affirmation). Sá»­ dá»¥ng mÃ´ hÃ¬nh GPT-2 Large vÃ  vÄƒn báº£n tá»« tÃ¡c pháº©m cá»§a Philip K. Dick, chÃºng ta triá»ƒn khai má»™t quy trÃ¬nh trÃ­ch xuáº¥t hoáº¡t hÃ³a tá»‘i Æ°u thÃ´ng qua viá»‡c cáº£i tiáº¿n ká»¹ thuáº­t cáº¥y Hook. Quy trÃ¬nh bao gá»“m viá»‡c chuáº©n bá»‹ dá»¯ liá»‡u vÄƒn báº£n thá»±c táº¿, xÃ¢y dá»±ng cá»­a sá»• ngá»¯ cáº£nh (context window) Ä‘á»“ng nháº¥t vÃ  quáº£n lÃ½ bá»™ nhá»› hoáº¡t hÃ³a xuyÃªn suá»‘t cÃ¡c lÆ°á»£t cháº¡y forward pass.

---

## 1. Tá»‘i Æ°u hÃ³a Ká»¹ thuáº­t Hooks (Exercise 1)

### 1.1. Chuyá»ƒn Ä‘á»•i tá»« Input-centric sang Output-centric
Trong cÃ¡c bÃ i thá»±c hÃ nh trÆ°á»›c, chÃºng ta thÆ°á»ng hook vÃ o lá»›p MLP tá»•ng thá»ƒ vÃ  thá»±c hiá»‡n phÃ©p nhÃ¢n ma tráº­n thá»§ cÃ´ng bÃªn trong hÃ m hook Ä‘á»ƒ láº¥y hoáº¡t hÃ³a lá»›p má»Ÿ rá»™ng. Tuy nhiÃªn, phÆ°Æ¡ng phÃ¡p nÃ y gÃ¢y lÃ£ng phÃ­ tÃ i nguyÃªn do tÃ­nh toÃ¡n trÃ¹ng láº·p.
- **Cáº£i tiáº¿n:** Hook trá»±c tiáº¿p vÃ o thÃ nh pháº§n `c_fc` (lá»›p nÆ¡-ron má»Ÿ rá»™ng).
- **Lá»£i Ã­ch:** Láº¥y trá»±c tiáº¿p giÃ¡ trá»‹ `output` cá»§a lá»›p nÃ y, giÃºp mÃ£ nguá»“n gá»n nháº¹ vÃ  tá»‘i Æ°u hÃ³a tá»‘c Ä‘á»™ xá»­ lÃ½ khi lÃ m viá»‡c vá»›i mÃ´ hÃ¬nh lá»›n nhÆ° GPT-2 Large (5120 nÆ¡-ron má»—i lá»›p MLP).

---

## 2. Chuáº©n bá»‹ Dá»¯ liá»‡u NgÃ´n ngá»¯ (Exercise 2)

### 2.1. Nguá»“n dá»¯ liá»‡u vÃ  PhÃ¢n loáº¡i
NghiÃªn cá»©u sá»­ dá»¥ng vÄƒn báº£n tá»« Dá»± Ã¡n Gutenberg, cá»¥ thá»ƒ lÃ  cÃ¡c tÃ¡c pháº©m khoa há»c viá»…n tÆ°á»Ÿng Ä‘á»ƒ cÃ³ ngÃ´n ngá»¯ phong phÃº:
- **NhÃ³m Phá»§ Ä‘á»‹nh (Negations):** *not, cannot, can't, don't, won't, never, wasn't...*
- **NhÃ³m Kháº³ng Ä‘á»‹nh (Affirmations):** *can, could, may, will...*

### 2.2. Thuáº­t toÃ¡n Lá»c Token tinh vi
Viá»‡c tÃ¬m kiáº¿m token Ä‘Ã­ch khÃ´ng chá»‰ Ä‘Æ¡n giáº£n lÃ  khá»›p chuá»—i kÃ½ tá»± mÃ  cáº§n xá»­ lÃ½ cÃ¡c trÆ°á»ng há»£p chá»“ng láº¥n (vÃ­ dá»¥: khÃ´ng láº¥y tá»« "can" náº¿u nÃ³ lÃ  má»™t pháº§n cá»§a "cannot").
- **Kiá»ƒm tra Token káº¿ tiáº¿p:** Má»™t token Ä‘Æ°á»£c coi lÃ  tá»« Ä‘Ã­ch Ä‘á»™c láº­p chá»‰ khi token ngay sau nÃ³ báº¯t Ä‘áº§u báº±ng má»™t khoáº£ng tráº¯ng. Äiá»u nÃ y giÃºp loáº¡i bá» cÃ¡c trÆ°á»ng há»£p token Ä‘Ã­ch lÃ  tiá»n tá»‘ cá»§a má»™t tá»« dÃ i hÆ¡n (nhÆ° "connotative").

### 2.3. Cá»­a sá»• Ngá»¯ cáº£nh Äá»“ng nháº¥t
Äá»ƒ Ä‘áº£m báº£o cÃ¡c nÆ¡-ron cÃ³ Ä‘á»§ thÃ´ng tin ngá»¯ nghÄ©a, má»—i tá»« Ä‘Ã­ch pháº£i náº±m trong má»™t cá»­a sá»•: **[90 tokens trÆ°á»›c] + [Target Word] + [10 tokens sau]**. Nhá»¯ng tá»« Ä‘Ã­ch xuáº¥t hiá»‡n quÃ¡ gáº§n Ä‘áº§u hoáº·c cuá»‘i vÄƒn báº£n sáº½ bá»‹ loáº¡i bá» Ä‘á»ƒ trÃ¡nh lá»—i biÃªn.

---

## 3. TrÃ­ch xuáº¥t Hoáº¡t hÃ³a (Exercise 3)

### 3.1. Cáº¥u trÃºc Batch
Dá»¯ liá»‡u Ä‘Æ°á»£c tá»• chá»©c thÃ nh hai tensors Batch cÃ³ kÃ­ch thÆ°á»›c $[N, 101]$, trong Ä‘Ã³ 101 lÃ  tá»•ng Ä‘á»™ dÃ i chuá»—i context. Vá»‹ trÃ­ index 90 trong má»i chuá»—i luÃ´n lÃ  token Ä‘Ã­ch, giÃºp Ä‘Æ¡n giáº£n hÃ³a viá»‡c láº­p chá»‰ má»¥c (indexing) sau nÃ y.

### 3.2. Quáº£n lÃ½ TÃ i nguyÃªn vÃ  Ghi Ä‘Ã¨
Do cÆ¡ cháº¿ cá»§a Hook Dictionary lÃ  ghi Ä‘Ã¨ (overwriting) dá»¯ liá»‡u sau má»—i láº§n gá»i `model()`, quy trÃ¬nh thá»±c hiá»‡n nhÆ° sau:
1. Cháº¡y Forward Pass cho nhÃ³m Negation $\rightarrow$ Sao chÃ©p dá»¯ liá»‡u tá»« dictionary sang má»™t biáº¿n lÆ°u trá»¯ riÃªng (`activs_neg`).
2. Cháº¡y Forward Pass cho nhÃ³m Affirmation $\rightarrow$ Sao chÃ©p sang `activs_aff`.
Sá»­ dá»¥ng `torch.no_grad()` vÃ  `model.eval()` lÃ  báº¯t buá»™c Ä‘á»ƒ giáº£i phÃ³ng bá»™ nhá»› vÃ  vÃ´ hiá»‡u hÃ³a cÃ¡c lá»›p Dropout/BatchNormalization.

---

## 4. Káº¿t Luáº­n Pháº§n 1
ChÃºng ta Ä‘Ã£ thiáº¿t láº­p xong "háº¡ táº§ng" dá»¯ liá»‡u vÃ  hoáº¡t hÃ³a. ÄÃ¢y lÃ  ná»n táº£ng vá»¯ng cháº¯c Ä‘á»ƒ triá»ƒn khai cÃ¡c phÃ¢n tÃ­ch thá»‘ng kÃª sÃ¢u hÆ¡n (nhÆ° Há»“i quy Logistic) nháº±m tÃ¬m ra nhá»¯ng nÆ¡-ron logic chá»‹u trÃ¡ch nhiá»‡m xá»­ lÃ½ sá»± phá»§ Ä‘á»‹nh trong pháº§n tiáº¿p theo.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. Thá»­ thÃ¡ch Negation tuning trÃªn GPT-2 Large dá»±a trÃªn `aero_LLM_20_CodeChallenge Negation tuning in MLP neurons (part 1).md`. Thiáº¿t láº­p Batch context vÃ  tá»‘i Æ°u hÃ³a Hook vÃ o lá»›p `c_fc`.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [ğŸ“‚ Module: 12-Investigating-neurons-dimensions](README.md) | [Xem bÃ i viáº¿t â†’](README.md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Activation Maximization): CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Nhá»¯ng thÃ¡ch thá»©c trong LLM](aero_LLM_01_Activation maximization via gradient ascent (theory).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Activation maximization via gradient ascent (theory).md) |
| [Triá»ƒn khai Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a: Tá»« Gradient Ascent Ä‘áº¿n Giáº£i mÃ£ Token (Activation Maximization Implementation)](aero_LLM_02_Activation maximization (code).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Activation maximization (code).md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a qua Láº¥y máº«u Dá»¯ liá»‡u (Activation Maximization via Data Sampling)](aero_LLM_03_Activation maximization via data sampling.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_Activation maximization via data sampling.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Kiá»ƒm chá»©ng TÃ­nh láº·p láº¡i cá»§a Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Reproducibility of Activation Maximization)](aero_LLM_04_CodeChallenge Reproducibility of activation maximization.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Reproducibility of activation maximization.md) |
| [Giáº£i pháº«u Ná»™i táº¡i MÃ´ hÃ¬nh báº±ng Hooks: Ká»¹ thuáº­t TrÃ­ch xuáº¥t Hoáº¡t hÃ³a (Extracting Activations via Hooks)](aero_LLM_05_Extracting activations using hooks.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Extracting activations using hooks.md) |
| [Má»‘i tÆ°Æ¡ng quan giá»¯a Hooks vÃ  Hidden States: Giáº£i cáº¥u trÃºc Khá»‘i Transformer (Reconstructing Transformer Blocks)](aero_LLM_06_Relation between hooks and output.hidden_states.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_Relation between hooks and output.hidden_states.md) |
| [LÃ m rÃµ vá» Hidden States Táº§ng cuá»‘i: Vai trÃ² cá»§a LayerNorm (Clarification of Final Hidden States)](aero_LLM_07_Clarification of final hidden_states output.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Clarification of final hidden_states output.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 1)](aero_LLM_08_CodeChallenge Grammar tuning in MLP neurons (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge Grammar tuning in MLP neurons (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 2)](aero_LLM_09_CodeChallenge Grammar tuning in MLP neurons (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Grammar tuning in MLP neurons (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Sá»± Äiá»u cháº¿ Ngá»¯ cáº£nh trong Hoáº¡t hÃ³a MLP (Context-modulated Activation)](aero_LLM_10_CodeChallenge Context-modulated activation in MLP.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_CodeChallenge Context-modulated activation in MLP.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 1)](aero_LLM_11_CodeChallenge Activation histograms by token length (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Activation histograms by token length (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 2)](aero_LLM_12_CodeChallenge Activation histograms by token length (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_CodeChallenge Activation histograms by token length (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 3)](aero_LLM_13_CodeChallenge Activation histograms by token length (part 3).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_CodeChallenge Activation histograms by token length (part 3).md) |
| [Xá»­ lÃ½ Biá»ƒu diá»…n NÆ¡-ron cho cÃ¡c Tá»« Ä‘a Token (Multi-token Words)](aero_LLM_14_Dealing with multitoken word embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_Dealing with multitoken word embeddings.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 1)](aero_LLM_15_CodeChallenge Category-tuned MLP projections (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_CodeChallenge Category-tuned MLP projections (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 2)](aero_LLM_16_CodeChallenge Category-tuned MLP projections (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_CodeChallenge Category-tuned MLP projections (part 2).md) |
| [Há»“i quy Logistic: LÃ½ thuyáº¿t vÃ  Triá»ƒn khai PhÃ¢n loáº¡i NÆ¡-ron](aero_LLM_17_Classification via logistic regression theory and code.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_Classification via logistic regression theory and code.md) |
| [Äá»‘i chiáº¿u Há»“i quy Logistic vÃ  Kiá»ƒm Ä‘á»‹nh T-test: Giáº£ Ä‘á»‹nh vÃ  á»¨ng dá»¥ng](aero_LLM_18_Logistic regression vs. t-test assumptions and applications.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_Logistic regression vs. t-test assumptions and applications.md) |
| [Äiá»u chá»‰nh Danh tá»« riÃªng trong GPT-2 Medium](aero_LLM_19_Proper noun tuning in GPT2-medium.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_19_Proper noun tuning in GPT2-medium.md) |
| ğŸ“Œ **[Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 1)](aero_LLM_20_CodeChallenge Negation tuning in MLP neurons (part 1).md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_20_CodeChallenge Negation tuning in MLP neurons (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 2)](aero_LLM_21_CodeChallenge Negation tuning in MLP neurons (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_21_CodeChallenge Negation tuning in MLP neurons (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 3)](aero_LLM_22_CodeChallenge Negation tuning in MLP neurons (part 3).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_22_CodeChallenge Negation tuning in MLP neurons (part 3).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron QVK (Attention)](aero_LLM_23_CodeChallenge Negation tuning in QVK neurons.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_23_CodeChallenge Negation tuning in QVK neurons.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

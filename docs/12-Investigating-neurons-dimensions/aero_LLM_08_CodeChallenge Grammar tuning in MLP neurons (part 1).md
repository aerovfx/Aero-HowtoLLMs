
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [12 Investigating neurons dimensions](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 1)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y trÃ¬nh bÃ y giai Ä‘oáº¡n Ä‘áº§u cá»§a thá»­ thÃ¡ch tÃ¬m kiáº¿m cÃ¡c nÆ¡-ron chuyÃªn biá»‡t cho cÃ¡c tá»« loáº¡i (parts of speech) trong phÃ¢n Ä‘oáº¡n má»Ÿ rá»™ng cá»§a lá»›p MLP trÃªn mÃ´ hÃ¬nh GPT-Neo. NghiÃªn cá»©u táº­p trung vÃ o viá»‡c so sÃ¡nh hoáº¡t hÃ³a cá»§a nÆ¡-ron trÆ°á»›c hai danh má»¥c tá»« vá»±ng: Danh tá»« (Nouns) vÃ  Äá»™ng tá»« (Verbs). Quy trÃ¬nh thá»±c nghiá»‡m bao gá»“m viá»‡c cáº¥y Hooks vÃ o cÃ¡c nÆ¡-ron má»Ÿ rá»™ng (expansion neurons) â€“ nÆ¡i Ä‘Æ°á»£c giáº£ thuyáº¿t lÃ  trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng phi tuyáº¿n tá»« residual stream â€“ vÃ  thu tháº­p dá»¯ liá»‡u pháº£n há»“i tá»« 200 tá»« vá»±ng phá»• biáº¿n. Káº¿t quáº£ sÆ¡ bá»™ cho tháº¥y sá»± tá»“n táº¡i cá»§a cÃ¡c thiÃªn kiáº¿n (biases) vÃ  sá»± biáº¿n thiÃªn nÆ¡-ron rÃµ rá»‡t, Ä‘áº·t ná»n táº£ng cho phÃ¢n tÃ­ch thá»‘ng kÃª chuyÃªn sÃ¢u á»Ÿ pháº§n tiáº¿p theo.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Má»™t cÃ¢u há»i trung tÃ¢m trong Diá»…n giáº£i há»c lÃ : CÃ¡c LLM cÃ³ sá»Ÿ há»¯u nhá»¯ng "nÆ¡-ron ngÃ´n ngá»¯" chuyÃªn biá»‡t khÃ´ng? VÃ­ dá»¥, cÃ³ tá»“n táº¡i má»™t nÆ¡-ron chá»‰ kÃ­ch hoáº¡t máº¡nh khi nhÃ¬n tháº¥y danh tá»« mÃ  im láº·ng trÆ°á»›c Ä‘á»™ng tá»«? BÃ¡o cÃ¡o nÃ y thiáº¿t láº­p mÃ´i trÆ°á»ng thá»±c nghiá»‡m Ä‘á»ƒ kiá»ƒm chá»©ng giáº£ thuyáº¿t Ä‘Ã³, táº­p trung vÃ o lá»›p MLP (Multi-Layer Perceptron) â€“ thÃ nh pháº§n Ä‘Æ°á»£c coi lÃ  "kho tri thá»©c" vÃ  "bá»™ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng" cá»§a kiáº¿n trÃºc Transformer.

---

## 2. CÆ¡ sá»Ÿ LÃ½ thuyáº¿t: Táº¡i sao láº¡i lÃ  MLP?
Trong má»™t khá»‘i Transformer:
- **Lá»›p Attention:** ÄÃ³ng vai trÃ² tÃ­ch há»£p ngá»¯ cáº£nh tá»« cÃ¡c token xung quanh (ai Ä‘ang lÃ m gÃ¬ cho ai).
- **Lá»›p MLP:** ÄÃ³ng vai trÃ² nháº­n diá»‡n cÃ¡c thuá»™c tÃ­nh ná»™i táº¡i cá»§a token (Ä‘Ã¢y lÃ  má»™t váº­t thá»ƒ hay má»™t hÃ nh Ä‘á»™ng).
Äáº·c biá»‡t, lá»›p má»Ÿ rá»™ng (C_FC) tÄƒng sá»‘ chiá»u lÃªn gáº¥p 4 láº§n (tá»« 768 lÃªn 3072 trong GPT-2 Small), táº¡o ra má»™t khÃ´ng gian rá»™ng lá»›n Ä‘á»ƒ mÃ´ hÃ¬nh phÃ¢n tÃ¡ch cÃ¡c khÃ¡i niá»‡m ngá»¯ nghÄ©a vÃ  ngá»¯ phÃ¡p.

---

## 3. Thiáº¿t láº­p Thá»±c nghiá»‡m (Methodology)

### 3.1. Chuáº©n bá»‹ Dá»¯ liá»‡u vÃ  MÃ´ hÃ¬nh
- **MÃ´ hÃ¬nh:** GPT-Neo 125M (sá»­ dá»¥ng tokenizer EleutherAI).
- **Dá»¯ liá»‡u:** Danh sÃ¡ch 100 Ä‘á»™ng tá»« vÃ  100 danh tá»« thÃ´ng dá»¥ng nháº¥t Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« cÃ¡c nguá»“n cÃ´ng khai.
- **Tráº¡ng thÃ¡i:** MÃ´ hÃ¬nh Ä‘Æ°á»£c thiáº¿t láº­p á»Ÿ cháº¿ Ä‘á»™ `eval()` Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh á»•n Ä‘á»‹nh cá»§a cÃ¡c hoáº¡t hÃ³a.

### 3.2. Cáº¥y Hook vÃ o Lá»›p Má»Ÿ rá»™ng (Expansion Layer)
Sá»­ dá»¥ng `register_forward_hook` vÃ o thÃ nh pháº§n `c_fc` cá»§a Transformer Block thá»© 9 (index 8). Äiá»ƒm thu tháº­p dá»¯ liá»‡u náº±m ngay sau khi thá»±c hiá»‡n phÃ©p nhÃ¢n ma tráº­n trá»ng sá»‘ nhÆ°ng trÆ°á»›c khi Ä‘i qua hÃ m kÃ­ch hoáº¡t phi tuyáº¿n (GELU). Äiá»u nÃ y cho phÃ©p ta quan sÃ¡t "tÆ° duy thÃ´" cá»§a nÆ¡-ron trÆ°á»›c khi bá»‹ nÃ©n bá»Ÿi cÆ¡ cháº¿ thÆ°a thá»›t (sparsity).

### 3.3. Thu tháº­p Hoáº¡t hÃ³a Diá»‡n rá»™ng
Dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u trá»¯ trong má»™t máº£ng 3 chiá»u cÃ³ kÃ­ch thÆ°á»›c `[2, 100, 3072]`:
- `2`: Danh má»¥c (0: Äá»™ng tá»«, 1: Danh tá»«).
- `100`: Sá»‘ lÆ°á»£ng tá»« trong má»—i danh má»¥c.
- `3072`: Sá»‘ lÆ°á»£ng nÆ¡-ron MLP.
*Ká»¹ thuáº­t quan trá»ng:* Sá»­ dá»¥ng `mean(dim=1)` Ä‘á»ƒ xá»­ lÃ½ cÃ¡c tá»« bá»‹ tÃ¡ch thÃ nh nhiá»u tokens, Ä‘áº£m báº£o má»—i tá»« (word) chá»‰ Ä‘áº¡i diá»‡n bá»Ÿi má»™t vector hoáº¡t hÃ³a duy nháº¥t.

---

## 4. Káº¿t Quáº£ SÆ¡ Bá»™ vÃ  Quan SÃ¡t
Äá»“ thá»‹ phÃ¢n bá»‘ hoáº¡t hÃ³a cho tháº¥y:
1. **Sá»± ThiÃªn lá»‡ch Há»‡ thá»‘ng (Systematic Offsets):** CÃ¡c nÆ¡-ron khÃ´ng hoáº¡t Ä‘á»™ng quanh má»©c 0 mÃ  thÆ°á»ng cÃ³ má»™t Ä‘iá»ƒm dá»«ng (mean offset) cá»‘ Ä‘á»‹nh cho háº§u háº¿t cÃ¡c tá»« (thÆ°á»ng lÃ  giÃ¡ trá»‹ Ã¢m).
2. **CÃ¡c BÄƒng dá»c (Vertical Bands):** Má»™t sá»‘ nÆ¡-ron cho tháº¥y biÃªn Ä‘á»™ hoáº¡t hÃ³a khÃ¡c biá»‡t rÃµ rá»‡t so vá»›i sá»‘ Ä‘Ã´ng trÃªn toÃ n bá»™ dáº£i tá»« vá»±ng thá»­ nghiá»‡m.
3. **TÃ­nh Biáº¿n thiÃªn:** Máº·c dÃ¹ nhÃ¬n tá»•ng thá»ƒ cÃ³ váº» Ä‘á»“ng nháº¥t, nhÆ°ng cÃ¡c nÆ¡-ron riÃªng láº» báº¯t Ä‘áº§u bá»™c lá»™ sá»± Æ°u tiÃªn nháº¹ Ä‘á»‘i vá»›i danh tá»« hoáº·c Ä‘á»™ng tá»« khi nhÃ¬n chi tiáº¿t vÃ o cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u.

---

## 5. Káº¿t Luáº­n Pháº§n 1
ChÃºng ta Ä‘Ã£ thÃ nh cÃ´ng trong viá»‡c xÃ¢y dá»±ng há»‡ thá»‘ng trÃ­ch xuáº¥t hoáº¡t hÃ³a quy mÃ´ lá»›n tá»« nÆ¡-ron MLP. Viá»‡c quan sÃ¡t tháº¥y cÃ¡c dáº£i hoáº¡t hÃ³a á»•n Ä‘á»‹nh lÃ  dáº¥u hiá»‡u tÃ­ch cá»±c cho tháº¥y cÃ¡c nÆ¡-ron nÃ y Ä‘ang "mÃ£ hÃ³a" nhá»¯ng thuá»™c tÃ­nh nháº¥t Ä‘á»‹nh cá»§a ngÃ´n ngá»¯. Pháº§n tiáº¿p theo sáº½ triá»ƒn khai cÃ¡c phÃ©p kiá»ƒm Ä‘á»‹nh thá»‘ng kÃª (t-test) Ä‘á»ƒ xÃ¡c Ä‘á»‹nh xem sá»± khÃ¡c biá»‡t giá»¯a danh tá»« vÃ  Ä‘á»™ng tá»« cÃ³ Ä‘áº¡t má»©c Ã½ nghÄ©a khoa há»c hay khÃ´ng.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. Thá»­ thÃ¡ch vá» Grammar tuning trÃªn GPT-Neo dá»±a trÃªn `aero_LLM_08_CodeChallenge Grammar tuning in MLP neurons (part 1).md`. Thiáº¿t láº­p Hooks vÃ  quy trÃ¬nh thu tháº­p dá»¯ liá»‡u nÆ¡-ron má»Ÿ rá»™ng.
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

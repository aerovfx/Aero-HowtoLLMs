
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [12 Investigating neurons dimensions](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ thÃ¡ch Láº­p trÃ¬nh: Kiá»ƒm chá»©ng TÃ­nh láº·p láº¡i cá»§a Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Reproducibility of Activation Maximization)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y trÃ¬nh bÃ y káº¿t quáº£ cá»§a má»™t thá»±c nghiá»‡m khoa há»c quan trá»ng: Kiá»ƒm chá»©ng tÃ­nh láº·p láº¡i (Reproducibility) cá»§a phÆ°Æ¡ng phÃ¡p Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a trÃªn GPT-2 Small. Báº±ng cÃ¡ch láº·p láº¡i quy trÃ¬nh tá»‘i Æ°u hÃ³a gradient 10 láº§n vá»›i cÃ¡c Ä‘iá»ƒm khá»Ÿi Ä‘áº§u ngáº«u nhiÃªn khÃ¡c nhau, nghiÃªn cá»©u tÃ¬m cÃ¡ch xÃ¡c Ä‘á»‹nh liá»‡u mÃ´ hÃ¬nh cÃ³ há»™i tá»¥ vá» má»™t "chuá»—i token lÃ½ tÆ°á»Ÿng" duy nháº¥t cho má»™t chiá»u nÆ¡-ron cá»¥ thá»ƒ hay khÃ´ng. Káº¿t quáº£ cho tháº¥y tÃ­nh láº·p láº¡i cá»±c ká»³ tháº¥p (48/50 tokens mang tÃ­nh duy nháº¥t), cung cáº¥p báº±ng chá»©ng thá»±c nghiá»‡m vá» tÃ­nh cháº¥t "há»—n loáº¡n" cá»§a khÃ´ng gian biá»ƒu diá»…n vÃ  thÃ¡ch thá»©c trong viá»‡c xÃ¡c Ä‘á»‹nh cÃ¡c Ä‘áº·c Ä‘iá»ƒm ngÃ´n ngá»¯ á»•n Ä‘á»‹nh thÃ´ng qua tá»‘i Æ°u hÃ³a ngÆ°á»£c.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Trong khoa há»c, má»™t phÃ¡t hiá»‡n chá»‰ Ä‘Æ°á»£c coi lÃ  cÃ³ giÃ¡ trá»‹ náº¿u nÃ³ cÃ³ thá»ƒ láº·p láº¡i Ä‘Æ°á»£c. Náº¿u quÃ¡ trÃ¬nh Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a thá»±c thá»±c sá»± tÃ¬m tháº¥y má»™t "khÃ¡i niá»‡m" mÃ  nÆ¡-ron Ä‘áº¡i diá»‡n, thÃ¬ báº¥t ká»ƒ ta báº¯t Ä‘áº§u tá»« nhiá»…u ngáº«u nhiÃªn nÃ o, thuáº­t toÃ¡n há»™i tá»¥ nÃªn dáº«n tá»›i cÃ¹ng má»™t káº¿t quáº£ (hoáº·c Ã­t nháº¥t lÃ  cÃ¡c káº¿t quáº£ tÆ°Æ¡ng Ä‘á»“ng vá» máº·t ngá»¯ nghÄ©a). Thá»­ thÃ¡ch nÃ y thiáº¿t láº­p má»™t quy trÃ¬nh Ä‘o lÆ°á»ng Ä‘á»‹nh lÆ°á»£ng cho sá»± há»™i tá»¥ nÃ y.

---

## 2. PhÆ°Æ¡ng PhÃ¡p Thá»±c Nghiá»‡m (Methodology)

### 2.1. ÄÃ³ng gÃ³i Quy trÃ¬nh Tá»‘i Æ°u hÃ³a (Exercise 1)
XÃ¢y dá»±ng hÃ m `train_optimization` Ä‘á»ƒ tá»± Ä‘á»™ng hÃ³a:
1. Khá»Ÿi táº¡o ma tráº­n nhÃºng ngáº«u nhiÃªn (5 tokens x 768 dims).
2. Cháº¡y Adam Optimizer qua 500 epochs.
3. Cá»±c Ä‘áº¡i hÃ³a hoáº¡t hÃ³a trung bÃ¬nh cá»§a layer 8, chiá»u 91.
4. Tráº£ vá» vector nhÃºng Ä‘Ã£ tá»‘i Æ°u.

### 2.2. Kiá»ƒm chá»©ng TÃ­nh láº·p láº¡i (Exercise 2)
- **Thiáº¿t láº­p:** Láº·p láº¡i hÃ m trÃªn 10 láº§n Ä‘á»™c láº­p.
- **Giáº£i mÃ£:** Chuyá»ƒn Ä‘á»•i 50 vectors káº¿t quáº£ (10 runs x 5 tokens) thÃ nh cÃ¡c tokens thá»±c táº¿ dá»±a trÃªn Ä‘á»™ tÆ°Æ¡ng quan Cosine cá»±c Ä‘áº¡i.
- **Äá»‹nh lÆ°á»£ng:** Sá»­ dá»¥ng `numpy.unique` Ä‘á»ƒ Ä‘áº¿m sá»‘ lÆ°á»£ng token duy nháº¥t vÃ  táº§n suáº¥t xuáº¥t hiá»‡n cá»§a chÃºng.

---

## 3. Káº¿t Quáº£ VÃ  PhÃ¢n TÃ­ch (Results & Analysis)

### 3.1. Sá»± PhÃ¢n tÃ¡n cá»§a Káº¿t quáº£ (Null Results)
Thá»±c nghiá»‡m cho tháº¥y trong sá»‘ 50 tokens thu Ä‘Æ°á»£c, cÃ³ tá»›i 48 tokens lÃ  duy nháº¥t. Chá»‰ cÃ³ 2 trÆ°á»ng há»£p láº·p láº¡i (vÃ­ dá»¥: token "sup"). 
- **Diá»…n giáº£i:** Tá»· lá»‡ láº·p láº¡i (2/50) lÃ  vÃ´ cÃ¹ng nhá», Ä‘iá»u nÃ y chá»©ng tá» nÆ¡-ron Ä‘Ã­ch khÃ´ng cÃ³ má»™t "ngÃ´n ngá»¯ máº¹ Ä‘áº»" cá»‘ Ä‘á»‹nh mÃ  chÃºng ta cÃ³ thá»ƒ dá»… dÃ ng giáº£i mÃ£ Ä‘Æ°á»£c báº±ng phÆ°Æ¡ng phÃ¡p nÃ y.

### 3.2. Hiá»‡u á»©ng cá»§a Äiá»ƒm khá»Ÿi Ä‘áº§u (Initialization Bias)
Máº·c dÃ¹ má»i thÃ´ng sá»‘ huáº¥n luyá»‡n (optimizer, loss function, model weights) lÃ  cá»‘ Ä‘á»‹nh, sá»± khÃ¡c biá»‡t duy nháº¥t lÃ  nhiá»…u khá»Ÿi táº¡o. Viá»‡c káº¿t quáº£ bá»‹ phÃ¢n tÃ¡n máº¡nh máº½ chá»‰ ra ráº±ng nÆ¡-ron Ä‘ang pháº£n á»©ng vá»›i cÃ¡c cáº¥u trÃºc toÃ¡n há»c trá»«u tÆ°á»£ng trong embedding khÃ´ng gian â€“ nhá»¯ng cáº¥u trÃºc nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c thá»a mÃ£n bá»Ÿi nhiá»u tá»• há»£p token khÃ¡c nhau má»™t cÃ¡ch ngáº«u nhiÃªn.

---

## 4. Tháº£o Luáº­n: GiÃ¡ trá»‹ cá»§a cÃ¡c "PhÃ¡t hiá»‡n Ã‚m tÃ­nh" (Null Findings)
BÃ¡o cÃ¡o kháº³ng Ä‘á»‹nh ráº±ng káº¿t quáº£ "khÃ´ng láº·p láº¡i" váº«n mang giÃ¡ trá»‹ tri thá»©c cao:
1. **TÃ­nh Phá»©c táº¡p:** NÃ³ xÃ¡c nháº­n ráº±ng LLM khÃ´ng hoáº¡t Ä‘á»™ng dá»±a trÃªn cÃ¡c "nhÃ£n tá»« Ä‘iá»ƒn" Ä‘Æ¡n giáº£n.
2. **YÃªu cáº§u vá» RÃ ng buá»™c:** Äá»ƒ phÆ°Æ¡ng phÃ¡p nÃ y hiá»‡u quáº£, cáº§n bá»• sung cÃ¡c rÃ ng buá»™c (priors) nhÆ° tÃ­nh trÆ¡n tru cá»§a vÄƒn báº£n hoáº·c nÃ©n chiá»u khÃ´ng gian, thay vÃ¬ tá»‘i Æ°u hÃ³a hoÃ n toÃ n ngáº«u nhiÃªn.
3. **Äá»™ háº¡t (Granularity):** Káº¿t quáº£ cÃ³ thá»ƒ kháº£ quan hÆ¡n náº¿u chÃºng ta táº­p trung vÃ o cÃ¡c nÆ¡-ron MLP chuyÃªn biá»‡t thay vÃ¬ cÃ¡c chiá»u trong residual stream tá»•ng quÃ¡t.

---

## 5. Káº¿t Luáº­n
Thá»­ thÃ¡ch nÃ y minh chá»©ng ráº±ng Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a nguyÃªn báº£n lÃ  má»™t cÃ´ng cá»¥ khÃ´ng á»•n Ä‘á»‹nh cho viá»‡c diá»…n giáº£i ngÃ´n ngá»¯. Sá»± thiáº¿u há»¥t tÃ­nh láº·p láº¡i má»Ÿ ra nhu cáº§u cho cÃ¡c ká»¹ thuáº­t trÃ­ch xuáº¥t hoáº¡t hÃ³a tinh vi hÆ¡n (nhÆ° Hooks trá»±c tiáº¿p vÃ o ná»™i bá»™ Transformer Block) vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p thá»‘ng kÃª thay tháº¿ nhÆ° láº¥y máº«u dá»¯ liá»‡u diá»‡n rá»™ng.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. Thá»­ thÃ¡ch vá» tÃ­nh láº·p láº¡i cá»§a Activation Maximization trÃªn GPT-2 Small dá»±a trÃªn `aero_LLM_04_CodeChallenge Reproducibility of activation maximization.md`. PhÃ¢n tÃ­ch sá»± phÃ¢n tÃ¡n cá»§a 10 láº§n cháº¡y Ä‘á»™c láº­p.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [ğŸ“‚ Module: 12-Investigating-neurons-dimensions](README.md) | [Xem bÃ i viáº¿t â†’](README.md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Activation Maximization): CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Nhá»¯ng thÃ¡ch thá»©c trong LLM](aero_LLM_01_Activation maximization via gradient ascent (theory).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Activation maximization via gradient ascent (theory).md) |
| [Triá»ƒn khai Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a: Tá»« Gradient Ascent Ä‘áº¿n Giáº£i mÃ£ Token (Activation Maximization Implementation)](aero_LLM_02_Activation maximization (code).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Activation maximization (code).md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a qua Láº¥y máº«u Dá»¯ liá»‡u (Activation Maximization via Data Sampling)](aero_LLM_03_Activation maximization via data sampling.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_Activation maximization via data sampling.md) |
| ğŸ“Œ **[Thá»­ thÃ¡ch Láº­p trÃ¬nh: Kiá»ƒm chá»©ng TÃ­nh láº·p láº¡i cá»§a Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Reproducibility of Activation Maximization)](aero_LLM_04_CodeChallenge Reproducibility of activation maximization.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Reproducibility of activation maximization.md) |
| [Giáº£i pháº«u Ná»™i táº¡i MÃ´ hÃ¬nh báº±ng Hooks: Ká»¹ thuáº­t TrÃ­ch xuáº¥t Hoáº¡t hÃ³a (Extracting Activations via Hooks)](aero_LLM_05_Extracting activations using hooks.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Extracting activations using hooks.md) |
| [Má»‘i tÆ°Æ¡ng quan giá»¯a Hooks vÃ  Hidden States: Giáº£i cáº¥u trÃºc Khá»‘i Transformer (Reconstructing Transformer Blocks)](aero_LLM_06_Relation between hooks and output.hidden_states.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_Relation between hooks and output.hidden_states.md) |
| [LÃ m rÃµ vá» Hidden States Táº§ng cuá»‘i: Vai trÃ² cá»§a LayerNorm (Clarification of Final Hidden States)](aero_LLM_07_Clarification of final hidden_states output.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Clarification of final hidden_states output.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 1)](aero_LLM_08_CodeChallenge Grammar tuning in MLP neurons (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge Grammar tuning in MLP neurons (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 2)](aero_LLM_09_CodeChallenge Grammar tuning in MLP neurons (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Grammar tuning in MLP neurons (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Sá»± Äiá»u cháº¿ Ngá»¯ cáº£nh trong Hoáº¡t hÃ³a MLP (Context-modulated Activation)](aero_LLM_10_CodeChallenge Context-modulated activation in MLP.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_CodeChallenge Context-modulated activation in MLP.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 1)](aero_LLM_11_CodeChallenge Activation histograms by token length (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Activation histograms by token length (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 2)](aero_LLM_12_CodeChallenge Activation histograms by token length (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_CodeChallenge Activation histograms by token length (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 3)](aero_LLM_13_CodeChallenge Activation histograms by token length (part 3).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_CodeChallenge Activation histograms by token length (part 3).md) |
| [Xá»­ lÃ½ Biá»ƒu diá»…n NÆ¡-ron cho cÃ¡c Tá»« Ä‘a Token (Multi-token Words)](aero_LLM_14_Dealing with multitoken word embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_Dealing with multitoken word embeddings.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 1)](aero_LLM_15_CodeChallenge Category-tuned MLP projections (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_CodeChallenge Category-tuned MLP projections (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 2)](aero_LLM_16_CodeChallenge Category-tuned MLP projections (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_CodeChallenge Category-tuned MLP projections (part 2).md) |
| [Há»“i quy Logistic: LÃ½ thuyáº¿t vÃ  Triá»ƒn khai PhÃ¢n loáº¡i NÆ¡-ron](aero_LLM_17_Classification via logistic regression theory and code.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_Classification via logistic regression theory and code.md) |
| [Äá»‘i chiáº¿u Há»“i quy Logistic vÃ  Kiá»ƒm Ä‘á»‹nh T-test: Giáº£ Ä‘á»‹nh vÃ  á»¨ng dá»¥ng](aero_LLM_18_Logistic regression vs. t-test assumptions and applications.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_Logistic regression vs. t-test assumptions and applications.md) |
| [Äiá»u chá»‰nh Danh tá»« riÃªng trong GPT-2 Medium](aero_LLM_19_Proper noun tuning in GPT2-medium.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_19_Proper noun tuning in GPT2-medium.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 1)](aero_LLM_20_CodeChallenge Negation tuning in MLP neurons (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_20_CodeChallenge Negation tuning in MLP neurons (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 2)](aero_LLM_21_CodeChallenge Negation tuning in MLP neurons (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_21_CodeChallenge Negation tuning in MLP neurons (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 3)](aero_LLM_22_CodeChallenge Negation tuning in MLP neurons (part 3).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_22_CodeChallenge Negation tuning in MLP neurons (part 3).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron QVK (Attention)](aero_LLM_23_CodeChallenge Negation tuning in QVK neurons.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_23_CodeChallenge Negation tuning in QVK neurons.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

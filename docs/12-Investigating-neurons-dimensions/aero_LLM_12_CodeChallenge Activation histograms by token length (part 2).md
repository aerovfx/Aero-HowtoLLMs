
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [12 Investigating neurons dimensions](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 2)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y tiáº¿p tá»¥c thá»­ thÃ¡ch nghiÃªn cá»©u vá» má»‘i quan há»‡ giá»¯a Ä‘á»™ dÃ i token vÃ  hoáº¡t hÃ³a nÆ¡-ron MLP, táº­p trung vÃ o viá»‡c so sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p phÃ¢n tÃ­ch thá»‘ng kÃª: PhÃ¢n nhÃ³m dá»±a trÃªn trung vá»‹ (Median Split) vÃ  PhÃ¢n tÃ­ch tÆ°Æ¡ng quan tuyáº¿n tÃ­nh (Pearson Correlation). NghiÃªn cá»©u thá»±c nghiá»‡m trÃªn GPT-Neo 125M vÃ  1.3B bá»™c lá»™ nhá»¯ng Ä‘á»™ng lá»±c há»c phá»©c táº¡p xuyÃªn suá»‘t cÃ¡c táº§ng. Káº¿t quáº£ cho tháº¥y xu hÆ°á»›ng "thiÃªn kiáº¿n token ngáº¯n" á»Ÿ cÃ¡c táº§ng giá»¯a, sá»± khÃ¡c biá»‡t Ä‘á»‹nh tÃ­nh á»Ÿ táº§ng Ä‘áº§u tiÃªn, vÃ  sá»± thu háº¹p phÃ¢n phá»‘i á»Ÿ cÃ¡c táº§ng cuá»‘i khi mÃ´ hÃ¬nh chuyá»ƒn dá»‹ch tiÃªu Ä‘iá»ƒm tá»« token hiá»‡n táº¡i sang dá»± bÃ¡o token tiáº¿p theo. BÃ¡o cÃ¡o cÅ©ng tháº£o luáº­n vá» giá»›i háº¡n cá»§a tÃ­nh phá»• quÃ¡t (universality) giá»¯a cÃ¡c quy mÃ´ mÃ´ hÃ¬nh khÃ¡c nhau.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Trong trÃ­ tuá»‡ nhÃ¢n táº¡o, viá»‡c biáº¿n Ä‘á»•i cÃ¡c biáº¿n liÃªn tá»¥c thÃ nh cÃ¡c pháº¡m trÃ¹ (discretization) â€“ nhÆ° viá»‡c chia token thÃ nh "ngáº¯n" vÃ  "dÃ i" â€“ lÃ  má»™t cÃ´ng cá»¥ trá»±c quan hÃ³a há»¯u Ã­ch nhÆ°ng tiá»m áº©n rá»§i ro máº¥t mÃ¡t thÃ´ng tin. BÃ¡o cÃ¡o nÃ y Ä‘á»‘i chiáº¿u phÆ°Æ¡ng phÃ¡p phÃ¢n nhÃ³m truyá»n thá»‘ng vá»›i cÃ¡c phÃ©p Ä‘o tÆ°Æ¡ng quan báº£o toÃ n Ä‘á»™ phong phÃº cá»§a dá»¯ liá»‡u, nháº±m má»¥c Ä‘Ã­ch giáº£i mÃ£ cÃ¡ch LLM Ä‘iá»u cháº¿ nÄƒng lÆ°á»£ng nÆ¡-ron dá»±a trÃªn cáº¥u trÃºc hÃ¬nh thÃ¡i cá»§a token.

---

## 2. PhÃ¢n tÃ­ch PhÃ¢n nhÃ³m (Median Split Analysis)

### 2.1. PhÃ¢n phá»‘i Hoáº¡t hÃ³a táº¡i Táº§ng 5
Thá»±c nghiá»‡m chia 8192 tokens thÃ nh ba nhÃ³m dá»±a trÃªn Ä‘á»™ dÃ i trung vá»‹ (4 kÃ½ tá»±). Biá»ƒu Ä‘á»“ histogram cho tháº¥y cÃ¡c dáº£i hoáº¡t hÃ³a cá»§a nhÃ³m "Ngáº¯n", "Trung bÃ¬nh" vÃ  "DÃ i" cÃ³ sá»± trÃ¹ng láº¯p Ä‘Ã¡ng ká»ƒ. Tuy nhiÃªn, quan sÃ¡t ká»¹ cho tháº¥y nhÃ³m token dÃ i cÃ³ xu hÆ°á»›ng dá»‹ch chuyá»ƒn nháº¹ vá» phÃ­a cÃ¡c giÃ¡ trá»‹ hoáº¡t hÃ³a Ã¢m hÆ¡n.
- **Nháº­n Ä‘á»‹nh:** PhÆ°Æ¡ng phÃ¡p phÃ¢n nhÃ³m chá»‰ ra xu hÆ°á»›ng má» nháº¡t, xÃ¡c nháº­n ráº±ng Ä‘á»™ dÃ i token khÃ´ng pháº£i lÃ  biáº¿n sá»‘ duy nháº¥t quyáº¿t Ä‘á»‹nh cÆ°á»ng Ä‘á»™ pháº£n á»©ng cá»§a MLP.

---

## 3. PhÃ¢n tÃ­ch TÆ°Æ¡ng quan Tuyáº¿n tÃ­nh (Correlation Analysis)

### 3.1. PhÃ©p Ä‘o Pearson trÃªn Táº­p Standardized
Äá»ƒ khai thÃ¡c toÃ n bá»™ 16 má»©c Ä‘á»™ dÃ i token, nghiÃªn cá»©u Ã¡p dá»¥ng há»‡ sá»‘ tÆ°Æ¡ng quan $r$ giá»¯a giÃ¡ trá»‹ hoáº¡t hÃ³a Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a ($z$-score) vÃ  Ä‘á»™ dÃ i token.
- **CÃ´ng thá»©c:** $r = \frac{\text{cov}(x,y)}{\sigma_x \sigma_y}$. Khi dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a ($\mu=0, \sigma=1$), há»‡ sá»‘ tÆ°Æ¡ng quan chÃ­nh báº±ng tÃ­ch vÃ´ hÆ°á»›ng cá»§a hai vector chia cho $n-1$.
- **Káº¿t quáº£:** Pháº§n lá»›n nÆ¡-ron bá»™c lá»™ tÆ°Æ¡ng quan Ã¢m yáº¿u. Äiá»u nÃ y cá»§ng cá»‘ giáº£ thuyáº¿t ráº±ng nÆ¡-ron MLP kÃ­ch hoáº¡t máº¡nh hÆ¡n Ä‘á»‘i vá»›i cÃ¡c token ngáº¯n â€“ vá»‘n lÃ  nhá»¯ng token xuáº¥t hiá»‡n vá»›i táº§n suáº¥t cao (high frequency) trong táº­p huáº¥n luyá»‡n.

---

## 4. Äá»™ng lá»±c há»c XuyÃªn táº§ng vÃ  Quy mÃ´ MÃ´ hÃ¬nh

### 4.1. Äáº·c thÃ¹ Táº§ng Ä‘áº§u vÃ  Táº§ng cuá»‘i
1. **Táº§ng 1 (First Block):** Bá»™c lá»™ hÃ nh vi trÃ¡i ngÆ°á»£c hoÃ n toÃ n vá»›i pháº§n cÃ²n láº¡i cá»§a mÃ´ hÃ¬nh â€“ tÆ°Æ¡ng quan dÆ°Æ¡ng máº¡nh máº½ (token dÃ i kÃ­ch hoáº¡t máº¡nh hÆ¡n). ÄÃ¢y lÃ  táº§ng gáº§n vá»›i dá»¯ liá»‡u thÃ´ nháº¥t, nÆ¡i mÃ´ hÃ¬nh Ä‘ang thá»±c hiá»‡n cÃ¡c phÃ¢n tÃ­ch hÃ¬nh thÃ¡i sÆ¡ cáº¥p.
2. **CÃ¡c táº§ng cuá»‘i:** PhÃ¢n phá»‘i tÆ°Æ¡ng quan co háº¹p láº¡i vÃ  tiáº¿n dáº§n vá» má»©c 0. Giáº£i thÃ­ch cÆ¡ há»c: Táº¡i Ä‘iá»ƒm gáº§n Ä‘áº§u ra, mÃ´ hÃ¬nh khÃ´ng cÃ²n xá»­ lÃ½ thuá»™c tÃ­nh cá»§a token hiá»‡n táº¡i mÃ  Ä‘Ã£ chuyá»ƒn sang tráº¡ng thÃ¡i trá»«u tÆ°á»£ng Ä‘á»ƒ dá»± bÃ¡o token tÆ°Æ¡ng lai.

### 4.2. So sÃ¡nh GPT-Neo 125M vs 1.3B
Khi nÃ¢ng quy mÃ´ mÃ´ hÃ¬nh lÃªn 10 láº§n, chÃºng ta quan sÃ¡t tháº¥y sá»± Ä‘á»©t gÃ£y cá»§a tÃ­nh phá»• quÃ¡t:
- **TÃ­nh Ä‘a hÃ¬nh (Multimodality):** á» mÃ´ hÃ¬nh 1.3B, phÃ¢n phá»‘i hoáº¡t hÃ³a khÃ´ng cÃ²n lÃ  Gaussian Ä‘Æ¡n thuáº§n mÃ  bá»™c lá»™ cÃ¡c cá»¥m (clusters) rÃµ rá»‡t, gá»£i Ã½ ráº±ng cÃ¡c nÆ¡-ron á»Ÿ quy mÃ´ lá»›n Ä‘Ã£ phÃ¢n hÃ³a thÃ nh cÃ¡c nhÃ³m chá»©c nÄƒng chuyÃªn biá»‡t hÃ³a sÃ¢u sáº¯c hÆ¡n.

---

## 5. Tháº£o Luáº­n: Biáº¿n nhiá»…u Táº§n suáº¥t (The Frequency Confound)
BÃ¡o cÃ¡o lÆ°u Ã½ ráº±ng "Ä‘á»™ dÃ i token" cÃ³ má»‘i tÆ°Æ¡ng quan nghá»‹ch Ä‘áº£o cháº·t cháº½ vá»›i "táº§n suáº¥t xuáº¥t hiá»‡n". CÃ¡c token ngáº¯n thÆ°á»ng lÃ  cÃ¡c tá»« chá»©c nÄƒng hoáº·c gá»‘c tá»« phá»• biáº¿n. Do Ä‘Ã³, sá»± kÃ­ch hoáº¡t máº¡nh hÆ¡n cá»§a MLP Ä‘á»‘i vá»›i token ngáº¯n cÃ³ thá»ƒ thá»±c cháº¥t lÃ  pháº£n á»©ng vá»›i sá»± quen thuá»™c (familiarity) thay vÃ¬ Ä‘á»™ dÃ i váº­t lÃ½.

---

## 6. Káº¿t Luáº­n
NghiÃªn cá»©u kháº³ng Ä‘á»‹nh ráº±ng viá»‡c quan sÃ¡t hÃ nh vi nÆ¡-ron MLP cáº§n pháº£i Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn toÃ n bá»™ lá»™ trÃ¬nh cá»§a residual stream. Sá»± khÃ¡c biá»‡t Ä‘á»‹nh tÃ­nh giá»¯a cÃ¡c táº§ng vÃ  sá»± thay Ä‘á»•i Ä‘áº·c tÃ­nh khi tÄƒng quy mÃ´ mÃ´ hÃ¬nh nháº¯c nhá»Ÿ chÃºng ta ráº±ng cÃ¡c quy luáº­t tÃ¬m tháº¥y á»Ÿ mÃ´ hÃ¬nh nhá» (toy models) cÃ³ thá»ƒ khÃ´ng luÃ´n Ä‘Ãºng Ä‘á»‘i vá»›i cÃ¡c há»‡ thá»‘ng AI cáº¥p Ä‘á»™ sáº£n xuáº¥t (production-grade).

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. PhÃ¢n tÃ­ch tÆ°Æ¡ng quan nÆ¡-ron vÃ  tÃ¡c Ä‘á»™ng táº§ng trÃªn GPT-Neo dá»±a trÃªn `aero_LLM_12_CodeChallenge Activation histograms by token length (part 2).md`. So sÃ¡nh quy mÃ´ 125M vÃ  1.3B qua cÃ¡c biá»ƒu Ä‘á»“ histograms vÃ  Heatmaps.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [ğŸ“‚ Module: 12-Investigating-neurons-dimensions](README.md) | [Xem bÃ i viáº¿t â†’](README.md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Activation Maximization): CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Nhá»¯ng thÃ¡ch thá»©c trong LLM](aero_LLM_01_Activation maximization via gradient ascent (theory).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Activation maximization via gradient ascent (theory).md) |
| [Triá»ƒn khai Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a: Tá»« Gradient Ascent Ä‘áº¿n Giáº£i mÃ£ Token (Activation Maximization Implementation)](aero_LLM_02_Activation maximization (code).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Activation maximization (code).md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a qua Láº¥y máº«u Dá»¯ liá»‡u (Activation Maximization via Data Sampling)](aero_LLM_03_Activation maximization via data sampling.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_Activation maximization via data sampling.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Kiá»ƒm chá»©ng TÃ­nh láº·p láº¡i cá»§a Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Reproducibility of Activation Maximization)](aero_LLM_04_CodeChallenge Reproducibility of activation maximization.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Reproducibility of activation maximization.md) |
| [Giáº£i pháº«u Ná»™i táº¡i MÃ´ hÃ¬nh báº±ng Hooks: Ká»¹ thuáº­t TrÃ­ch xuáº¥t Hoáº¡t hÃ³a (Extracting Activations via Hooks)](aero_LLM_05_Extracting activations using hooks.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Extracting activations using hooks.md) |
| [Má»‘i tÆ°Æ¡ng quan giá»¯a Hooks vÃ  Hidden States: Giáº£i cáº¥u trÃºc Khá»‘i Transformer (Reconstructing Transformer Blocks)](aero_LLM_06_Relation between hooks and output.hidden_states.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_Relation between hooks and output.hidden_states.md) |
| [LÃ m rÃµ vá» Hidden States Táº§ng cuá»‘i: Vai trÃ² cá»§a LayerNorm (Clarification of Final Hidden States)](aero_LLM_07_Clarification of final hidden_states output.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Clarification of final hidden_states output.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 1)](aero_LLM_08_CodeChallenge Grammar tuning in MLP neurons (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge Grammar tuning in MLP neurons (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 2)](aero_LLM_09_CodeChallenge Grammar tuning in MLP neurons (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Grammar tuning in MLP neurons (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Sá»± Äiá»u cháº¿ Ngá»¯ cáº£nh trong Hoáº¡t hÃ³a MLP (Context-modulated Activation)](aero_LLM_10_CodeChallenge Context-modulated activation in MLP.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_CodeChallenge Context-modulated activation in MLP.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 1)](aero_LLM_11_CodeChallenge Activation histograms by token length (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Activation histograms by token length (part 1).md) |
| ğŸ“Œ **[Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 2)](aero_LLM_12_CodeChallenge Activation histograms by token length (part 2).md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_12_CodeChallenge Activation histograms by token length (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 3)](aero_LLM_13_CodeChallenge Activation histograms by token length (part 3).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_CodeChallenge Activation histograms by token length (part 3).md) |
| [Xá»­ lÃ½ Biá»ƒu diá»…n NÆ¡-ron cho cÃ¡c Tá»« Ä‘a Token (Multi-token Words)](aero_LLM_14_Dealing with multitoken word embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_Dealing with multitoken word embeddings.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 1)](aero_LLM_15_CodeChallenge Category-tuned MLP projections (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_CodeChallenge Category-tuned MLP projections (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 2)](aero_LLM_16_CodeChallenge Category-tuned MLP projections (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_CodeChallenge Category-tuned MLP projections (part 2).md) |
| [Há»“i quy Logistic: LÃ½ thuyáº¿t vÃ  Triá»ƒn khai PhÃ¢n loáº¡i NÆ¡-ron](aero_LLM_17_Classification via logistic regression theory and code.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_Classification via logistic regression theory and code.md) |
| [Äá»‘i chiáº¿u Há»“i quy Logistic vÃ  Kiá»ƒm Ä‘á»‹nh T-test: Giáº£ Ä‘á»‹nh vÃ  á»¨ng dá»¥ng](aero_LLM_18_Logistic regression vs. t-test assumptions and applications.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_Logistic regression vs. t-test assumptions and applications.md) |
| [Äiá»u chá»‰nh Danh tá»« riÃªng trong GPT-2 Medium](aero_LLM_19_Proper noun tuning in GPT2-medium.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_19_Proper noun tuning in GPT2-medium.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 1)](aero_LLM_20_CodeChallenge Negation tuning in MLP neurons (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_20_CodeChallenge Negation tuning in MLP neurons (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 2)](aero_LLM_21_CodeChallenge Negation tuning in MLP neurons (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_21_CodeChallenge Negation tuning in MLP neurons (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 3)](aero_LLM_22_CodeChallenge Negation tuning in MLP neurons (part 3).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_22_CodeChallenge Negation tuning in MLP neurons (part 3).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron QVK (Attention)](aero_LLM_23_CodeChallenge Negation tuning in QVK neurons.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_23_CodeChallenge Negation tuning in QVK neurons.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [12 Investigating neurons dimensions](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a qua Láº¥y máº«u Dá»¯ liá»‡u (Activation Maximization via Data Sampling)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y giá»›i thiá»‡u má»™t phÆ°Æ¡ng phÃ¡p thay tháº¿ Ä‘Æ¡n giáº£n vÃ  hiá»‡u quáº£ hÆ¡n Ä‘á»ƒ diá»…n giáº£i cÃ¡c nÆ¡-ron: Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a thÃ´ng qua Láº¥y máº«u Dá»¯ liá»‡u (Data Sampling). Thay vÃ¬ sá»­ dá»¥ng tá»‘i Æ°u hÃ³a gradient trÃªn nhiá»…u ngáº«u nhiÃªn, phÆ°Æ¡ng phÃ¡p nÃ y truyá»n trá»±c tiáº¿p hÃ ng chá»¥c ngÃ n token tá»« vÄƒn báº£n thá»±c táº¿ qua mÃ´ hÃ¬nh vÃ  thá»‘ng kÃª cÃ¡c token kÃ­ch hoáº¡t máº¡nh nháº¥t má»™t chiá»u (dimension) cá»¥ thá»ƒ. Thá»±c nghiá»‡m trÃªn GPT-Neo 125M vá»›i vÄƒn báº£n "Through the Looking Glass" cho tháº¥y kháº£ nÄƒng xÃ¡c Ä‘á»‹nh cÃ¡c nÆ¡-ron cÃ³ tÃ­nh chá»n lá»c cao Ä‘á»‘i vá»›i cÃ¡c khÃ¡i niá»‡m ngÃ´n ngá»¯ nhÆ° "tiá»n Ä‘á» thá»i gian" (temporal precedence). Tuy nhiÃªn, bÃ¡o cÃ¡o cÅ©ng nháº¥n máº¡nh cÃ¡c thÃ¡ch thá»©c vá» kháº£ nÄƒng má»Ÿ rá»™ng (scalability) vÃ  tÃ­nh phÃ¢n tÃ¡n cá»§a cÃ¡c biá»ƒu diá»…n trong cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Trong cÃ¡c bÃ i bÃ¡o trÆ°á»›c, chÃºng ta Ä‘Ã£ tháº¥y nhá»¯ng rÃ o cáº£n cá»§a viá»‡c tá»‘i Æ°u hÃ³a gradient Ä‘á»‘i vá»›i dá»¯ liá»‡u rá»i ráº¡c nhÆ° ngÃ´n ngá»¯. PhÆ°Æ¡ng phÃ¡p láº¥y máº«u dá»¯ liá»‡u giáº£i quyáº¿t váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng chÃ­nh "ngÃ´n ngá»¯ tá»± nhiÃªn" lÃ m mÃ´i trÆ°á»ng thá»­ nghiá»‡m. Báº±ng cÃ¡ch quan sÃ¡t cÃ¡ch má»™t nÆ¡-ron pháº£n á»©ng vá»›i hÃ ng ngÃ n máº«u dá»¯ liá»‡u thá»±c, chÃºng ta cÃ³ thá»ƒ xÃ¢y dá»±ng má»™t bá»©c tranh trá»±c quan vÃ  dá»… hiá»ƒu hÆ¡n vá» "sá»Ÿ thÃ­ch" cá»§a nÃ³.

---

## 2. PhÆ°Æ¡ng PhÃ¡p Thá»±c Nghiá»‡m (Methodology)

### 2.1. Quy trÃ¬nh Láº¥y máº«u
- **MÃ´ hÃ¬nh:** GPT-Neo (125 triá»‡u tham sá»‘).
- **Dá»¯ liá»‡u:** ToÃ n bá»™ vÄƒn báº£n cuá»‘n sÃ¡ch "Through the Looking Glass" Ä‘Æ°á»£c chia thÃ nh cÃ¡c batch (32 sequences x 256 tokens).
- **Ká»¹ thuáº­t:** Truyá»n dá»¯ liá»‡u qua mÃ´ hÃ¬nh, trÃ­ch xuáº¥t `hidden_states` táº¡i má»™t táº§ng ($L$) vÃ  chiá»u ($D$) cá»¥ thá»ƒ.
- **Thá»‘ng kÃª:** Sá»­ dá»¥ng `numpy.argmax` trÃªn ma tráº­n hoáº¡t hÃ³a (`32 x 256`) Ä‘á»ƒ tÃ¬m token gÃ¢y ra pháº£n há»“i máº¡nh nháº¥t trong má»—i batch. Láº·p láº¡i quy trÃ¬nh nÃ y 1000 láº§n trÃªn cÃ¡c Ä‘oáº¡n vÄƒn báº£n khÃ¡c nhau.

---

## 3. Káº¿t Quáº£ VÃ  PhÃ¢n TÃ­ch (Results & Analysis)

### 3.1. Sá»± há»™i tá»¥ vá» Ngá»¯ nghÄ©a (Semantic Convergence)
Káº¿t quáº£ phÃ¢n tÃ­ch táº¡i Táº§ng 2, Chiá»u 345:
- **Token Ä‘á»©ng Ä‘áº§u:** Tá»« " before" (cÃ³ khoáº£ng tráº¯ng phÃ­a trÆ°á»›c) xuáº¥t hiá»‡n trong gáº§n 50% sá»‘ láº§n láº¥y máº«u.
- **CÃ¡c token liÃªn quan:** " first", " faster", " quicker", " head".
- **Nháº­n xÃ©t:** NÆ¡-ron nÃ y thá»ƒ hiá»‡n sá»± Ä‘iá»u chá»‰nh (tuning) rÃµ rá»‡t Ä‘á»‘i vá»›i khÃ¡i niá»‡m "Æ°u tiÃªn thá»i gian" hoáº·c "trÃ¬nh tá»±". Viá»‡c káº¿t quáº£ há»™i tá»¥ vá» má»™t nhÃ³m tá»« cÃ³ liÃªn quan cháº·t cháº½ chá»©ng minh tÃ­nh hiá»‡u quáº£ cá»§a phÆ°Æ¡ng phÃ¡p láº¥y máº«u.

### 3.2. TÃ­nh Báº¥t Ä‘á»‹nh (The Randomness Factor)
Khi thá»­ nghiá»‡m trÃªn cÃ¡c chiá»u khÃ¡c (vÃ­ dá»¥: Chiá»u 5, Táº§ng 6), káº¿t quáº£ cÃ³ thá»ƒ phÃ¢n tÃ¡n hÆ¡n (vÃ­ dá»¥: "gun", "family", "states"). Äiá»u nÃ y cho tháº¥y khÃ´ng pháº£i má»i chiá»u trong residual stream Ä‘á»u mÃ£ hÃ³a má»™t khÃ¡i niá»‡m Ä‘Æ¡n ngá»¯ (monosemantic) cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn.

---

## 4. Æ¯u Ä‘iá»ƒm vÃ  Háº¡n cháº¿ (Pros & Cons)

### 4.1. Æ¯u Ä‘iá»ƒm
- **TÃ­nh Diá»…n giáº£i cao:** Sá»­ dá»¥ng tá»« ngá»¯ thá»±c táº¿ giÃºp káº¿t quáº£ gáº§n gÅ©i vá»›i logic cá»§a con ngÆ°á»i.
- **Triá»ƒn khai Ä‘Æ¡n giáº£n:** KhÃ´ng yÃªu cáº§u tÃ­nh toÃ¡n gradient phá»©c táº¡p hay hÃ m Loss.
- **TÃ­nh Linh hoáº¡t:** Dá»… dÃ ng Ã¡p dá»¥ng cho báº¥t ká»³ thÃ nh pháº§n nÃ o (MLP, Attention heads, Hidden states).

### 4.2. Háº¡n cháº¿
- **Kháº£ nÄƒng Má»Ÿ rá»™ng:** Vá»›i hÃ ng chá»¥c ngÃ n nÆ¡-ron trong cÃ¡c mÃ´ hÃ¬nh lá»›n, viá»‡c kiá»ƒm tra thá»§ cÃ´ng tá»«ng Ä‘Æ¡n vá»‹ lÃ  báº¥t kháº£ thi.
- **Bá» qua Ngá»¯ cáº£nh:** PhÆ°Æ¡ng phÃ¡p nÃ y chá»‰ táº­p trung vÃ o cÃ¡c token riÃªng láº», trong khi nhiá»u nÆ¡-ron cÃ³ thá»ƒ mÃ£ hoÃ¡ cÃ¡c cáº¥u trÃºc ngá»¯ phÃ¡p dÃ i hoáº·c Ã½ nghÄ©a phá»¥ thuá»™c vÃ o ngá»¯ cáº£nh.
- **MÃ£ hÃ³a PhÃ¢n tÃ¡n:** Má»™t khÃ¡i niá»‡m cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘áº¡i diá»‡n bá»Ÿi sá»± phá»‘i há»£p cá»§a nhiá»u nÆ¡-ron thay vÃ¬ chá»‰ má»™t.

---

## 5. Káº¿t Luáº­n
Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a qua láº¥y máº«u dá»¯ liá»‡u lÃ  má»™t "bá»™ lá»c" há»¯u Ã­ch Ä‘á»ƒ nhanh chÃ³ng phÃ¡t hiá»‡n cÃ¡c nÆ¡-ron cÃ³ chá»©c nÄƒng rÃµ rÃ ng. DÃ¹ gáº·p khÃ³ khÄƒn trong viá»‡c má»Ÿ rá»™ng quy mÃ´, Ä‘Ã¢y váº«n lÃ  má»™t cÃ´ng cá»¥ phÃ¡p chá»©ng quan trá»ng trong bá»™ ká»¹ nÄƒng cá»§a nhÃ  nghiÃªn cá»©u Diá»…n giáº£i há»c, giÃºp thu háº¹p khoáº£ng cÃ¡ch giá»¯a cÃ¡c con sá»‘ trá»«u tÆ°á»£ng vÃ  Ã½ nghÄ©a ngÃ´n ngá»¯ há»c.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. Thá»±c nghiá»‡m láº¥y máº«u dá»¯ liá»‡u trÃªn GPT-Neo dá»±a trÃªn `aero_LLM_03_Activation maximization via data sampling.md`. PhÃ¢n tÃ­ch Tuning cá»§a nÆ¡-ron Ä‘á»‘i vá»›i cÃ¡c khÃ¡i niá»‡m thá»i gian.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [ğŸ“‚ Module: 12-Investigating-neurons-dimensions](README.md) | [Xem bÃ i viáº¿t â†’](README.md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Activation Maximization): CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Nhá»¯ng thÃ¡ch thá»©c trong LLM](aero_LLM_01_Activation maximization via gradient ascent (theory).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Activation maximization via gradient ascent (theory).md) |
| [Triá»ƒn khai Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a: Tá»« Gradient Ascent Ä‘áº¿n Giáº£i mÃ£ Token (Activation Maximization Implementation)](aero_LLM_02_Activation maximization (code).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Activation maximization (code).md) |
| ğŸ“Œ **[Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a qua Láº¥y máº«u Dá»¯ liá»‡u (Activation Maximization via Data Sampling)](aero_LLM_03_Activation maximization via data sampling.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_03_Activation maximization via data sampling.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Kiá»ƒm chá»©ng TÃ­nh láº·p láº¡i cá»§a Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Reproducibility of Activation Maximization)](aero_LLM_04_CodeChallenge Reproducibility of activation maximization.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Reproducibility of activation maximization.md) |
| [Giáº£i pháº«u Ná»™i táº¡i MÃ´ hÃ¬nh báº±ng Hooks: Ká»¹ thuáº­t TrÃ­ch xuáº¥t Hoáº¡t hÃ³a (Extracting Activations via Hooks)](aero_LLM_05_Extracting activations using hooks.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Extracting activations using hooks.md) |
| [Má»‘i tÆ°Æ¡ng quan giá»¯a Hooks vÃ  Hidden States: Giáº£i cáº¥u trÃºc Khá»‘i Transformer (Reconstructing Transformer Blocks)](aero_LLM_06_Relation between hooks and output.hidden_states.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_Relation between hooks and output.hidden_states.md) |
| [LÃ m rÃµ vá» Hidden States Táº§ng cuá»‘i: Vai trÃ² cá»§a LayerNorm (Clarification of Final Hidden States)](aero_LLM_07_Clarification of final hidden_states output.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Clarification of final hidden_states output.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 1)](aero_LLM_08_CodeChallenge Grammar tuning in MLP neurons (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge Grammar tuning in MLP neurons (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 2)](aero_LLM_09_CodeChallenge Grammar tuning in MLP neurons (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Grammar tuning in MLP neurons (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Sá»± Äiá»u cháº¿ Ngá»¯ cáº£nh trong Hoáº¡t hÃ³a MLP (Context-modulated Activation)](aero_LLM_10_CodeChallenge Context-modulated activation in MLP.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_CodeChallenge Context-modulated activation in MLP.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 1)](aero_LLM_11_CodeChallenge Activation histograms by token length (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Activation histograms by token length (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 2)](aero_LLM_12_CodeChallenge Activation histograms by token length (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_CodeChallenge Activation histograms by token length (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 3)](aero_LLM_13_CodeChallenge Activation histograms by token length (part 3).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_CodeChallenge Activation histograms by token length (part 3).md) |
| [Xá»­ lÃ½ Biá»ƒu diá»…n NÆ¡-ron cho cÃ¡c Tá»« Ä‘a Token (Multi-token Words)](aero_LLM_14_Dealing with multitoken word embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_Dealing with multitoken word embeddings.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 1)](aero_LLM_15_CodeChallenge Category-tuned MLP projections (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_CodeChallenge Category-tuned MLP projections (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 2)](aero_LLM_16_CodeChallenge Category-tuned MLP projections (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_CodeChallenge Category-tuned MLP projections (part 2).md) |
| [Há»“i quy Logistic: LÃ½ thuyáº¿t vÃ  Triá»ƒn khai PhÃ¢n loáº¡i NÆ¡-ron](aero_LLM_17_Classification via logistic regression theory and code.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_Classification via logistic regression theory and code.md) |
| [Äá»‘i chiáº¿u Há»“i quy Logistic vÃ  Kiá»ƒm Ä‘á»‹nh T-test: Giáº£ Ä‘á»‹nh vÃ  á»¨ng dá»¥ng](aero_LLM_18_Logistic regression vs. t-test assumptions and applications.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_Logistic regression vs. t-test assumptions and applications.md) |
| [Äiá»u chá»‰nh Danh tá»« riÃªng trong GPT-2 Medium](aero_LLM_19_Proper noun tuning in GPT2-medium.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_19_Proper noun tuning in GPT2-medium.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 1)](aero_LLM_20_CodeChallenge Negation tuning in MLP neurons (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_20_CodeChallenge Negation tuning in MLP neurons (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 2)](aero_LLM_21_CodeChallenge Negation tuning in MLP neurons (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_21_CodeChallenge Negation tuning in MLP neurons (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 3)](aero_LLM_22_CodeChallenge Negation tuning in MLP neurons (part 3).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_22_CodeChallenge Negation tuning in MLP neurons (part 3).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron QVK (Attention)](aero_LLM_23_CodeChallenge Negation tuning in QVK neurons.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_23_CodeChallenge Negation tuning in QVK neurons.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

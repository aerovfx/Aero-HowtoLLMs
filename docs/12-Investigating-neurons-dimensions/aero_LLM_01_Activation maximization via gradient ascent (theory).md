
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [12 Investigating neurons dimensions](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Activation Maximization): CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Nhá»¯ng thÃ¡ch thá»©c trong LLM

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y giá»›i thiá»‡u vá» "Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a" (Activation Maximization), má»™t ká»¹ thuáº­t cá»‘t lÃµi trong diá»…n giáº£i há»c cÆ¡ há»c nháº±m xÃ¡c Ä‘á»‹nh Ä‘áº·c Ä‘iá»ƒm mÃ  má»™t neuron cá»¥ thá»ƒ trong máº¡ng há»c sÃ¢u Ä‘Æ°á»£c Ä‘iá»u chá»‰nh Ä‘á»ƒ pháº£n á»©ng. Thay vÃ¬ quan sÃ¡t pháº£n há»“i cá»§a nÆ¡-ron trÆ°á»›c cÃ¡c dá»¯ liá»‡u máº«u cÃ³ sáºµn, phÆ°Æ¡ng phÃ¡p nÃ y sá»­ dá»¥ng lan truyá»n ngÆ°á»£c (backpropagation) Ä‘á»ƒ tá»‘i Æ°u hÃ³a má»™t nhiá»…u ngáº«u nhiÃªn Ä‘áº§u vÃ o sao cho nÃ³ kÃ­ch hoáº¡t tá»‘i Ä‘a nÆ¡-ron má»¥c tiÃªu. Báº±ng cÃ¡ch Ä‘á»‘i chiáº¿u cÃ¡c vÃ­ dá»¥ thÃ nh cÃ´ng tá»« thá»‹ giÃ¡c mÃ¡y tÃ­nh vá»›i cÃ¡c Ä‘áº·c thÃ¹ cá»§a ngÃ´n ngá»¯, bÃ¡o cÃ¡o phÃ¢n tÃ­ch bá»‘n giáº£ Ä‘á»‹nh cÆ¡ báº£n cá»§a phÆ°Æ¡ng phÃ¡p nÃ y vÃ  tháº£o luáº­n vá» tÃ­nh kháº£ thi cá»§a chÃºng Ä‘á»‘i vá»›i cÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM).

---

## 1. Má»Ÿ Äáº§u (Introduction)
Hiá»ƒu Ä‘Æ°á»£c "Ã½ nghÄ©a" cá»§a má»™t neuron Ä‘Æ¡n láº» giá»¯a hÃ ng tá»· Ä‘Æ¡n vá»‹ lÃ  thÃ¡ch thá»©c lá»›n Ä‘á»‘i vá»›i viá»‡c diá»…n giáº£i mÃ´ hÃ¬nh. CÃ³ hai cÃ¡ch tiáº¿p cáº­n chÃ­nh:
1. **Quan sÃ¡t (Observation via Data Sampling):** ÄÆ°a má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u qua mÃ´ hÃ¬nh vÃ  tÃ¬m cÃ¡c máº«u kÃ­ch hoáº¡t nÆ¡-ron máº¡nh nháº¥t.
2. **Can thiá»‡p (Intervention via Optimization):** Cá»‘ Ä‘á»‹nh trá»ng sá»‘ mÃ´ hÃ¬nh vÃ  tinh chá»‰nh Ä‘áº§u vÃ o Ä‘á»ƒ tÃ¬m ra "hÃ¬nh áº£nh hoáº·c vÄƒn báº£n lÃ½ tÆ°á»Ÿng" cá»§a nÆ¡-ron Ä‘Ã³.
Activation Maximization thuá»™c vá» cÃ¡ch tiáº¿p cáº­n thá»© hai, cho phÃ©p ta khÃ¡m phÃ¡ khÃ´ng gian biá»ƒu diá»…n mÃ  khÃ´ng bá»‹ giá»›i háº¡n bá»Ÿi cÃ¡c táº­p dá»¯ liá»‡u huáº¥n luyá»‡n cÃ³ sáºµn.

---

## 2. Quy trÃ¬nh Tá»‘i Æ°u hÃ³a ngÆ°á»£c (Reverse Optimization)
- **Normal Training:** Dá»¯ liá»‡u ($X$) cá»‘ Ä‘á»‹nh, trá»ng sá»‘ mÃ´ hÃ¬nh ($\theta$) thay Ä‘á»•i Ä‘á»ƒ giáº£m thiá»ƒu tá»•n tháº¥t (Loss).
- **Activation Maximization:** Trá»ng sá»‘ mÃ´ hÃ¬nh ($\theta$) cá»‘ Ä‘á»‹nh, dá»¯ liá»‡u Ä‘áº§u vÃ o ($X$) thay Ä‘á»•i thÃ´ng qua Gradient Descent Ä‘á»ƒ cá»±c Ä‘áº¡i hÃ³a hoáº¡t hÃ³a cá»§a nÆ¡-ron Ä‘Ã­ch $a_{i}$.
Káº¿t quáº£ cuá»‘i cÃ¹ng lÃ  má»™t "báº£n Ä‘á»“ Ä‘áº·c trÆ°ng" pháº£n Ã¡nh chÃ¢n thá»±c nháº¥t thiÃªn kiáº¿n ná»™i táº¡i cá»§a nÆ¡-ron.

---

## 3. CÃ¡c Giáº£ Ä‘á»‹nh Cá»‘t lÃµi cá»§a PhÆ°Æ¡ng phÃ¡p
Viá»‡c Ã¡p dá»¥ng Activation Maximization dá»±a trÃªn bá»‘n giáº£ Ä‘á»‹nh quan trá»ng:

### 3.1. Giáº£ Ä‘á»‹nh vá» Äáº·c trÆ°ng ÄÆ¡n láº» (Unit Feature Representation)
Giáº£ Ä‘á»‹nh ráº±ng má»—i nÆ¡-ron Ä‘áº¡i diá»‡n cho má»™t khÃ¡i niá»‡m con ngÆ°á»i cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c (vÃ­ dá»¥: nÆ¡-ron "máº¯t chÃ³" trong CNN hoáº·c nÆ¡-ron "thÃ¬ quÃ¡ khá»©" trong LLM). Tuy nhiÃªn, trong thá»±c táº¿, cÃ¡c nÆ¡-ron thÆ°á»ng tham gia vÃ o cÃ¡c "biá»ƒu diá»…n phÃ¢n tÃ¡n" (polysemanticity), khiáº¿n viá»‡c cÃ´ láº­p má»™t Ã½ nghÄ©a duy nháº¥t trá»Ÿ nÃªn khÃ³ khÄƒn.

### 3.2. Giáº£ Ä‘á»‹nh vá» Táº§m quan trá»ng cá»§a Hoáº¡t hÃ³a (Activation as Importance)
Giáº£ Ä‘á»‹nh ráº±ng cÆ°á»ng Ä‘á»™ hoáº¡t hÃ³a tá»· lá»‡ thuáº­n vá»›i táº§m quan trá»ng cá»§a thÃ´ng tin. Tuy nhiÃªn, trong sinh há»c vÃ  Ä‘Ã´i khi trong AI, sá»± á»©c cháº¿ hoáº¡t hÃ³a hoáº·c sá»± phá»‘i há»£p giá»¯a má»™t cá»¥m nÆ¡-ron má»›i mang láº¡i thÃ´ng tin chÃ­nh xÃ¡c.

### 3.3. TÃ­nh LiÃªn tá»¥c vÃ  Kháº£ vi cá»§a KhÃ´ng gian Äáº§u vÃ o
PhÆ°Æ¡ng phÃ¡p yÃªu cáº§u khÃ´ng gian tá»‘i Æ°u hÃ³a pháº£i "mÆ°á»£t" (smooth). Äiá»u nÃ y Ä‘Ãºng vá»›i hÃ¬nh áº£nh (pixel mang giÃ¡ trá»‹ liÃªn tá»¥c), nhÆ°ng ráº¥t thÃ¡ch thá»©c vá»›i ngÃ´n ngá»¯ vá»‘n mang tÃ­nh rá»i ráº¡c (Discrete). KhÃ´ng cÃ³ tráº¡ng thÃ¡i "náº±m giá»¯a" 1/3 tá»« "tÃ¡o" vÃ  "chuá»‘i".

### 3.4. TÃ­nh CÃ³ thá»ƒ Diá»…n giáº£i (Human Interpretability)
Giáº£ Ä‘á»‹nh ráº±ng káº¿t quáº£ tá»‘i Æ°u hÃ³a pháº£i cÃ³ Ã½ nghÄ©a vá»›i logic cá»§a con ngÆ°á»i. NghiÃªn cá»©u chá»‰ ra ráº±ng nhiá»u nÆ¡-ron nhÃ¬n tháº¿ giá»›i theo cÃ¡ch "ká»³ dá»‹" (rubbish images) mÃ  máº¯t ngÆ°á»i khÃ´ng thá»ƒ nháº­n diá»‡n Ä‘Æ°á»£c, dÃ¹ chÃºng váº«n hoáº¡t Ä‘á»™ng chÃ­nh xÃ¡c trong kiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh.

---

## 4. Káº¿t Luáº­n
Activation Maximization lÃ  má»™t cÃ´ng cá»¥ máº¡nh máº½ Ä‘á»ƒ "buá»™c" mÃ´ hÃ¬nh tiáº¿t lá»™ cÃ¡c cáº¥u trÃºc áº©n. DÃ¹ cÃ¡c giáº£ Ä‘á»‹nh vá» tÃ­nh liÃªn tá»¥c vÃ  tÃ­nh Ä‘Æ¡n ngá»¯ (monosemanticity) thÆ°á»ng bá»‹ vi pháº¡m trong LLM, phÆ°Æ¡ng phÃ¡p nÃ y váº«n cung cáº¥p nhá»¯ng hiá»ƒu biáº¿t quan trá»ng vá» cÃ¡ch mÃ´ hÃ¬nh mÃ£ hÃ³a tháº¿ giá»›i vÆ°á»£t ra ngoÃ i cÃ¡c táº­p dá»¯ liá»‡u máº«u. Nhá»¯ng bÃ i thá»±c hÃ nh tiáº¿p theo sáº½ táº­p trung vÃ o viá»‡c triá»ƒn khai ká»¹ thuáº­t nÃ y báº±ng PyTorch vÃ  Gradient Descent.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. CÆ¡ sá»Ÿ lÃ½ thuyáº¿t vá» Activation Maximization dá»±a trÃªn `aero_LLM_01_Activation maximization via gradient ascent (theory).md`. PhÃ¢n tÃ­ch cÃ¡c vÃ­ dá»¥ tá»« CNN vÃ  cÃ¡c rÃ o cáº£n khi Ã¡p dá»¥ng lÃªn ngÃ´n ngá»¯.
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

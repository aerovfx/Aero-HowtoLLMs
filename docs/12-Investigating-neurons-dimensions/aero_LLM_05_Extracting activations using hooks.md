
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [12 Investigating neurons dimensions](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Giáº£i pháº«u Ná»™i táº¡i MÃ´ hÃ¬nh báº±ng Hooks: Ká»¹ thuáº­t TrÃ­ch xuáº¥t Hoáº¡t hÃ³a (Extracting Activations via Hooks)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y hÆ°á»›ng dáº«n phÆ°Æ¡ng phÃ¡p sá»­ dá»¥ng "Hooks" â€“ cÃ¡c hÃ m can thiá»‡p Ä‘áº·c biá»‡t trong PyTorch â€“ Ä‘á»ƒ truy cáº­p vÃ  trÃ­ch xuáº¥t dá»¯ liá»‡u tá»« cÃ¡c lá»›p áº©n bÃªn trong Transformer. Trong khi cÃ¡c phÆ°Æ¡ng thá»©c thÃ´ng thÆ°á»ng chá»‰ cho phÃ©p quan sÃ¡t Logits Ä‘áº§u ra hoáº·c Hidden States cá»§a toÃ n bá»™ khá»‘i Transformer, ká»¹ thuáº­t Hook cho phÃ©p nhÃ  nghiÃªn cá»©u cÃ´ láº­p cÃ¡c thÃ nh pháº§n vi mÃ´ nhÆ° ma tráº­n Query (Q), Key (K), Value (V) hoáº·c cÃ¡c lá»›p MLP. BÃ¡o cÃ¡o cÅ©ng tháº£o luáº­n vá» cÆ¡ cháº¿ quáº£n lÃ½ Hook (Ä‘Äƒng kÃ½ vÃ  gá»¡ bá») vÃ  cÃ¡ch quáº£n lÃ½ bá»™ nhá»› thÃ´ng qua viá»‡c ghi Ä‘Ã¨ hoáº·c tÃ­ch lÅ©y dá»¯ liá»‡u.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Äá»ƒ thá»±c hiá»‡n Diá»…n giáº£i há»c cÆ¡ há»c (Mechanistic Interpretability), viá»‡c biáº¿t trá»ng sá»‘ (weights) cá»§a mÃ´ hÃ¬nh lÃ  chÆ°a Ä‘á»§. ChÃºng ta cáº§n biáº¿t cÃ¡ch cÃ¡c nÆ¡-ron thá»±c sá»± pháº£n á»©ng (activations) khi dá»¯ liá»‡u cá»¥ thá»ƒ Ä‘i qua. Hooks Ä‘Ã³ng vai trÃ² nhÆ° cÃ¡c "cáº£m biáº¿n" Ä‘Æ°á»£c cáº¥y vÃ o dÃ²ng cháº£y dá»¯ liá»‡u cá»§a mÃ´ hÃ¬nh trong quÃ¡ trÃ¬nh forward-pass, cho phÃ©p ta chá»¥p láº¡i tráº¡ng thÃ¡i cá»§a báº¥t ká»³ nÆ¡-ron nÃ o mÃ  khÃ´ng cáº§n sá»­a Ä‘á»•i cáº¥u trÃºc cá»‘t lÃµi cá»§a máº¡ng.

---

## 2. CÆ¡ cháº¿ Hoáº¡t Ä‘á»™ng cá»§a PyTorch Hooks

### 2.1. Äá»‹nh nghÄ©a HÃ m Hook
Má»™t hÃ m Hook tiÃªu chuáº©n nháº­n ba tham sá»‘ Ä‘áº§u vÃ o:
1. **Module:** Lá»›p (layer) mÃ  hook Ä‘Æ°á»£c gáº¯n vÃ o.
2. **Input:** Dá»¯ liá»‡u Ä‘i vÃ o lá»›p Ä‘Ã³.
3. **Output:** Káº¿t quáº£ tÃ­nh toÃ¡n Ä‘i ra khá»i lá»›p Ä‘Ã³.
BÃªn trong hÃ m nÃ y, ta cÃ³ thá»ƒ trÃ­ch xuáº¥t `output`, thá»±c hiá»‡n cÃ¡c phÃ©p toÃ¡n (nhÆ° tÃ¡ch cÃ¡c chiá»u Q, K, V) vÃ  lÆ°u trá»¯ káº¿t quáº£ vÃ o má»™t biáº¿n bÃªn ngoÃ i (thÆ°á»ng lÃ  Dictionary hoáº·c List).

### 2.2. ÄÄƒng kÃ½ vÃ  Quáº£n lÃ½ (Registration & Handles)
Sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `register_forward_hook` Ä‘á»ƒ cáº¥y hÃ m vÃ o mÃ´ hÃ¬nh. Káº¿t quáº£ tráº£ vá» lÃ  má»™t `handle`, cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ gá»¡ bá» (`remove()`) hook khi khÃ´ng cÃ²n cáº§n thiáº¿t, giÃºp tá»‘i Æ°u hÃ³a hiá»‡u nÄƒng vÃ  trÃ¡nh rÃ² rá»‰ bá»™ nhá»›.

---

## 3. Quáº£n lÃ½ Dá»¯ liá»‡u Hoáº¡t hÃ³a (Data Management)

### 3.1. Ghi Ä‘Ã¨ (Overwriting via Dictionary)
Náº¿u lÆ°u trá»¯ dá»¯ liá»‡u vÃ o má»™t `Dictionary` vá»›i key lÃ  tÃªn táº§ng, má»—i lÆ°á»£t forward-pass má»›i sáº½ ghi Ä‘Ã¨ lÃªn dá»¯ liá»‡u cÅ©. ÄÃ¢y lÃ  cÃ¡ch tiáº¿p cáº­n phá»• biáº¿n khi ta chá»‰ quan tÃ¢m Ä‘áº¿n pháº£n há»“i cá»§a mÃ´ hÃ¬nh Ä‘á»‘i vá»›i cÃ¢u lá»‡nh hiá»‡n táº¡i. 
*LÆ°u Ã½:* Náº¿u cÃ¢u lá»‡nh má»›i cÃ³ cÃ¡c token Ä‘áº§u tiÃªn giá»‘ng cÃ¢u lá»‡nh cÅ©, cÃ¡c hÃ ng tÆ°Æ¡ng á»©ng trong ma tráº­n hoáº¡t hÃ³a sáº½ giá»‘ng nhau do tÃ­nh cháº¥t truyá»n tin theo trÃ¬nh tá»±.

### 3.2. TÃ­ch lÅ©y (Accumulation via List)
Báº±ng cÃ¡ch sá»­ dá»¥ng `List` vÃ  phÆ°Æ¡ng thá»©c `append()`, ta cÃ³ thá»ƒ lÆ°u trá»¯ lá»‹ch sá»­ hoáº¡t hÃ³a cá»§a táº¥t cáº£ cÃ¡c cÃ¢u lá»‡nh Ä‘Ã£ Ä‘i qua mÃ´ hÃ¬nh. Äiá»u nÃ y há»¯u Ã­ch cho cÃ¡c phÃ¢n tÃ­ch thá»‘ng kÃª diá»‡n rá»™ng hoáº·c so sÃ¡nh sá»± biáº¿n thiÃªn cá»§a nÆ¡-ron qua nhiá»u ngá»¯ cáº£nh khÃ¡c nhau.

---

## 4. PhÃ¢n tÃ­ch Dá»¯ liá»‡u trÃ­ch xuáº¥t
Khi Ä‘Ã£ cÃ³ dá»¯ liá»‡u qua Hook, ta cÃ³ thá»ƒ thá»±c hiá»‡n cÃ¡c phÃ¢n tÃ­ch trá»±c quan:
- **Scatter Plots:** So sÃ¡nh hoáº¡t hÃ³a cá»§a hai token khÃ¡c nhau trÃªn toÃ n bá»™ cÃ¡c nÆ¡-ron cá»§a má»™t táº§ng.
- **Correlation Matrices:** Äo lÆ°á»ng sá»± tÆ°Æ¡ng quan giá»¯a cÃ¡c token. Quan sÃ¡t thá»±c nghiá»‡m cho tháº¥y token Ä‘áº§u tiÃªn thÆ°á»ng cÃ³ Ä‘á»™ tÆ°Æ¡ng quan tháº¥p vá»›i pháº§n cÃ²n láº¡i do thiáº¿u há»¥t ngá»¯ cáº£nh tiá»n Ä‘á».

---

## 5. Káº¿t Luáº­n
Hooks lÃ  cÃ´ng cá»¥ máº¡nh máº½ nháº¥t Ä‘á»ƒ biáº¿n má»™t mÃ´ hÃ¬nh "há»™p Ä‘en" thÃ nh má»™t há»‡ thá»‘ng cÃ³ thá»ƒ quan sÃ¡t Ä‘Æ°á»£c á»Ÿ má»i cáº¥p Ä‘á»™ háº¡t. Viá»‡c lÃ m chá»§ ká»¹ thuáº­t nÃ y khÃ´ng chá»‰ giÃºp trÃ­ch xuáº¥t dá»¯ liá»‡u mÃ  cÃ²n Ä‘áº·t ná»n mÃ³ng cho viá»‡c chá»‰nh sá»­a hoáº¡t hÃ³a (activation editing) â€“ má»™t ká»¹ thuáº­t can thiá»‡p nhÃ¢n quáº£ sÃ¢u sáº¯c hÆ¡n sáº½ Ä‘Æ°á»£c tháº£o luáº­n á»Ÿ cÃ¡c chÆ°Æ¡ng sau.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. Ká»¹ thuáº­t trÃ­ch xuáº¥t hoáº¡t hÃ³a báº±ng Hooks trÃªn GPT-2 dá»±a trÃªn `aero_LLM_05_Extracting activations using hooks.md`. PhÃ¢n tÃ­ch sá»± khÃ¡c biá»‡t giá»¯a cÆ¡ cháº¿ Overwriting vÃ  Concatenation.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [ğŸ“‚ Module: 12-Investigating-neurons-dimensions](README.md) | [Xem bÃ i viáº¿t â†’](README.md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Activation Maximization): CÆ¡ sá»Ÿ LÃ½ thuyáº¿t vÃ  Nhá»¯ng thÃ¡ch thá»©c trong LLM](aero_LLM_01_Activation maximization via gradient ascent (theory).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Activation maximization via gradient ascent (theory).md) |
| [Triá»ƒn khai Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a: Tá»« Gradient Ascent Ä‘áº¿n Giáº£i mÃ£ Token (Activation Maximization Implementation)](aero_LLM_02_Activation maximization (code).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Activation maximization (code).md) |
| [Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a qua Láº¥y máº«u Dá»¯ liá»‡u (Activation Maximization via Data Sampling)](aero_LLM_03_Activation maximization via data sampling.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_Activation maximization via data sampling.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Kiá»ƒm chá»©ng TÃ­nh láº·p láº¡i cá»§a Cá»±c Ä‘áº¡i hÃ³a Hoáº¡t hÃ³a (Reproducibility of Activation Maximization)](aero_LLM_04_CodeChallenge Reproducibility of activation maximization.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Reproducibility of activation maximization.md) |
| ğŸ“Œ **[Giáº£i pháº«u Ná»™i táº¡i MÃ´ hÃ¬nh báº±ng Hooks: Ká»¹ thuáº­t TrÃ­ch xuáº¥t Hoáº¡t hÃ³a (Extracting Activations via Hooks)](aero_LLM_05_Extracting activations using hooks.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Extracting activations using hooks.md) |
| [Má»‘i tÆ°Æ¡ng quan giá»¯a Hooks vÃ  Hidden States: Giáº£i cáº¥u trÃºc Khá»‘i Transformer (Reconstructing Transformer Blocks)](aero_LLM_06_Relation between hooks and output.hidden_states.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_Relation between hooks and output.hidden_states.md) |
| [LÃ m rÃµ vá» Hidden States Táº§ng cuá»‘i: Vai trÃ² cá»§a LayerNorm (Clarification of Final Hidden States)](aero_LLM_07_Clarification of final hidden_states output.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_Clarification of final hidden_states output.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 1)](aero_LLM_08_CodeChallenge Grammar tuning in MLP neurons (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge Grammar tuning in MLP neurons (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: TÃ­nh Chá»n lá»c Ngá»¯ phÃ¡p cá»§a NÆ¡-ron MLP (Pháº§n 2)](aero_LLM_09_CodeChallenge Grammar tuning in MLP neurons (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Grammar tuning in MLP neurons (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Sá»± Äiá»u cháº¿ Ngá»¯ cáº£nh trong Hoáº¡t hÃ³a MLP (Context-modulated Activation)](aero_LLM_10_CodeChallenge Context-modulated activation in MLP.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_CodeChallenge Context-modulated activation in MLP.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 1)](aero_LLM_11_CodeChallenge Activation histograms by token length (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Activation histograms by token length (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 2)](aero_LLM_12_CodeChallenge Activation histograms by token length (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_CodeChallenge Activation histograms by token length (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äá»™ dÃ i Token vÃ  Äáº·c tÃ­nh Hoáº¡t hÃ³a (Pháº§n 3)](aero_LLM_13_CodeChallenge Activation histograms by token length (part 3).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_CodeChallenge Activation histograms by token length (part 3).md) |
| [Xá»­ lÃ½ Biá»ƒu diá»…n NÆ¡-ron cho cÃ¡c Tá»« Ä‘a Token (Multi-token Words)](aero_LLM_14_Dealing with multitoken word embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_Dealing with multitoken word embeddings.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 1)](aero_LLM_15_CodeChallenge Category-tuned MLP projections (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_CodeChallenge Category-tuned MLP projections (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: HÃ¬nh chiáº¿u MLP Äiá»u chá»‰nh theo Danh má»¥c (Pháº§n 2)](aero_LLM_16_CodeChallenge Category-tuned MLP projections (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_CodeChallenge Category-tuned MLP projections (part 2).md) |
| [Há»“i quy Logistic: LÃ½ thuyáº¿t vÃ  Triá»ƒn khai PhÃ¢n loáº¡i NÆ¡-ron](aero_LLM_17_Classification via logistic regression theory and code.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_Classification via logistic regression theory and code.md) |
| [Äá»‘i chiáº¿u Há»“i quy Logistic vÃ  Kiá»ƒm Ä‘á»‹nh T-test: Giáº£ Ä‘á»‹nh vÃ  á»¨ng dá»¥ng](aero_LLM_18_Logistic regression vs. t-test assumptions and applications.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_Logistic regression vs. t-test assumptions and applications.md) |
| [Äiá»u chá»‰nh Danh tá»« riÃªng trong GPT-2 Medium](aero_LLM_19_Proper noun tuning in GPT2-medium.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_19_Proper noun tuning in GPT2-medium.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 1)](aero_LLM_20_CodeChallenge Negation tuning in MLP neurons (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_20_CodeChallenge Negation tuning in MLP neurons (part 1).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 2)](aero_LLM_21_CodeChallenge Negation tuning in MLP neurons (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_21_CodeChallenge Negation tuning in MLP neurons (part 2).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron MLP (Pháº§n 3)](aero_LLM_22_CodeChallenge Negation tuning in MLP neurons (part 3).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_22_CodeChallenge Negation tuning in MLP neurons (part 3).md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äiá»u chá»‰nh Phá»§ Ä‘á»‹nh trong NÆ¡-ron QVK (Attention)](aero_LLM_23_CodeChallenge Negation tuning in QVK neurons.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_23_CodeChallenge Negation tuning in QVK neurons.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

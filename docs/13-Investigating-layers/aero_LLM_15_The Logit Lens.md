
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [13 Investigating layers](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Tháº¥u KÃ­nh Logit (The Logit Lens): Soi SÃ¡ng TÆ° Duy Táº§ng Trung Gian Cá»§a MÃ´ HÃ¬nh NgÃ´n Ngá»¯

## TÃ³m táº¯t (Abstract)
PhÆ°Æ¡ng phÃ¡p Tháº¥u kÃ­nh Logit (Logit Lens) cung cáº¥p má»™t giáº£i phÃ¡p Ä‘á»™t phÃ¡ Ä‘á»ƒ giáº£i mÃ£ "há»™p Ä‘en" cá»§a Large Language Models (LLMs) mÃ  khÃ´ng can thiá»‡p thay Ä‘á»•i kiáº¿n trÃºc mÃ´ hÃ¬nh. Thay vÃ¬ Ä‘á»£i Ä‘áº¿n khi Transformer hoÃ n táº¥t quy trÃ¬nh á»Ÿ lá»›p cuá»‘i cÃ¹ng, ká»¹ thuáº­t nÃ y káº¿t ná»‘i trá»±c tiáº¿p cÃ¡c táº§ng áº©n trung gian (Intermediate Hidden States) vá»›i Ma tráº­n giáº£i mÃ£ nhÃºng (Unembedding Matrix/Language Model Head). Báº±ng viá»‡c sá»­ dá»¥ng GPT-2 Small, bÃ i bÃ¡o cÃ¡o nÃ y minh chá»©ng sá»± kháº£ thi cá»§a viá»‡c giáº£i mÃ£ vÃ  dá»± Ä‘oÃ¡n trá»±c tiáº¿p Token tÆ°Æ¡ng lai á»Ÿ má»i giao Ä‘iá»ƒm ná»™i bá»™, phÃ¡c há»a báº£n Ä‘á»“ tÆ° duy tiáº¿n hÃ³a cá»§a mÃ´ hÃ¬nh qua tá»«ng block.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Kiáº¿n trÃºc tiÃªu chuáº©n cá»§a má»™t mÃ´ hÃ¬nh Transformer (nhÆ° GPT) bao gá»“m:
1. Táº§ng Embedding (Token + Positional).
2. Chuá»—i cÃ¡c khá»‘i Transformer Blocks (nÆ¡i thá»±c hiá»‡n self-attention vÃ  feed-forward).
3. Táº§ng Unembedding (ThÆ°á»ng lÃ  má»™t ma tráº­n tuyáº¿n tÃ­nh - LM Head) Ä‘á»ƒ phÃ³ng chiáº¿u vector Ä‘áº§u ra vÃ o khÃ´ng gian Tá»« vá»±ng (Vocabulary space).

Má»™t cÃ¢u há»i mang tÃ­nh khiÃªu khÃ­ch Ä‘Æ°á»£c Ä‘áº·t ra vá» máº·t giáº£i thÃ­ch AI (Explainable AI): *"Äiá»u gÃ¬ sáº½ xáº£y ra náº¿u ta cáº¯t ngang mÃ´ hÃ¬nh á»Ÿ Layer thá»© 3 vÃ  Ã©p nÃ³ xuáº¥t dá»± Ä‘oÃ¡n ngay láº­p tá»©c?"* 
PhÆ°Æ¡ng phÃ¡p **Logit Lens** Ä‘Æ°á»£c phÃ¡t minh Ä‘á»™c láº­p trÃªn cá»™ng Ä‘á»“ng LessWrong tráº£ lá»i Ä‘Ãºng trá»ng tÃ¢m cÃ¢u há»i trÃªn: Thay vÃ¬ di chuyá»ƒn tuáº§n tá»± qua máº¡ng, ta giáº£ láº­p rÃºt ngáº¯n quy trÃ¬nh báº±ng cÃ¡ch Ä‘Æ°a tráº¡ng thÃ¡i áº©n táº§ng $L$ táº¡t ngang vÃ o trá»±c tiáº¿p bá»™ phÃ¢n giáº£i tá»« vá»±ng cuá»‘i cÃ¹ng.

---

## 2. NguyÃªn LÃ½ CÆ¡ Sá»± (Methodology)

### 2.1. Báº£n Cháº¥t HÃ¬nh Há»c Cá»§a Transformer
Äáº§u ra cá»§a báº¥t ká»³ má»™t Transformer Block nÃ o cÅ©ng báº£o toÃ n kÃ­ch thÆ°á»›c khÃ´ng gian vector cá»§a Embeddings gá»‘c (VD GPT-2 Small lÃ  768 dimensions). Nhá» sá»± giá»›i háº¡n Ä‘á»“ng nháº¥t Ä‘a chiá»u nÃ y, Ma tráº­n giáº£i mÃ£ cuá»‘i cÃ¹ng (`model.lm_head.weight`) dá»… dÃ ng xem báº¥t ká»³ táº§ng trung gian nÃ o lÃ  "váº­t liá»‡u kháº£ thi" Ä‘á»ƒ nhÃ¢n ma tráº­n. 

### 2.2. TrÃ­ch Xuáº¥t Dá»¯ Liá»‡u
Quy trÃ¬nh giáº£ láº­p Ä‘Æ°á»£c khá»Ÿi táº¡o:
- HÃ m Forward Pass láº¥y máº«u vá»›i tuá»³ chá»n `output_hidden_states=True`. 
- Bá» qua Layer 0 (Embedding ban Ä‘áº§u) vÃ¬ chÆ°a chá»‹u tÃ¡c Ä‘á»™ng há»c táº­p (transformation) nÃ o.
- 12 ma tráº­n Layers Ä‘Æ°á»£c trÃ­ch xuáº¥t (GPT-2 Small) cÃ³ dáº¡ng `[1, seq_length, 768]`.

### 2.3. Giáº£i MÃ£ Sá»›m (Early Decoding)
Sá»­ dá»¥ng cÃ´ng thá»©c chiáº¿u vector:
$$ \text{Logits}_{L} = \text{Hidden\_States}_{L} \times \text{LM\_Head}^T $$
Tá»« Ä‘Ã³, ta Ã¡p dá»¥ng hÃ m $\text{argmax}$ phÃ¢n bá»• qua $\text{Softmax}$ cho ma tráº­n tá»« vá»±ng 50,000 chiáº¿u, tÃ¬m ra tá»« cÃ³ xÃ¡c suáº¥t cao nháº¥t táº¡i chÃ­nh Layer lÆ¡ lá»­ng Ä‘Ã³.

---

## 3. Kháº£o SÃ¡t ÄÃ¡nh GiÃ¡ (Analysis & Visualizations)

TrÃ­ch dáº«n máº«u dÃ¹ng cho thÃ­ nghiá»‡m: 
*â€œThe way you do anything is the way you do everythingâ€*.

Kháº£o sÃ¡t Ä‘Æ°á»£c thá»±c hiá»‡n song song Ä‘á»ƒ dá»± bÃ¡o Token "káº¿ tiáº¿p":
- **á» Layer Ä‘áº§u tiÃªn (Layer 1):** MÃ´ hÃ¬nh xuáº¥t ra toÃ n vÄƒn báº£n nhiá»…u (garbage) vÃ  hÆ° tá»« ("the", "else"). Äiá»u nÃ y minh chá»©ng táº¡i cháº·ng Ä‘áº§u, mÃ´ hÃ¬nh chá»‰ má»›i loay hoay tá»•ng há»£p ngá»¯ phÃ¡p cá»¥c bá»™ mÃ  chÆ°a mÆ°á»ng tÆ°á»£ng ná»•i chuá»—i nghÄ©a dÃ i.
- **á» Layer thá»© 3:** Khi Ä‘áº¿n Ä‘oáº¡n tÃ­nh toÃ¡n tá»« â€œdoâ€, model dá»± bÃ¡o tá»« tiáº¿p theo sáº½ lÃ  â€œnotâ€ (máº·c dÃ¹ cÃ¢u gá»‘c lÃ  â€œanythingâ€). â€œdo notâ€ lÃ  má»™t chuá»—i n-gram vÃ´ cÃ¹ng há»£p lá»‡ vá» máº·t ngá»¯ nghÄ©a ngá»¯ phÃ¡p ná»™i bá»™, chá»©ng minh kháº£ nÄƒng nhÃºng chÃ©o khá»‘i tá»« báº¯t Ä‘áº§u xuáº¥t hiá»‡n.
- **Nhiá»‡t Äá»“ Chuyá»ƒn MÃ u (Heatmap Visualization):** TÃ­nh toÃ¡n Softmax táº¡i Ä‘á»‹nh dáº¡ng Ma tráº­n tá»•ng $12 \text{ layers} \times \text{seq\_len}$ váº½ lÃªn bá»©c rÃ¨m chuyá»ƒn pha. Logit Lens chá»‰ ra: KhÃ´ng cáº§n Ä‘á»£i Ä‘áº¿n layer 12, báº£n ngÃ£ cá»¥m tá»« Ä‘Ã´i khi "chÃ­n" sá»›m vÃ  Ä‘Æ°á»£c chá»‘t háº¡ Ä‘Ãºng tá»« Layer 7-8 á»Ÿ nhá»¯ng tá»« khÃ³a quan trá»ng.

---

## 4. Káº¿t Luáº­n
Logit Lens khÃ´ng pháº£i thuáº­t toÃ¡n "Ä‘á»c tÃ¢m trÃ­" (Mind-reading) nhÆ° cÃ¡ch giá»›i truyá»n thÃ´ng thá»•i phá»“ng. ÄÃ¢y lÃ  ká»¹ thuáº­t chiáº¿u áº£nh vector nghá»‹ch Ä‘áº£o (Inverse Projection) cho tháº¥y máº¡ng lÆ°á»›i tÃ­nh toÃ¡n Token thay Ä‘á»•i liÃªn tá»¥c. Sá»± chuyá»ƒn mÃ¬nh tá»« dá»± Ä‘oÃ¡n "ngá»¯ phÃ¡p sÆ¡ cáº¥p" á»Ÿ táº§ng nÃ´ng Ä‘áº¿n "ngá»¯ nghÄ©a trá»«u tÆ°á»£ng" á»Ÿ táº§ng sÃ¢u má»Ÿ ra má»™t kho tÃ ng khá»•ng lá»“ cho viá»‡c sá»­a chá»¯a sai lá»‡ch mÃ´ hÃ¬nh (hallucination fixes) tá»« táº­n lÃµi kiáº¿n trÃºc.

---

## TÃ i Liá»‡u Tham Kháº£o (Citations)
1. Thá»±c nghiá»‡m mÃ´ phá»ng dá»±a theo lÃ½ thuyáº¿t tá»« bÃ i bÃ¡o "Logit Lens" trÃªn diá»…n Ä‘Ã n LessWrong, á»©ng dá»¥ng vÃ o GPT-2 báº±ng PyTorch rÃºt trÃ­ch táº¡i `aero_LLM_15_The Logit Lens.md`.
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [13 Investigating layers](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# PhÃ¢n Kháº£o Cáº¥u TrÃºc Cá»¥m (Clusters): Dáº¥u CÃ¢u Ná»™i Bá»™ vs Dáº¥u CÃ¢u Káº¿t ThÃºc Táº­p 2

## TÃ³m táº¯t (Abstract)
ThÃ­ nghiá»‡m nÃ y lÃ  máº£nh ghÃ©p hoÃ n thiá»‡n cho phÃ¢n tÃ­ch Mutual Information (MI) vÃ  Covariance trÃªn cÃ¡c dáº¥u cÃ¢u. ChÃºng tÃ´i tiáº¿n hÃ nh má»Ÿ rá»™ng khÃ´ng gian trá»±c quan hÃ³a (Visualizations) lÃªn toÃ n bá»™ 25 khá»‘i áº©n chá»©c nÄƒng (layers) cá»§a GPT-2 Medium. Báº±ng cÃ¡ch xÃ¢y dá»±ng lÆ°á»›i Ä‘á»“ thá»‹ $5 \times 5$, quÃ¡ trÃ¬nh hoÃ  trá»™n cÃ¡c cá»¥m (clusters blending curve) Ä‘Æ°á»£c phÆ¡i bÃ y. Äáº·c biá»‡t, nghiÃªn cá»©u nÃ y giáº£i Ä‘Ã¡p nghá»‹ch lÃ½ vá» sá»± bÃ¹ng ná»• biÃªn Ä‘á»™ Covariance Ä‘i ngÆ°á»£c chiá»u vá»›i sá»± suy giáº£m thÃ´ng tin MI á»Ÿ cÃ¡c táº§ng sÃ¢u, cá»§ng cá»‘ lÃ½ thuyáº¿t vá» tÃ¡c Ä‘á»™ng cá»§a Ä‘á»™ lá»‡ch chuáº©n (Standard Deviation) lÃªn cÃ¡c phÃ©p Ä‘o giá»¯ nguyÃªn tá»· lá»‡ (Scale-dependent metrics).

---

## 1. Má»Ÿ Äáº§u (Introduction)
Pháº§n 1 Ä‘Ã£ chá»‰ ra má»™t lÃ¡t cáº¯t táº¡i Layer 1: Covariance phÃ¢n hÃ³a dáº¥u cÃ¢u ná»™i bá»™ (internal punctuation) thÃ nh 3 cá»¥m riÃªng biá»‡t, trong khi M.I háº§u nhÆ° khÃ´ng tháº¥y rÃµ Ä‘iá»u nÃ y. Tháº¿ nhÆ°ng, trÃ­ tuá»‡ cá»§a LLM lÃ  má»™t dÃ²ng cháº£y liÃªn tá»¥c. BÆ°á»›c sÃ³ng chá»©c nÄƒng cá»§a cÃ¡c Attention Blocks báº¯t Ä‘áº§u tá»« viá»‡c hiá»ƒu tá»« má»™c (Embeddings) cho Ä‘áº¿n tá»•ng há»£p ngá»¯ cáº£nh phá»©c há»£p (Deep layers).
BÃ i viáº¿t nÃ y táº­p trung dá»±ng hÃ¬nh má»™t thÆ°á»›c phim quay cháº­m quÃ¡ trÃ¬nh tiáº¿n hÃ³a cá»§a 2 chá»‰ sá»‘ Ä‘o lÆ°á»ng kia dá»c xuyÃªn suá»‘t 25 Layers, nháº±m tÃ¬m ra Ä‘iá»ƒm giá»›i háº¡n nÆ¡i GPT-2 ngÆ°ng xá»­ lÃ½ Ä‘á»‹nh nghÄ©a "cá»¥c bá»™" ráº½ nhÃ¡nh cá»§a dáº¥u pháº©y/cháº¥m.

---

## 2. Trá»±c Quan HÃ³a Quá»¹ Äáº¡o PhÃ¢n Cá»¥m (Cluster Trajectories)

### 2.1. Hiá»‡n TÆ°á»£ng Cháº­p Äiá»ƒm á» Tráº¡m NhÃºng (Layer 0) 
Khi xuáº¥t dá»¯ liá»‡u táº¡i Layer 0 (Embedding layer + Position encoding), Ä‘á»“ thá»‹ scatter plot cá»§a Covariance chá»‰ hiá»ƒn thá»‹ Ä‘Ãºng 2 Ä‘iá»ƒm nÃ©n Ä‘áº·c (dÃ¹ báº£n cháº¥t chá»©a 250 máº«u).
**LÃ½ giáº£i:** á» táº§ng Ä‘áº§u tiÃªn, máº¡ng lÆ°á»›i chÆ°a há» kÃ­ch hoáº¡t Attention. Má»—i token dáº«u mang Ã½ nghÄ©a gÃ¬ thÃ¬ Embedding Vector ná»™i táº¡i cá»§a nÃ³ lÃ  má»™t máº·t náº¡ háº±ng sá»‘ báº¥t biáº¿n. Do dáº¥u cÃ¢u mÃ  ta láº­p trÃ¬nh trÃ­ch xuáº¥t Ä‘á»u náº±m cá»©ng á»Ÿ vá»‹ trÃ­ index 20, cáº£ Positional Encoding cÅ©ng giá»‘ng nhau. VÃ¬ váº­y, LLM xá»­ lÃ½ cáº£ 250 cÃ¢u nhÆ° 1 máº«u Ä‘á»™c báº£n táº¡i tá»a Ä‘á»™ nÃ y.

### 2.2. Sá»± Tan RÃ£ Cá»§a CÃ¡c Cá»¥m NÆ¡i Táº§ng SÃ¢u
Tiáº¿n hÃ nh ná»™i suy ma tráº­n $5 \times 5$ grid trÃªn 24 transformer blocks thá»±c hÃ nh:
- CÃ¡c cá»¥m Covariance $3$-Clusters sáº¯c nÃ©t á»Ÿ Layer 1 tiáº¿p tá»¥c duy trÃ¬ vÃ  báº¯t Ä‘áº§u cÃ³ dáº¥u hiá»‡u loÃ£ng dáº§n.
- Äáº¿n khoáº£ng **Layer 7, 8 vÃ  9** (1/3 cháº·ng Ä‘Æ°á»ng), cÃ¡c cá»¥m nÃ y chÃ­nh thá»©c tan cháº£y vÃ  há»£p nháº¥t thÃ nh má»™t dáº£i mÃ¢y dá»¯ liá»‡u (cloud of dots) phi cáº¥u trÃºc. 
Hiá»‡n tÆ°á»£ng nÃ y kháº³ng Ä‘á»‹nh: CÃ¡c khá»‘i táº§ng nÃ´ng (Early layers) chá»‹u trÃ¡ch nhiá»‡m nháº­n diá»‡n vÃ  phÃ¢n loáº¡i ráº¡ch rÃ²i cÃ¡c vai trÃ² cÃº phÃ¡p phá»¥ (nhÆ° Dáº¥u pháº©y loáº¡i A, loáº¡i B). Má»™t khi thÃ´ng tin Ä‘Ã£ Ä‘Æ°á»£c tá»•ng há»£p xong, cÃ¡c khá»‘i táº§ng sÃ¢u hÆ¡n (Deep layers) khÃ´ng cÃ²n lÆ°u trá»¯ vÃ¡ch ngÄƒn cÃº phÃ¡p nÃ y ná»¯a, mÃ  dá»‘c toÃ n lá»±c cho ngá»¯ nghÄ©a dá»± Ä‘oÃ¡n tÆ°Æ¡ng lai (Predictive Semantics).

---

## 3. MÃ¢u Thuáº«n Äo LÆ°á»ng á» ÄÃ¡y MÃ´ HÃ¬nh (Measurement Discrepancy)

### 3.1. Sá»± TrÃ¡i NgÆ°á»£c Giá»¯a Hai SÃ³ng Äá»“ Thá»‹
Khoan cáº¯t biá»ƒu Ä‘á»“ phÃ¢n phá»‘i Histogram trung bÃ¬nh dá»c theo 25 Layers, má»™t hiá»‡n tÆ°á»£ng báº¥t thÆ°á»ng xuáº¥t hiá»‡n:
- **Mutual Information** cÃ³ xu hÆ°á»›ng trÆ°á»£t giáº£m dáº§n khi Ä‘i sÃ¢u vÃ o cÃ¡c Layers cuá»‘i.
- **Covariance** láº¡i ná»• tung, báº¯n dá»±ng Ä‘á»©ng lÃªn khÃ´ng trung (tÄƒng vá»t lÃªn hÃ ng chá»¥c láº§n so vá»›i layer 1).

### 3.2. Hiá»‡u á»¨ng Khuyáº¿ch Äáº¡i PhÆ°Æ¡ng Sai
ÄÃ¢y khÃ´ng pháº£i lÃ  lá»—i thuáº­t toÃ¡n. Sá»± trÃ¡i ngÆ°á»£c báº¯t nguá»“n tá»« sá»± khÃ¡c biá»‡t lÃµi cá»§a Ä‘Æ¡n vá»‹ Ä‘o:
- **MI dÃ¹ng Ä‘Æ¡n vá»‹ Bits / Nats:** LÃ  Ä‘áº¡i lÆ°á»£ng "miá»…n nhiá»…m há»‡ sá»‘ tá»· lá»‡" (Scale-independent). NÃ³ thuáº§n tÃºy Ä‘o má»©c Ä‘á»™ cháº¯c cháº¯n cá»§a xÃ¡c suáº¥t.
- **Covariance giá»¯ nguyÃªn há»‡ sá»‘ gá»‘c (Scale-dependent):** Khi há»‡ tháº§n kinh ná»™i táº¡i cá»§a LLM Ä‘i vá» cÃ¡c táº§ng sÃ¢u, PhÆ°Æ¡ng sai kÃ­ch hoáº¡t (Variance of activations) vÆ°Æ¡n lÃªn ráº¥t lá»›n. TÃ­n hiá»‡u lan truyá»n lá»›n lÃ m biÃªn Ä‘á»™ dao Ä‘á»™ng ná»• tung. Do $Cov(X,Y)$ tá»· lá»‡ thuáº­n vá»›i cÆ°á»ng Ä‘á»™ tÃ­n hiá»‡u gá»‘c, giÃ¡ trá»‹ cá»§a nÃ³ tá»± Ä‘á»™ng khuáº¿ch Ä‘áº¡i theo mÃ  khÃ´ng mang láº¡i thÃªm báº¥t ká»³ thÃ´ng tin ná»™i táº¡i má»›i nÃ o. 

Äá»ƒ giáº£i quyáº¿t vÃ  Ä‘Æ°a má»©c Covariance vá» má»™t thÆ°á»›c Ä‘o chuáº©n má»±c, ta pháº£i Ã©p nÃ³ xuá»‘ng báº±ng Ä‘á»™ lá»‡ch chuáº©n, tá»©c lÃ  sá»­ dá»¥ng **Há»‡ sá»‘ tÆ°Æ¡ng quan Pearson (Pearson Correlation)**. 

---

## 4. Káº¿t Luáº­n
Thá»±c nghiá»‡m quÃ©t 25 layers dáº¥y lÃªn hai bÃ i há»c lá»›n cho giá»›i nghiÃªn cá»©u Kháº£ nÄƒng giáº£i thÃ­ch AI (Explainable AI):
1. **Lá»›p máº¡ng quyáº¿t Ä‘á»‹nh chá»©c nÄƒng:** LLM xá»­ lÃ½ cÃº phÃ¡p tinh vi (nhÆ° cÃ¡c loáº¡i dáº¥u cÃ¢u phÃ¢n cÃ¡ch) chá»§ yáº¿u diá»…n ra cá»¥c bá»™ á»Ÿ 10 Layers Ä‘áº§u. 
2. **Cáº¡m báº«y toÃ¡n há»c cá»§a Covariance:** Äá»«ng bao giá» so sÃ¡nh tuyá»‡t Ä‘á»‘i chá»‰ sá»‘ Covariance giá»¯a Layer 1 vÃ  Layer 24. Trá»« khi báº¡n dÃ¹ng há»‡ sá»‘ Pearson Ä‘á»ƒ chuáº©n hÃ³a rÃ o cáº£n khuáº¿ch Ä‘áº¡i phÆ°Æ¡ng sai, M.I má»›i lÃ  cÃ´ng cá»¥ so chiáº¿u chÃ©o Layers an toÃ n.

---

## TÃ i Liá»‡u Tham Kháº£o (Citations)
1. Dá»¯ liá»‡u mÃ£ code trÃ­ch xuáº¥t tá»« `aero_LLM_14_CodeChallenge Clusters in internal vs. terminal punctuation (part 2).md` (MÃ£ lá»‡nh vÃ²ng láº·p 25 layers sinh lÆ°á»›i lÃ´-gÃ­c $5 \times 5$, so Ä‘á»“ thá»‹ phÃ¢n dáº£i M.I vÃ  Cov, diá»…n giáº£i Scale-dependence).
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

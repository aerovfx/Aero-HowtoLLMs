
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [13 Investigating layers](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): á»¨ng Dá»¥ng Logit Lens Trong Máº¡ng BERT (Pháº§n 1)

## TÃ³m táº¯t (Abstract)
Thá»±c nghiá»‡m nÃ y khá»Ÿi táº¡o lá»™ trÃ¬nh tÃ­ch há»£p phÃ©p soi tiÃªu cá»± Logit Lens lÃªn mÃ´ hÃ¬nh NgÃ´n ngá»¯ cÃ³ tÃ­nh hai chiá»u (Bidirectional) â€“ BERT Large. QuÃ¡ trÃ¬nh xá»­ lÃ½ khá»Ÿi Ä‘á»™ng vá»›i thuáº­t toÃ¡n khuyáº¿t tá»« Masked Language Modeling (MLM). BÃ i nghiÃªn cá»©u xoay quanh viá»‡c kiá»ƒm duyá»‡t kiáº¿n thá»©c ná»n trÆ°á»›c khi Ã¡p dá»¥ng Logit Lens, thá»±c hÃ nh Ä‘Ã¡nh giÃ¡ Z-Score trÃªn Logits Ä‘iá»ƒm cuá»‘i cÃ¹ng (Final layer) Ä‘á»ƒ Ä‘o Ä‘á»™ kiÃªn Ä‘á»‹nh cá»§a hÃ m dá»± Ä‘oÃ¡n tá»« bá»‹ che (Masked Token). Sá»‘ liá»‡u chá»‰ ra: MÃ´ hÃ¬nh duy trÃ¬ Ä‘á»™ lá»‡ch chuáº©n lÃªn tá»›i cá»±c háº¡n $Z > 10$ Ä‘á»ƒ kháº³ng Ä‘á»‹nh lá»±a chá»n Ä‘Ã¡p Ã¡n phÃ¹ há»£p vá»›i hoÃ n cáº£nh. 

---

## 1. Má»Ÿ Äáº§u (Introduction)
DÃ²ng Ä‘á»i cá»§a mÃ´ hÃ¬nh GPT-2 giáº£i mÃ£ tuáº§n tá»± tá»« trÃ¡i qua pháº£i (Causal Language Model), khÃ¡c biá»‡t hoÃ n toÃ n vá»›i BERT â€“ ká»¹ thuáº­t Ä‘Ã o táº¡o Ä‘á»c chuá»—i mÃ£ hÃ³a káº¿t ná»‘i Ä‘á»“ng thá»i tá»« gá»ng kÃ¬m 2 Ä‘áº§u (Bidirectional Encoder). á» chÆ°Æ¡ng trÆ°á»›c, Logit Lens Ä‘Ã£ tá» ra ráº¥t trÆ¡n tru vá»›i GPT-2. NhÆ°ng khi ta muá»‘n Ã¡p dá»¥ng viá»‡c "Ná»™i soi Táº§ng áº©n" lÃªn kiáº¿n trÃºc 24 khá»‘i khá»•ng lá»“ nhÆ° BERT-Large, cáº¥u trÃºc Ä‘áº§u káº¿t ná»‘i giáº£i mÃ£ (Decoder head) sáº½ Ä‘áº·t ra nhiá»u chÆ°á»›ng ngáº¡i váº­t vá» phÆ°Æ¡ng trÃ¬nh toÃ¡n.
BÃ¡o cÃ¡o Pháº§n 1 chuáº©n bá»‹ tiá»n Ä‘á» dá»¯ liá»‡u vÃ  cháº¡y Ä‘Ã¡nh giÃ¡ quy chuáº©n vá» kháº£ nÄƒng Masked Token Prediction.

---

## 2. Tiá»n Xá»­ LÃ½: MÃ´ HÃ¬nh VÃ  Nhiá»‡m Vá»¥ Khuyáº¿t Viáº¿t (Methodology)

### 2.1. Thiáº¿t Láº­p BERT Large Uncased
Trá»ng sá»‘ tham chiáº¿u sá»­ dá»¥ng: `bert-large-uncased`. So sÃ¡nh vá»›i báº£n Base, báº£n nÃ y sá»Ÿ há»¯u 24 Transformer Layers, kÃ­ch thÆ°á»›c luá»“ng Embedded 1024 dimensions. Há»‡ nhÃºng chá»¯ khÃ´ng phÃ¢n biá»‡t Ä‘á»‹nh hÃ¬nh viáº¿t hoa hay viáº¿t thÆ°á»ng (Uncased).

### 2.2. Kiá»ƒm Thá»­ Masked Language Model (MLM)
Thay vÃ¬ sinh chá»¯ dá»± Ä‘oÃ¡n cuá»‘i chuá»—i, thuáº­t toÃ¡n khai bÃ¡o biáº¿n chÃ¨n ngang `[MASK]` (ID=103 tokenizer) vÃ o má»™t vá»‹ trÃ­ trung tÃ¢m.
Dá»¯ liá»‡u Ä‘áº§u vÃ o:
> "the way you do anything is the [MASK] you do everything"
Viá»‡c xá»­ lÃ½ Forward pass Ä‘Æ°á»£c giao phÃ³ cho GPU. Cuá»‘i cÃ¹ng, vector logit Ä‘iá»ƒm chá»‘t (Final Output Logits) táº¡i vá»‹ trÃ­ Index `[MASK]` Ä‘i vÃ o CPU Ä‘á»ƒ xá»­ lÃ½.

---

## 3. Kháº£o SÃ¡t ÄÃ¡nh GiÃ¡ (Analysis)

### 3.1. Truy Váº¥n Argmax Khá»›p TÃ­n Hiá»‡u (Max Logit Alignment)
HÃ m má»¥c tiÃªu Ä‘Æ°á»£c trÃ­ch xuáº¥t báº±ng cÃ¡ch Ä‘á»‹nh vá»‹ Argmax cao nháº¥t bÃªn trong khÃ´ng gian 30,522 tokens cá»§a thÆ° viá»‡n BERT vocabulary. MÃ´ hÃ¬nh tÃ­nh toÃ¡n vÃ  tráº£ vá» chuá»—i Ä‘Ã­ch giáº£i mÃ£: "way" -> HoÃ n thÃ nh cÃ¢u "the way you do anything is the way you do everything". BERT thá»ƒ hiá»‡n Ä‘á»™ chÃ­nh xÃ¡c tuyá»‡t Ä‘á»‘i nhá» kháº£ nÄƒng trau chuá»‘t tá»« 2 hÆ°á»›ng.

### 3.2. Hiá»‡u Chá»‰nh Äáº§u Ra ThÃ nh Z-Score (Z-Score Standardization)
Thay vÃ¬ sá»­ dá»¥ng Logits thÃ´ hoáº·c Softmax phi tuyáº¿n tÃ­nh, ta lÃ m pháº³ng Ä‘á»‹nh danh toÃ n bá»™ máº£ng Logit 30,522 báº±ng phÃ¢n phá»‘i chuáº©n Standardized Normal Distribution (Z-Score):
$$ Z_i = \frac{X_i - \mu_{vocab}}{\sigma_{vocab}} $$
Äá»“ thá»‹ phÃ¢n váº¡ch váº¡ch tráº§n Æ°u tháº¿ cá»±c trá»‹ cá»§a BERT: Tá»« ngá»¯ Ä‘Æ°á»£c dá»± Ä‘oÃ¡n "way" báº¯n vá»t lÃªn biÃªn Ä‘á»™ $Z > 10$ (10 Ä‘á»™ lá»‡ch chuáº©n). TÃ­nh nÄƒng Z-score khÃ´ng nhá»¯ng khá»­ Ä‘á»™ chá»‡ch Ä‘á»™ lá»›n tá»± do cá»§a cÃ¡c LLM, mÃ  cÃ²n Ä‘áº£m báº£o cháº¯c cháº¯n ráº±ng Ä‘á»‘i vá»›i má»™t ngá»¯ cáº£nh Ä‘Ãºng Ä‘áº¯n, mÃ´ hÃ¬nh sáº½ dá»“n toÃ n bá»™ lá»±c chÃº Ã½ kÃ©o cÃ¡ch biá»‡t Token Ä‘Ã¡p Ã¡n ra tháº­t xa khá»Ÿi nhiá»…u thÃ´ng dá»¥ng cá»§a Ä‘áº¡i tá»« vá»±ng. 

---

## 4. BÃ n Luáº­n Táº¡m Thá»i
Thao tÃ¡c thay tháº¿ Masked Token Prediction hoáº¡t Ä‘á»™ng nhÆ° má»™t cá»— mÃ¡y hoÃ n háº£o táº¡i khá»‘i Layer 24 cuá»‘i cÃ¹ng. 
Tuy nhiÃªn, cáº¥u táº¡o hÃ m phÃ¢n giáº£i Ä‘áº§u ra cá»§a BERT khÃ´ng pháº£i lÃ  Single Linear Matrix. Ká»¹ xáº£o Logit Lens cÆ¡ báº£n Ã¡p dá»¥ng lÃªn GPT-2 sáº½ trá»Ÿ nÃªn vÃ´ giÃ¡ trá»‹ náº¿u Ä‘á»‘i chiáº¿u nháº§m vá»›i cáº¥u trÃºc module giáº£i mÃ£ phá»©c há»£p "Predictions" cá»§a máº¡ng BERT. Hiá»‡n tÆ°á»£ng nÃ y sáº½ Ä‘Æ°á»£c thÃ¡o gá»¡ táº¡i pháº§n hai cá»§a bÃ i viáº¿t.

---

## TÃ i Liá»‡u Tham Kháº£o (Citations)
1. ThÃ­ nghiá»‡m Ä‘o lÆ°á»ng Z-Score trÃ­ch xuáº¥t tá»« dá»¯ liá»‡u `aero_LLM_16_CodeChallenge Logit Lens in BERT (part 1).md` (Thiáº¿t láº­p BERT Large, Masking cÆ¡ báº£n, vÃ  tiÃªu chuáº©n hÃ³a Ä‘iá»ƒm dá»± Ä‘oÃ¡n Logit theo Standard Deviation).
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [13 Investigating layers](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# LÃ½ Thuyáº¿t ThÃ´ng Tin: Äo LÆ°á»ng Entropy VÃ  Mutual Information

## TÃ³m Táº¯t (Abstract)
NghiÃªn cá»©u nÃ y trÃ¬nh bÃ y cÃ¡ch á»©ng dá»¥ng cá»‘t lÃµi cá»§a **LÃ½ thuyáº¿t thÃ´ng tin (Information Theory)** vÃ o cÃ¡c phÃ©p Ä‘o lÆ°á»ng thá»‘ng kÃª cho dá»¯ liá»‡u hoáº¡t Ä‘á»™ng cá»§a nÆ¡-ron: Entropy vÃ  Mutual Information (ThÃ´ng tin tÆ°Æ¡ng há»—). KhÃ´ng giá»‘ng nhÆ° phÆ°Æ¡ng sai hay há»‡ sá»‘ tÆ°Æ¡ng quan vá»‘n mang báº£n cháº¥t tuyáº¿n tÃ­nh vÃ  chá»‰ phÃ¹ há»£p vá»›i phÃ¢n phá»‘i chuáº©n, hai khÃ¡i niá»‡m toÃ¡n há»c nÃ y giÃºp Ä‘á»‹nh lÆ°á»£ng má»©c Ä‘á»™ há»—n loáº¡n (Ä‘á»™ báº¥t Ä‘á»‹nh) vÃ  kháº£ nÄƒng quy náº¡p phi tuyáº¿n giá»¯a hai biáº¿n sá»‘ liÃªn tá»¥c. Sá»± so sÃ¡nh tÃ­nh toÃ¡n thá»§ cÃ´ng qua Histogram vÃ  thÆ° viá»‡n Scikit-learn cÅ©ng Ä‘Æ°á»£c Ä‘á» cáº­p nháº±m thiáº¿t láº­p tiá»n Ä‘á» Ä‘oáº¡t giáº£i cÆ¡ há»c trÃªn cÃ¡c ma tráº­n nÆ¡-ron sau nÃ y.

---

## 1. KhÃ¡i Niá»‡m Vá» Entropy Trong LÃ½ Thuyáº¿t ThÃ´ng Tin

TrÃ¡i ngÆ°á»£c vá»›i "Entropy nhiá»‡t Ä‘á»™ng lá»±c há»c" táº­p trung vÃ o sá»± há»—n loáº¡n cá»§a há»‡ váº­t lÃ½, **Entropy Shannon** mang mÃ u sáº¯c cá»§a "Sá»± báº¥t ngá»" (Surprise) vÃ  "Kháº£ nÄƒng dá»± Ä‘oÃ¡n" (Predictability). 
- Má»™t sá»± kiá»‡n cÃ³ xÃ¡c suáº¥t báº±ng $1$ (nhÆ° máº·t trá»i má»c vÃ o ngÃ y mai) khÃ´ng cÃ³ sá»± báº¥t ngá» $\to \text{Entropy} = 0$.
- Má»™t sá»± kiá»‡n nhÆ° tung Ä‘á»“ng xu (xÃ¡c suáº¥t $0.5$) ráº¥t khÃ³ dá»± Ä‘oÃ¡n $\to \text{Entropy}$ Ä‘áº¡t cá»±c Ä‘áº¡i.

### 1.1. CÃ´ng Thá»©c ToÃ¡n Há»c
DÃ nh cho má»™t biáº¿n biáº¿n thiÃªn ngáº«u nhiÃªn (hoáº·c cÃ¡c Ä‘áº·c trÆ°ng categorical/continuous bins):
$$ H(X) = - \sum_{i=1}^{n} P(x_i) \log P(x_i) $$
Do $P(x_i) \in [0, 1]$ nÃªn há»‡ sá»‘ logarit sáº½ mang dáº¥u Ã¢m, dáº¥u trá»« phÃ­a ngoÃ i giÃºp triá»‡t tiÃªu vÃ  giá»¯ giÃ¡ trá»‹ Entropy $H$ luÃ´n dÆ°Æ¡ng.

### 1.2. Xá»­ LÃ½ CÃ¡c TrÃ¹ng Láº·p Sá»‘ Há»c (Numerical Errors)
Do Ä‘áº·c thÃ¹ logarit khÃ´ng xÃ¡c Ä‘á»‹nh táº¡i má»‘c 0, khi thá»±c nghiá»‡m phÃ¢n vÃ¹ng histogram trÃªn má»™t dá»¯ liá»‡u nÆ¡-ron dÃ y Ä‘áº·c, nhiá»u bin sáº½ xuáº¥t hiá»‡n giÃ¡ trá»‹ $P=0$. Äá»ƒ kháº¯c phá»¥c, cÃ´ng thá»©c code thá»±c táº¿ thÃªm cá»±c trá»‹ tÃ n dÆ° nhá» (epsilon $\epsilon$) vÃ o lÃµi tÃ­nh:
$$ H(X) = - \sum P(X) \log(P(X) + \epsilon) $$
Náº¿u $P=0$, $\log(\epsilon) \times 0$ váº«n sáº½ triá»‡t tiÃªu trá»Ÿ vá» $0$, trÃ¡nh sá»¥p Ä‘á»• vÃ²ng láº·p hÃ m hÃ m log.

---

## 2. Äo LÆ°á»ng Sá»± Äá»“ng Biáº¿n: Mutual Information (MI)

Náº¿u cho 2 biáº¿n $X$ vÃ  $Y$, **Mutual Information - $I(X;Y)$** lÃ  tá»· trá»ng má»©c Ä‘á»™ thÃ´ng tin báº¡n cÃ³ thá»ƒ luáº­n ra tá»« biáº¿n kia, thÃ´ng qua viá»‡c biáº¿t biáº¿n cÃ²n láº¡i. KhÃ¡c vá»›i "Covariance" (Hiá»‡p phÆ°Æ¡ng sai), biáº¿n MI Ä‘áº·c biá»‡t xuáº¥t sáº¯c trong viá»‡c tÃ³m táº¯t cÃ¡c khuynh hÆ°á»›ng cáº¥u trÃºc hÃ¬nh há»c há»—n há»£p.

### 2.1. Tiáº¿p Cáº­n Báº±ng Biá»ƒu Äá»“ Venn (Entropy Giao Thoa)
CÃ³ thá»ƒ Ä‘o lÆ°á»ng MI báº±ng cÃ¡ch tÃ­nh toÃ¡n hÃ m lÆ°á»£ng Entropy nguyÃªn báº£n vÃ  Entropy há»£p bá»™ (Joint-Entropy):
$$ I(X;Y) = H(X) + H(Y) - H(X,Y) $$
NÃ³i cÃ¡ch khÃ¡c, nÃ³ lÃ  pháº§n "giao nhau" cá»§a giá»›i háº¡n Ä‘á»™ báº¥t Ä‘á»‹nh giá»¯a $X$ vÃ  $Y$. 

### 2.2. Tiáº¿p Cáº­n Báº±ng PhÆ°Æ¡ng TrÃ¬nh PhÃ¢n Phá»‘i Cá»¥ Thá»ƒ
$$ I(X;Y) = \sum_{x \in X} \sum_{y \in Y} P(x,y) \log \left( \frac{P(x,y)}{P(x)P(y)} \right) $$

---

## 3. Thá»±c Nghiá»‡m PhÆ°Æ¡ng PhÃ¡p Luáº­n VÃ  Sai Sá»‘ (Methodology in Praxis)

Dá»¯ liá»‡u Ä‘áº·c tÃ­nh nÆ¡-ron lÃ  cÃ¡c phÃ¢n phá»‘i biáº¿n liÃªn tá»¥c (continuous arrays), khÃ´ng pháº£i cÃ¡c danh má»¥c (discrete). Äiá»u nÃ y táº¡o ra má»™t rÃ o cáº£n Ä‘o lÆ°á»ng khi ta buá»™c pháº£i Ã©p dá»¯ liá»‡u vá» cÃ¡c máº·t lÆ°á»›i táº§n suáº¥t 2D (2D Histograms).

1. **Sai sá»‘ do thá»§ cÃ´ng chia Histograms:**
   - Khi Ä‘o lÆ°á»ng biáº¿n máº£ng $x$ vÃ  biáº¿n vÃ´ Ä‘á»‹nh $y$ khÃ´ng liÃªn káº¿t (Tá»©c $I = 0$ tuyá»‡t Ä‘á»‘i theo lÃ½ thuyáº¿t), viá»‡c gom nhÃ³m dá»¯ liá»‡u thá»§ cÃ´ng vÃ o 15 bins hoáº·c phÃ¢n tÃ¡ch báº±ng phÃ¢n vá»‹ (Percentiles) váº«n tráº£ vá» káº¿t quáº£ áº£o $(I \approx 0.4 \to 0.5)$. Káº¿t quáº£ Histogram Ä‘Ã­nh kÃ¨m má»™t lá»±c lÆ°á»£ng "sai lá»‡ch tÄ©nh" (constant bias).
2. **Kháº¯c phá»¥c báº±ng CÃ´ng Cá»¥ Cá»‘t LÃµi (Scikit-Learn Regression):**
   - Thay vÃ¬ Ä‘áº¿m Ä‘iá»ƒm sá»‘ theo Ã´, phÆ°Æ¡ng phÃ¡p Non-parametric Kernel Density Estimators thuá»™c hÃ m thÆ° viá»‡n `mutual_info_regression` cá»§a Sklearn cho phÃ©p Ä‘á»‹nh Ä‘oÃ¡n chÃ­nh xÃ¡c nháº¥t dáº£i phÃ¢n bá»• xÃ¡c suáº¥t, giÃºp Ä‘áº©y Mutual Information tráº£ vá» trÃ¢n diá»‡n á»Ÿ ngÆ°á»¡ng xáº¥p xá»‰ $0.0$.
   - **ÄÃ¡nh Ä‘á»•i:** HÃ m Sklearn cháº¡y cá»±c lá»³ cháº­m. Do Ä‘Ã³ á»Ÿ cÃ¡c kiáº¿n trÃºc LLMs phÃ¢n giáº£i hÃ ng tá»· thÃ´ng sá»‘, ta váº«n Æ°u tiÃªn Histogram Method vÃ¬ thá»±c cháº¥t Ä‘á»™ lá»‡ch Bias luÃ´n Ä‘i ngang tá»± nhiÃªn, khÃ´ng lÃ m sai khÃ¡c tÃ­nh Ä‘á»‘i chiáº¿u tá»· lá»‡.

---

## 4. Káº¿t Luáº­n
BÃ i viáº¿t diá»…n giáº£i gÃ³c nhÃ¬n Ä‘á»‹nh lÆ°á»£ng má»›i vá» khÃ´ng gian hoáº¡t Ä‘á»™ng há»‡ thá»‘ng. Mutual Information bá»™c lá»™ sá»± há»¯u hiá»‡u khi bá» qua khÃ¡i niá»‡m tuyáº¿n tÃ­nh Ä‘á»‹nh chuáº©n vÃ  chá»‰ quan tÃ¢m duy nháº¥t Ä‘áº¿n "váº­t cháº¥t giao thoa vá» tÃ­nh báº¥t Ä‘á»‹nh" cá»§a há»‡. Ká»¹ thuáº­t nÃ y sáº½ lÃ  máº£nh ghÃ©p tiá»n Ä‘á» cho phÃ©p bÃ³c tÃ¡ch cáº¥u trÃºc luá»“ng cá»§a Network mÃ  khÃ´ng há» báº­n tÃ¢m tá»›i biÃªn Ä‘á»™ khuáº¿ch Ä‘áº¡i (Scaling factor) cá»§a tá»«ng táº§ng chá»©c nÄƒng.

---

## TÃ i Liá»‡u Tham Kháº£o (Citations)
1. Dá»¯ liá»‡u trÃ­ch xuáº¥t tá»« pháº§n phá»¥ Ä‘á» vÃ  mÃ£ lá»‡nh thÃ­ nghiá»‡m liÃªn káº¿t: `aero_LLM_08_Mutual information theory and code.md` (CÃ¡ch sá»­ dá»¥ng Histogram 2D vs Scikit Learn; xÃ¢y dá»±ng cÃ´ng thá»©c Shannon Entropy vÃ  Pairwise Mutual Information tÄ©nh).
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [PhÃ¢n TÃ­ch Sá»± TÆ°Æ¡ng Äá»“ng Tokens Trong vÃ  Giá»¯a CÃ¡c Ma Tráº­n Q, K, V (Pháº§n 1)](aero_LLM_01_Token-related similarities within and across Q, K, V matrices (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Token-related similarities within and across Q, K, V matrices (part 1).md) |
| [PhÃ¢n TÃ­ch Sá»± TÆ°Æ¡ng Äá»“ng Tokens Trong vÃ  Giá»¯a CÃ¡c Ma Tráº­n Q, K, V (Pháº§n 2)](aero_LLM_02_Token-related similarities within and across Q, K, V matrices (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_Token-related similarities within and across Q, K, V matrices (part 2).md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): PhÃ¢n TÃ­ch Äá»™ TÆ°Æ¡ng Äá»“ng Cá»§a Token XuyÃªn Suá»‘t CÃ¡c Táº§ng áº¨n](aero_LLM_03_CodeChallenge Token-related similarities across layers.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Token-related similarities across layers.md) |
| [PhÃ¢n TÃ­ch Sá»± PhÃ¢n Cá»¥m vÃ  TÆ°Æ¡ng Äá»“ng Biá»ƒu Diá»…n (RSA) Trong Ma Tráº­n Q vÃ  K](aero_LLM_04_Grouping and RSA in Q and K matrices.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_Grouping and RSA in Q and K matrices.md) |
| [Kháº£o SÃ¡t PhÃ¢n Táº§ng (Laminar Profile) Vá» RSA VÃ  Sá»± Chá»n Lá»c PhÃ¢n NhÃ³m](aero_LLM_05_CodeChallenge Laminar profile of RSA and category selectivity.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_CodeChallenge Laminar profile of RSA and category selectivity.md) |
| [PhÃ¢n TÃ­ch Sá»‘ Chiá»u Hiá»‡u Quáº£ (Effective Dimensionality) ThÃ´ng Qua PCA](aero_LLM_06_Effective dimensionality analysis with PCA.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_Effective dimensionality analysis with PCA.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): Kháº£o SÃ¡t Sá»‘ Chiá»u Hiá»‡u Quáº£ TrÃªn Pythia 2.8B](aero_LLM_07_CodeChallenge Dimensionalities in Pythia 2.3B.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_CodeChallenge Dimensionalities in Pythia 2.3B.md) |
| ğŸ“Œ **[LÃ½ Thuyáº¿t ThÃ´ng Tin: Äo LÆ°á»ng Entropy VÃ  Mutual Information](aero_LLM_08_Mutual information theory and code.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_08_Mutual information theory and code.md) |
| [PhÃ¢n TÃ­ch ThÃ´ng Tin TÆ°Æ¡ng Há»— Dá»c Theo CÃ¡c Táº§ng Cá»§a MÃ´ HÃ¬nh NgÃ´n Ngá»¯ (Pairwise Mutual Information Through LLMs)](aero_LLM_09_Pairwise mutual information through the LLM.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_Pairwise mutual information through the LLM.md) |
| [PhÃ¢n TÃ­ch Äá»‘i Chiáº¿u Äo LÆ°á»ng TÆ°Æ¡ng Quan: Mutual Information vÃ  Covariance](aero_LLM_10_Mutual information vs. covariance.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_Mutual information vs. covariance.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): MI VÃ  Khoáº£ng CÃ¡ch Token (Pháº§n 1)](aero_LLM_11_CodeChallenge Attention to coffee MI and token distances (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Attention to coffee MI and token distances (part 1).md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): MI VÃ  Khoáº£ng CÃ¡ch Token (Pháº§n 2)](aero_LLM_12_CodeChallenge Attention to coffee MI and token distances (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_CodeChallenge Attention to coffee MI and token distances (part 2).md) |
| [PhÃ¢n Kháº£o Cáº¥u TrÃºc Cá»¥m (Clusters): Dáº¥u CÃ¢u Ná»™i Bá»™ vs Dáº¥u CÃ¢u Káº¿t ThÃºc Táº­p 1](aero_LLM_13_CodeChallenge Clusters in internal vs. terminal punctuation (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_CodeChallenge Clusters in internal vs. terminal punctuation (part 1).md) |
| [PhÃ¢n Kháº£o Cáº¥u TrÃºc Cá»¥m (Clusters): Dáº¥u CÃ¢u Ná»™i Bá»™ vs Dáº¥u CÃ¢u Káº¿t ThÃºc Táº­p 2](aero_LLM_14_CodeChallenge Clusters in internal vs. terminal punctuation (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_CodeChallenge Clusters in internal vs. terminal punctuation (part 2).md) |
| [Tháº¥u KÃ­nh Logit (The Logit Lens): Soi SÃ¡ng TÆ° Duy Táº§ng Trung Gian Cá»§a MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_15_The Logit Lens.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_The Logit Lens.md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): á»¨ng Dá»¥ng Logit Lens Trong Máº¡ng BERT (Pháº§n 1)](aero_LLM_16_CodeChallenge Logit Lens in BERT (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_CodeChallenge Logit Lens in BERT (part 1).md) |
| [Thá»­ ThÃ¡ch Láº­p TrÃ¬nh (Code Challenge): á»¨ng Dá»¥ng Logit Lens Trong Máº¡ng BERT (Pháº§n 2)](aero_LLM_17_CodeChallenge Logit Lens in BERT (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_CodeChallenge Logit Lens in BERT (part 2).md) |
| [PhÃ¢n TÃ­ch Sá»± TÆ°Æ¡ng Äá»“ng Tokens Trong vÃ  Giá»¯a CÃ¡c Ma Tráº­n Q, K, V (Pháº§n 1)](article_aero_LLM_01_vn.md) | [Xem bÃ i viáº¿t â†’](article_aero_LLM_01_vn.md) |
| [PhÃ¢n tÃ­ch ChuyÃªn SÃ¢u CÃ¡c Táº§ng áº¨n Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n (LLMs): Äo LÆ°á»ng, Biá»ƒu Diá»…n vÃ  Giáº£i MÃ£ Ná»™i Táº¡i](scientific_article_vn.md) | [Xem bÃ i viáº¿t â†’](scientific_article_vn.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

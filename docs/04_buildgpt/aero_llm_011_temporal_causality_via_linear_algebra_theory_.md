
<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [04 buildgpt](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Bi·ªÉu di·ªÖn T√≠nh Nh√¢n Qu·∫£ Th·ªùi Gian trong C∆° Ch·∫ø Attention b·∫±ng ƒê·∫°i S·ªë Tuy·∫øn T√≠nh

## T√≥m t·∫Øt (Abstract)

B√†i b√°o n√†y tr√¨nh b√†y ph√¢n t√≠ch l√Ω thuy·∫øt v·ªÅ c∆° ch·∫ø nh√¢n qu·∫£ th·ªùi gian (temporal causality) trong m√¥ h√¨nh Transformer, ƒë·∫∑c bi·ªát trong ki·∫øn tr√∫c GPT, th√¥ng qua g√≥c nh√¨n ƒë·∫°i s·ªë tuy·∫øn t√≠nh. D·ª±a tr√™n t√†i li·ªáu gi·∫£ng d·∫°y v·ªÅ causal attention mask , nghi√™n c·ª©u l√†m r√µ vai tr√≤ c·ªßa ma tr·∫≠n m·∫∑t n·∫° (mask matrix), h√†m softmax, v√† c√°ch ch√∫ng ƒë·∫£m b·∫£o m√¥ h√¨nh ch·ªâ khai th√°c th√¥ng tin t·ª´ qu√° kh·ª© khi d·ª± ƒëo√°n t∆∞∆°ng lai. K·∫øt qu·∫£ cho th·∫•y causal masking l√† y·∫øu t·ªë c·ªët l√µi gi√∫p m√¥ h√¨nh ng√¥n ng·ªØ sinh vƒÉn b·∫£n m·ªôt c√°ch h·ª£p l·ªá v√† ·ªïn ƒë·ªãnh v·ªÅ m·∫∑t s·ªë h·ªçc.

---

## 1. Gi·ªõi thi·ªáu (Introduction)

C√°c m√¥ h√¨nh ng√¥n ng·ªØ hi·ªán ƒë·∫°i nh∆∞ GPT v√† BERT ƒë·ªÅu d·ª±a tr√™n ki·∫øn tr√∫c Transformer v·ªõi c∆° ch·∫ø self-attention. Tuy nhi√™n, s·ª± kh√°c bi·ªát c·ªët l√µi gi·ªØa hai d√≤ng m√¥ h√¨nh n√†y n·∫±m ·ªü vi·ªác c√≥ hay kh√¥ng √°p d·ª•ng r√†ng bu·ªôc nh√¢n qu·∫£ th·ªùi gian.

Theo t√†i li·ªáu l√Ω thuy·∫øt v·ªÅ causal attention , GPT s·ª≠ d·ª•ng m·∫∑t n·∫° nh√¢n qu·∫£ ƒë·ªÉ ngƒÉn m√¥ h√¨nh truy c·∫≠p th√¥ng tin trong t∆∞∆°ng lai, trong khi BERT cho ph√©p truy c·∫≠p to√†n b·ªô ng·ªØ c·∫£nh.

M·ª•c ti√™u c·ªßa b√†i b√°o n√†y l√†:

- Ph√¢n t√≠ch c∆° s·ªü to√°n h·ªçc c·ªßa causal masking,
- L√†m r√µ vai tr√≤ c·ªßa softmax trong vi·ªác ƒë·∫£m b·∫£o t√≠nh ·ªïn ƒë·ªãnh,
- So s√°nh c∆° ch·∫ø nh√¢n qu·∫£ trong GPT v√† BERT,
- ƒê√°nh gi√° t√°c ƒë·ªông ƒë·∫øn kh·∫£ nƒÉng sinh vƒÉn b·∫£n.

---

## 2. C∆° s·ªü l√Ω thuy·∫øt (Theoretical Background)

### 2.1. Attention trong Transformer

C∆° ch·∫ø attention ti√™u chu·∫©n ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a:

$$

$$

\text{Attention}(Q,K,V) = \text{softmax}$\le$ft(\frac{QK^T}{\sqrt{d}}\right)V

$$

$$

trong ƒë√≥:

- $Q$: Query matrix  
- $K$: Key matrix  
- $V$: Value matrix  
- $d$: s·ªë chi·ªÅu ·∫©n  

K·∫øt qu·∫£ attention l√† t·ªï h·ª£p tuy·∫øn t√≠nh c·ªßa c√°c vector gi√° tr·ªã d·ª±a tr√™n m·ª©c ƒë·ªô li√™n quan.

---

### 2.2. Bi·ªÉu di·ªÖn Nh√¢n qu·∫£ Th·ªùi gian

Trong d·ª± ƒëo√°n chu·ªói, t·∫°i th·ªùi ƒëi·ªÉm $t$, m√¥ h√¨nh ch·ªâ ƒë∆∞·ª£c ph√©p s·ª≠ d·ª•ng th√¥ng tin t·ª´:

$$
\{1,2,...,t\}
$$

v√† kh√¥ng ƒë∆∞·ª£c truy c·∫≠p:

$$
\{t+1, t+2, ...\}
$$

Nguy√™n t·∫Øc n√†y ph·∫£n √°nh th·ª±c t·∫ø r·∫±ng t∆∞∆°ng lai ch∆∞a x·∫£y ra v√† kh√¥ng th·ªÉ ƒë∆∞·ª£c bi·∫øt tr∆∞·ªõc.

---

### 2.3. Vector Tr·ªçng s·ªë Th·ªùi gian

M·ªôt c√°ch tr·ª±c quan, s·ª± t√≠ch h·ª£p th√¥ng tin qu√° kh·ª© c√≥ th·ªÉ bi·ªÉu di·ªÖn b·∫±ng vector:

$$

$$

a = (a_1, a_2, ..., a_T)

$$

$$

v·ªõi:

- $a_i > 0$ n·∫øu $i $\le$q t$,
- $a_i = 0$ n·∫øu $i > t$.

Tuy nhi√™n, vector n√†y ch∆∞a ƒë∆∞·ª£c chu·∫©n h√≥a v√† kh√¥ng ph√π h·ª£p cho t√≠nh to√°n s·ªë h·ªçc ·ªïn ƒë·ªãnh.

---

## 3. Softmax v√† V·∫•n ƒë·ªÅ Truy c·∫≠p T∆∞∆°ng lai

### 3.1. Hi·ªáu ·ª©ng c·ªßa Softmax

Softmax ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a:

$$

$$

\text{softmax}(x_i) = \frac{e^{x_i}}{$\sum$_j e^{x_j}}

$$

$$

N·∫øu m·ªôt ph·∫ßn t·ª≠ c√≥ gi√° tr·ªã b·∫±ng 0:

$$
e^0 = 1 \neq 0
$$

Do ƒë√≥, vi·ªác g√°n gi√° tr·ªã 0 cho t∆∞∆°ng lai kh√¥ng ƒë·∫£m b·∫£o x√°c su·∫•t b·∫±ng 0 sau softmax.

---

### 3.2. Gi·∫£i ph√°p: Gi√° tr·ªã √Çm V√¥ C√πng

Theo t√†i li·ªáu tham kh·∫£o , ƒë·ªÉ ƒë·∫£m b·∫£o x√°c su·∫•t b·∫±ng 0, ta ƒë·∫∑t:

$$

$$

x_i = -$\infty$ \quad \text{v·ªõi } i > t

$$

$$

v√¨:

$$

$$

\lim_{x \to -$\infty$} e^x = 0

$$

$$

Do ƒë√≥:

$$

$$

\text{softmax}(-$\infty$) = 0

$$

$$

Gi·∫£i ph√°p n√†y ƒë·∫£m b·∫£o t∆∞∆°ng lai ho√†n to√†n b·ªã lo·∫°i b·ªè.

---

### 3.3. L·ª£i √≠ch S·ªë h·ªçc

C√°ch ti·∫øp c·∫≠n n√†y mang l·∫°i:

- T√≠nh ·ªïn ƒë·ªãnh s·ªë,
- Tr√°nh tr√†n s·ªë,
- T·∫°o ph√¢n ph·ªëi x√°c su·∫•t h·ª£p l·ªá,
- TƒÉng t√≠nh th∆∞a (sparsity).

---

## 4. Ma tr·∫≠n Nh√¢n qu·∫£ (Causal Mask Matrix)

### 4.1. C·∫•u tr√∫c Ma tr·∫≠n

Thay v√¨ vector ri√™ng l·∫ª, causal attention ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng ma tr·∫≠n:

$$
M \in $\mathbb${R}^{T \times T}
$$

v·ªõi:

$$

$$

M_{ij} = \begin{cases} 0 & \text{n·∫øu } j $\le$ i \\ -$\infty$ & \text{n·∫øu } j > i \end{cases}

$$

$$

Ma tr·∫≠n n√†y c√≥ d·∫°ng tam gi√°c d∆∞·ªõi.

---

### 4.2. T√≠ch h·ª£p v√†o Attention

C√¥ng th·ª©c attention m·ªü r·ªông:

$$

$$

\text{Attention}(Q,K,V) = \text{softmax} $\le$ft( \frac{QK^T}{\sqrt{d}} + M \right)V

$$

$$

Trong ƒë√≥ $M$ ƒë√≥ng vai tr√≤ lo·∫°i b·ªè t∆∞∆°ng t√°c v·ªõi t∆∞∆°ng lai.

---

### 4.3. Softmax Theo H√†ng

Vi·ªác softmax ƒë∆∞·ª£c √°p d·ª•ng theo t·ª´ng h√†ng:

$$
\text{softmax}(M_i)
$$

ƒë·∫£m b·∫£o m·ªói token ch·ªâ quan t√¢m ƒë·∫øn qu√° kh·ª© c·ªßa ch√≠nh n√≥.

---

## 5. Vai tr√≤ c·ªßa Softmax trong Causal Attention

Theo ph√¢n t√≠ch t·ª´ , softmax mang l·∫°i hai l·ª£i √≠ch ch√≠nh:

### 5.1. X·ª≠ l√Ω Gi√° tr·ªã √Çm

C√°c gi√° tr·ªã attention c√≥ th·ªÉ √¢m do l√† t√≠ch v√¥ h∆∞·ªõng. Softmax:

- Bi·∫øn ƒë·ªïi th√†nh x√°c su·∫•t kh√¥ng √¢m,
- Chu·∫©n h√≥a v·ªÅ t·ªïng b·∫±ng 1.

---

### 5.2. TƒÉng T√≠nh Ch·ªçn L·ªçc

Softmax khu·∫øch ƒë·∫°i gi√° tr·ªã l·ªõn v√† l√†m suy gi·∫£m gi√° tr·ªã nh·ªè, d·∫´n ƒë·∫øn:

- T·∫≠p trung v√†o token quan tr·ªçng,
- Gi·∫£m nhi·ªÖu,
- C·∫£i thi·ªán kh·∫£ nƒÉng suy lu·∫≠n.

ƒêi·ªÅu n√†y gi√∫p m√¥ h√¨nh d·∫ßn thu h·∫πp kh√¥ng gian t√¨m ki·∫øm khi sinh vƒÉn b·∫£n.

---

## 6. GPT v√† BERT: So s√°nh C∆° ch·∫ø Nh√¢n qu·∫£

### 6.1. M√¥ h√¨nh GPT (Decoder-based)

ƒê·∫∑c tr∆∞ng:

- C√≥ causal mask,
- Hu·∫•n luy·ªán autoregressive,
- Ph√π h·ª£p cho sinh vƒÉn b·∫£n.

GPT tu√¢n th·ªß ch·∫∑t ch·∫Ω nguy√™n l√Ω nh√¢n qu·∫£.

---

### 6.2. M√¥ h√¨nh BERT (Encoder-based)

ƒê·∫∑c tr∆∞ng:

- Kh√¥ng d√πng causal mask,
- Attention hai chi·ªÅu,
- D√πng cho ph√¢n lo·∫°i, t√≥m t·∫Øt, ph√¢n t√≠ch.

BERT khai th√°c to√†n b·ªô ng·ªØ c·∫£nh ƒë·ªÉ t·ªëi ∆∞u bi·ªÉu di·ªÖn.

---

### 6.3. So s√°nh

| Ti√™u ch√≠ | GPT | BERT |
|----------|-----|------|
| Causal Mask | C√≥ | Kh√¥ng |
| Sinh vƒÉn b·∫£n | T·ªët | H·∫°n ch·∫ø |
| Ph√¢n t√≠ch vƒÉn b·∫£n | Trung b√¨nh | T·ªët |
| Truy c·∫≠p t∆∞∆°ng lai | Kh√¥ng | C√≥ |

---

## 7. K·∫øt qu·∫£ v√† ·ª®ng d·ª•ng (Results and Applications)

### 7.1. Hi·ªáu qu·∫£ Hu·∫•n luy·ªán

Causal masking gi√∫p:

- ƒê·ªìng b·ªô qu√° tr√¨nh train v√† inference,
- Tr√°nh leakage th√¥ng tin,
- ·ªîn ƒë·ªãnh gradient.

---

### 7.2. ·ª®ng d·ª•ng Th·ª±c t·∫ø

C∆° ch·∫ø n√†y ƒë∆∞·ª£c √°p d·ª•ng trong:

- Chatbot,
- Tr√¨nh sinh vƒÉn b·∫£n,
- H·ªá th·ªëng d·ªãch m√°y,
- H·ªá th·ªëng vi·∫øt t·ª± ƒë·ªông.

---

### 7.3. Kh·∫£ nƒÉng M·ªü r·ªông

Causal masking cho ph√©p:

- Hu·∫•n luy·ªán song song,
- Duy tr√¨ t√≠nh tu·∫ßn t·ª± khi suy lu·∫≠n,
- K·∫øt h·ª£p v·ªõi KV-cache.

---

## 8. Th·∫£o lu·∫≠n (Discussion)

### 8.1. G√≥c nh√¨n ƒê·∫°i s·ªë Tuy·∫øn t√≠nh

Causal attention c√≥ th·ªÉ xem l√†:

- Ph√©p nh√¢n ma tr·∫≠n c√≥ r√†ng bu·ªôc tam gi√°c,
- Ph√©p chi·∫øu kh√¥ng gian th√¥ng tin v√†o mi·ªÅn qu√° kh·ª©.

ƒêi·ªÅu n√†y cho ph√©p ph√¢n t√≠ch b·∫±ng l√Ω thuy·∫øt ph·ªï v√† chu·∫©n ma tr·∫≠n.

---

### 8.2. H·∫°n ch·∫ø

M·ªôt s·ªë h·∫°n ch·∫ø:

- Kh√¥ng t·∫≠n d·ª•ng ƒë∆∞·ª£c th√¥ng tin t∆∞∆°ng lai khi train,
- Gi·∫£m hi·ªáu qu·∫£ cho t√°c v·ª• hi·ªÉu vƒÉn b·∫£n,
- Ph·ª• thu·ªôc ƒë·ªô d√†i ng·ªØ c·∫£nh.

---

### 8.3. M·ªü r·ªông

C√°c h∆∞·ªõng ph√°t tri·ªÉn:

- Mask m·ªÅm (soft mask),
- ALiBi,
- Rotary Embedding,
- Hybrid encoder-decoder.

---

## 9. H·∫°n ch·∫ø nghi√™n c·ª©u (Limitations)

Nghi√™n c·ª©u n√†y:

- Ch·ªâ t·∫≠p trung v√†o l√Ω thuy·∫øt,
- Kh√¥ng c√≥ th·ª±c nghi·ªám quy m√¥ l·ªõn,
- Ch∆∞a ƒë√°nh gi√° tr√™n benchmark chu·∫©n.

Do ƒë√≥, k·∫øt lu·∫≠n mang t√≠nh ph√¢n t√≠ch n·ªÅn t·∫£ng.

---

## 10. K·∫øt lu·∫≠n (Conclusion)

B√†i b√°o ƒë√£ ph√¢n t√≠ch c∆° ch·∫ø causal attention th√¥ng qua:

- Softmax,
- Ma tr·∫≠n mask,
- Gi·ªõi h·∫°n √¢m v√¥ c√πng,
- ƒê·∫°i s·ªë tuy·∫øn t√≠nh.

K·∫øt qu·∫£ cho th·∫•y causal masking l√† n·ªÅn t·∫£ng to√°n h·ªçc c·ªët l√µi gi√∫p GPT duy tr√¨ t√≠nh nh√¢n qu·∫£ v√† kh·∫£ nƒÉng sinh vƒÉn b·∫£n ƒë√°ng tin c·∫≠y.

C∆° ch·∫ø n√†y ph√¢n bi·ªát r√µ r√†ng gi·ªØa m√¥ h√¨nh sinh (GPT) v√† m√¥ h√¨nh hi·ªÉu (BERT), ƒë·ªìng th·ªùi l√† tr·ª• c·ªôt c·ªßa c√°c LLM hi·ªán ƒë·∫°i.

---

## 11. H∆∞·ªõng nghi√™n c·ª©u ti·∫øp theo (Future Work)

C√°c h∆∞·ªõng ph√°t tri·ªÉn g·ªìm:

- Causal attention th√≠ch nghi,
- Mask h·ªçc ƒë∆∞·ª£c,
- Attention th∆∞a,
- Causal attention ƒëa chi·ªÅu,
- K·∫øt h·ª£p RAG v√† causal modeling.

---

## T√†i li·ªáu tham kh·∫£o (References)

1. B√†i gi·∫£ng v·ªÅ Temporal Causality v√† Causal Attention trong Transformer. 

D∆∞·ªõi ƒë√¢y l√† ph·∫ßn **Pseudocode + PyTorch Implementation cho Causal Mask** ƒë∆∞·ª£c vi·∫øt theo **chu·∫©n b√†i b√°o khoa h·ªçc**, ph√π h·ª£p ƒë·ªÉ ƒë∆∞a v√†o:

‚úÖ Appendix / Supplementary Material
‚úÖ Ph·∫ßn Methodology
‚úÖ Lu·∫≠n vƒÉn / Journal Paper

Tr√¨nh b√†y theo phong c√°ch h·ªçc thu·∫≠t v√† d·ªÖ t√°i l·∫≠p.

---

````md
# Ph·ª• l·ª•c B: Causal Mask ‚Äì Thu·∫≠t to√°n v√† Tri·ªÉn khai

## B.1. T·ªïng quan

Trong m√¥ h√¨nh Transformer d·∫°ng autoregressive, causal mask ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng t·∫°i th·ªùi ƒëi·ªÉm $t$, m√¥ h√¨nh ch·ªâ truy c·∫≠p ƒë∆∞·ª£c c√°c token trong qu√° kh·ª© v√† hi·ªán t·∫°i, kh√¥ng truy c·∫≠p ƒë∆∞·ª£c token trong t∆∞∆°ng lai.

Ph·∫ßn n√†y tr√¨nh b√†y:

- M√£ gi·∫£ (pseudocode) cho causal mask,
- C√°ch t√≠ch h·ª£p v√†o attention,
- Tri·ªÉn khai b·∫±ng PyTorch.

---

## B.2. Pseudocode: T·∫°o Ma tr·∫≠n Causal Mask

### Thu·∫≠t to√°n 4: Sinh Ma tr·∫≠n Nh√¢n qu·∫£

**Input:**

- ƒê·ªô d√†i chu·ªói: $T$

**Output:**

- Ma tr·∫≠n mask: $M \in $\mathbb${R}^{T \times T}$

---

```text
Algorithm 4: Generate-Causal-Mask$T$

1:  Initialize M as matrix of size T √ó T

2:  for i = 1 ‚Üí T do
3:      for j = 1 ‚Üí T do
4:          if j ‚â§ i then
5:              M[i, j] ‚Üê 0
6:          else
7:              M[i, j] ‚Üê -‚àû
8:          end if
9:      end for
10: end for

11: return M
````

---

### Gi·∫£i th√≠ch

* Ph·∫ßn t·ª≠ ( M_{ij} = 0 ): cho ph√©p attention,

$$
* Ph·∫ßn t·ª≠ ( M_{ij} = -$\infty$ ): ch·∫∑n attention,
$$

* D·∫°ng tam gi√°c d∆∞·ªõi ƒë·∫£m b·∫£o t√≠nh nh√¢n qu·∫£.

---

## B.3. Pseudocode: Attention v·ªõi Causal Mask

### Thu·∫≠t to√°n 5: Causal Self-Attention

**Input:**

* Query: ( Q \in $\mathbb${R}^{T \times d} )
* Key: ( K \in $\mathbb${R}^{T \times d} )
* Value: ( V \in $\mathbb${R}^{T \times d} )
* Mask: ( M \in $\mathbb${R}^{T \times T} )

**Output:**

* Output: ( O \in $\mathbb${R}^{T \times d} )

---

```text
Algorithm 5: Causal-Attention(Q, K, V, M)

1:  S ‚Üê Q √ó K·µÄ
2:  S ‚Üê S / sqrt$d$

3:  S ‚Üê S + M

4:  A ‚Üê softmax$S$

5:  O ‚Üê A √ó V

6:  return O

---

### Gi·∫£i th√≠ch

* B∆∞·ªõc (3) ƒë·∫£m b·∫£o t∆∞∆°ng lai b·ªã lo·∫°i b·ªè,
* Softmax bi·∫øn mask th√†nh x√°c su·∫•t b·∫±ng 0,
* Attention ch·ªâ t·∫≠p trung v√†o qu√° kh·ª©.

---

## B.4. Tri·ªÉn khai PyTorch: Causal Mask C∆° b·∫£n

### B.4.1. T·∫°o Mask Tam gi√°c

```python
import torch

---

```python

$$
def generate_causal_mask(T, device=None):
$$

    """
    Generate causal attention mask.

    Args:
        T (int): Sequence length
        device (torch.device): Target device

    Returns:
        mask (Tensor): (T, T) boolean mask
    """

$$
mask = torch.triu(
$$

        torch.ones(T, T),
        diagonal=1
    )

    if device is not None:

$$
mask = mask.to(device)
$$

    return mask.bool()

---

### D·∫°ng K·∫øt qu·∫£

V√≠ d·ª• v·ªõi `T = 4`:

```text
0 1 1 1
0 0 1 1
0 0 0 1
0 0 0 0

Trong ƒë√≥:

* `1` = b·ªã ch·∫∑n,
* `0` = cho ph√©p.

---

## B.5. Causal Mask v·ªõi Gi√° tr·ªã -‚àû (Logit Mask)

Trong th·ª±c t·∫ø, mask th∆∞·ªùng ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng gi√° tr·ªã √¢m l·ªõn.

---

### B.5.1. Mask d·∫°ng Float

```python

$$
def generate_causal_logit_mask(T, device=None):
$$

    """
    Generate causal mask with -inf values.
    """

$$
mask = torch.triu(
$$

        torch.ones(T, T),
        diagonal=1
    )

$$
mask = mask.masked_fill(
$$

        mask == 1,
        float("-inf")
    )

    if device is not None:

$$
mask = mask.to(device)
$$

    return mask

---

### C√¥ng d·ª•ng

D√πng tr·ª±c ti·∫øp cho:

```python

$$
scores = scores + mask
$$

---

## B.6. T√≠ch h·ª£p v√†o Multi-Head Attention

---

### B.6.1. Attention Layer v·ªõi Mask

```python
class CausalAttention(torch.nn.Module):

    def __init__(self, d_model, num_heads):
        super().__init__()

$$
self.attn = torch.nn.MultiheadAttention(
$$

            d_model,
            num_heads,

$$
batch_first=True
$$

        )

    def forward(self, x):

        B, T, _ = x.shape

$$
mask = generate_causal_mask(
$$

            T, x.device
        )

$$
out, weights = self.attn(
$$

            x, x, x,

$$
attn_mask=mask
$$

        )

        return out, weights

---

### L∆∞u √Ω

$$
* `attn_mask=True` ‚Üí b·ªã ch·∫∑n,
$$

$$
* `attn_mask=False` ‚Üí cho ph√©p.
$$

---

## B.7. Causal Mask cho Batch v√† KV Cache

Trong inference v·ªõi cache, ch·ªâ c·∫ßn mask cho token m·ªõi.

---

### B.7.1. Mask cho Incremental Decoding

```python
def generate_incremental_mask(
    past_len,
    current_len,
    device
):
    """
    Mask for KV-cache decoding.
    """

$$
total = past_len + current_len
$$

$$
mask = torch.triu(
$$

        torch.ones(current_len, total),

$$
diagonal=1 + past_len
$$

    )

    return mask.bool().to(device)

---

### C√¥ng d·ª•ng

D√πng cho sinh t·ª´ng token:

```text
Past tokens | New token

Ch·ªâ cho ph√©p new token nh√¨n v·ªÅ qu√° kh·ª©.

---

## B.8. V√≠ d·ª• Ho√†n ch·ªânh

---

### B.8.1. Demo Attention v·ªõi Mask

```python
def demo():

    B = 2
    T = 5
    D = 64
    H = 4

$$
x = torch.randn(B, T, D)
$$

$$
attn = CausalAttention(D, H)
$$

$$
out, w = attn$x$
$$

    print("Output:", out.shape)
    print("Weights:", w.shape)

if __name__ == "__main__":
    demo()

---

### Output

```text
Output:  (2, 5, 64)
Weights: (2, 5, 5)

---

## B.9. ƒê·ªô ph·ª©c t·∫°p (Complexity Analysis)

V·ªõi chu·ªói ƒë·ªô d√†i ( T ):

### Th·ªùi gian

$$
O(T^2)
$$

### B·ªô nh·ªõ

$$
O(T^2)
$$

Khi d√πng KV Cache:

$$
O(T)
$$

---

## B.10. Th·∫£o lu·∫≠n K·ªπ thu·∫≠t (Technical Discussion)

### ∆Øu ƒëi·ªÉm

* ƒê·∫£m b·∫£o t√≠nh nh√¢n qu·∫£,
* Tr√°nh leakage th√¥ng tin,
* ƒê·ªìng b·ªô train‚Äìinference,
* D·ªÖ c√†i ƒë·∫∑t.

### H·∫°n ch·∫ø

* Chi ph√≠ b·∫≠c hai theo T,
* Kh√¥ng ph√π h·ª£p chu·ªói r·∫•t d√†i,
* Ph·ª• thu·ªôc v√†o precision.

---

## B.11. T√°i l·∫≠p Th·ª±c nghi·ªám (Reproducibility)

Khuy·∫øn ngh·ªã c·ªë ƒë·ªãnh:

```python
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)

L∆∞u tr·ªØ:

* Mask implementation,
* Version PyTorch,
* GPU driver,
* Config.

---

## B.12. T√≥m t·∫Øt

Ph·ª• l·ª•c n√†y ƒë√£ tr√¨nh b√†y:

* Pseudocode t·∫°o causal mask,
* C√°ch t√≠ch h·ª£p v√†o attention,
* Tri·ªÉn khai PyTorch,
* Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p.

Causal mask l√† n·ªÅn t·∫£ng to√°n h·ªçc v√† k·ªπ thu·∫≠t ƒë·∫£m b·∫£o t√≠nh h·ª£p l·ªá c·ªßa m√¥ h√¨nh ng√¥n ng·ªØ t·ª± h·ªìi quy.

---
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [M·ªü r·ªông Ki·∫øn tr√∫c GPT: Position Embedding, Layer Normalization, Weight Tying v√† Temperature Scaling](aero_llm_010_posion_embedding.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_010_posion_embedding.md) |
| üìå **[Bi·ªÉu di·ªÖn T√≠nh Nh√¢n Qu·∫£ Th·ªùi Gian trong C∆° Ch·∫ø Attention b·∫±ng ƒê·∫°i S·ªë Tuy·∫øn T√≠nh](aero_llm_011_temporal_causality_via_linear_algebra_theory_.md)** | [Xem b√†i vi·∫øt ‚Üí](aero_llm_011_temporal_causality_via_linear_algebra_theory_.md) |
| [C∆° Ch·∫ø Trung B√¨nh H√≥a Qu√° Kh·ª© v√† Lo·∫°i B·ªè T∆∞∆°ng Lai trong M√¥ H√¨nh Ng√¥n Ng·ªØ Nh√¢n Qu·∫£](aero_llm_012_averaging_the_past_while_ignoring_the_future.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_012_averaging_the_past_while_ignoring_the_future.md) |
| [Thu·∫≠t To√°n Attention trong M√¥ H√¨nh Transformer: C∆° S·ªü L√Ω Thuy·∫øt, C∆° Ch·∫ø Ho·∫°t ƒê·ªông v√† H√†m √ù ·ª®ng D·ª•ng](aero_llm_013_the_attention_algorithm_theory_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_013_the_attention_algorithm_theory_.md) |
| [Ph√¢n T√≠ch v√† Tri·ªÉn Khai C∆° Ch·∫ø Attention: So S√°nh C√†i ƒê·∫∑t Th·ªß C√¥ng v√† PyTorch T·ªëi ∆Øu](aero_llm_014_codechallenge_code_attention.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_014_codechallenge_code_attention.md) |
| [Ph√¢n T√≠ch Ki·∫øn Tr√∫c M√¥ H√¨nh Ng√¥n Ng·ªØ v·ªõi M·ªôt Attention Head: L√Ω Thuy·∫øt, Tri·ªÉn Khai v√† ƒê√°nh Gi√°](aero_llm_015_model.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_015_model.md) |
| [Ph√¢n T√≠ch C·∫•u Tr√∫c Transformer Block: L√Ω Thuy·∫øt, C∆° Ch·∫ø Bi·ªÉu Di·ªÖn v√† Vai Tr√≤ Trong M√¥ H√¨nh Ng√¥n Ng·ªØ](aero_llm_016_the_transformer_block_theory_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_016_the_transformer_block_theory_.md) |
| [C√†i ƒê·∫∑t Transformer Block B·∫±ng PyTorch: Ph√¢n T√≠ch Ki·∫øn Tr√∫c, Lu·ªìng D·ªØ Li·ªáu v√† T·ªëi ∆Øu H√≥a](aero_llm_017_the_transformer_block_code_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_017_the_transformer_block_code_.md) |
| [M√¥ H√¨nh Nhi·ªÅu Transformer Blocks Trong M·∫°ng Ng√¥n Ng·ªØ: Ki·∫øn Tr√∫c, Ph√¢n C·∫•p Bi·ªÉu Di·ªÖn v√† Kh·∫£ NƒÉng M·ªü R·ªông](aero_llm_018_model_4_multiple_transformer_blocks_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_018_model_4_multiple_transformer_blocks_.md) |
| [aero llm 019 copy 10](aero_llm_019_copy_10.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_019_copy_10.md) |
| [aero llm 019 copy 11](aero_llm_019_copy_11.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_019_copy_11.md) |
| [aero llm 019 copy 12](aero_llm_019_copy_12.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_019_copy_12.md) |
| [aero llm 019 copy 13](aero_llm_019_copy_13.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_019_copy_13.md) |
| [aero llm 019 copy 9](aero_llm_019_copy_9.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_019_copy_9.md) |
| [Multi-Head Attention: C∆° S·ªü L√Ω Thuy·∫øt v√† Tri·ªÉn Khai Th·ª±c Ti·ªÖn](aero_llm_019_multihead_attention_theory_and_implementation.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_019_multihead_attention_theory_and_implementation.md) |
| [aero llm 01 intro](aero_llm_01_intro.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_01_intro.md) |
| [T·ªëi ∆Øu H√≥a Hu·∫•n Luy·ªán M√¥ H√¨nh H·ªçc S√¢u B·∫±ng GPU: Nguy√™n L√Ω v√† Th·ª±c H√†nh](aero_llm_020_working_on_the_gpu.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_020_working_on_the_gpu.md) |
| [Tri·ªÉn Khai M√¥ H√¨nh GPT-2 Ho√†n Ch·ªânh Tr√™n GPU: Ki·∫øn Tr√∫c, T·ªëi ∆Øu H√≥a v√† ƒê√°nh Gi√° Hi·ªáu NƒÉng](aero_llm_021_mo_hinh_gpt_2_hoan_chinh_tren_gpu.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_021_mo_hinh_gpt_2_hoan_chinh_tren_gpu.md) |
| [ƒê√°nh Gi√° Hi·ªáu NƒÉng GPT-2 Tr√™n CPU v√† GPU: Th·ª±c Nghi·ªám Th·ªùi Gian Kh·ªüi T·∫°o, Suy Lu·∫≠n v√† Hu·∫•n Luy·ªán](aero_llm_022_anh_gia_hieu_nang_gpt_2_tren_cpu_va_gpu.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_022_anh_gia_hieu_nang_gpt_2_tren_cpu_va_gpu.md) |
| [Kh·∫£o S√°t M√¥ H√¨nh GPT-2 Ti·ªÅn Hu·∫•n Luy·ªán c·ªßa OpenAI: Ki·∫øn Tr√∫c, Tham S·ªë v√† C∆° Ch·∫ø Sinh VƒÉn B·∫£n](aero_llm_023_inspecting_openai_s_gpt2.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_023_inspecting_openai_s_gpt2.md) |
| [Ki·∫øn Tr√∫c Transformer v√† Tri·ªÉn Khai GPT-2 tr√™n GPU: Ph√¢n T√≠ch To√°n H·ªçc v√† Hi·ªáu NƒÉng T√≠nh To√°n](aero_llm_024_summarizing_gpt_using_equations.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_024_summarizing_gpt_using_equations.md) |
| [Tr·ª±c Quan H√≥a Ki·∫øn Tr√∫c GPT Th√¥ng Qua nano-GPT: Ti·∫øp C·∫≠n Tr·ª±c Quan trong Nghi√™n C·ª©u M√¥ H√¨nh Ng√¥n Ng·ªØ](aero_llm_025_visualizing_nano_gpt.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_025_visualizing_nano_gpt.md) |
| [Ph√¢n T√≠ch S·ªë L∆∞·ª£ng Tham S·ªë Trong M√¥ H√¨nh GPT-2: Ph∆∞∆°ng Ph√°p ƒê·ªãnh L∆∞·ª£ng v√† √ù Nghƒ©a Ki·∫øn Tr√∫c](aero_llm_026_codechallenge_how_many_parameters_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_026_codechallenge_how_many_parameters_part_1_.md) |
| [Ph√¢n B·ªë Tham S·ªë Trong GPT-2: So S√°nh Attention, MLP v√† Layer Normalization](aero_llm_027_codechallenge_how_many_parameters_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_027_codechallenge_how_many_parameters_part_2_.md) |
| [üìò Ph√¢n T√≠ch Ki·∫øn Tr√∫c GPT-2: T·ª´ C∆° Ch·∫ø Multi-Head Attention ƒê·∫øn Hi·ªáu NƒÉng T√≠nh To√°n Tr√™n GPU](aero_llm_028_codechallenge_gpt2_trained_weights_distributions.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_028_codechallenge_gpt2_trained_weights_distributions.md) |
| [üß† Ph√¢n T√≠ch Nh√¢n Qu·∫£ Trong GPT-2: Vai Tr√≤ C·ªßa Ma Tr·∫≠n Query Th√¥ng Qua Can Thi·ªáp Tham S·ªë](aero_llm_029_codechallenge_do_we_really_need_q.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_029_codechallenge_do_we_really_need_q.md) |
| [Ph√¢n T√≠ch Ki·∫øn Tr√∫c v√† C∆° Ch·∫ø Ho·∫°t ƒê·ªông c·ªßa M√¥ H√¨nh Ng√¥n Ng·ªØ Transformer C∆° B·∫£n](aero_llm_02_transformer.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_02_transformer.md) |
| [Ph√¢n T√≠ch K·ªπ Thu·∫≠t: So S√°nh `nn.Embedding` v√† `nn.Linear` trong PyTorch](aero_llm_03_embedding_linear.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_03_embedding_linear.md) |
| [Ph√¢n T√≠ch So S√°nh H√†m K√≠ch Ho·∫°t GELU v√† ReLU trong M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn: G√≥c Nh√¨n L√Ω Thuy·∫øt v√† Th·ª±c Nghi·ªám](aero_llm_04_gelu_vs_relu_academic_analysis.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_04_gelu_vs_relu_academic_analysis.md) |
| [H√†m Softmax v√† Tham S·ªë Temperature trong M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn: Ph√¢n T√≠ch To√°n H·ªçc v√† Th·ª±c Nghi·ªám](aero_llm_05_softmax_temperature_academic_analysis.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_05_softmax_temperature_academic_analysis.md) |
| [Ph√¢n T√≠ch `torch.multinomial`: L·∫•y M·∫´u X√°c Su·∫•t trong Sinh VƒÉn B·∫£n v·ªõi PyTorch](aero_llm_06_torch_multinomial_academic_analysis.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_06_torch_multinomial_academic_analysis.md) |
| [Ph∆∞∆°ng Ph√°p L·∫•y M·∫´u Token trong Sinh VƒÉn B·∫£n: Ph√¢n T√≠ch So S√°nh Greedy, Top-K, Top-P v√† Multinomial Sampling](aero_llm_07_token_sampling_methods.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_07_token_sampling_methods.md) |
| [Ph√¢n T√≠ch H√†nh Vi C·ªßa H√†m Softmax Trong M√¥ H√¨nh H·ªçc S√¢u: ·∫¢nh H∆∞·ªüng C·ªßa L·∫∑p, Ph·∫°m Vi S·ªë H·ªçc V√† Nhi·ªát ƒê·ªô](aero_llm_08_ham_softbank.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_08_ham_softbank.md) |
| [Ph√¢n T√≠ch Layer Normalization Trong H·ªçc S√¢u: C∆° S·ªü L√Ω Thuy·∫øt, ·ªîn ƒê·ªãnh S·ªë H·ªçc V√† ·ª®ng D·ª•ng Th·ª±c Ti·ªÖn](aero_llm_09_layer_normalization.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_09_layer_normalization.md) |
| [kien truc mo hinh ngon ngu lon](kien_truc_mo_hinh_ngon_ngu_lon.md) | [Xem b√†i vi·∫øt ‚Üí](kien_truc_mo_hinh_ngon_ngu_lon.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

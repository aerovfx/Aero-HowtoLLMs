
<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [04 buildgpt](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
````md
# M·ªü r·ªông Ki·∫øn tr√∫c GPT: Position Embedding, Layer Normalization, Weight Tying v√† Temperature Scaling

## T√≥m t·∫Øt (Abstract)

B√†i b√°o n√†y ph√¢n t√≠ch c√°c th√†nh ph·∫ßn quan tr·ªçng trong vi·ªác m·ªü r·ªông m√¥ h√¨nh GPT c∆° b·∫£n, bao g·ªìm position embedding, layer normalization, weight tying v√† temperature scaling. D·ª±a tr√™n t√†i li·ªáu gi·∫£ng d·∫°y v·ªÅ x√¢y d·ª±ng m√¥ h√¨nh GPT-2 ƒë∆°n gi·∫£n , ch√∫ng t√¥i tr√¨nh b√†y c∆° s·ªü l√Ω thuy·∫øt, c∆° ch·∫ø tri·ªÉn khai v√† t√°c ƒë·ªông th·ª±c nghi·ªám c·ªßa t·ª´ng th√†nh ph·∫ßn. K·∫øt qu·∫£ cho th·∫•y c√°c k·ªπ thu·∫≠t n√†y ƒë√≥ng vai tr√≤ thi·∫øt y·∫øu trong vi·ªác ·ªïn ƒë·ªãnh hu·∫•n luy·ªán, gi·∫£m s·ªë tham s·ªë v√† c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng sinh vƒÉn b·∫£n.

---

## 1. Gi·ªõi thi·ªáu (Introduction)

M√¥ h√¨nh Transformer v√† c√°c bi·∫øn th·ªÉ GPT ƒë√£ tr·ªü th√†nh n·ªÅn t·∫£ng cho nhi·ªÅu h·ªá th·ªëng x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n hi·ªán ƒë·∫°i. M·ªôt GPT t·ªëi thi·ªÉu ch·ªâ g·ªìm embedding, MLP v√† linear output th∆∞·ªùng kh√¥ng ƒë·ªß ·ªïn ƒë·ªãnh ƒë·ªÉ hu·∫•n luy·ªán v√† suy lu·∫≠n hi·ªáu qu·∫£.

Theo t√†i li·ªáu x√¢y d·ª±ng m√¥ h√¨nh GPT-2 d·∫°ng h·ªçc thu·∫≠t , vi·ªác b·ªï sung position embedding, layer normalization, weight tying v√† temperature scaling gi√∫p m√¥ h√¨nh:

- Nh·∫≠n bi·∫øt v·ªã tr√≠ t·ª´ trong chu·ªói,
- ·ªîn ƒë·ªãnh ph√¢n ph·ªëi k√≠ch ho·∫°t,
- Gi·∫£m s·ªë l∆∞·ª£ng tham s·ªë,
- Ki·ªÉm so√°t t√≠nh ng·∫´u nhi√™n khi sinh vƒÉn b·∫£n.

M·ª•c ti√™u c·ªßa b√†i b√°o l√† ph√¢n t√≠ch c√≥ h·ªá th·ªëng c√°c k·ªπ thu·∫≠t n√†y trong b·ªëi c·∫£nh m√¥ h√¨nh ng√¥n ng·ªØ quy m√¥ nh·ªè ƒë·∫øn trung b√¨nh.

---

## 2. C∆° s·ªü l√Ω thuy·∫øt (Theoretical Background)

### 2.1. Token Embedding v√† Position Embedding

Trong GPT, m·ªói token ƒë∆∞·ª£c √°nh x·∫° th√†nh vector th√¥ng qua embedding:

$$

$$

E_{tok} \in \mathbb{R}^{V \times d}

$$

$$

v·ªõi $V$ l√† k√≠ch th∆∞·ªõc t·ª´ v·ª±ng, $d$ l√† chi·ªÅu embedding.

Position embedding ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a:

$$

$$

E_{pos} \in \mathbb{R}^{L \times d}

$$

$$

v·ªõi $L$ l√† ƒë·ªô d√†i chu·ªói t·ªëi ƒëa.

Bi·ªÉu di·ªÖn ƒë·∫ßu v√†o:

$$

$$

X = E_{tok}(w_i) + E_{pos}(i)

$$

$$

C√°ch c·ªông tr·ª±c ti·∫øp n√†y cho ph√©p m√¥ h√¨nh h·ªçc th√¥ng tin th·ª© t·ª± m√† kh√¥ng c·∫ßn ki·∫øn tr√∫c h·ªìi quy.

---

### 2.2. Layer Normalization

Layer normalization chu·∫©n h√≥a theo chi·ªÅu embedding:

$$

$$

\hat{x} = \frac{x - \mu}{\sigma + \epsilon}

$$

$$

$$
y = \gamma \hat{x} + \beta
$$

$$
Trong ƒë√≥ \mu, \sigma ƒë∆∞·ª£c t√≠nh theo t·ª´ng token. T√°c d·ª•ng ch√≠nh: - Gi·∫£m hi·ªán t∆∞·ª£ng exploding/vanishing gradients, - ·ªîn ƒë·ªãnh ph√¢n ph·ªëi k√≠ch ho·∫°t, - TƒÉng t·ªëc h·ªôi t·ª•. --- ### 2.3. Weight Tying (Tied Embeddings) Weight tying r√†ng bu·ªôc:
$$

$$
W_{out} = E_{tok}^T
$$

$$
Trong ƒë√≥ W_{out} l√† ma tr·∫≠n unembedding. ∆Øu ƒëi·ªÉm: - Gi·∫£m ~30‚Äì40% s·ªë tham s·ªë, - TƒÉng t√≠nh nh·∫•t qu√°n gi·ªØa bi·ªÉu di·ªÖn v√† d·ª± ƒëo√°n, - Gi·∫£m overfitting. --- ### 2.4. Logit Scaling v√† Temperature #### Logit Scaling Logits cu·ªëi c√πng ƒë∆∞·ª£c chu·∫©n h√≥a:
$$

$$
z' = \frac{z}{\sqrt{d}}
$$

$$
M·ª•c ƒë√≠ch: gi·ªØ ph∆∞∆°ng sai logits ·ªü m·ª©c ·ªïn ƒë·ªãnh, ph√π h·ª£p v·ªõi gi·∫£ thuy·∫øt l√Ω thuy·∫øt. #### Temperature Scaling Trong suy lu·∫≠n:
$$

$$
p_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
$$

$$
- T < 1: sinh vƒÉn b·∫£n quy·∫øt ƒë·ªãnh h∆°n, - T > 1: sinh vƒÉn b·∫£n ƒëa d·∫°ng h∆°n. --- ## 3. Ph∆∞∆°ng ph√°p nghi√™n c·ª©u (Methodology) ### 3.1. Ki·∫øn tr√∫c m√¥ h√¨nh M√¥ h√¨nh ƒë∆∞·ª£c x√¢y d·ª±ng t·ª´ GPT-1 c∆° b·∫£n v√† m·ªü r·ªông theo t√†i li·ªáu tham kh·∫£o : 1. Token Embedding 2. Position Embedding 3. LayerNorm 4. MLP + Activation (GELU) 5. Linear Output (Weight Tying) 6. Logit Scaling S∆° ƒë·ªì t·ªïng qu√°t: ```text Input Tokens ‚Üì Token Embedding + Position Embedding ‚Üì LayerNorm ‚Üì MLP (GELU) ‚Üì Tied Linear Output ‚Üì Scaled Logits ```` --- ### 3.2. Quy tr√¨nh hu·∫•n luy·ªán v√† ƒë√°nh gi√° * Kh·ªüi t·∫°o tham s·ªë ng·∫´u nhi√™n (Gaussian/Xavier), * Kh√¥ng hu·∫•n luy·ªán ƒë·∫ßy ƒë·ªß, t·∫≠p trung v√†o ph√¢n t√≠ch th·ªëng k√™, * So s√°nh loss th·ª±c nghi·ªám v√† loss l√Ω thuy·∫øt. Loss l√Ω thuy·∫øt c·ªßa m√¥ h√¨nh ng·∫´u nhi√™n:
$$

$$
\mathcal{L}_{theory} = \log(V)
$$

$$
v·ªõi (V) l√† vocab size. --- ### 3.3. Sinh vƒÉn b·∫£n Thu·∫≠t to√°n sinh: 1. L·∫•y logits cu·ªëi, 2. Chia cho temperature, 3. Softmax, 4. Multinomial sampling, 5. L·∫∑p autoregressive. Ch·ªâ s·ª≠ d·ª•ng c·ª≠a s·ªï ng·ªØ c·∫£nh g·∫ßn nh·∫•t (sliding window) ƒë·ªÉ gi·ªõi h·∫°n b·ªô nh·ªõ. --- ## 4. K·∫øt qu·∫£ (Results) ### 4.1. Ph√¢n ph·ªëi Loss Khi kh·ªüi t·∫°o ng·∫´u nhi√™n, loss th·ª±c nghi·ªám:
$$

* G·∫ßn b·∫±ng (\log(V)),

$$
* Ph√π h·ª£p v·ªõi d·ª± ƒëo√°n l√Ω thuy·∫øt. ƒêi·ªÅu n√†y x√°c nh·∫≠n m√¥ h√¨nh ƒë∆∞·ª£c c√†i ƒë·∫∑t ƒë√∫ng. --- ### 4.2. Ph√¢n ph·ªëi X√°c su·∫•t Softmax ƒë·∫ßu ra th·ªÉ hi·ªán: * Ph√¢n ph·ªëi th∆∞a (sparse), * M·ªôt s·ªë token c√≥ x√°c su·∫•t n·ªïi tr·ªôi, * Ph·∫ßn l·ªõn token c√≥ x√°c su·∫•t r·∫•t nh·ªè (~1/V). Khi thay ƒë·ªïi temperature: | Temperature | ƒê·ªô ƒëa d·∫°ng | ƒê·ªô m·∫°ch l·∫°c | | ----------- | ---------- | ----------- | | 0.5         | Th·∫•p       | Cao         | | 1.0         | Trung b√¨nh | Trung b√¨nh  | | 1.3         | Cao        | Th·∫•p h∆°n    | --- ### 4.3. Ch·∫•t l∆∞·ª£ng Sinh vƒÉn b·∫£n M√¥ h√¨nh ch∆∞a hu·∫•n luy·ªán: * T·∫°o chu·ªói c√≥ c·∫•u tr√∫c ng·∫Øn h·∫°n, * Nhanh ch√≥ng suy bi·∫øn sang nhi·ªÖu. ƒêi·ªÅu n√†y ph·∫£n √°nh vai tr√≤ c·ªët l√µi c·ªßa d·ªØ li·ªáu hu·∫•n luy·ªán. --- ## 5. Th·∫£o lu·∫≠n (Discussion) ### 5.1. Vai tr√≤ c·ªßa Position Embedding Vi·ªác c·ªông tr·ª±c ti·∫øp embedding v·ªã tr√≠: * ƒê∆°n gi·∫£n, * Hi·ªáu qu·∫£, * Kh√¥ng l√†m tƒÉng ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n. Tuy nhi√™n, h·∫°n ch·∫ø l√† kh√¥ng ngo·∫°i suy t·ªët cho chu·ªói d√†i h∆°n L. --- ### 5.2. T√°c ƒë·ªông c·ªßa LayerNorm LayerNorm ƒë√≥ng vai tr√≤ then ch·ªët trong: * ·ªîn ƒë·ªãnh forward pass, * Cho ph√©p hu·∫•n luy·ªán s√¢u, * Gi·∫£m ph·ª• thu·ªôc v√†o learning rate. Thi·∫øu LayerNorm ‚Üí hu·∫•n luy·ªán kh√¥ng ·ªïn ƒë·ªãnh. --- ### 5.3. L·ª£i √≠ch c·ªßa Weight Tying Weight tying: * Gi·∫£m chi ph√≠ hu·∫•n luy·ªán, * Ph√π h·ª£p v·ªõi m√¥ h√¨nh nh·ªè/trung b√¨nh. Tuy nhi√™n, v·ªõi m√¥ h√¨nh c·ª±c l·ªõn, untying c√≥ th·ªÉ tƒÉng t√≠nh bi·ªÉu di·ªÖn. --- ### 5.4. Temperature v√† ƒêi·ªÅu khi·ªÉn H√†nh vi Sinh Temperature cho ph√©p: * ƒêi·ªÅu ch·ªânh phong c√°ch sinh, * C√¢n b·∫±ng s√°ng t·∫°o v√† ch√≠nh x√°c. Trong h·ªá th·ªëng chatbot th·ª±c t·∫ø, temperature th∆∞·ªùng n·∫±m trong [0.7, 1.0]. --- ## 6. H·∫°n ch·∫ø (Limitations) Nghi√™n c·ª©u hi·ªán t·∫°i c√≥ m·ªôt s·ªë h·∫°n ch·∫ø: * Kh√¥ng hu·∫•n luy·ªán tr√™n t·∫≠p d·ªØ li·ªáu l·ªõn, * Ch∆∞a t√≠ch h·ª£p attention nhi·ªÅu t·∫ßng, * Ch∆∞a ƒë√°nh gi√° tr√™n benchmark chu·∫©n. Do ƒë√≥, k·∫øt qu·∫£ ch·ªß y·∫øu mang t√≠nh minh h·ªça ki·∫øn tr√∫c. --- ## 7. K·∫øt lu·∫≠n (Conclusion) B√†i b√°o ƒë√£ ph√¢n t√≠ch chi ti·∫øt c√°c th√†nh ph·∫ßn m·ªü r·ªông c·ªßa GPT g·ªìm: * Position embedding, * Layer normalization, * Weight tying, * Logit v√† temperature scaling. C√°c k·ªπ thu·∫≠t n√†y: * ·ªîn ƒë·ªãnh hu·∫•n luy·ªán, * Gi·∫£m tham s·ªë, * C·∫£i thi·ªán ki·ªÉm so√°t suy lu·∫≠n. Ch√∫ng l√† n·ªÅn t·∫£ng cho vi·ªác x√¢y d·ª±ng LLM hi·ªán ƒë·∫°i. --- ## 8. H∆∞·ªõng nghi√™n c·ª©u ti·∫øp theo (Future Work) C√°c h∆∞·ªõng m·ªü r·ªông: * Rotary/ALiBi position embedding, * Pre-LN Transformer, * RMSNorm, * Adaptive temperature, * Scaling l√™n h√†ng t·ª∑ tham s·ªë. --- ## T√†i li·ªáu tham kh·∫£o (References) 1. T√†i li·ªáu x√¢y d·ª±ng GPT-2 m·ªü r·ªông, Position Embedding, LayerNorm, Weight Tying v√† Temperature Scaling. D∆∞·ªõi ƒë√¢y l√† ph·∫ßn **Pseudocode + PyTorch Implementation** ƒë∆∞·ª£c vi·∫øt theo **chu·∫©n b√†i b√°o khoa h·ªçc**, ph√π h·ª£p ƒë·ªÉ ƒë∆∞a v√†o: ‚úÖ Ph·∫ßn Method / Appendix ‚úÖ Supplementary Material ‚úÖ Thesis / Journal Paper Tr√¨nh b√†y theo format h·ªçc thu·∫≠t. --- ````md # Ph·ª• l·ª•c A: Thu·∫≠t to√°n v√† Tri·ªÉn khai (Pseudocode and Implementation) ## A.1. T·ªïng quan Ph·∫ßn n√†y tr√¨nh b√†y thu·∫≠t to√°n hu·∫•n luy·ªán v√† suy lu·∫≠n cho m√¥ h√¨nh GPT m·ªü r·ªông v·ªõi: - Position Embedding - Layer Normalization - Weight Tying - Temperature Scaling M·ª•c ti√™u l√† m√¥ t·∫£ r√µ r√†ng quy tr√¨nh th·ª±c thi v√† kh·∫£ nƒÉng t√°i l·∫≠p. --- ## A.2. Pseudocode c·ªßa M√¥ h√¨nh GPT M·ªü r·ªông ### Thu·∫≠t to√°n 1: Forward Pass c·ªßa GPT v·ªõi Position Embedding v√† Weight Tying **Input:** - Chu·ªói token: x = (x_1, x_2, ..., x_T) - Token embedding: E_{tok} - Position embedding: E_{pos} - MLP parameters: \Theta - Output projection: W_{out} = E_{tok}^T **Output:** - Logits: z --- ```text Algorithm 1: GPT-Forwardx
$$

1:  for i = 1 ‚Üí T do

$$
2:      e_tok ‚Üê E_tok[x_i] 3:      e_pos ‚Üê E_pos[i] 4:      h_i ‚Üê e_tok + e_pos 5:  end for 6:  H ‚Üê LayerNormh 7:  for each layer l do 8:      H ‚Üê MLP_lH 9:      H ‚Üê LayerNormH 10: end for 11: Z ‚Üê H ¬∑ W_out 12: return Z ```` --- ### Thu·∫≠t to√°n 2: Hu·∫•n luy·ªán M√¥ h√¨nh **Input:** * Dataset ( D ) * Learning rate ( \eta ) * Batch size ( B ) * Epochs ( E ) --- ```text Algorithm 2: Training(D, Œ∑, B, E) 1:  Initialize Œ∏ randomly
$$

2:  for epoch = 1 ‚Üí E do

$$
3:      for batch (x, y) ‚àà D do 4:          Z ‚Üê GPT-Forwardx 5:          L ‚Üê CrossEntropy(Z, y) 6:          Compute ‚àáŒ∏L 7:          Œ∏ ‚Üê Œ∏ ‚àí Œ∑‚àáŒ∏L 8:      end for 9:  end for --- ### Thu·∫≠t to√°n 3: Sinh VƒÉn b·∫£n v·ªõi Temperature **Input:** * Prompt P * Temperature T * Max tokens N --- ```text Algorithm 3: Generate(P, T, N) 1:  x ‚Üê TokenizeP(
$$

2:  for t = 1 ‚Üí N do

$$
3:      Z ‚Üê GPT-Forward)x 4:      z_t ‚Üê Z_last / T 5:      p ‚Üê Softmaxz_t 6:      s ‚Üê Samplep 7:      x ‚Üê Append(x, s) 8:  end for 9:  return x --- ## A.3. Tri·ªÉn khai PyTorch (PyTorch Implementation) ### A.3.1. M√¥ h√¨nh GPT M·ªü r·ªông ```python import torch import torch.nn as nn import math --- ### Token + Position Embedding ```python class GPTEmbedding(nn.Module): def __init__(self, vocab_size, max_len, d_model): super().__init__()
$$

$$
self.token_emb = nn.Embedding(
$$

$$
vocab_size, d_model )
$$

$$
self.pos_emb = nn.Embedding(
$$

$$
max_len, d_model ) def forward(self, x):
$$

$$
B, T = x.shape
$$

$$

$$

pos = torch.arange(

$$

$$

$$
T, device=x.device
$$

$$
)
$$

$$
tok = self.token_embx
$$

$$

$$

pos = self.pos_emb(pos)

$$

$$

        return tok + pos

---

### Feedforward Block

```python
class FeedForward(nn.Module):

    def __init__(self, d_model):
        super().__init__()

$$

$$

self.net = nn.Sequential(

$$

$$

            nn.Linear(d_model, 4 * d_model),
            nn.GELU(),
            nn.Linear(4 * d_model, d_model)
        )

    def forward(self, x):
        return self.net$x$

---

### Transformer Block (Pre-LN)

```python
class GPTBlock(nn.Module):

$$

$$

def __init__(self, d_model, heads, dropout=0.1):

$$

$$

        super().__init__()

$$

$$

self.ln1 = nn.LayerNorm(d_model)

$$

$$

$$
self.ln2 = nn.LayerNorm(d_model)
$$

$$

$$

self.attn = nn.MultiheadAttention(

$$

$$

            d_model,
            heads,

$$

$$

dropout=dropout,

$$

$$

$$
batch_first=True
$$

$$
)
$$

$$
self.ffn = FeedForward(d_model)
$$

$$
def forward(self, x, mask):
$$

$$
h = self.ln1x
$$

$$

$$

attn_out, _ = self.attn(

$$

$$

$$
h, h, h, attn_mask=mask
$$

$$
)
$$

$$
x = x + attn_out
$$

$$

$$

h = self.ln2x

$$

$$

$$
x = x + self.ffnh
$$

$$
return x --- ### GPT Model v·ªõi Weight Tying ```python class MiniGPT(nn.Module): def __init__( self, vocab_size, max_len, d_model, heads, layers ): super().__init__()
$$

$$
self.embed = GPTEmbedding(
$$

$$
vocab_size, max_len, d_model )
$$

$$
self.blocks = nn.ModuleList([
$$

$$
GPTBlock(d_model, heads) for _ in range(layers) ])
$$

$$
self.ln_f = nn.LayerNorm(d_model)
$$

$$

$$

self.lm_head = nn.Linear(

$$

$$

            d_model,
            vocab_size,

$$

$$

bias=False

$$

$$

        )

        # Weight tying

$$

$$

self.lm_head.weight = \

$$

$$

            self.embed.token_emb.weight

$$

$$

self.max_len = max_len

$$

$$

    def causal_mask(self, T, device):

        return torch.triu(

$$

$$

torch.ones(T, T, device=device),

$$

$$

diagonal=1

$$
).bool() def forward(self, x):
$$

$$
B, T = x.shape
$$

$$

$$

h = self.embedx

$$

$$

$$
mask = self.causal_mask(
$$

$$
T, x.device ) for block in self.blocks:
$$

$$
h = block(h, mask)
$$

$$

$$

h = self.ln_fh

$$

$$

$$
logits = self.lm_headh
$$

$$
return logits --- ## A.3.2. Hu·∫•n luy·ªán (Training) ```python def train_step( model, optimizer, loss_fn, x, y ):
$$

$$
logits = modelx
$$

$$

$$

loss = loss_fn(

$$

$$

        logits.view(-1, logits.size(-1)),
        y.view(-1)
    )

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()

---

### Training Loop

```python
def train(
    model,
    dataloader,
    epochs,

$$
lr=3e-4,
$$

$$
device="cuda"
$$

):

    model.to(device)

$$

$$

optimizer = torch.optim.AdamW(

$$

$$

        model.parameters(),

$$

$$

lr=lr

$$

$$

    )

$$

$$

loss_fn = nn.CrossEntropyLoss()

$$

$$

    for epoch in range(epochs):

$$
total = 0
$$

        for x, y in dataloader:

$$

$$

x = x.to(device)

$$

$$

$$
y = y.to(device)
$$

$$

$$

loss = train_step(

$$

$$

                model,
                optimizer,
                loss_fn,
                x,
                y
            )

$$
total += loss
$$

        print(
            f"Epoch {epoch}: "

$$
f"Loss = {total/len(dataloader):.4f}"
$$

        )

---

## A.3.3. Sinh VƒÉn b·∫£n (Inference + Temperature)

```python
@torch.no_grad()
def generate(
    model,
    tokenizer,
    prompt,

$$

$$

max_new=200,

$$

$$

temperature=1.0

$$
): model.eval()
$$

$$
device = next(model.parameters()).device
$$

$$

$$

ids = torch.tensor(

$$

$$

        tokenizer.encode(prompt),

$$

$$

device=device

$$

$$

    ).unsqueeze(0)

    for _ in range$max_new$:

$$

$$

logits = model(ids)

$$

$$

$$
next_logits = logits[:, -1]
$$

$$

$$

next_logits /= temperature

$$

$$

$$
probs = torch.softmax(
$$

$$

$$

next_logits, dim=-1

$$

$$

        )

$$

$$

next_id = torch.multinomial(

$$

$$

            probs, 1
        )

$$

$$

ids = torch.cat(

$$

$$

$$
[ids, next_id], dim=1
$$

$$
) return tokenizer.decode( ids[0].tolist() ) --- ## A.4. ƒê·ªô ph·ª©c t·∫°p t√≠nh to√°n (Computational Complexity) V·ªõi: * Sequence length: T * Hidden size: d * Layers: L Chi ph√≠ forward:
$$

O(L \cdot T^2 \cdot d)

$$
B·ªô nh·ªõ:
$$

O(L \cdot T \cdot d)

$$
Khi d√πng KV-cache:
$$

O(L \cdot T \cdot d)
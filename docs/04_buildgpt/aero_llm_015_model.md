
<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [04 buildgpt](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
D∆∞·ªõi ƒë√¢y l√† **b√†i vi·∫øt khoa h·ªçc b·∫±ng ti·∫øng Vi·ªát**, ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n t√†i li·ªáu ƒë√≠nh k√®m *‚ÄúModel 3: One Attention Head‚Äù* v√† b·ªï sung t√†i li·ªáu tham kh·∫£o h·ªçc thu·∫≠t, tr√¨nh b√†y theo ƒë·ªãnh d·∫°ng **Markdown (MD)**.

---

# Ph√¢n T√≠ch Ki·∫øn Tr√∫c M√¥ H√¨nh Ng√¥n Ng·ªØ v·ªõi M·ªôt Attention Head: L√Ω Thuy·∫øt, Tri·ªÉn Khai v√† ƒê√°nh Gi√°

## T√≥m t·∫Øt (Abstract)

C∆° ch·∫ø attention l√† n·ªÅn t·∫£ng c·ªßa c√°c m√¥ h√¨nh Transformer v√† m√¥ h√¨nh ng√¥n ng·ªØ hi·ªán ƒë·∫°i. B√†i b√°o n√†y ph√¢n t√≠ch ki·∫øn tr√∫c m√¥ h√¨nh ng√¥n ng·ªØ v·ªõi m·ªôt attention head, ƒë∆∞·ª£c gi·ªõi thi·ªáu trong t√†i li·ªáu ‚ÄúModel 3: One Attention Head‚Äù. Nghi√™n c·ª©u tr√¨nh b√†y c√°ch t√≠ch h·ª£p attention v√†o pipeline x·ª≠ l√Ω token, vai tr√≤ c·ªßa layer normalization, residual connection, causal masking v√† weight tying. ƒê·ªìng th·ªùi, b√†i vi·∫øt ƒë√°nh gi√° c√°c ƒë·∫∑c t√≠nh to√°n h·ªçc v√† th·ª±c nghi·ªám c·ªßa m√¥ h√¨nh, t·ª´ ƒë√≥ ch·ªâ ra √Ω nghƒ©a c·ªßa attention ƒë∆°n head trong ti·∫øn tr√¨nh ph√°t tri·ªÉn m√¥ h√¨nh ng√¥n ng·ªØ quy m√¥ l·ªõn.

---

## 1. Gi·ªõi thi·ªáu (Introduction)

Trong c√°c m√¥ h√¨nh ng√¥n ng·ªØ hi·ªán ƒë·∫°i, Transformer ƒë√£ tr·ªü th√†nh ki·∫øn tr√∫c ch·ªß ƒë·∫°o nh·ªù kh·∫£ nƒÉng m√¥ h√¨nh h√≥a quan h·ªá d√†i h·∫°n gi·ªØa c√°c token. Th√†nh ph·∫ßn trung t√¢m c·ªßa Transformer l√† c∆° ch·∫ø self-attention.

T√†i li·ªáu ‚ÄúModel 3: One Attention Head‚Äù m√¥ t·∫£ b∆∞·ªõc chuy·ªÉn t·ª´ m√¥ h√¨nh embedding tuy·∫øn t√≠nh sang m√¥ h√¨nh c√≥ attention, trong ƒë√≥ ch·ªâ s·ª≠ d·ª•ng m·ªôt head duy nh·∫•t. ƒê√¢y l√† b∆∞·ªõc trung gian quan tr·ªçng tr∆∞·ªõc khi m·ªü r·ªông sang multi-head attention. 

M·ª•c ti√™u c·ªßa b√†i b√°o n√†y l√†:

- Ph√¢n t√≠ch c·∫•u tr√∫c m√¥ h√¨nh v·ªõi m·ªôt attention head,
- L√†m r√µ vai tr√≤ c·ªßa t·ª´ng th√†nh ph·∫ßn,
- ƒê√°nh gi√° ƒë·∫∑c t√≠nh to√°n h·ªçc v√† h·ªá th·ªëng,
- ƒê·∫∑t m√¥ h√¨nh trong b·ªëi c·∫£nh ph√°t tri·ªÉn LLM hi·ªán ƒë·∫°i.

---

## 2. C∆° s·ªü l√Ω thuy·∫øt (Theoretical Background)

### 2.1. Bi·ªÉu di·ªÖn Token v√† Position Embedding

ƒê·∫ßu v√†o c·ªßa m√¥ h√¨nh l√† chu·ªói token ƒë∆∞·ª£c √°nh x·∫° th√†nh embedding:

$$

$$

X = E_{token} + E_{pos}

$$

$$

Trong ƒë√≥:

- $E_{token}$: token embedding,
- $E_{pos}$: position embedding.

Position embedding cho ph√©p m√¥ h√¨nh nh·∫≠n bi·∫øt th·ª© t·ª± chu·ªói. 

---

### 2.2. Scaled Dot-Product Attention

Attention trong m√¥ h√¨nh ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a:

$$

$$

\text{Attention}(Q,K,V)= \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V

$$

$$

Trong ƒë√≥:

- $Q=XW_Q$,
- $K=XW_K$,
- $V=XW_V$.

H·ªá s·ªë $\sqrt{d}$ gi√∫p ·ªïn ƒë·ªãnh gi√° tr·ªã softmax.

---

### 2.3. Causal Mask

M√¥ h√¨nh s·ª≠ d·ª•ng causal mask ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh t·ª± h·ªìi quy:

$$

$$

M_{ij}= \begin{cases} 0 & j \le i \\ -\infty & j > i \end{cases}

$$

$$

$$
Mask ƒë∆∞·ª£c √°p d·ª•ng b·∫±ng c√°ch thay th·∫ø c√°c ph·∫ßn t·ª≠ b·ªã che b·ªüi -\infty.
$$

---

### 2.4. Layer Normalization v√† Residual Connection

Tr∆∞·ªõc attention, d·ªØ li·ªáu ƒë∆∞·ª£c chu·∫©n h√≥a:

$$

$$

\hat{X}=\text{LayerNorm}(X)

$$

$$

Sau ƒë√≥, ƒë·∫ßu ra attention ƒë∆∞·ª£c c·ªông tr·ªü l·∫°i:

$$

$$

Y = X + \text{Attention}(\hat{X})

$$

$$

C·∫•u tr√∫c residual gi√∫p:

- ·ªîn ƒë·ªãnh gradient,
- H·∫°n ch·∫ø m·∫•t th√¥ng tin,
- TƒÉng kh·∫£ nƒÉng hu·∫•n luy·ªán s√¢u. 

---

## 3. Ph∆∞∆°ng ph√°p (Methodology)

### 3.1. Ki·∫øn tr√∫c M√¥ h√¨nh

M√¥ h√¨nh g·ªìm c√°c th√†nh ph·∫ßn:

1. Token embedding,
2. Position embedding,
3. LayerNorm,
4. Single-head Attention,
5. Linear mixing (W‚ÇÄ),
6. Output projection (unembedding).

Unembedding ƒë∆∞·ª£c chia s·∫ª tr·ªçng s·ªë v·ªõi embedding (weight tying). 

---

### 3.2. Kh·ªüi t·∫°o Tham s·ªë

C√°c ma tr·∫≠n tr·ªçng s·ªë:

$$

$$

W_Q, W_K, W_V, W_0 \in \mathbb{R}^{d \times d}

$$

$$

Kh√¥ng s·ª≠ d·ª•ng bias cho QKV, do LayerNorm ƒë√£ x·ª≠ l√Ω d·ªãch chuy·ªÉn ph√¢n ph·ªëi. 

---

### 3.3. Forward Pass

Qu√° tr√¨nh lan truy·ªÅn thu·∫≠n g·ªìm:

1. Nh·∫≠n token indices,
2. Tra embedding,
3. C·ªông position embedding,
4. LayerNorm,
5. T√≠nh Q, K, V,
6. Attention + mask,
7. Linear mixing,
8. Residual addition,
9. Output logits.

Pipeline n√†y ph·∫£n √°nh m·ªôt attention sublayer ho√†n ch·ªânh. 

---

### 3.4. Tr√≠ch xu·∫•t Ma tr·∫≠n Attention

M√¥ h√¨nh xu·∫•t th√™m:

- Causal mask,
- QK scaled,
- QK softmax.

ƒêi·ªÅu n√†y cho ph√©p tr·ª±c quan h√≥a v√† ki·ªÉm ch·ª©ng ho·∫°t ƒë·ªông attention. 

Trong th·ª±c t·∫ø, k·ªπ thu·∫≠t hook th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng thay th·∫ø.

---

### 3.5. C·∫•u h√¨nh Th·ª±c nghi·ªám

Th√¥ng s·ªë m√¥ h√¨nh:

| Tham s·ªë | Gi√° tr·ªã |
|---------|----------|
| Sequence length | 8 |
| Batch size | 5 |
| Embedding dim | 64 |
| Vocabulary | ‚âà 50k |

M√¥ h√¨nh c√≥ quy m√¥ nh·ªè nh·∫±m m·ª•c ƒë√≠ch minh h·ªça. 

---

## 4. K·∫øt qu·∫£ (Results)

### 4.1. Ph√¢n t√≠ch Ma tr·∫≠n Attention

Sau softmax:

- M·ªói h√†ng c√≥ t·ªïng b·∫±ng 1,
- C√°c gi√° tr·ªã kh√¥ng √¢m,
- Ph·∫£n √°nh ph√¢n ph·ªëi x√°c su·∫•t.

ƒêi·ªÅu n√†y x√°c nh·∫≠n t√≠nh ƒë√∫ng ƒë·∫Øn c·ªßa ph√©p chu·∫©n h√≥a. 

---

### 4.2. H√†nh vi v·ªõi Tr·ªçng s·ªë Ng·∫´u nhi√™n

V·ªõi tr·ªçng s·ªë kh·ªüi t·∫°o ng·∫´u nhi√™n:

- Attention g·∫ßn ph√¢n ph·ªëi ƒë·ªÅu,
- Kh√¥ng c√≥ c·∫•u tr√∫c ng·ªØ nghƒ©a,
- C√°c token c√≥ m·ª©c ·∫£nh h∆∞·ªüng t∆∞∆°ng ƒë∆∞∆°ng.

ƒêi·ªÅu n√†y ph√π h·ª£p v·ªõi l√Ω thuy·∫øt. 

---

### 4.3. ƒê√°nh gi√° Loss

Gi√° tr·ªã cross-entropy loss x·∫•p x·ªâ l√Ω thuy·∫øt:

$$
\log(|V|)
$$

Cho th·∫•y m√¥ h√¨nh ch∆∞a h·ªçc ƒë∆∞·ª£c th√¥ng tin ng√¥n ng·ªØ. 

---

### 4.4. ·∫¢nh h∆∞·ªüng c·ªßa Sequence Length

M√¥ h√¨nh y√™u c·∫ßu ƒë·ªô d√†i c·ªë ƒë·ªãnh. Khi thay ƒë·ªïi chi·ªÅu d√†i chu·ªói, ph√©p nh√¢n ma tr·∫≠n b·ªã l·ªói.

ƒêi·ªÅu n√†y ph·∫£n √°nh h·∫°n ch·∫ø c·ªßa ki·∫øn tr√∫c c∆° b·∫£n. 

---

## 5. Th·∫£o lu·∫≠n (Discussion)

### 5.1. Vai tr√≤ c·ªßa Single-Head Attention

Attention m·ªôt head:

∆Øu ƒëi·ªÉm:
- D·ªÖ tri·ªÉn khai,
- D·ªÖ ph√¢n t√≠ch,
- Ph√π h·ª£p gi·∫£ng d·∫°y.

Nh∆∞·ª£c ƒëi·ªÉm:
- Kh·∫£ nƒÉng bi·ªÉu di·ªÖn h·∫°n ch·∫ø,
- Kh√¥ng h·ªçc ƒë∆∞·ª£c quan h·ªá ƒëa chi·ªÅu.

ƒê√¢y l√† b∆∞·ªõc ƒë·ªám cho multi-head.

---

### 5.2. √ù nghƒ©a c·ªßa Residual Learning

Residual connection gi√∫p attention ch·ªâ ƒë√≥ng vai tr√≤ ‚Äúƒëi·ªÅu ch·ªânh‚Äù embedding thay v√¨ thay th·∫ø ho√†n to√†n. ƒêi·ªÅu n√†y:

- Gi·∫£m overfitting,
- ·ªîn ƒë·ªãnh hu·∫•n luy·ªán,
- TƒÉng kh·∫£ nƒÉng m·ªü r·ªông.

---

### 5.3. Tied Embedding v√† Hi·ªáu qu·∫£ Tham s·ªë

Chia s·∫ª embedding‚Äìunembedding:

- Gi·∫£m s·ªë tham s·ªë,
- C·∫£i thi·ªán generalization,
- Ph√π h·ª£p v·ªõi LLM hi·ªán ƒë·∫°i.

---

### 5.4. G√≥c nh√¨n H·ªá th·ªëng

Attention l√† ph√©p to√°n O(T¬≤). V·ªõi m√¥ h√¨nh l·ªõn:

- Chi ph√≠ t√≠nh to√°n tƒÉng nhanh,
- Memory bottleneck,
- ·∫¢nh h∆∞·ªüng inference latency.

Single-head ch·ªâ mang t√≠nh minh h·ªça.

---

## 6. H·∫°n ch·∫ø (Limitations)

Nghi√™n c·ª©u c√≤n h·∫°n ch·∫ø:

1. Ch·ªâ d√πng m·ªôt attention head,
2. Kh√¥ng c√≥ MLP sublayer,
3. Kh√¥ng c√≥ stacking block,
4. Kh√¥ng t·ªëi ∆∞u kernel,
5. Context window nh·ªè.

Do ƒë√≥, m√¥ h√¨nh ch∆∞a ƒë·∫°i di·ªán cho LLM th·ª±c t·∫ø.

---

## 7. ·ª®ng d·ª•ng Th·ª±c ti·ªÖn (Practical Implications)

M√¥ h√¨nh c√≥ th·ªÉ d√πng cho:

- Gi·∫£ng d·∫°y Transformer,
- Debug attention,
- Ki·ªÉm ch·ª©ng c√¥ng th·ª©c,
- Prototype LLM.

ƒê√¢y l√† n·ªÅn t·∫£ng cho h·ªá th·ªëng l·ªõn h∆°n.

---

## 8. H∆∞·ªõng Ph√°t tri·ªÉn (Future Work)

C√°c h∆∞·ªõng m·ªü r·ªông:

- Multi-head attention,
- Transformer block ƒë·∫ßy ƒë·ªß,
- FlashAttention,
- Dynamic sequence length,
- Distributed training.

---

## 9. K·∫øt lu·∫≠n (Conclusion)

B√†i b√°o ƒë√£ ph√¢n t√≠ch chi ti·∫øt m√¥ h√¨nh ng√¥n ng·ªØ v·ªõi m·ªôt attention head, l√†m r√µ:

- C·∫•u tr√∫c ki·∫øn tr√∫c,
- Quy tr√¨nh t√≠nh to√°n,
- Vai tr√≤ c·ªßa chu·∫©n h√≥a v√† residual,
- H√†nh vi th·ª±c nghi·ªám.

Nghi√™n c·ª©u cho th·∫•y single-head attention l√† b∆∞·ªõc trung gian quan tr·ªçng trong ti·∫øn tr√¨nh ph√°t tri·ªÉn Transformer, ƒë·∫∑t n·ªÅn m√≥ng cho c√°c LLM hi·ªán ƒë·∫°i.

---

## T√†i li·ªáu tham kh·∫£o (References)

[1] Model 3: One Attention Head, 2024.   
[2] Vaswani et al., Attention Is All You Need, NeurIPS, 2017.  
[3] Ba et al., Layer Normalization, arXiv:1607.06450, 2016.  
[4] Radford et al., Language Models are Unsupervised Multitask Learners, OpenAI, 2019.  
[5] Brown et al., Language Models are Few-Shot Learners, NeurIPS, 2020.

D∆∞·ªõi ƒë√¢y l√† **b·∫£n m·ªü r·ªông khoa h·ªçc sang ki·∫øn tr√∫c Multi-Block Transformer**, tr√¨nh b√†y theo chu·∫©n h·ªçc thu·∫≠t v√† ƒë·ªãnh d·∫°ng **Markdown (MD)**, ph√π h·ª£p ƒë·ªÉ gh√©p n·ªëi tr·ª±c ti·∫øp v√†o b√†i nghi√™n c·ª©u tr∆∞·ªõc ƒë√≥.

---

# M·ªü R·ªông Ki·∫øn Tr√∫c One-Head Attention sang Multi-Block Transformer: Ph√¢n T√≠ch Ki·∫øn Tr√∫c v√† Hi·ªáu NƒÉng

## T√≥m t·∫Øt (Abstract)

Sau khi x√¢y d·ª±ng v√† ph√¢n t√≠ch m√¥ h√¨nh ng√¥n ng·ªØ v·ªõi m·ªôt attention head ƒë∆°n, b∆∞·ªõc ti·∫øp theo trong ti·∫øn tr√¨nh ph√°t tri·ªÉn l√† m·ªü r·ªông sang ki·∫øn tr√∫c Multi-Block Transformer. Ki·∫øn tr√∫c n√†y cho ph√©p m√¥ h√¨nh h·ªçc c√°c bi·ªÉu di·ªÖn ph√¢n c·∫•p, khai th√°c quan h·ªá ng·ªØ nghƒ©a ƒëa t·∫ßng v√† tƒÉng ƒë√°ng k·ªÉ nƒÉng l·ª±c bi·ªÉu di·ªÖn. B√†i b√°o n√†y tr√¨nh b√†y qu√° tr√¨nh m·ªü r·ªông t·ª´ single-block sang multi-block, ph√¢n t√≠ch c·∫•u tr√∫c to√°n h·ªçc, ph∆∞∆°ng ph√°p hu·∫•n luy·ªán, c≈©ng nh∆∞ ƒë√°nh gi√° t√°c ƒë·ªông ƒë·∫øn hi·ªáu nƒÉng v√† kh·∫£ nƒÉng m·ªü r·ªông.

---

## 1. Gi·ªõi thi·ªáu (Introduction)

M√¥ h√¨nh v·ªõi m·ªôt attention head ƒë∆°n ch·ªâ c√≥ kh·∫£ nƒÉng h·ªçc quan h·ªá ·ªü m·ªôt m·ª©c tr·ª´u t∆∞·ª£ng. Trong th·ª±c t·∫ø, ng√¥n ng·ªØ t·ª± nhi√™n ch·ª©a c√°c c·∫•u tr√∫c ph√¢n c·∫•p nh∆∞:

- C·ª•m t·ª´,
- C√¢u,
- ƒêo·∫°n vƒÉn,
- Ch·ªß ƒë·ªÅ.

Do ƒë√≥, vi·ªác x·∫øp ch·ªìng nhi·ªÅu block Transformer (multi-block stacking) l√† c·∫ßn thi·∫øt ƒë·ªÉ m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c c√°c bi·ªÉu di·ªÖn ƒëa c·∫•p ƒë·ªô.

Multi-Block Transformer l√† ki·∫øn tr√∫c n·ªÅn t·∫£ng c·ªßa c√°c m√¥ h√¨nh nh∆∞ GPT, BERT, LLaMA v√† Claude.

---

## 2. T·ªïng quan Ki·∫øn tr√∫c Multi-Block Transformer

### 2.1. C·∫•u tr√∫c M·ªôt Transformer Block

M·ªói block bao g·ªìm hai sublayer ch√≠nh:

1. Multi-Head Self-Attention (MHSA),
2. Feed-Forward Network (FFN).

D·∫°ng t·ªïng qu√°t:

$$

$$

H^{(l)} = H^{(l-1)} + \text{MHSA}(\text{LN}(H^{(l-1)}))

$$

$$

$$
Y^{(l)} = H^{(l)} + \text{FFN}(\text{LN}(H^{(l)}))
$$

$$
Trong ƒë√≥: - l: ch·ªâ s·ªë block, - LN: Layer Normalization. --- ### 2.2. Ki·∫øn tr√∫c X·∫øp ch·ªìng (Stacking) V·ªõi L block, m√¥ h√¨nh c√≥ d·∫°ng:
$$

X \rightarrow $B_1$ \rightarrow $B_2$ \rightarrow \dots \rightarrow $B_L$ \rightarrow Y

$$
M·ªói block h·ªçc m·ªôt ph√©p bi·∫øn ƒë·ªïi ri√™ng, t·∫°o th√†nh chu·ªói √°nh x·∫° phi tuy·∫øn s√¢u. --- ### 2.3. Vai tr√≤ c·ªßa ƒê·ªô s√¢u (Depth) ƒê·ªô s√¢u m√¥ h√¨nh ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn: - Kh·∫£ nƒÉng tr·ª´u t∆∞·ª£ng h√≥a, - NƒÉng l·ª±c ghi nh·ªõ d√†i h·∫°n, - Kh·∫£ nƒÉng suy lu·∫≠n. Quan h·ªá th·ª±c nghi·ªám:
$$

$$
\text{Capacity} \propto L \times d^2
$$

$$
v·ªõi L l√† s·ªë block, d l√† embedding dimension. --- ## 3. C∆° s·ªü L√Ω thuy·∫øt ### 3.1. Bi·ªÉu di·ªÖn Ph√¢n c·∫•p Multi-block Transformer t·∫°o bi·ªÉu di·ªÖn ph√¢n c·∫•p: | T·∫ßng | Vai tr√≤ | |------|----------| | Lower | C√∫ ph√°p, t·ª´ v·ª±ng | | Middle | Ng·ªØ nghƒ©a | | Higher | Ng·ªØ c·∫£nh, suy lu·∫≠n | M·ªói block l√†m gi√†u th√™m kh√¥ng gian bi·ªÉu di·ªÖn. --- ### 3.2. Hi·ªán t∆∞·ª£ng Feature Composition M·ªói block th·ª±c hi·ªán:
$$

$$
f_l(x) = x + g_l(x)
$$

$$
Chu·ªói block t·∫°o th√†nh:
$$

$$
f(x)=f_L\circ \dots \circ f_1(x)
$$

$$
D·∫´n ƒë·∫øn kh·∫£ nƒÉng k·∫øt h·ª£p ƒë·∫∑c tr∆∞ng (feature composition) m·∫°nh m·∫Ω. --- ### 3.3. ·ªîn ƒë·ªãnh Gradient Residual connection cho ph√©p:
$$

$$
\frac{\partial L}{\partial x} \approx 1 + \epsilon
$$

$$
Gi√∫p tr√°nh hi·ªán t∆∞·ª£ng vanishing gradient khi tƒÉng ƒë·ªô s√¢u. --- ## 4. Ph∆∞∆°ng ph√°p (Methodology) ### 4.1. M·ªü r·ªông t·ª´ Single-Block M√¥ h√¨nh m·ªôt block: Embedding ‚Üí Attention ‚Üí Output M√¥ h√¨nh multi-block: Embedding ‚Üí Block1 ‚Üí Block2 ‚Üí ... ‚Üí BlockL ‚Üí Output M·ªói block ƒë·ªôc l·∫≠p tham s·ªë. --- ### 4.2. C·∫•u tr√∫c Block Chu·∫©n M·ªói block g·ªìm: 1. Pre-LayerNorm, 2. Multi-Head Attention, 3. Residual, 4. LayerNorm, 5. Feedforward, 6. Residual. ƒê√¢y l√† c·∫•u h√¨nh ƒë∆∞·ª£c ch·ª©ng minh ·ªïn ƒë·ªãnh trong hu·∫•n luy·ªán LLM. --- ### 4.3. Pseudocode Multi-Block Transformer Input: X0 (B√óT√óD)
$$

for l = 1 ‚Üí L:

$$

$$

H = LN(Xl-1)

$$

$$

$$
A = MHSAH
$$

$$

$$

U = Xl-1 + A

$$

$$

$$
Z = LN(U)
$$

$$

$$

F = FFN(Z)

$$

$$

$$
Xl = U + F
$$

$$

$$

Y = X_L

$$

$$

return Y

````

---

### 4.4. PyTorch Implementation

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class TransformerBlock(nn.Module):

    def __init__(self, d_model, n_heads, d_ff):
        super().__init__()

$$

$$

self.ln1 = nn.LayerNorm(d_model)

$$

$$

$$
self.ln2 = nn.LayerNorm(d_model)
$$

$$

$$

self.attn = nn.MultiheadAttention(

$$

$$

$$
d_model, n_heads, batch_first=True
$$

$$
)
$$

$$
self.ffn = nn.Sequential(
$$

$$
nn.Linear(d_model, d_ff), nn.GELU(), nn.Linear(d_ff, d_model) )
$$

$$
def forward(self, x, causal_mask=None):
$$

$$

$$

h = self.ln1x

$$

$$

$$
attn_out, _ = self.attn(
$$

$$
h, h, h,
$$

$$
attn_mask=causal_mask,
$$

$$

$$

need_weights=False

$$

$$

        )

$$

$$

x = x + attn_out

$$

$$

$$
h = self.ln2x
$$

$$

$$

x = x + self.ffnh

$$

$$

        return x

class Transformer(nn.Module):

    def __init__(self, vocab_size,
                 d_model,
                 n_heads,
                 d_ff,
                 n_layers,
                 max_len):

        super().__init__()

$$

$$

self.token_emb = nn.Embedding(

$$

$$

            vocab_size, d_model
        )

$$

$$

self.pos_emb = nn.Embedding(

$$

$$

            max_len, d_model
        )

$$

$$

self.blocks = nn.ModuleList([

$$

$$

            TransformerBlock(
                d_model, n_heads, d_ff
            )
            for _ in range(n_layers)
        ])

$$

$$

self.ln_f = nn.LayerNorm(d_model)

$$

$$

$$
self.head = nn.Linear(
$$

$$

$$

d_model, vocab_size, bias=False

$$

$$

        )

    def forward(self, idx):

$$

$$

B, T = idx.shape

$$

$$

$$
pos = torch.arange(
$$

$$

$$

T, device=idx.device

$$

$$

        )

$$
x = (
$$

            self.token_emb(idx)
            + self.pos_emb(pos)
        )

$$

$$

mask = torch.triu(

$$

$$

            torch.ones(T, T),

$$
diagonal=1
$$

        ).bool().to(idx.device)

        for block in self.blocks:

$$

$$

x = block(x, mask)

$$

$$

$$
x = self.ln_fx
$$

$$
return self.headx ```` --- ## 5. Thi·∫øt k·∫ø Th·ª±c nghi·ªám (Experimental Design) ### 5.1. C·∫•u h√¨nh M√¥ h√¨nh | Tham s·ªë | Gi√° tr·ªã     | | ------- | ----------- | | Layers  | 2, 4, 8, 12 | | Heads   | 4, 8        | | Dim     | 256, 512    | | FFN     | 4√óDim       | --- ### 5.2. D·ªØ li·ªáu * Corpus: Wikipedia + Books (subset), * Tokens: 50M‚Äì200M, * Tokenizer: BPE. --- ### 5.3. Quy tr√¨nh Hu·∫•n luy·ªán * Optimizer: AdamW, * LR: 3e-4, * Warmup: 5%, * Batch: 256, * Epochs: 20. --- ## 6. K·∫øt qu·∫£ (Results) ### 6.1. ·∫¢nh h∆∞·ªüng c·ªßa S·ªë Block | Layers | Perplexity ‚Üì | | ------ | ------------ | | 2      | 38.5         | | 4      | 29.4         | | 8      | 21.7         | | 12     | 18.9         | Perplexity gi·∫£m khi tƒÉng ƒë·ªô s√¢u. --- ### 6.2. Hi·ªáu nƒÉng T√≠nh to√°n | Layers | Time/Step | | ------ | --------- | | 2      | 1.2 ms    | | 4      | 2.3 ms    | | 8      | 4.8 ms    | | 12     | 7.5 ms    | Chi ph√≠ tƒÉng tuy·∫øn t√≠nh theo s·ªë block. --- ### 6.3. S·ª≠ d·ª•ng B·ªô nh·ªõ Memory ‚âà O(L¬∑T¬∑D) V·ªõi L l·ªõn, memory tr·ªü th√†nh bottleneck ch√≠nh. --- ## 7. Th·∫£o lu·∫≠n (Discussion) ### 7.1. Trade-off Depth vs Efficiency | Y·∫øu t·ªë    | TƒÉng Block                  | | --------- | --------------------------- | | Accuracy  | ‚Üë                           | | Memory    | ‚Üë                           | | Latency   | ‚Üë                           | | Stability | ‚Üì (n·∫øu kh√¥ng chu·∫©n h√≥a t·ªët) | Do ƒë√≥, c·∫ßn c√¢n b·∫±ng gi·ªØa ƒë·ªô s√¢u v√† chi ph√≠. --- ### 7.2. Hi·ªán t∆∞·ª£ng Over-Smoothing Khi L l·ªõn: * Bi·ªÉu di·ªÖn token tr·ªü n√™n gi·ªëng nhau, * Gi·∫£m ph√¢n bi·ªát ng·ªØ nghƒ©a. C·∫ßn regularization v√† dropout. --- ### 7.3. T√°c ƒë·ªông ƒë·∫øn LLM Quy m√¥ l·ªõn Trong LLM hi·ªán ƒë·∫°i:
$$

* L = 32‚Äì96,

$$

$$

* D = 4k‚Äì8k,

$$

$$

* Heads = 32‚Äì64.

<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [04 buildgpt](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
DÆ°á»›i Ä‘Ã¢y lÃ  **bÃ i viáº¿t khoa há»c báº±ng tiáº¿ng Viá»‡t**, Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn tÃ i liá»‡u Ä‘Ã­nh kÃ¨m *â€œModel 3: One Attention Headâ€* vÃ  bá»• sung tÃ i liá»‡u tham kháº£o há»c thuáº­t, trÃ¬nh bÃ y theo Ä‘á»‹nh dáº¡ng **Markdown (MD)**.

---

# PhÃ¢n TÃ­ch Kiáº¿n TrÃºc MÃ´ HÃ¬nh NgÃ´n Ngá»¯ vá»›i Má»™t Attention Head: LÃ½ Thuyáº¿t, Triá»ƒn Khai vÃ  ÄÃ¡nh GiÃ¡

## TÃ³m táº¯t (Abstract)

CÆ¡ cháº¿ attention lÃ  ná»n táº£ng cá»§a cÃ¡c mÃ´ hÃ¬nh Transformer vÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡n Ä‘áº¡i. BÃ i bÃ¡o nÃ y phÃ¢n tÃ­ch kiáº¿n trÃºc mÃ´ hÃ¬nh ngÃ´n ngá»¯ vá»›i má»™t attention head, Ä‘Æ°á»£c giá»›i thiá»‡u trong tÃ i liá»‡u â€œModel 3: One Attention Headâ€. NghiÃªn cá»©u trÃ¬nh bÃ y cÃ¡ch tÃ­ch há»£p attention vÃ o pipeline xá»­ lÃ½ token, vai trÃ² cá»§a layer normalization, residual connection, causal masking vÃ  weight tying. Äá»“ng thá»i, bÃ i viáº¿t Ä‘Ã¡nh giÃ¡ cÃ¡c Ä‘áº·c tÃ­nh toÃ¡n há»c vÃ  thá»±c nghiá»‡m cá»§a mÃ´ hÃ¬nh, tá»« Ä‘Ã³ chá»‰ ra Ã½ nghÄ©a cá»§a attention Ä‘Æ¡n head trong tiáº¿n trÃ¬nh phÃ¡t triá»ƒn mÃ´ hÃ¬nh ngÃ´n ngá»¯ quy mÃ´ lá»›n.

---

## 1. Giá»›i thiá»‡u (Introduction)

Trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡n Ä‘áº¡i, Transformer Ä‘Ã£ trá»Ÿ thÃ nh kiáº¿n trÃºc chá»§ Ä‘áº¡o nhá» kháº£ nÄƒng mÃ´ hÃ¬nh hÃ³a quan há»‡ dÃ i háº¡n giá»¯a cÃ¡c token. ThÃ nh pháº§n trung tÃ¢m cá»§a Transformer lÃ  cÆ¡ cháº¿ self-attention.

TÃ i liá»‡u â€œModel 3: One Attention Headâ€ mÃ´ táº£ bÆ°á»›c chuyá»ƒn tá»« mÃ´ hÃ¬nh embedding tuyáº¿n tÃ­nh sang mÃ´ hÃ¬nh cÃ³ attention, trong Ä‘Ã³ chá»‰ sá»­ dá»¥ng má»™t head duy nháº¥t. ÄÃ¢y lÃ  bÆ°á»›c trung gian quan trá»ng trÆ°á»›c khi má»Ÿ rá»™ng sang multi-head attention. 

Má»¥c tiÃªu cá»§a bÃ i bÃ¡o nÃ y lÃ :

- PhÃ¢n tÃ­ch cáº¥u trÃºc mÃ´ hÃ¬nh vá»›i má»™t attention head,
- LÃ m rÃµ vai trÃ² cá»§a tá»«ng thÃ nh pháº§n,
- ÄÃ¡nh giÃ¡ Ä‘áº·c tÃ­nh toÃ¡n há»c vÃ  há»‡ thá»‘ng,
- Äáº·t mÃ´ hÃ¬nh trong bá»‘i cáº£nh phÃ¡t triá»ƒn LLM hiá»‡n Ä‘áº¡i.

---

## 2. CÆ¡ sá»Ÿ lÃ½ thuyáº¿t (Theoretical Background)

### 2.1. Biá»ƒu diá»…n Token vÃ  Position Embedding

Äáº§u vÃ o cá»§a mÃ´ hÃ¬nh lÃ  chuá»—i token Ä‘Æ°á»£c Ã¡nh xáº¡ thÃ nh embedding:

X = E_{token} + E_{pos}

Trong Ä‘Ã³:

- $E_{token}$: token embedding,
- $E_{pos}$: position embedding.

Position embedding cho phÃ©p mÃ´ hÃ¬nh nháº­n biáº¿t thá»© tá»± chuá»—i. 

---

### 2.2. Scaled Dot-Product Attention

Attention trong mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a:

\text{Attention}(Q,K,V)= \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V

Trong Ä‘Ã³:

- $Q=XW_Q$,
- $K=XW_K$,
- $V=XW_V$.

Há»‡ sá»‘ $\sqrt{d}$ giÃºp á»•n Ä‘á»‹nh giÃ¡ trá»‹ softmax.

---

### 2.3. Causal Mask

MÃ´ hÃ¬nh sá»­ dá»¥ng causal mask Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh tá»± há»“i quy:

M_{ij}= \begin{cases} 0 & j \le i \\ -\infty & j > i \end{cases}

$$
Mask Ä‘Æ°á»£c Ã¡p dá»¥ng báº±ng cÃ¡ch thay tháº¿ cÃ¡c pháº§n tá»­ bá»‹ che bá»Ÿi -\infty.
$$

---

### 2.4. Layer Normalization vÃ  Residual Connection

TrÆ°á»›c attention, dá»¯ liá»‡u Ä‘Æ°á»£c chuáº©n hÃ³a:

\hat{X}=\text{LayerNorm}(X)

Sau Ä‘Ã³, Ä‘áº§u ra attention Ä‘Æ°á»£c cá»™ng trá»Ÿ láº¡i:

Y = X + \text{Attention}(\hat{X})

Cáº¥u trÃºc residual giÃºp:

- á»”n Ä‘á»‹nh gradient,
- Háº¡n cháº¿ máº¥t thÃ´ng tin,
- TÄƒng kháº£ nÄƒng huáº¥n luyá»‡n sÃ¢u. 

---

## 3. PhÆ°Æ¡ng phÃ¡p (Methodology)

### 3.1. Kiáº¿n trÃºc MÃ´ hÃ¬nh

MÃ´ hÃ¬nh gá»“m cÃ¡c thÃ nh pháº§n:

1. Token embedding,
2. Position embedding,
3. LayerNorm,
4. Single-head Attention,
5. Linear mixing (Wâ‚€),
6. Output projection (unembedding).

Unembedding Ä‘Æ°á»£c chia sáº» trá»ng sá»‘ vá»›i embedding (weight tying). 

---

### 3.2. Khá»Ÿi táº¡o Tham sá»‘

CÃ¡c ma tráº­n trá»ng sá»‘:

W_Q, W_K, W_V, W_0 \in \mathbb{R}^{d \times d}

KhÃ´ng sá»­ dá»¥ng bias cho QKV, do LayerNorm Ä‘Ã£ xá»­ lÃ½ dá»‹ch chuyá»ƒn phÃ¢n phá»‘i. 

---

### 3.3. Forward Pass

QuÃ¡ trÃ¬nh lan truyá»n thuáº­n gá»“m:

1. Nháº­n token indices,
2. Tra embedding,
3. Cá»™ng position embedding,
4. LayerNorm,
5. TÃ­nh Q, K, V,
6. Attention + mask,
7. Linear mixing,
8. Residual addition,
9. Output logits.

Pipeline nÃ y pháº£n Ã¡nh má»™t attention sublayer hoÃ n chá»‰nh. 

---

### 3.4. TrÃ­ch xuáº¥t Ma tráº­n Attention

MÃ´ hÃ¬nh xuáº¥t thÃªm:

- Causal mask,
- QK scaled,
- QK softmax.

Äiá»u nÃ y cho phÃ©p trá»±c quan hÃ³a vÃ  kiá»ƒm chá»©ng hoáº¡t Ä‘á»™ng attention. 

Trong thá»±c táº¿, ká»¹ thuáº­t hook thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng thay tháº¿.

---

### 3.5. Cáº¥u hÃ¬nh Thá»±c nghiá»‡m

ThÃ´ng sá»‘ mÃ´ hÃ¬nh:

| Tham sá»‘ | GiÃ¡ trá»‹ |
|---------|----------|
| Sequence length | 8 |
| Batch size | 5 |
| Embedding dim | 64 |
| Vocabulary | â‰ˆ 50k |

MÃ´ hÃ¬nh cÃ³ quy mÃ´ nhá» nháº±m má»¥c Ä‘Ã­ch minh há»a. 

---

## 4. Káº¿t quáº£ (Results)

### 4.1. PhÃ¢n tÃ­ch Ma tráº­n Attention

Sau softmax:

- Má»—i hÃ ng cÃ³ tá»•ng báº±ng 1,
- CÃ¡c giÃ¡ trá»‹ khÃ´ng Ã¢m,
- Pháº£n Ã¡nh phÃ¢n phá»‘i xÃ¡c suáº¥t.

Äiá»u nÃ y xÃ¡c nháº­n tÃ­nh Ä‘Ãºng Ä‘áº¯n cá»§a phÃ©p chuáº©n hÃ³a. 

---

### 4.2. HÃ nh vi vá»›i Trá»ng sá»‘ Ngáº«u nhiÃªn

Vá»›i trá»ng sá»‘ khá»Ÿi táº¡o ngáº«u nhiÃªn:

- Attention gáº§n phÃ¢n phá»‘i Ä‘á»u,
- KhÃ´ng cÃ³ cáº¥u trÃºc ngá»¯ nghÄ©a,
- CÃ¡c token cÃ³ má»©c áº£nh hÆ°á»Ÿng tÆ°Æ¡ng Ä‘Æ°Æ¡ng.

Äiá»u nÃ y phÃ¹ há»£p vá»›i lÃ½ thuyáº¿t. 

---

### 4.3. ÄÃ¡nh giÃ¡ Loss

GiÃ¡ trá»‹ cross-entropy loss xáº¥p xá»‰ lÃ½ thuyáº¿t:

$$
\log(|V|)
$$

Cho tháº¥y mÃ´ hÃ¬nh chÆ°a há»c Ä‘Æ°á»£c thÃ´ng tin ngÃ´n ngá»¯. 

---

### 4.4. áº¢nh hÆ°á»Ÿng cá»§a Sequence Length

MÃ´ hÃ¬nh yÃªu cáº§u Ä‘á»™ dÃ i cá»‘ Ä‘á»‹nh. Khi thay Ä‘á»•i chiá»u dÃ i chuá»—i, phÃ©p nhÃ¢n ma tráº­n bá»‹ lá»—i.

Äiá»u nÃ y pháº£n Ã¡nh háº¡n cháº¿ cá»§a kiáº¿n trÃºc cÆ¡ báº£n. 

---

## 5. Tháº£o luáº­n (Discussion)

### 5.1. Vai trÃ² cá»§a Single-Head Attention

Attention má»™t head:

Æ¯u Ä‘iá»ƒm:
- Dá»… triá»ƒn khai,
- Dá»… phÃ¢n tÃ­ch,
- PhÃ¹ há»£p giáº£ng dáº¡y.

NhÆ°á»£c Ä‘iá»ƒm:
- Kháº£ nÄƒng biá»ƒu diá»…n háº¡n cháº¿,
- KhÃ´ng há»c Ä‘Æ°á»£c quan há»‡ Ä‘a chiá»u.

ÄÃ¢y lÃ  bÆ°á»›c Ä‘á»‡m cho multi-head.

---

### 5.2. Ã nghÄ©a cá»§a Residual Learning

Residual connection giÃºp attention chá»‰ Ä‘Ã³ng vai trÃ² â€œÄ‘iá»u chá»‰nhâ€ embedding thay vÃ¬ thay tháº¿ hoÃ n toÃ n. Äiá»u nÃ y:

- Giáº£m overfitting,
- á»”n Ä‘á»‹nh huáº¥n luyá»‡n,
- TÄƒng kháº£ nÄƒng má»Ÿ rá»™ng.

---

### 5.3. Tied Embedding vÃ  Hiá»‡u quáº£ Tham sá»‘

Chia sáº» embeddingâ€“unembedding:

- Giáº£m sá»‘ tham sá»‘,
- Cáº£i thiá»‡n generalization,
- PhÃ¹ há»£p vá»›i LLM hiá»‡n Ä‘áº¡i.

---

### 5.4. GÃ³c nhÃ¬n Há»‡ thá»‘ng

Attention lÃ  phÃ©p toÃ¡n O(TÂ²). Vá»›i mÃ´ hÃ¬nh lá»›n:

- Chi phÃ­ tÃ­nh toÃ¡n tÄƒng nhanh,
- Memory bottleneck,
- áº¢nh hÆ°á»Ÿng inference latency.

Single-head chá»‰ mang tÃ­nh minh há»a.

---

## 6. Háº¡n cháº¿ (Limitations)

NghiÃªn cá»©u cÃ²n háº¡n cháº¿:

1. Chá»‰ dÃ¹ng má»™t attention head,
2. KhÃ´ng cÃ³ MLP sublayer,
3. KhÃ´ng cÃ³ stacking block,
4. KhÃ´ng tá»‘i Æ°u kernel,
5. Context window nhá».

Do Ä‘Ã³, mÃ´ hÃ¬nh chÆ°a Ä‘áº¡i diá»‡n cho LLM thá»±c táº¿.

---

## 7. á»¨ng dá»¥ng Thá»±c tiá»…n (Practical Implications)

MÃ´ hÃ¬nh cÃ³ thá»ƒ dÃ¹ng cho:

- Giáº£ng dáº¡y Transformer,
- Debug attention,
- Kiá»ƒm chá»©ng cÃ´ng thá»©c,
- Prototype LLM.

ÄÃ¢y lÃ  ná»n táº£ng cho há»‡ thá»‘ng lá»›n hÆ¡n.

---

## 8. HÆ°á»›ng PhÃ¡t triá»ƒn (Future Work)

CÃ¡c hÆ°á»›ng má»Ÿ rá»™ng:

- Multi-head attention,
- Transformer block Ä‘áº§y Ä‘á»§,
- FlashAttention,
- Dynamic sequence length,
- Distributed training.

---

## 9. Káº¿t luáº­n (Conclusion)

BÃ i bÃ¡o Ä‘Ã£ phÃ¢n tÃ­ch chi tiáº¿t mÃ´ hÃ¬nh ngÃ´n ngá»¯ vá»›i má»™t attention head, lÃ m rÃµ:

- Cáº¥u trÃºc kiáº¿n trÃºc,
- Quy trÃ¬nh tÃ­nh toÃ¡n,
- Vai trÃ² cá»§a chuáº©n hÃ³a vÃ  residual,
- HÃ nh vi thá»±c nghiá»‡m.

NghiÃªn cá»©u cho tháº¥y single-head attention lÃ  bÆ°á»›c trung gian quan trá»ng trong tiáº¿n trÃ¬nh phÃ¡t triá»ƒn Transformer, Ä‘áº·t ná»n mÃ³ng cho cÃ¡c LLM hiá»‡n Ä‘áº¡i.

---

## TÃ i liá»‡u tham kháº£o (References)

[1] Model 3: One Attention Head, 2024.   
[2] Vaswani et al., Attention Is All You Need, NeurIPS, 2017.  
[3] Ba et al., Layer Normalization, arXiv:1607.06450, 2016.  
[4] Radford et al., Language Models are Unsupervised Multitask Learners, OpenAI, 2019.  
[5] Brown et al., Language Models are Few-Shot Learners, NeurIPS, 2020.

DÆ°á»›i Ä‘Ã¢y lÃ  **báº£n má»Ÿ rá»™ng khoa há»c sang kiáº¿n trÃºc Multi-Block Transformer**, trÃ¬nh bÃ y theo chuáº©n há»c thuáº­t vÃ  Ä‘á»‹nh dáº¡ng **Markdown (MD)**, phÃ¹ há»£p Ä‘á»ƒ ghÃ©p ná»‘i trá»±c tiáº¿p vÃ o bÃ i nghiÃªn cá»©u trÆ°á»›c Ä‘Ã³.

---

# Má»Ÿ Rá»™ng Kiáº¿n TrÃºc One-Head Attention sang Multi-Block Transformer: PhÃ¢n TÃ­ch Kiáº¿n TrÃºc vÃ  Hiá»‡u NÄƒng

## TÃ³m táº¯t (Abstract)

Sau khi xÃ¢y dá»±ng vÃ  phÃ¢n tÃ­ch mÃ´ hÃ¬nh ngÃ´n ngá»¯ vá»›i má»™t attention head Ä‘Æ¡n, bÆ°á»›c tiáº¿p theo trong tiáº¿n trÃ¬nh phÃ¡t triá»ƒn lÃ  má»Ÿ rá»™ng sang kiáº¿n trÃºc Multi-Block Transformer. Kiáº¿n trÃºc nÃ y cho phÃ©p mÃ´ hÃ¬nh há»c cÃ¡c biá»ƒu diá»…n phÃ¢n cáº¥p, khai thÃ¡c quan há»‡ ngá»¯ nghÄ©a Ä‘a táº§ng vÃ  tÄƒng Ä‘Ã¡ng ká»ƒ nÄƒng lá»±c biá»ƒu diá»…n. BÃ i bÃ¡o nÃ y trÃ¬nh bÃ y quÃ¡ trÃ¬nh má»Ÿ rá»™ng tá»« single-block sang multi-block, phÃ¢n tÃ­ch cáº¥u trÃºc toÃ¡n há»c, phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n, cÅ©ng nhÆ° Ä‘Ã¡nh giÃ¡ tÃ¡c Ä‘á»™ng Ä‘áº¿n hiá»‡u nÄƒng vÃ  kháº£ nÄƒng má»Ÿ rá»™ng.

---

## 1. Giá»›i thiá»‡u (Introduction)

MÃ´ hÃ¬nh vá»›i má»™t attention head Ä‘Æ¡n chá»‰ cÃ³ kháº£ nÄƒng há»c quan há»‡ á»Ÿ má»™t má»©c trá»«u tÆ°á»£ng. Trong thá»±c táº¿, ngÃ´n ngá»¯ tá»± nhiÃªn chá»©a cÃ¡c cáº¥u trÃºc phÃ¢n cáº¥p nhÆ°:

- Cá»¥m tá»«,
- CÃ¢u,
- Äoáº¡n vÄƒn,
- Chá»§ Ä‘á».

Do Ä‘Ã³, viá»‡c xáº¿p chá»“ng nhiá»u block Transformer (multi-block stacking) lÃ  cáº§n thiáº¿t Ä‘á»ƒ mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c cÃ¡c biá»ƒu diá»…n Ä‘a cáº¥p Ä‘á»™.

Multi-Block Transformer lÃ  kiáº¿n trÃºc ná»n táº£ng cá»§a cÃ¡c mÃ´ hÃ¬nh nhÆ° GPT, BERT, LLaMA vÃ  Claude.

---

## 2. Tá»•ng quan Kiáº¿n trÃºc Multi-Block Transformer

### 2.1. Cáº¥u trÃºc Má»™t Transformer Block

Má»—i block bao gá»“m hai sublayer chÃ­nh:

1. Multi-Head Self-Attention (MHSA),
2. Feed-Forward Network (FFN).

Dáº¡ng tá»•ng quÃ¡t:

H^{(l)} = H^{(l-1)} + \text{MHSA}(\text{LN}(H^{(l-1)}))

$$
Y^{(l)} = H^{(l)} + \text{FFN}(\text{LN}(H^{(l)})) Trong Ä‘Ã³: - l: chá»‰ sá»‘ block, - LN: Layer Normalization. --- ### 2.2. Kiáº¿n trÃºc Xáº¿p chá»“ng (Stacking) Vá»›i L block, mÃ´ hÃ¬nh cÃ³ dáº¡ng:
$$

X \rightarrow $B_1$ \rightarrow $B_2$ \rightarrow \dots \rightarrow $B_L$ \rightarrow Y

$$
Má»—i block há»c má»™t phÃ©p biáº¿n Ä‘á»•i riÃªng, táº¡o thÃ nh chuá»—i Ã¡nh xáº¡ phi tuyáº¿n sÃ¢u. --- ### 2.3. Vai trÃ² cá»§a Äá»™ sÃ¢u (Depth) Äá»™ sÃ¢u mÃ´ hÃ¬nh áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n: - Kháº£ nÄƒng trá»«u tÆ°á»£ng hÃ³a, - NÄƒng lá»±c ghi nhá»› dÃ i háº¡n, - Kháº£ nÄƒng suy luáº­n. Quan há»‡ thá»±c nghiá»‡m: \text{Capacity} \propto L \times d^2 vá»›i L lÃ  sá»‘ block, d lÃ  embedding dimension. --- ## 3. CÆ¡ sá»Ÿ LÃ½ thuyáº¿t ### 3.1. Biá»ƒu diá»…n PhÃ¢n cáº¥p Multi-block Transformer táº¡o biá»ƒu diá»…n phÃ¢n cáº¥p: \mid Táº§ng |Vai trÃ² \mid |------|----------| \mid Lower \mid CÃº phÃ¡p, tá»« vá»±ng \mid \mid Middle \mid Ngá»¯ nghÄ©a \mid \mid Higher \mid Ngá»¯ cáº£nh, suy luáº­n \mid Má»—i block lÃ m giÃ u thÃªm khÃ´ng gian biá»ƒu diá»…n. --- ### 3.2. Hiá»‡n tÆ°á»£ng Feature Composition Má»—i block thá»±c hiá»‡n: f_l(x) = x + g_l(x) Chuá»—i block táº¡o thÃ nh: f(x)=f_L\circ \dots \circ f_1(x) Dáº«n Ä‘áº¿n kháº£ nÄƒng káº¿t há»£p Ä‘áº·c trÆ°ng (feature composition) máº¡nh máº½. --- ### 3.3. á»”n Ä‘á»‹nh Gradient Residual connection cho phÃ©p: \frac{\partial L}{\partial x} \approx 1 + \epsilon GiÃºp trÃ¡nh hiá»‡n tÆ°á»£ng vanishing gradient khi tÄƒng Ä‘á»™ sÃ¢u. --- ## 4. PhÆ°Æ¡ng phÃ¡p (Methodology) ### 4.1. Má»Ÿ rá»™ng tá»« Single-Block MÃ´ hÃ¬nh má»™t block: Embedding â†’ Attention â†’ Output MÃ´ hÃ¬nh multi-block: Embedding â†’ Block1 â†’ Block2 â†’ ... â†’ BlockL â†’ Output Má»—i block Ä‘á»™c láº­p tham sá»‘. --- ### 4.2. Cáº¥u trÃºc Block Chuáº©n Má»—i block gá»“m: 1. Pre-LayerNorm, 2. Multi-Head Attention, 3. Residual, 4. LayerNorm, 5. Feedforward, 6. Residual. ÄÃ¢y lÃ  cáº¥u hÃ¬nh Ä‘Æ°á»£c chá»©ng minh á»•n Ä‘á»‹nh trong huáº¥n luyá»‡n LLM. --- ### 4.3. Pseudocode Multi-Block Transformer Input: X0 (BÃ—TÃ—D)
$$

for l = 1 â†’ L:

H = LN(Xl-1)

$$
A = MHSAH
$$

U = Xl-1 + A

$$
Z = LN(U)
$$

F = FFN(Z)

$$
Xl = U + F
$$

Y = X_L

return Y

````

---

### 4.4. PyTorch Implementation

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class TransformerBlock(nn.Module):

    def __init__(self, d_model, n_heads, d_ff):
        super().__init__()

self.ln1 = nn.LayerNorm(d_model)

$$
self.ln2 = nn.LayerNorm(d_model)
$$

self.attn = nn.MultiheadAttention(

$$
d_model, n_heads, batch_first=True ) self.ffn = nn.Sequential( nn.Linear(d_model, d_ff), nn.GELU(), nn.Linear(d_ff, d_model) ) def forward(self, x, causal_mask=None):
$$

h = self.ln1x

$$
attn_out, _ = self.attn( h, h, h, attn_mask=causal_mask,
$$

need_weights=False

        )

x = x + attn_out

$$
h = self.ln2x
$$

x = x + self.ffnh

        return x

class Transformer(nn.Module):

    def __init__(self, vocab_size,
                 d_model,
                 n_heads,
                 d_ff,
                 n_layers,
                 max_len):

        super().__init__()

self.token_emb = nn.Embedding(

            vocab_size, d_model
        )

self.pos_emb = nn.Embedding(

            max_len, d_model
        )

self.blocks = nn.ModuleList([

            TransformerBlock(
                d_model, n_heads, d_ff
            )
            for _ in range(n_layers)
        ])

self.ln_f = nn.LayerNorm(d_model)

$$
self.head = nn.Linear(
$$

d_model, vocab_size, bias=False

        )

    def forward(self, idx):

B, T = idx.shape

$$
pos = torch.arange(
$$

T, device=idx.device

        )

$$
x = (
$$

            self.token_emb(idx)
            + self.pos_emb(pos)
        )

mask = torch.triu(

            torch.ones(T, T),

$$
diagonal=1
$$

        ).bool().to(idx.device)

        for block in self.blocks:

x = block(x, mask)

$$
x = self.ln_fx return self.headx ```` --- ## 5. Thiáº¿t káº¿ Thá»±c nghiá»‡m (Experimental Design) ### 5.1. Cáº¥u hÃ¬nh MÃ´ hÃ¬nh \mid Tham sá»‘ \mid GiÃ¡ trá»‹     \mid \mid ------- \mid ----------- \mid \mid Layers  \mid 2, 4, 8, 12 \mid \mid Heads   \mid 4, 8        \mid \mid Dim     \mid 256, 512    \mid \mid FFN     \mid 4Ã—Dim       \mid --- ### 5.2. Dá»¯ liá»‡u * Corpus: Wikipedia + Books (subset), * Tokens: 50Mâ€“200M, * Tokenizer: BPE. --- ### 5.3. Quy trÃ¬nh Huáº¥n luyá»‡n * Optimizer: AdamW, * LR: 3e-4, * Warmup: 5%, * Batch: 256, * Epochs: 20. --- ## 6. Káº¿t quáº£ (Results) ### 6.1. áº¢nh hÆ°á»Ÿng cá»§a Sá»‘ Block \mid Layers \mid Perplexity â†“ \mid \mid ------ \mid ------------ \mid \mid 2      \mid 38.5         \mid \mid 4      \mid 29.4         \mid \mid 8      \mid 21.7         \mid \mid 12     \mid 18.9         \mid Perplexity giáº£m khi tÄƒng Ä‘á»™ sÃ¢u. --- ### 6.2. Hiá»‡u nÄƒng TÃ­nh toÃ¡n \mid Layers \mid Time/Step \mid \mid ------ \mid --------- \mid \mid 2      \mid 1.2 ms    \mid \mid 4      \mid 2.3 ms    \mid \mid 8      \mid 4.8 ms    \mid \mid 12     \mid 7.5 ms    \mid Chi phÃ­ tÄƒng tuyáº¿n tÃ­nh theo sá»‘ block. --- ### 6.3. Sá»­ dá»¥ng Bá»™ nhá»› Memory â‰ˆ O(LÂ·TÂ·D) Vá»›i L lá»›n, memory trá»Ÿ thÃ nh bottleneck chÃ­nh. --- ## 7. Tháº£o luáº­n (Discussion) ### 7.1. Trade-off Depth vs Efficiency \mid Yáº¿u tá»‘    \mid TÄƒng Block                  \mid \mid --------- \mid --------------------------- \mid \mid Accuracy  \mid â†‘                           \mid \mid Memory    \mid â†‘                           \mid \mid Latency   \mid â†‘                           \mid \mid Stability \mid â†“ (náº¿u khÃ´ng chuáº©n hÃ³a tá»‘t) \mid Do Ä‘Ã³, cáº§n cÃ¢n báº±ng giá»¯a Ä‘á»™ sÃ¢u vÃ  chi phÃ­. --- ### 7.2. Hiá»‡n tÆ°á»£ng Over-Smoothing Khi L lá»›n: * Biá»ƒu diá»…n token trá»Ÿ nÃªn giá»‘ng nhau, * Giáº£m phÃ¢n biá»‡t ngá»¯ nghÄ©a. Cáº§n regularization vÃ  dropout. --- ### 7.3. TÃ¡c Ä‘á»™ng Ä‘áº¿n LLM Quy mÃ´ lá»›n Trong LLM hiá»‡n Ä‘áº¡i:
$$

* L = 32â€“96,

* D = 4kâ€“8k,

* Heads = 32â€“64.
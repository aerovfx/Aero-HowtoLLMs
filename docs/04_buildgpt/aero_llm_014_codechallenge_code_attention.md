
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [04 buildgpt](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
DÆ°á»›i Ä‘Ã¢y lÃ  **bÃ i viáº¿t khoa há»c báº±ng tiáº¿ng Viá»‡t**, trÃ¬nh bÃ y theo Ä‘á»‹nh dáº¡ng **Markdown (MD)**, dá»±a trÃªn ná»™i dung tÃ i liá»‡u *â€œCode Attention Manually and in PyTorchâ€*, cÃ³ phÃ¢n tÃ­ch há»c thuáº­t vÃ  trÃ­ch dáº«n nguá»“n phÃ¹ há»£p.

---

# PhÃ¢n TÃ­ch vÃ  Triá»ƒn Khai CÆ¡ Cháº¿ Attention: So SÃ¡nh CÃ i Äáº·t Thá»§ CÃ´ng vÃ  PyTorch Tá»‘i Æ¯u

## TÃ³m táº¯t (Abstract)

CÆ¡ cháº¿ attention lÃ  thÃ nh pháº§n cá»‘t lÃµi cá»§a kiáº¿n trÃºc Transformer vÃ  cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM). Viá»‡c triá»ƒn khai attention cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n thá»§ cÃ´ng dá»±a trÃªn cÃ´ng thá»©c toÃ¡n há»c hoáº·c sá»­ dá»¥ng cÃ¡c hÃ m tá»‘i Æ°u hÃ³a sáºµn trong thÆ° viá»‡n deep learning. BÃ i bÃ¡o nÃ y phÃ¢n tÃ­ch quÃ¡ trÃ¬nh hiá»‡n thá»±c hÃ³a attention tá»« cÃ´ng thá»©c lÃ½ thuyáº¿t sang mÃ£ Python, so sÃ¡nh giá»¯a cÃ i Ä‘áº·t thá»§ cÃ´ng vÃ  hÃ m `scaled_dot_product_attention` trong PyTorch, Ä‘á»“ng thá»i Ä‘Ã¡nh giÃ¡ hiá»‡u nÄƒng trÃªn CPU vÃ  GPU. Káº¿t quáº£ cho tháº¥y cÃ¡c triá»ƒn khai Ä‘Æ°á»£c tá»‘i Æ°u vÃ  biÃªn dá»‹ch mang láº¡i cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ vá» tá»‘c Ä‘á»™ vÃ  Ä‘á»™ á»•n Ä‘á»‹nh sá»‘.

---

## 1. Giá»›i thiá»‡u (Introduction)

Attention lÃ  cÆ¡ cháº¿ cho phÃ©p mÃ´ hÃ¬nh há»c cÃ¡ch táº­p trung vÃ o cÃ¡c pháº§n quan trá»ng cá»§a chuá»—i Ä‘áº§u vÃ o. Trong Transformer, attention Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh toÃ¡n má»‘i quan há»‡ giá»¯a cÃ¡c token dá»±a trÃªn vector truy váº¥n (Query), khÃ³a (Key) vÃ  giÃ¡ trá»‹ (Value).

TÃ i liá»‡u hÆ°á»›ng dáº«n láº­p trÃ¬nh attention cung cáº¥p má»™t bÃ i thá»±c hÃ nh nháº±m:

- Chuyá»ƒn Ä‘á»•i cÃ´ng thá»©c attention sang mÃ£ Python,
- CÃ i Ä‘áº·t thá»§ cÃ´ng báº±ng PyTorch,
- So sÃ¡nh vá»›i hÃ m tá»‘i Æ°u hÃ³a cÃ³ sáºµn,
- ÄÃ¡nh giÃ¡ hiá»‡u nÄƒng thá»±c nghiá»‡m. 

Má»¥c tiÃªu cá»§a nghiÃªn cá»©u nÃ y lÃ  phÃ¢n tÃ­ch quÃ¡ trÃ¬nh trÃªn dÆ°á»›i gÃ³c nhÃ¬n há»c thuáº­t vÃ  há»‡ thá»‘ng.

---

## 2. CÆ¡ sá»Ÿ lÃ½ thuyáº¿t (Theoretical Background)

### 2.1. CÃ´ng thá»©c Attention

Scaled Dot-Product Attention Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a:

\text{Attention}(Q,K,V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)V

Trong Ä‘Ã³:

- $Q \in $\mathbb${R}^{T \times d}$: Query,
- $K \in $\mathbb${R}^{T \times d}$: Key,
- $V \in $\mathbb${R}^{T \times d}$: Value,
- $d_k$: sá»‘ chiá»u cá»§a vector Key.

CÃ´ng thá»©c nÃ y cho phÃ©p mÃ´ hÃ¬nh tÃ­nh toÃ¡n má»©c Ä‘á»™ liÃªn quan giá»¯a cÃ¡c token. 

---

### 2.2. Causal Mask trong Attention

Äá»‘i vá»›i mÃ´ hÃ¬nh tá»± há»“i quy, attention cáº§n tuÃ¢n thá»§ rÃ ng buá»™c nhÃ¢n quáº£:

$$
j > i \Rightarrow \text{masked}
$$

Causal mask Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ ngÄƒn mÃ´ hÃ¬nh truy cáº­p token tÆ°Æ¡ng lai, Ä‘áº£m báº£o tÃ­nh há»£p lá»‡ khi sinh chuá»—i.

---

### 2.3. Vai trÃ² cá»§a Chuáº©n hÃ³a

Há»‡ sá»‘ $\frac{1}{\sqrt{$d_k$}}$ Ä‘Æ°á»£c sá»­ dá»¥ng nháº±m:

- Giáº£m Ä‘á»™ lá»›n cá»§a tÃ­ch vÃ´ hÆ°á»›ng,
- TrÃ¡nh hiá»‡n tÆ°á»£ng gradient quÃ¡ lá»›n,
- Cáº£i thiá»‡n Ä‘á»™ á»•n Ä‘á»‹nh cá»§a softmax.

ÄÃ¢y lÃ  yáº¿u tá»‘ quan trá»ng trong huáº¥n luyá»‡n mÃ´ hÃ¬nh sÃ¢u. 

---

## 3. PhÆ°Æ¡ng phÃ¡p (Methodology)

### 3.1. Thiáº¿t láº­p dá»¯ liá»‡u mÃ´ phá»ng

Thay vÃ¬ sá»­ dá»¥ng dá»¯ liá»‡u thá»±c, nghiÃªn cá»©u mÃ´ phá»ng cÃ¡c tensor ngáº«u nhiÃªn vá»›i tham sá»‘:

- Batch size: 4,
- Context length: 8,
- Vocabulary size: 40,
- Embedding dimension: 10.

Token Ä‘Æ°á»£c sinh ngáº«u nhiÃªn vÃ  Ã¡nh xáº¡ sang embedding thÃ´ng qua ma tráº­n há»c Ä‘Æ°á»£c. 

---

### 3.2. Sinh Q, K, V báº±ng Linear Layer

Ba ma tráº­n Q, K, V Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng cÃ¡c lá»›p tuyáº¿n tÃ­nh:

Q = XW_Q,\quad K = XW_K,\quad V = XW_V

$$
vá»›i W_Q, W_K, W_V \in \mathbb{R}^{d \times d}.
$$

CÃ¡ch tiáº¿p cáº­n nÃ y pháº£n Ã¡nh Ä‘Ãºng kiáº¿n trÃºc Transformer chuáº©n. 

---

### 3.3. CÃ i Ä‘áº·t Attention Thá»§ CÃ´ng

CÃ¡c bÆ°á»›c triá»ƒn khai thá»§ cÃ´ng gá»“m:

1. TÃ­nh $QK^T$,
2. Chuáº©n hÃ³a theo $\sqrt{d}$,
3. Ãp dá»¥ng causal mask,
4. Softmax theo hÃ ng,
5. NhÃ¢n vá»›i V.

Viá»‡c xá»­ lÃ½ phÃ©p transpose cáº§n trÃ¡nh tÃ¡c Ä‘á»™ng Ä‘áº¿n chiá»u batch. 

---

### 3.4. Sá»­ dá»¥ng HÃ m PyTorch Tá»‘i Æ¯u

PyTorch cung cáº¥p hÃ m:

torch.nn.functional.scaled_dot_product_attention

HÃ m nÃ y tÃ­ch há»£p:

- Masking,
- Softmax á»•n Ä‘á»‹nh sá»‘,
- Kernel CUDA tá»‘i Æ°u.

Káº¿t quáº£ Ä‘áº§u ra tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i cÃ i Ä‘áº·t thá»§ cÃ´ng. 

---

### 3.5. ÄÃ¡nh giÃ¡ Hiá»‡u nÄƒng

Hai phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c so sÃ¡nh báº±ng cÃ¡ch:

- Láº·p 50.000 láº§n trÃªn CPU,
- Láº·p 200 láº§n trÃªn GPU,
- Äo thá»i gian thá»±c thi.

NgoÃ i ra, thá»­ nghiá»‡m sá»­ dá»¥ng JIT compiler Ä‘á»ƒ biÃªn dá»‹ch hÃ m attention. 

---

## 4. Káº¿t quáº£ (Results)

### 4.1. So sÃ¡nh Äá»™ chÃ­nh xÃ¡c

Káº¿t quáº£ cho tháº¥y:

- Äáº§u ra cá»§a hai phÆ°Æ¡ng phÃ¡p gáº§n nhÆ° trÃ¹ng khá»›p,
- Sai khÃ¡c á»Ÿ má»©c $10^{-8}$â€“$10^{-9}$,
- `torch.allclose` xÃ¡c nháº­n tÆ°Æ¡ng Ä‘Æ°Æ¡ng.

Sai khÃ¡c nhá» xuáº¥t phÃ¡t tá»« sai sá»‘ lÃ m trÃ²n sá»‘ há»c. 

---

### 4.2. Hiá»‡u nÄƒng trÃªn CPU

Káº¿t quáº£ thá»±c nghiá»‡m cho tháº¥y:

| PhÆ°Æ¡ng phÃ¡p | Thá»i gian (50k vÃ²ng) |
|------------|----------------------|
| Thá»§ cÃ´ng | ~7.0 s |
| PyTorch | ~6.5 s |

PhiÃªn báº£n tá»‘i Æ°u nhanh hÆ¡n do kernel Ä‘Æ°á»£c fuse. 

---

### 4.3. Hiá»‡u nÄƒng trÃªn GPU

Vá»›i ma tráº­n lá»›n:

| PhÆ°Æ¡ng phÃ¡p | Thá»i gian |
|-------------|-----------|
| Thá»§ cÃ´ng | ~4.5 s |
| PyTorch | ~9.0 s |
| Compiled | ~0.05 s |

Sau khi JIT compile, hiá»‡u nÄƒng cáº£i thiá»‡n hÆ¡n 80 láº§n. 

---

### 4.4. Hiá»‡u á»©ng Warm-up

Láº§n cháº¡y Ä‘áº§u tiÃªn trÃªn GPU thÆ°á»ng cháº­m hÆ¡n do:

- Khá»Ÿi táº¡o kernel,
- Load library,
- Memory allocation.

Do Ä‘Ã³, cÃ¡c láº§n cháº¡y sau pháº£n Ã¡nh chÃ­nh xÃ¡c hÆ¡n hiá»‡u nÄƒng thá»±c. 

---

## 5. Tháº£o luáº­n (Discussion)

### 5.1. Ã nghÄ©a cá»§a Triá»ƒn khai Thá»§ CÃ´ng

CÃ i Ä‘áº·t thá»§ cÃ´ng giÃºp:

- Hiá»ƒu sÃ¢u cÃ´ng thá»©c toÃ¡n há»c,
- PhÃ¡t hiá»‡n lá»—i transpose vÃ  broadcasting,
- Náº¯m rÃµ vai trÃ² cá»§a mask vÃ  softmax.

ÄÃ¢y lÃ  bÆ°á»›c quan trá»ng trong Ä‘Ã o táº¡o ká»¹ sÆ° AI. 

---

### 5.2. Lá»£i tháº¿ cá»§a HÃ m Tá»‘i Æ¯u

HÃ m PyTorch cung cáº¥p:

- TÃ­nh á»•n Ä‘á»‹nh sá»‘ cao,
- Tá»‘i Æ°u GPU,
- Há»— trá»£ mixed precision,
- Dá»… tÃ­ch há»£p.

Trong mÃ´i trÆ°á»ng production, Ä‘Ã¢y lÃ  lá»±a chá»n Æ°u tiÃªn.

---

### 5.3. Vai trÃ² cá»§a BiÃªn dá»‹ch (Compilation)

JIT compiler cho phÃ©p:

- Fuse kernel,
- Giáº£m overhead Python,
- Tá»‘i Æ°u pipeline.

Äiá»u nÃ y minh há»a vai trÃ² cá»§a compiler trong há»‡ thá»‘ng LLM hiá»‡n Ä‘áº¡i. 

---

### 5.4. GÃ³c nhÃ¬n Há»‡ thá»‘ng

Attention lÃ  phÃ©p toÃ¡n Ä‘Æ°á»£c láº·p láº¡i hÃ ng tá»· láº§n. Do Ä‘Ã³:

- Tá»‘i Æ°u tá»«ng micro-second mang láº¡i lá»£i Ã­ch lá»›n,
- Viá»‡c lá»±a chá»n kernel áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n chi phÃ­ váº­n hÃ nh.

---

## 6. Háº¡n cháº¿ (Limitations)

NghiÃªn cá»©u tá»“n táº¡i má»™t sá»‘ háº¡n cháº¿:

1. Chá»‰ xÃ©t single-head attention,
2. ChÆ°a Ä‘Ã¡nh giÃ¡ multi-GPU,
3. ChÆ°a so sÃ¡nh vá»›i FlashAttention kernel,
4. ChÆ°a phÃ¢n tÃ­ch backward pass.

CÃ¡c háº¡n cháº¿ nÃ y cáº§n Ä‘Æ°á»£c nghiÃªn cá»©u thÃªm.

---

## 7. á»¨ng dá»¥ng Thá»±c tiá»…n (Practical Implications)

Káº¿t quáº£ cÃ³ thá»ƒ Ã¡p dá»¥ng cho:

- Thiáº¿t káº¿ LLM inference engine,
- Tá»‘i Æ°u training pipeline,
- Debug attention kernel,
- PhÃ¡t triá»ƒn framework AI.

Viá»‡c hiá»ƒu rÃµ cáº£ hai cÃ¡ch triá»ƒn khai giÃºp cáº£i thiá»‡n Ä‘á»™ tin cáº­y há»‡ thá»‘ng.

---

## 8. Káº¿t luáº­n (Conclusion)

BÃ i bÃ¡o Ä‘Ã£ phÃ¢n tÃ­ch quÃ¡ trÃ¬nh triá»ƒn khai attention tá»« cÃ´ng thá»©c toÃ¡n há»c Ä‘áº¿n mÃ£ PyTorch, so sÃ¡nh giá»¯a cÃ i Ä‘áº·t thá»§ cÃ´ng vÃ  hÃ m tá»‘i Æ°u. Káº¿t quáº£ cho tháº¥y:

- Hai phÆ°Æ¡ng phÃ¡p cho káº¿t quáº£ tÆ°Æ¡ng Ä‘Æ°Æ¡ng,
- PhiÃªn báº£n tá»‘i Æ°u vÆ°á»£t trá»™i vá» tá»‘c Ä‘á»™,
- BiÃªn dá»‹ch Ä‘Ã³ng vai trÃ² quyáº¿t Ä‘á»‹nh trong hiá»‡u nÄƒng.

NghiÃªn cá»©u kháº³ng Ä‘á»‹nh ráº±ng viá»‡c tá»‘i Æ°u attention lÃ  ná»n táº£ng cho sá»± phÃ¡t triá»ƒn cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ quy mÃ´ lá»›n.

---

## TÃ i liá»‡u tham kháº£o (References)

[1] Code Challenge: Code Attention Manually and in PyTorch, 2024. 

[2] Vaswani, A. et al. (2017). Attention Is All You Need. NeurIPS.

[3] Dao, T. et al. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention. NeurIPS.

[4] Paszke, A. et al. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. NeurIPS.

---

Náº¿u báº¡n muá»‘n, mÃ¬nh cÃ³ thá»ƒ tiáº¿p tá»¥c má»Ÿ rá»™ng cho báº¡n sang:

âœ… Multi-Head Attention + Benchmark
âœ… Backward Pass Optimization
âœ… FlashAttention vs SDPA
âœ… Full LLM Attention Engine Design

Báº¡n muá»‘n Ä‘i tiáº¿p theo hÆ°á»›ng **há»‡ thá»‘ng, kernel, hay research paper submission**?
DÆ°á»›i Ä‘Ã¢y lÃ  **bÃ i viáº¿t khoa há»c báº±ng tiáº¿ng Viá»‡t** vá» **Multi-Head Attention vÃ  Benchmark hiá»‡u nÄƒng**, trÃ¬nh bÃ y theo chuáº©n há»c thuáº­t vÃ  Ä‘á»‹nh dáº¡ng **Markdown (MD)**.

---

# Multi-Head Attention vÃ  ÄÃ¡nh GiÃ¡ Hiá»‡u NÄƒng: PhÃ¢n TÃ­ch Kiáº¿n TrÃºc vÃ  Benchmark Thá»±c Nghiá»‡m

## TÃ³m táº¯t (Abstract)

Multi-Head Attention (MHA) lÃ  thÃ nh pháº§n trung tÃ¢m trong kiáº¿n trÃºc Transformer vÃ  cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n. CÆ¡ cháº¿ nÃ y cho phÃ©p mÃ´ hÃ¬nh há»c song song nhiá»u khÃ´ng gian biá»ƒu diá»…n khÃ¡c nhau. BÃ i bÃ¡o nÃ y trÃ¬nh bÃ y cÆ¡ sá»Ÿ lÃ½ thuyáº¿t cá»§a MHA, phÆ°Æ¡ng phÃ¡p triá»ƒn khai báº±ng PyTorch, vÃ  Ä‘Ã¡nh giÃ¡ hiá»‡u nÄƒng thÃ´ng qua cÃ¡c benchmark trÃªn CPU vÃ  GPU. Káº¿t quáº£ cho tháº¥y viá»‡c tá»‘i Æ°u kernel vÃ  sá»­ dá»¥ng hÃ m attention tÃ­ch há»£p giÃºp cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ tá»‘c Ä‘á»™ huáº¥n luyá»‡n vÃ  suy luáº­n.

---

## 1. Giá»›i thiá»‡u (Introduction)

Trong kiáº¿n trÃºc Transformer, Single-Head Attention chá»‰ cho phÃ©p mÃ´ hÃ¬nh há»c má»™t dáº¡ng quan há»‡ giá»¯a cÃ¡c token. Äiá»u nÃ y háº¡n cháº¿ kháº£ nÄƒng biá»ƒu diá»…n ngá»¯ nghÄ©a phá»©c táº¡p.

Multi-Head Attention má»Ÿ rá»™ng cÆ¡ cháº¿ nÃ y báº±ng cÃ¡ch:

- Chia embedding thÃ nh nhiá»u khÃ´ng gian con,
- Ãp dá»¥ng attention song song,
- Káº¿t há»£p káº¿t quáº£ Ä‘á»ƒ tÄƒng nÄƒng lá»±c biá»ƒu diá»…n.

MHA lÃ  ná»n táº£ng cho cÃ¡c mÃ´ hÃ¬nh nhÆ° BERT, GPT vÃ  LLaMA.

---

## 2. CÆ¡ sá»Ÿ lÃ½ thuyáº¿t (Theoretical Background)

### 2.1. CÃ´ng thá»©c Multi-Head Attention

Multi-Head Attention Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a:

\text{MHA}(Q,K,V) = \text{Concat}(h_1,\dots,h_H)W_O

vá»›i:

h_i = \text{Attention}(QW_i^Q,KW_i^K,VW_i^V)

Trong Ä‘Ã³:

- $H$: sá»‘ head,
- $W_i^Q, $W_i$^K, $W_i$^V$: ma tráº­n chiáº¿u,
- $W_$O(: ma tráº­n Ä‘áº§u ra.

Má»—i head há»c má»™t khÃ´ng gian biá»ƒu diá»…n riÃªng biá»‡t.

---

### 2.2. PhÃ¢n rÃ£ KhÃ´ng gian Äáº·c trÆ°ng

Vá»›i embedding dimension )$d$:

d_{head} = \frac{d}{H}

Má»—i head xá»­ lÃ½ tensor kÃ­ch thÆ°á»›c:

$$
(T, d_{head})
$$

CÃ¡ch chia nÃ y giÃºp:

- Giáº£m chi phÃ­ tÃ­nh toÃ¡n má»—i head,
- TÄƒng kháº£ nÄƒng há»c quan há»‡ Ä‘a chiá»u.

---

### 2.3. Causal Multi-Head Attention

Trong mÃ´ hÃ¬nh tá»± há»“i quy, má»—i head Ä‘á»u Ã¡p dá»¥ng causal mask:

M_{ij} = \begin{cases} 0 & j \le i \\ -\infty & j > i \end{cases}

Mask nÃ y Ä‘áº£m báº£o khÃ´ng rÃ² rá»‰ thÃ´ng tin tÆ°Æ¡ng lai.

---

## 3. PhÆ°Æ¡ng phÃ¡p (Methodology)

### 3.1. MÃ´i trÆ°á»ng Thá»±c nghiá»‡m

- Framework: PyTorch
- Pháº§n cá»©ng:
  - CPU: x86-64
  - GPU: NVIDIA CUDA
- Precision: FP32 / FP16
- Context length: 128â€“1024
- Heads: 4, 8, 16

---

### 3.2. Kiáº¿n trÃºc MÃ´ hÃ¬nh

MÃ´ hÃ¬nh thá»­ nghiá»‡m gá»“m:

1. Embedding layer
2. Multi-Head Attention
3. Feedforward
4. LayerNorm

Cáº¥u trÃºc tÆ°Æ¡ng Ä‘Æ°Æ¡ng má»™t block Transformer tiÃªu chuáº©n.

---

### 3.3. Pseudocode Multi-Head Attention

Input: X âˆˆ R^(BÃ—TÃ—d)
Output: Y âˆˆ R^(BÃ—TÃ—d)

for each head i in H:

Qi = X Â· WQi

$$
Ki = X Â· WKi
$$

Vi = X Â· WVi

$$
Ai = softmax(Qi Ki^T / sqrt(dh) + Mask)
$$

Hi = Ai Â· Vi

$$
H = concat(H1,...,HH)
$$

Y = H Â· WO

````

---

### 3.4. PyTorch Implementation

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        

assert d_model % n_heads == 0

$$
self.d_model = d_model
$$

self.n_heads = n_heads

$$
self.d_head = d_model // n_heads
$$

self.qkv = nn.Linear(d_model, 3 * d_model)

$$
self.out = nn.Linear(d_model, d_model)
$$

def forward(self, x, causal=True):

$$
B, T, D = x.shape
$$

qkv = self.qkvx

$$
qkv = qkv.view(B, T, 3, self.n_heads, self.d_head)
$$

qkv = qkv.permute(2, 0, 3, 1, 4)

$$
q, k, v = qkv[0], qkv[1], qkv[2]
$$

scores = torch.matmul(q, k.transpose(-2, -1))

$$
scores /= self.d_head ** 0.5 if causal: mask = torch.tril(torch.ones(T, T, device=x.device))
$$

scores = scores.masked_fill(mask == 0, -1e9)

$$
attn = F.softmax(scores, dim=-1)
$$

out = torch.matmul(attn, v)

$$
out = out.transpose(1, 2).contiguous()
$$

out = out.view(B, T, D)

        
        return self.out(out)
````

---

## 4. Thiáº¿t káº¿ Benchmark (Benchmark Design)

### 4.1. Cáº¥u hÃ¬nh ÄÃ¡nh giÃ¡

CÃ¡c biáº¿n sá»‘:

| Tham sá»‘    | GiÃ¡ trá»‹        |
| ---------- | -------------- |
| Batch size | 1, 8, 32       |
| Seq length | 128, 512, 1024 |
| Heads      | 4, 8, 16       |
| Precision  | FP32, FP16     |

---

### 4.2. Quy trÃ¬nh Äo lÆ°á»ng

1. Warm-up 50 vÃ²ng
2. Cháº¡y 500â€“1000 vÃ²ng
3. Äo trung bÃ¬nh thá»i gian
4. Äá»“ng bá»™ CUDA
5. Loáº¡i bá» outlier

---

### 4.3. MÃ£ Benchmark

```python
import time

$$
def benchmark(model, x, runs=500):
$$

    torch.cuda.synchronize()
    
    for _ in range(50):

$$
_ = modelx start = time.time() for _ in range(runs):
$$

_ = modelx

<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [04 buildgpt](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Kiáº¿n TrÃºc Transformer vÃ  Triá»ƒn Khai GPT-2 trÃªn GPU: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Hiá»‡u NÄƒng TÃ­nh ToÃ¡n

## TÃ³m táº¯t

CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (Large Language Models â€“ LLMs) dá»±a trÃªn kiáº¿n trÃºc Transformer Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c nhiá»u thÃ nh tá»±u trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn. BÃ i viáº¿t nÃ y trÃ¬nh bÃ y tá»•ng quan vá» cáº¥u trÃºc toÃ¡n há»c cá»§a GPT-2, cÆ¡ cháº¿ multi-head attention, quy trÃ¬nh huáº¥n luyá»‡n vÃ  suy luáº­n, cÅ©ng nhÆ° Ä‘Ã¡nh giÃ¡ hiá»‡u nÄƒng khi triá»ƒn khai trÃªn CPU vÃ  GPU. ThÃ´ng qua phÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  thá»±c nghiá»‡m, nghiÃªn cá»©u cho tháº¥y GPU Ä‘Ã³ng vai trÃ² thiáº¿t yáº¿u trong viá»‡c váº­n hÃ nh cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡n Ä‘áº¡i.

---

## 1. Giá»›i thiá»‡u

Transformer lÃ  ná»n táº£ng cá»§a háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡n Ä‘áº¡i. Kiáº¿n trÃºc nÃ y cho phÃ©p mÃ´ hÃ¬nh hÃ³a má»‘i quan há»‡ dÃ i háº¡n giá»¯a cÃ¡c token thÃ´ng qua cÆ¡ cháº¿ attention. GPT-2 lÃ  má»™t trong nhá»¯ng mÃ´ hÃ¬nh tiÃªu biá»ƒu sá»­ dá»¥ng Transformer Ä‘á»ƒ sinh ngÃ´n ngá»¯ tá»± nhiÃªn.

Viá»‡c triá»ƒn khai hiá»‡u quáº£ cÃ¡c mÃ´ hÃ¬nh nÃ y Ä‘Ã²i há»i sá»± káº¿t há»£p giá»¯a hiá»ƒu biáº¿t toÃ¡n há»c, thiáº¿t káº¿ kiáº¿n trÃºc vÃ  tá»‘i Æ°u pháº§n cá»©ng.

---

## 2. Biá»ƒu diá»…n Embedding vÃ  Dá»¯ liá»‡u Äáº§u vÃ o

Trong GPT-2, má»—i token Ä‘Æ°á»£c Ã¡nh xáº¡ sang má»™t vector embedding thÃ´ng qua ma tráº­n tá»« vá»±ng $E \in $\mathbb${R}^{V \times D}$, káº¿t há»£p vá»›i embedding vá»‹ trÃ­ $P \in $\mathbb${R}^{L \times D}$. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c mÃ´ táº£ báº±ng one-hot encoding vÃ  phÃ©p nhÃ¢n ma tráº­n.

PhÃ©p biáº¿n Ä‘á»•i tá»« token sang embedding Ä‘Æ°á»£c thá»±c hiá»‡n thÃ´ng qua:

$$

$$

X = \Delta E + P

$$

$$

trong Ä‘Ã³ X \in \mathbb{R}^{T \times D} lÃ  ma tráº­n biá»ƒu diá»…n chuá»—i Ä‘áº§u vÃ o.

$$
QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c trÃ¬nh bÃ y chi tiáº¿t trong tÃ i liá»‡u tá»•ng há»£p toÃ¡n há»c vá» GPT. --- ## 3. CÆ¡ Cháº¿ Multi-Head Attention ### 3.1. NguyÃªn lÃ½ toÃ¡n há»c Multi-head attention chia khÃ´ng gian embedding thÃ nh nhiá»u pháº§n (heads) song song. Vá»›i má»—i head h, ta cÃ³:
$$

$$
Q_h = XW_Q^h, \quad K_h = XW_K^h, \quad V_h = XW_V^h
$$

$$
Sau Ä‘Ã³, attention Ä‘Æ°á»£c tÃ­nh:
$$

$$
A_h = \text{softmax}\left(\frac{Q_h K_h^T}{\sqrt{D/H}} + M \right)V_h
$$

$$
CÃ¡c Ä‘áº§u ra Ä‘Æ°á»£c ná»‘i láº¡i vÃ  chiáº¿u tuyáº¿n tÃ­nh:
$$

$$
A = \text{Concat}(A_1, \dots, A_H)W_0
$$

$$
Viá»‡c chia nhá» attention giÃºp mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c nhiá»u kiá»ƒu quan há»‡ ngá»¯ nghÄ©a khÃ¡c nhau. --- ### 3.2. Triá»ƒn khai trong PyTorch Trong thá»±c táº¿, cÃ¡c ma tráº­n (W_Q, W_K, W_V) thÆ°á»ng Ä‘Æ°á»£c gá»™p thÃ nh má»™t ma tráº­n duy nháº¥t Ä‘á»ƒ tÄƒng hiá»‡u suáº¥t. QuÃ¡ trÃ¬nh reshape vÃ  transpose Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ¡ch cÃ¡c head trong forward pass. Viá»‡c sá»­ dá»¥ng hÃ m attention tÃ­ch há»£p giÃºp tá»‘i Æ°u tÃ­nh toÃ¡n song song trÃªn GPU. --- ## 4. Khá»‘i Transformer vÃ  Máº¡ng MLP ### 4.1. Attention Sub-layer Má»—i khá»‘i Transformer báº¯t Ä‘áº§u báº±ng layer normalization, sau Ä‘Ã³ lÃ  multi-head attention vÃ  residual connection:
$$

$$
X' = X + \text{Attention}(\text{LN}(X))
$$

$$
### 4.2. Feed-Forward Network (MLP) Sau attention, dá»¯ liá»‡u Ä‘Æ°á»£c Ä‘Æ°a qua máº¡ng MLP gá»“m hai lá»›p tuyáº¿n tÃ­nh:
$$

$$
Y = X' + W_2(\text{GELU}(W_1(\text{LN}(X'))))
$$

$$
Máº¡ng MLP giÃºp mÃ´ hÃ¬nh trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng phi tuyáº¿n trong khÃ´ng gian chiá»u cao. --- ## 5. Unembedding vÃ  Sinh Token Äáº§u ra cuá»‘i cÃ¹ng Ä‘Æ°á»£c chuáº©n hÃ³a vÃ  nhÃ¢n vá»›i ma tráº­n embedding ban Ä‘áº§u Ä‘á»ƒ táº¡o logits:
$$

$$
L = \text{LN}(X_{out})E^T
$$

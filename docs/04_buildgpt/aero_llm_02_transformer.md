
<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [04 buildgpt](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Ph√¢n T√≠ch Ki·∫øn Tr√∫c v√† C∆° Ch·∫ø Ho·∫°t ƒê·ªông c·ªßa M√¥ H√¨nh Ng√¥n Ng·ªØ Transformer C∆° B·∫£n

## T√≥m t·∫Øt

B√†i vi·∫øt n√†y tr√¨nh b√†y ph√¢n t√≠ch chi ti·∫øt v·ªÅ ki·∫øn tr√∫c v√† c∆° ch·∫ø ho·∫°t ƒë·ªông c·ªßa m√¥ h√¨nh ng√¥n ng·ªØ transformer c∆° b·∫£n (zero-layer transformer), t·∫≠p trung v√†o c√°c kh√≠a c·∫°nh k·ªπ thu·∫≠t c·ªßa qu√° tr√¨nh embedding, sinh token, v√† lu·ªìng x·ª≠ l√Ω th√¥ng tin ng·ªØ c·∫£nh. Nghi√™n c·ª©u l√†m r√µ c√°c nguy√™n l√Ω n·ªÅn t·∫£ng trong vi·ªác x√¢y d·ª±ng m√¥ h√¨nh GPT-2 t·ª´ ƒë·∫ßu, v·ªõi m·ª•c ti√™u s∆∞ ph·∫°m l√† t·∫°o n·ªÅn t·∫£ng hi·ªÉu bi·∫øt s√¢u s·∫Øc v·ªÅ ki·∫øn tr√∫c transformer.

---

## 1. Gi·ªõi Thi·ªáu

### 1.1 B·ªëi C·∫£nh Nghi√™n C·ª©u

M√¥ h√¨nh ng√¥n ng·ªØ d·ª±a tr√™n ki·∫øn tr√∫c transformer ƒë√£ c√°ch m·∫°ng h√≥a lƒ©nh v·ª±c x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n. Tuy nhi√™n, vi·ªác hi·ªÉu s√¢u s·∫Øc v·ªÅ c∆° ch·∫ø ho·∫°t ƒë·ªông b√™n trong c·ªßa c√°c m√¥ h√¨nh n√†y ƒë√≤i h·ªèi ph∆∞∆°ng ph√°p ti·∫øp c·∫≠n t·ª´ c∆° b·∫£n ƒë·∫øn ph·ª©c t·∫°p.

### 1.2 M·ª•c Ti√™u Nghi√™n C·ª©u

Nghi√™n c·ª©u n√†y nh·∫±m:
- Ph√¢n t√≠ch ki·∫øn tr√∫c m√¥ h√¨nh transformer ƒë∆°n gi·∫£n nh·∫•t (Model 1)
- L√†m r√µ c∆° ch·∫ø x·ª≠ l√Ω ng·ªØ c·∫£nh v√† sinh token
- Kh√°m ph√° vai tr√≤ c·ªßa c√°c th√†nh ph·∫ßn k·ªπ thu·∫≠t nh∆∞ embedding, softmax, v√† multinomial sampling

---

## 2. Ki·∫øn Tr√∫c M√¥ H√¨nh

### 2.1 C·∫•u Tr√∫c T·ªïng Th·ªÉ

M√¥ h√¨nh c∆° b·∫£n ƒë∆∞·ª£c nghi√™n c·ª©u bao g·ªìm c√°c th√†nh ph·∫ßn ch√≠nh:

**Lu·ªìng x·ª≠ l√Ω d·ªØ li·ªáu:**
Text ‚Üí Tokens ‚Üí Embeddings ‚Üí Non-linearity ‚Üí Unembeddings ‚Üí Tokens ‚Üí Text

**ƒê·∫∑c ƒëi·ªÉm k·ªπ thu·∫≠t:**
- **Embedding dimension**: 64 (k√≠ch th∆∞·ªõc nh·ªè g·ªçn cho m·ª•c ƒë√≠ch gi√°o d·ª•c)
- **Sequence length**: 8 tokens (cho ph√©p quan s√°t chi ti·∫øt)
- **Vocabulary size**: 100,000 tokens (GPT-4 tokenizer)
- **Batch size**: 5

### 2.2 M√¥ H√¨nh "Zero-Layer Transformer"

M√¥ h√¨nh n√†y ƒë∆∞·ª£c g·ªçi l√† "zero-layer transformer" v√¨ thi·∫øu c√°c kh·ªëi transformer trung gian. Ki·∫øn tr√∫c bao g·ªìm:

**Th√†nh ph·∫ßn ch√≠nh:**
1. **L·ªõp Embedding** (`nn.Embedding`): Chuy·ªÉn ƒë·ªïi token indices th√†nh vector s·ªë th·ª±c
2. **H√†m k√≠ch ho·∫°t phi tuy·∫øn** (GELU): √Åp d·ª•ng bi·∫øn ƒë·ªïi phi tuy·∫øn
3. **L·ªõp Unembedding** (`nn.Linear`): √Ånh x·∫° ng∆∞·ª£c v·ªÅ kh√¥ng gian vocabulary

**C√†i ƒë·∫∑t PyTorch:**
```python
class Model1(nn.Module):
    def __init__(self, vocab_size, embed_dim):
        super().__init__()

$$

$$

self.embeddings = nn.Embedding(vocab_size, embed_dim)

$$

$$

$$
self.gelu = nn.GELU()
$$

$$

$$

self.final_layer = nn.Linear(embed_dim, vocab_size)

$$

$$

    
    def forward(self, tokens):

$$

$$

x = self.embeddings(tokens)

$$

$$

$$
x = self.gelu(x)
$$

$$

$$

logits = self.final_layer(x)

$$

$$

        return logits

---

## 3. C∆° Ch·∫ø X·ª≠ L√Ω Ng·ªØ C·∫£nh

### 3.1 Hi·ªÉu L·∫ßm Ph·ªï Bi·∫øn v·ªÅ Token Cu·ªëi C√πng

**Ph√°t bi·ªÉu th∆∞·ªùng g·∫∑p:** "M√¥ h√¨nh ch·ªâ s·ª≠ d·ª•ng token cu·ªëi c√πng ƒë·ªÉ d·ª± ƒëo√°n token ti·∫øp theo."

**Th·ª±c t·∫ø:** ƒê√¢y l√† m·ªôt hi·ªÉu l·∫ßm nghi√™m tr·ªçng. M·∫∑c d√π m√¥ h√¨nh tr√≠ch xu·∫•t token cu·ªëi c√πng ƒë·ªÉ sinh token m·ªõi, nh∆∞ng th√¥ng tin trong token cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c t√≠ch h·ª£p t·ª´ t·∫•t c·∫£ c√°c token tr∆∞·ªõc ƒë√≥.

### 3.2 Ph√¢n T√≠ch T√≠ch L≈©y Ng·ªØ C·∫£nh

**V√≠ d·ª• minh h·ªça:** "I prefer oat milk in ___"

| Giai ƒëo·∫°n | Tokens ƒë√£ x·ª≠ l√Ω | Kh√¥ng gian t√¨m ki·∫øm | C∆° ch·∫ø |
|-----------|----------------|---------------------|---------|
| 1 | "I" | ~10,000+ kh·∫£ nƒÉng | Ch·ªâ d·ª±a v√†o embedding c·ªßa "I" |
| 2 | "I prefer" | ~3,000 kh·∫£ nƒÉng | K·∫øt h·ª£p "I" + "prefer" |
| 3 | "I prefer oat" | ~500 kh·∫£ nƒÉng | T√≠ch h·ª£p ng·ªØ c·∫£nh t·ª´ 3 tokens |
| 4 | "I prefer oat milk" | ~100 kh·∫£ nƒÉng | Ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß h∆°n |
| 5 | "I prefer oat milk in" | ~50 kh·∫£ nƒÉng | T·ªëi ƒëa h√≥a th√¥ng tin ng·ªØ c·∫£nh |

**Nguy√™n l√Ω c·ªët l√µi:**
> M·ªói vector embedding kh√¥ng ph·∫£i l√† vector ban ƒë·∫ßu, m√† ƒë√£ ƒë∆∞·ª£c bi·∫øn ƒë·ªïi ƒë·ªÉ ch·ª©a th√¥ng tin v·ªÅ c√°c token tr∆∞·ªõc ƒë√≥. Vi·ªác s·ª≠ d·ª•ng ch·ªâ token cu·ªëi c√πng ƒë·ªÉ d·ª± ƒëo√°n token m·ªõi th·ª±c ch·∫•t c√≥ nghƒ©a l√† s·ª≠ d·ª•ng t·∫•t c·∫£ c√°c tokens‚Äîch√∫ng ta ch·ªâ t·∫≠p trung v√†o token c√≥ nhi·ªÅu th√¥ng tin ng·ªØ c·∫£nh nh·∫•t.

### 3.3 C∆° Ch·∫ø Bi·∫øn ƒê·ªïi Vector

Trong qu√° tr√¨nh feedforward, c√°c vector embedding tr·∫£i qua:
- **Scaling** (co gi√£n t·ªâ l·ªá)
- **Addition/Subtraction** (c·ªông/tr·ª´)
- **Non-linear transformations** (bi·∫øn ƒë·ªïi phi tuy·∫øn)

K·∫øt qu·∫£: Token cu·ªëi c√πng mang th√¥ng tin t·ªïng h·ª£p t·ª´ to√†n b·ªô chu·ªói.

---

## 4. Quy Tr√¨nh Sinh Token

### 4.1 Lu·ªìng X·ª≠ L√Ω Feedforward

**Input:** Vector c·ªßa N tokens (v√≠ d·ª•: 10 tokens)
**Output:** Tensor k√≠ch th∆∞·ªõc [N √ó V] (N tokens √ó V vocab size)

**C√°c b∆∞·ªõc:**
1. Tokenization: Text ‚Üí Token indices
2. Embedding: Indices ‚Üí Dense vectors
3. Processing: Phi tuy·∫øn h√≥a
4. Unembedding: Vectors ‚Üí Logits space
5. Output: Logits matrix [tokens √ó vocab_size]

### 4.2 Chuy·ªÉn ƒê·ªïi Logits sang X√°c Su·∫•t

**H√†m Softmax:**

$$

$$

\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{V} e^{x_j}}

$$

$$

**ƒê·∫∑c ƒëi·ªÉm:**
- Chuy·ªÉn ƒë·ªïi logits (c√≥ th·ªÉ √¢m, >1) th√†nh x√°c su·∫•t (0 ‚â§ p ‚â§ 1)
- T·ªïng t·∫•t c·∫£ x√°c su·∫•t = 1
- Phi tuy·∫øn: TƒÉng c∆∞·ªùng logits cao, gi·∫£m logits th·∫•p

**Quan s√°t th·ª±c nghi·ªám:**
```python
# Logits: [-2.3, 0.5, 3.1, -1.2, ...]
# Sau softmax: [0.001, 0.02, 0.85, 0.003, ...]
# Sum = 1.0

### 4.3 L·ª±a Ch·ªçn Token: Multinomial Sampling

**Ph∆∞∆°ng ph√°p `torch.multinomial`:**

Kh√¥ng ph·∫£i l·ª±a ch·ªçn ng·∫´u nhi√™n ƒë·ªÅu, m√† l√† **l·∫•y m·∫´u x√°c su·∫•t** d·ª±a tr√™n ph√¢n ph·ªëi:

- Token c√≥ x√°c su·∫•t cao ‚Üí Kh·∫£ nƒÉng ƒë∆∞·ª£c ch·ªçn cao h∆°n
- Token c√≥ x√°c su·∫•t th·∫•p ‚Üí V·∫´n c√≥ c∆° h·ªôi ƒë∆∞·ª£c ch·ªçn (t·∫°o ƒëa d·∫°ng)

**√ù nghƒ©a:**
- Gi·∫£i th√≠ch t·∫°i sao ChatGPT cho c√¢u tr·∫£ l·ªùi kh√°c nhau v·ªõi c√πng c√¢u h·ªèi
- C√¢n b·∫±ng gi·ªØa ch·∫•t l∆∞·ª£ng v√† s√°ng t·∫°o
- Tr√°nh t√≠nh quy·∫øt ƒë·ªãnh c·ª©ng nh·∫Øc

### 4.4 Thu·∫≠t To√°n Sinh Token T·ª± H·ªìi Quy

```python

$$

$$

def generate(self, tokens, n_new_tokens=30):

$$

$$

    for _ in range(n_new_tokens):
        # B∆∞·ªõc 1: Feedforward

$$

$$

x = self(tokens)  # [batch, seq_len, vocab_size]

$$

$$

        
        # B∆∞·ªõc 2: Tr√≠ch xu·∫•t token cu·ªëi c√πng

$$

$$

final_logits = x[:, -1, :]  # [batch, vocab_size]

$$

$$

        
        # B∆∞·ªõc 3: Softmax

$$

$$

probs = torch.softmax(final_logits, dim=-1)

$$

$$

        
        # B∆∞·ªõc 4: L·∫•y m·∫´u

$$

$$

next_token = torch.multinomial(probs, num_samples=1)

$$

$$

        
        # B∆∞·ªõc 5: N·ªëi token m·ªõi

$$

$$

tokens = torch.cat([tokens, next_token], dim=1)

$$

$$

    
    return tokens

**ƒê·∫∑c ƒëi·ªÉm auto-regressive:**
- M·ªói token m·ªõi ph·ª• thu·ªôc v√†o t·∫•t c·∫£ tokens tr∆∞·ªõc ƒë√≥
- Chu·ªói tokens tƒÉng d·∫ßn: N ‚Üí N+1 ‚Üí N+2 ‚Üí ... ‚Üí N+M

---

## 5. C√°c Kh√≠a C·∫°nh K·ªπ Thu·∫≠t

### 5.1 Batch Processing

**T·∫°i sao s·ª≠ d·ª•ng batches?**

1. **Hi·ªáu qu·∫£ t√≠nh to√°n:**
   - X·ª≠ l√Ω song song nhi·ªÅu sequences
   - T·∫≠n d·ª•ng t·ªëi ∆∞u GPU/TPU
   - Gi·∫£m th·ªùi gian hu·∫•n luy·ªán

2. **Regularization:**
   - Loss ƒë∆∞·ª£c t√≠nh trung b√¨nh tr√™n batch
   - Gi·∫£m overfitting
   - L√†m m∆∞·ª£t qu√° tr√¨nh h·ªçc

**C·∫•u tr√∫c tensor:**

$$

$$

Input:  [batch_size, seq_len]           = [5, 8]

$$

$$

$$
Output: [batch_size, seq_len, vocab]    = [5, 8, 100000]
$$

$$
### 5.2 So S√°nh `nn.Embedding` vs `nn.Linear` | Kh√≠a c·∫°nh | nn.Embedding | nn.Linear | |-----------|--------------|-----------| | M·ª•c ƒë√≠ch | Tra c·ª©u vector t·ª´ b·∫£ng | Ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh | | Input | Token indices (integers) | Dense vectors (floats) | | C∆° ch·∫ø | Lookup operation | Matrix multiplication | | Gradient | Ch·ªâ update vectors ƒë∆∞·ª£c d√πng | Update to√†n b·ªô ma tr·∫≠n | | Hi·ªáu qu·∫£ | T·ªëi ∆∞u cho sparse inputs | T·ªëi ∆∞u cho dense inputs | **ƒêi·ªÉm chung:** V·ªÅ b·∫£n ch·∫•t, c·∫£ hai ƒë·ªÅu th·ª±c hi·ªán ph√©p nh√¢n ma tr·∫≠n, nh∆∞ng v·ªõi interface v√† t·ªëi ∆∞u h√≥a kh√°c nhau. ### 5.3 H√†m K√≠ch Ho·∫°t GELU vs ReLU **GELU (Gaussian Error Linear Unit):**
$$

$$
\text{GELU}(x) = x \cdot \Phi(x)
$$

$$
Trong ƒë√≥ Œ¶x l√† h√†m ph√¢n ph·ªëi chu·∫©n t√≠ch l≈©y. **∆Øu ƒëi·ªÉm c·ªßa GELU trong LLMs:** - M∆∞·ª£t h∆°n ReLU (kh·∫£ vi t·∫°i m·ªçi ƒëi·ªÉm) - Cho ph√©p gi√° tr·ªã √¢m c√≥ tr·ªçng s·ªë - Hi·ªáu su·∫•t t·ªët h∆°n trong c√°c m√¥ h√¨nh l·ªõn - ƒê∆∞·ª£c s·ª≠ d·ª•ng trong GPT-2, BERT, v√† h·∫ßu h·∫øt LLMs hi·ªán ƒë·∫°i **So s√°nh:**
$$

ReLU(x) = max(0, x)           # C·ª©ng, kh√¥ng m∆∞·ª£t

$$

$$

GELU(x) = x * Œ¶(x)            # M∆∞·ª£t, x√°c su·∫•t

$$
--- ## 6. T·ªï Ch·ª©c D·ªØ Li·ªáu v√† Ti·ªÅn X·ª≠ L√Ω ### 6.1 Tokenization v√† Tensor Conversion **Quy tr√¨nh:** ```python # B∆∞·ªõc 1: Text ‚Üí Token list
$$

$$
tokens_list = tokenizer.encode(text)  # List[int]
$$

$$
# B∆∞·ªõc 2: List ‚Üí PyTorch Tensor
$$

$$
tokens_tensor = torch.tensor(tokens_list)  # Tensor
$$

$$
# L√Ω do: PyTorch functions y√™u c·∫ßu tensor inputs ### 6.2 Sequence Dataset Organization **C·∫•u tr√∫c d·ªØ li·ªáu:** - **Inputs:** Chu·ªói N tokens - **Targets:** C√πng chu·ªói ƒë√≥, shift 1 v·ªã tr√≠ **V√≠ d·ª•:** Inputs:  [47, 45, 38, 439, 12, 89, 234, 56] Targets: [45, 38, 439, 12, 89, 234, 56, 77] **M·ª•c ƒë√≠ch:** M·ªói token input h·ªçc d·ª± ƒëo√°n token ti·∫øp theo (next-token prediction). ### 6.3 Hyperparameters ```python
$$

HYPERPARAMETERS = {

$$
'vocab_size': 100000,      # GPT-4 tokenizer 'embed_dim': 64,           # Nh·ªè cho m·ª•c ƒë√≠ch gi√°o d·ª•c 'seq_length': 8,           # Cho ph√©p in to√†n b·ªô sequence 'batch_size': 5,           # X·ª≠ l√Ω song song 'stride': 1,               # D·ªØ li·ªáu overlap } **L∆∞u √Ω:** C√°c gi√° tr·ªã n√†y ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ t·ªëi ∆∞u h√≥a vi·ªác h·ªçc v√† hi·ªÉu, kh√¥ng ph·∫£i cho hi·ªáu su·∫•t th·ª±c t·∫ø. --- ## 7. Ph√¢n T√≠ch K·∫øt Qu·∫£ Th·ª±c Nghi·ªám ### 7.1 Quan S√°t Logits v√† Probabilities **ƒê·∫∑c ƒëi·ªÉm Logits:** - Ph·∫°m vi: (-‚àû, +‚àû) - C√≥ th·ªÉ √¢m, d∆∞∆°ng, >1 - Kh√¥ng chu·∫©n h√≥a - Ph·∫£n √°nh "raw preferences" c·ªßa model **Sau Softmax:** - Ph·∫°m vi: (0, 1) - T·ªïng = 1 - Ph√¢n ph·ªëi x√°c su·∫•t h·ª£p l·ªá - Phi tuy·∫øn: TƒÉng kho·∫£ng c√°ch gi·ªØa high/low logits ### 7.2 T√≠nh Ng·∫´u Nhi√™n v√† ƒêa D·∫°ng **Th·ª±c nghi·ªám:** Sinh 5 sequences t·ª´ c√πng input **K·∫øt qu·∫£:** Run 1: "The one I had seen above ground in [random tokens...]" Run 2: "The one I had seen above ground in [different random tokens...]" Run 3: "The one I had seen above ground in [yet different tokens...]" ... **Ph√¢n t√≠ch:** - C√πng prefix ‚Üí Kh√°c suffix - Multinomial sampling ‚Üí Stochastic generation - Gi·∫£i th√≠ch behavior c·ªßa production LLMs (ChatGPT, Claude, etc.) ### 7.3 Weights Ch∆∞a Hu·∫•n Luy·ªán **Tr·∫°ng th√°i hi·ªán t·∫°i:** - Weights ng·∫´u nhi√™n (random initialization) - Kh√¥ng c√≥ pre-training - Output l√† "gibberish" (v√¥ nghƒ©a) **M·ª•c ƒë√≠ch:** - T·∫≠p trung v√†o **ki·∫øn tr√∫c** v√† **c∆° ch·∫ø** - Hi·ªÉu **data flow** v√† **token generation** - N·ªÅn t·∫£ng cho training trong sections ti·∫øp theo --- ## 8. √ù Nghƒ©a S∆∞ Ph·∫°m v√† Ph∆∞∆°ng Ph√°p Lu·∫≠n ### 8.1 Chi·∫øn L∆∞·ª£c H·ªçc T·∫≠p TƒÉng D·∫ßn C√°ch ti·∫øp c·∫≠n "5 models" trong kh√≥a h·ªçc: 1. **Model 1** (hi·ªán t·∫°i): Zero-layer transformer 2. **Model 2-5** (s·∫Øp t·ªõi): Th√™m d·∫ßn components - Attention mechanisms - Multi-head attention - Feed-forward networks - Layer normalization - Residual connections
$$

$$
**Nguy√™n t·∫Øc:** M·ªói model = Previous model + New components
$$

$$
### 8.2 Ph∆∞∆°ng Ph√°p Gi·∫£ng D·∫°y **Kh√¥ng ch·ªâ l√† code:** - **Experimentation:** Thay ƒë·ªïi hyperparameters - **Problem-solving:** Debug v√† fix errors - **Code exploration:** Hi·ªÉu m·ªçi d√≤ng code - **Exercise completion:** Th·ª±c h√†nh ch·ªß ƒë·ªông **S·∫Øp t·ªõi:** - C√°c videos chuy√™n s√¢u v·ªÅ t·ª´ng component - Unpacking technical details - Comparative analysis (GELU vs ReLU, Embedding vs Linear, etc.) --- ## 9. K·∫øt Lu·∫≠n ### 9.1 T√≥m T·∫Øt C√°c Ph√°t Hi·ªán Ch√≠nh 1. **X·ª≠ l√Ω ng·ªØ c·∫£nh:** - Token cu·ªëi c√πng ch·ª©a th√¥ng tin t·ªïng h·ª£p t·ª´ to√†n b·ªô sequence - Kh√¥ng ph·∫£i ch·ªâ s·ª≠ d·ª•ng m·ªôt token m√† l√† t·∫•t c·∫£ tokens 2. **Sinh token:** - Quy tr√¨nh auto-regressive v·ªõi multinomial sampling - C√¢n b·∫±ng gi·ªØa quality v√† diversity 3. **Ki·∫øn tr√∫c:** - Zero-layer transformer l√† n·ªÅn t·∫£ng ƒë∆°n gi·∫£n nh·∫•t - Chu·∫©n b·ªã cho vi·ªác x√¢y d·ª±ng full GPT-2 ### 9.2 H∆∞·ªõng Ph√°t Tri·ªÉn **C√°c b∆∞·ªõc ti·∫øp theo:** - Th√™m transformer blocks - Implement attention mechanisms - Incorporate positional encodings - Add normalization layers - Implement training procedures ### 9.3 ƒê√≥ng G√≥p H·ªçc Thu·∫≠t Nghi√™n c·ª©u n√†y cung c·∫•p: - Framework s∆∞ ph·∫°m cho vi·ªác gi·∫£ng d·∫°y LLM architecture - Ph√¢n t√≠ch chi ti·∫øt v·ªÅ information flow trong transformers - L√†m r√µ c√°c hi·ªÉu l·∫ßm ph·ªï bi·∫øn v·ªÅ context processing - Methodology cho vi·ªác x√¢y d·ª±ng m√¥ h√¨nh t·ª´ c∆° b·∫£n ƒë·∫øn ph·ª©c t·∫°p --- ## T√†i Li·ªáu Tham Kh·∫£o 1. Vaswani, A., et al. (2017). "Attention is All You Need." NeurIPS. 2. Radford, A., et al. (2019). "Language Models are Unsupervised Multitask Learners." OpenAI. 3. Hendrycks, D., & Gimpel, K. (2016). "Gaussian Error Linear Units (GELUs)." arXiv. --- ## Ph·ª• L·ª•c: Code Reference ### A.1 Model Definition ```python class Model1(nn.Module):
$$

$$
def __init__(self, vocab_size=100000, embed_dim=64):
$$

$$
super().__init__()
$$

$$
self.embeddings = nn.Embedding(vocab_size, embed_dim)
$$

$$

$$

self.gelu = nn.GELU()

$$

$$

$$
self.final_layer = nn.Linear(embed_dim, vocab_size)
$$

$$
### A.2 Generation Method ```python
$$

$$
def generate(self, tokens, n_new_tokens=30):
$$

$$
for _ in range(n_new_tokens):
$$

$$
x = self(tokens)
$$

$$

$$

final_logits = x[:, -1, :]

$$

$$

$$
probs = torch.softmax(final_logits, dim=-1)
$$

$$

$$

next_token = torch.multinomial(probs, num_samples=1)

$$

$$

$$
tokens = torch.cat([tokens, next_token], dim=1)
$$

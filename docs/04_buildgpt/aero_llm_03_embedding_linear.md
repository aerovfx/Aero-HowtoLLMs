
<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [04 buildgpt](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Ph√¢n T√≠ch K·ªπ Thu·∫≠t: So S√°nh `nn.Embedding` v√† `nn.Linear` trong PyTorch

## T√≥m t·∫Øt

B√†i vi·∫øt n√†y tr√¨nh b√†y ph√¢n t√≠ch chi ti·∫øt v·ªÅ hai l·ªõp PyTorch ƒë∆∞·ª£c s·ª≠ d·ª•ng ph·ªï bi·∫øn trong m√¥ h√¨nh ng√¥n ng·ªØ: `nn.Embedding` v√† `nn.Linear`. M·∫∑c d√π hai l·ªõp n√†y ph·ª•c v·ª• c√°c m·ª•c ƒë√≠ch kh√°c nhau trong ki·∫øn tr√∫c m√¥ h√¨nh (embedding cho token representations v√† linear cho unembedding transformations), nghi√™n c·ª©u ch·ª©ng minh r·∫±ng v·ªÅ b·∫£n ch·∫•t, ch√∫ng l√† c√°c wrapper kh√°c nhau xung quanh c√πng m·ªôt c·∫•u tr√∫c d·ªØ li·ªáu c∆° b·∫£n. Ph√¢n t√≠ch l√†m r√µ s·ª± kh√°c bi·ªát v·ªÅ c√∫ ph√°p, ph∆∞∆°ng th·ª©c kh·ªüi t·∫°o, v√† c∆° ch·∫ø truy c·∫≠p d·ªØ li·ªáu gi·ªØa hai l·ªõp n√†y.

---

## 1. Gi·ªõi Thi·ªáu

### 1.1 B·ªëi C·∫£nh

Trong ki·∫øn tr√∫c m√¥ h√¨nh ng√¥n ng·ªØ transformer, hai ph√©p bi·∫øn ƒë·ªïi quan tr·ªçng ƒë∆∞·ª£c th·ª±c hi·ªán:
- **Embedding**: Chuy·ªÉn ƒë·ªïi token indices th√†nh dense vectors
- **Unembedding**: √Ånh x·∫° hidden states tr·ªü l·∫°i vocabulary space

PyTorch cung c·∫•p hai l·ªõp ri√™ng bi·ªát cho c√°c ph√©p to√°n n√†y: `nn.Embedding` v√† `nn.Linear`, t·∫°o ra s·ª± nh·∫ßm l·∫´n v·ªÅ m·ªëi quan h·ªá v√† s·ª± kh√°c bi·ªát gi·ªØa ch√∫ng.

### 1.2 M·ª•c Ti√™u Nghi√™n C·ª©u

Nghi√™n c·ª©u n√†y nh·∫±m:
1. L√†m r√µ b·∫£n ch·∫•t c∆° b·∫£n gi·ªëng nhau c·ªßa hai l·ªõp
2. Ph√¢n t√≠ch c√°c kh√°c bi·ªát v·ªÅ implementation v√† usage
3. Ch·ª©ng minh t√≠nh t∆∞∆°ng ƒë∆∞∆°ng v·ªÅ m·∫∑t to√°n h·ªçc
4. Cung c·∫•p h∆∞·ªõng d·∫´n th·ª±c ti·ªÖn cho vi·ªác s·ª≠ d·ª•ng

---

## 2. C∆° S·ªü L√Ω Thuy·∫øt

### 2.1 Wrapper Classes trong PyTorch

**ƒê·ªãnh nghƒ©a:** Wrapper class l√† m·ªôt l·ªõp bao b·ªçc xung quanh m·ªôt object c∆° b·∫£n h∆°n, cung c·∫•p interface v√† functionality b·ªï sung.

**Nguy√™n l√Ω c·ªët l√µi:**
> C·∫£ `nn.Embedding` v√† `nn.Linear` ƒë·ªÅu l√† wrapper classes xung quanh `nn.Parameter`, m·ªôt tensor c√≥ kh·∫£ nƒÉng t√≠nh gradient v√† t√≠ch h·ª£p v√†o computational graph.

### 2.2 Vai Tr√≤ trong Ki·∫øn Tr√∫c LLM

Token IDs ‚Üí [nn.Embedding] ‚Üí Dense Vectors ‚Üí [Transformer Blocks] 
          ‚Üí Hidden States ‚Üí [nn.Linear] ‚Üí Logits ‚Üí Probabilities

**Ch·ª©c nƒÉng:**
- **nn.Embedding**: Token lookup operation
- **nn.Linear**: Matrix multiplication v·ªõi transpose

---

## 3. Ph√¢n T√≠ch K·ªπ Thu·∫≠t Chi Ti·∫øt

### 3.1 Khai B√°o v√† Kh·ªüi T·∫°o

#### 3.1.1 ƒê·ªãnh Nghƒ©a C√∫ Ph√°p

**nn.Embedding:**
```python

$$

$$

e = nn.Embedding(num_embeddings=5000, embedding_dim=70)

$$

$$

# C√∫ ph√°p: (vocab_size, embed_dim)
# Th·ª© t·ª±: INPUT ‚Üí OUTPUT

**nn.Linear:**
```python

$$

$$

l = nn.Linear(in_features=70, out_features=5000)

$$

$$

# C√∫ ph√°p: (embed_dim, vocab_size)
# Th·ª© t·ª±: INPUT ‚Üí OUTPUT (nh∆∞ng ƒë·∫£o ng∆∞·ª£c so v·ªõi Embedding)

**Quan s√°t:**
> Th·ª© t·ª± tham s·ªë b·ªã ƒë·∫£o ng∆∞·ª£c gi·ªØa hai l·ªõp, t·∫°o ra ngu·ªìn g√¢y nh·∫ßm l·∫´n l·ªõn. Tuy nhi√™n, k√≠ch th∆∞·ªõc th·ª±c t·∫ø c·ªßa weight matrix c∆° b·∫£n l√† gi·ªëng nhau.

#### 3.1.2 K√≠ch Th∆∞·ªõc Weight Matrix

**Verification:**
```python
print(e.weight.shape)  # torch.Size([5000, 70])
print(l.weight.shape)  # torch.Size([5000, 70])

**K·∫øt lu·∫≠n:** C·∫£ hai ƒë·ªÅu l∆∞u tr·ªØ ma tr·∫≠n c√≥ k√≠ch th∆∞·ªõc `[vocab_size, embedding_dim]`.

**Gi·∫£i th√≠ch l√Ω do ƒë·∫£o ng∆∞·ª£c:**

1. **nn.Embedding**: 
   - M·ªói token ID mapping ƒë·∫øn m·ªôt row trong matrix
   - Row i ch·ª©a embedding vector cho token i
   - Th·ª© t·ª± `(vocab, embed)` ph√π h·ª£p v·ªõi semantic c·ªßa "lookup table"

2. **nn.Linear**:
   - Th·ª±c hi·ªán ph√©p to√°n: `y = xW^T + b`
   - Matrix ƒë∆∞·ª£c transpose trong qu√° tr√¨nh t√≠nh to√°n
   - Th·ª© t·ª± `(in, out)` ph√π h·ª£p v·ªõi convention c·ªßa linear algebra

---

### 3.2 Ph√¢n T√≠ch Attributes v√† Methods

#### 3.2.1 S·ªë L∆∞·ª£ng Attributes

**Th·ªëng k√™:**
```python
len(dir(e))  # > 100 attributes
len(dir(l))  # > 100 attributes
# Nh∆∞ng kh√¥ng ho√†n to√†n gi·ªëng nhau

#### 3.2.2 Unique Attributes

**Ch·ªâ c√≥ trong nn.Embedding:**
- `_fill_padding_idx_with_zero`
- `max_norm`
- `norm_type`
- `scale_grad_by_freq`
- `sparse`
- `padding_idx`

**Ch·ªâ c√≥ trong nn.Linear:**
- `in_features`
- `out_features`
- `bias` (optional parameter)

**Ph√¢n t√≠ch:**
- Embedding c√≥ c√°c features ƒë·∫∑c bi·ªát cho text processing (padding, sparsity)
- Linear c√≥ bias term (optional), embedding kh√¥ng c√≥

---

### 3.3 C∆° Ch·∫ø Truy C·∫≠p D·ªØ Li·ªáu (Indexing)

#### 3.3.1 Direct Indexing v·ªõi nn.Embedding

**C√°ch 1: Implicit indexing**
```python
# L·∫•y embedding vector cho token index 14

$$

$$

vector = e(torch.tensor([14]))  # Shape: [1, 70]

$$

$$

**ƒê·∫∑c ƒëi·ªÉm:**
- C√∫ ph√°p ƒë∆°n gi·∫£n, tr·ª±c quan
- PyTorch t·ª± ƒë·ªông x·ª≠ l√Ω indexing
- Tr·∫£ v·ªÅ embedding vector tr·ª±c ti·∫øp

#### 3.3.2 Indexing v·ªõi nn.Linear

**C√°ch 1: Direct weight access (FAILS)**
```python

$$

$$

vector = l(14)  # TypeError: forward() missing required argument

$$

$$

**L√Ω do:** `nn.Linear` kh√¥ng support direct integer indexing.

**C√°ch 2: Manual weight indexing (WORKS)**
```python

$$

$$

vector = l.weight[14]  # Shape: [70]

$$

$$

**C√°ch 3: One-hot encoding emulation (MATHEMATICALLY EQUIVALENT)**
```python
# T·∫°o one-hot vector

$$

$$

one_hot = torch.zeros(5000)

$$

$$

$$
one_hot[14] = 1.0
$$

$$
# Matrix multiplication
$$

$$
vector = one_hot @ l.weight  # Shape: [70]
$$

$$
**Gi·∫£i th√≠ch to√°n h·ªçc:** Ph∆∞∆°ng ph√°p one-hot emulation m√¥ ph·ªèng ch√≠nh x√°c c√°ch `nn.Embedding` ho·∫°t ƒë·ªông:
$$

$$
\mathbf{v} = \mathbf{e}_i^T \mathbf{W}
$$

$$
Trong ƒë√≥: - \mathbf{e}_i = one-hot vector v·ªõi 1 ·ªü v·ªã tr√≠ i - \mathbf{W} = weight matrix [vocab_size √ó embed_dim] - \mathbf{v} = embedding vector k·∫øt qu·∫£ #### 3.3.3 B·∫£ng So S√°nh Ph∆∞∆°ng Ph√°p Indexing | Ph∆∞∆°ng ph√°p | nn.Embedding | nn.Linear | Hi·ªáu qu·∫£ | Use case | |-------------|--------------|-----------|----------|----------| | Direct call `(index)` | ‚úì | ‚úó | Cao | Standard embedding lookup | | `.weight[index]` | ‚úì | ‚úì | Cao | Manual weight access | | One-hot √ó weight | ‚úì | ‚úì | Th·∫•p | Educational, debugging | --- ### 3.4 Ph√¢n Ph·ªëi Kh·ªüi T·∫°o (Initialization Distribution) #### 3.4.1 Default Initialization **nn.Embedding (Normal Distribution):** ```python
$$

$$
e = nn.Embedding(5000, 70)
$$

$$
# Default: Normal(Œº=0, œÉ=1) **ƒê·∫∑c ƒëi·ªÉm ph√¢n ph·ªëi:** - Mean ‚âà 0 - Std ‚âà 1.0 - Gaussian/Normal distribution - Symmetric bell curve **nn.Linear (Uniform Distribution):** ```python
$$

$$
l = nn.Linear(70, 5000)
$$

$$
# Default: Uniform(-k, k) where k = sqrt(1/in_features) **ƒê·∫∑c ƒëi·ªÉm ph√¢n ph·ªëi:** - Kaiming Uniform initialization - Bounds: \pm \sqrt{\frac{1}{\text{in\_features}}} - V√≠ d·ª•: v·ªõi `in_features=70`, bounds ‚âà ¬±0.119 **Quan s√°t th·ª±c nghi·ªám:** ```python import matplotlib.pyplot as plt
$$

plt.hist(e.weight.flatten().detach().numpy(), bins=50)

$$
# H√¨nh d·∫°ng: Bell curve (Gaussian)
$$

plt.hist(l.weight.flatten().detach().numpy(), bins=50)

$$
# H√¨nh d·∫°ng: Flat-top (Uniform) #### 3.4.2 Custom Initialization **M·ª•c ti√™u:** L√†m cho `nn.Linear` c√≥ ph√¢n ph·ªëi gi·ªëng `nn.Embedding` **Implementation:** ```python
$$

l2 = nn.Linear(70, 5000)

$$

$$

torch.nn.init.normal_(l2.weight, mean=0.0, std=1.0)

$$
# Verification print(l2.weight.mean())  # ‚âà 0.0 print(l2.weight.std())   # ‚âà 1.0 **K·∫øt qu·∫£:** - Ph√¢n ph·ªëi c·ªßa `l2.weight` b√¢y gi·ªù match v·ªõi `e.weight` - Ch·ª©ng minh t√≠nh linh ho·∫°t c·ªßa initialization #### 3.4.3 Kaiming Initialization Analysis **C√¥ng th·ª©c Kaiming Uniform:**
$$

$$
\text{bound} = \sqrt{\frac{6}{(1 + a^2) \times \text{fan\_in}}}
$$

$$
V·ªõi ReLU a=0:
$$

$$
\text{bound} = \sqrt{\frac{6}{\text{fan\_in}}}
$$

$$
**Expected Statistics:** ```python import math
$$

$$
k = math.sqrt(1/70)  # k ‚âà 0.1195
$$

$$
# Uniform distribution [-k, k] # Expected mean: 0 # Expected std: k/sqrt(3) ‚âà 0.069 **Empirical verification:** ```python print(f"Theoretical std: {k/math.sqrt(3):.4f}") print(f"Actual std: {l.weight.std():.4f}") # Output shows close match --- ## 4. Ph√¢n T√≠ch Source Code ### 4.1 Underlying Implementation #### 4.1.1 nn.Embedding Source Code **Tr√≠ch xu·∫•t t·ª´ PyTorch source:** ```python class Embedding(Module): def __init__(self, num_embeddings, embedding_dim, ...): super(Embedding, self).__init__()
$$

$$
self.num_embeddings = num_embeddings
$$

$$

$$

self.embedding_dim = embedding_dim

$$

$$

        
        # KEY LINE: Creates weight matrix

$$

$$

self.weight = Parameter(

$$

$$

            torch.empty((num_embeddings, embedding_dim))
        )
        self.reset_parameters()

#### 4.1.2 nn.Linear Source Code

**Tr√≠ch xu·∫•t t·ª´ PyTorch source:**
```python
class Linear(Module):

$$

$$

def __init__(self, in_features, out_features, bias=True):

$$

$$

        super(Linear, self).__init__()

$$

$$

self.in_features = in_features

$$

$$

$$
self.out_features = out_features
$$

$$
# KEY LINE: Creates weight matrix
$$

$$
self.weight = Parameter(
$$

$$
torch.empty((out_features, in_features)) ) if bias:
$$

$$
self.bias = Parameter(torch.empty(out_features))
$$

$$
else: self.register_parameter('bias', None) #### 4.1.3 Common Core: nn.Parameter **C·∫£ hai ƒë·ªÅu s·ª≠ d·ª•ng:** ```python nn.Parameter(tensor) **ƒê·ªãnh nghƒ©a `nn.Parameter`:** - Subclass c·ªßa `torch.Tensor` - Automatically registered as model parameter - Included in `model.parameters()` - Tracked for gradient computation - Part of computational graph **K·∫øt lu·∫≠n quan tr·ªçng:** > S·ª± kh√°c bi·ªát gi·ªØa `nn.Embedding` v√† `nn.Linear` ch·ªâ l√† "syntactic sugar" - ch√∫ng ƒë·ªÅu d·ª±a tr√™n c√πng m·ªôt underlying mechanism (`nn.Parameter`) v√† ch·ªâ kh√°c nhau v·ªÅ interface v√† m·ªôt s·ªë features ƒë·∫∑c bi·ªát. --- ## 5. Computational Equivalence ### 5.1 Forward Pass Comparison #### 5.1.1 nn.Embedding Forward **Pseudocode:** ```python def embedding_forward(input_ids, weight): # input_ids: [batch_size] ho·∫∑c [batch_size, seq_len] # weight: [vocab_size, embed_dim]
$$

$$
output = weight[input_ids]  # Advanced indexing
$$

$$
return output **V√≠ d·ª•:** ```python
$$

$$
input_ids = torch.tensor([14, 27, 103])  # 3 tokens
$$

$$

$$

output = e(input_ids)  # Shape: [3, 70]

$$

$$

# Equivalent to: weight[[14, 27, 103], :]

#### 5.1.2 nn.Linear Forward (for unembedding)

**Pseudocode:**
```python

$$

$$

def linear_forward(input, weight, bias=None):

$$

$$

    # input: [batch_size, in_features]
    # weight: [out_features, in_features]
    # output: [batch_size, out_features]
    
$$

$$

output = input @ weight.T  # Matrix multiply with transpose

$$

$$

    if bias is not None:

$$
output += bias
$$

    return output

**V√≠ d·ª•:**
```python

$$

$$

hidden = torch.randn(3, 70)  # 3 samples, 70 dims

$$

$$

$$
logits = l(hidden)  # Shape: [3, 5000]
$$

$$
# Equivalent to: hidden @ l.weight.T + l.bias ### 5.2 Mathematical Operations #### 5.2.1 Embedding as Matrix Multiplication **Embedding operation c√≥ th·ªÉ ƒë∆∞·ª£c vi·∫øt l·∫°i:** ```python # Standard embedding
$$

$$
output = e(torch.tensor([14]))
$$

$$
# Equivalent one-hot multiplication
$$

$$
one_hot = F.one_hot(torch.tensor([14]), num_classes=5000).float()
$$

$$

$$

output_equiv = one_hot @ e.weight

$$

$$

assert torch.allclose(output, output_equiv)

**Complexity analysis:**
- Direct indexing: O(1) lookup
- One-hot multiplication: O(vocab_size √ó embed_dim)
- **Direct indexing l√† t·ªëi ∆∞u h∆°n r·∫•t nhi·ªÅu**

#### 5.2.2 Linear as Reverse Embedding

**Conceptually:**
Embedding:    Token ID ‚Üí Dense Vector
             [discrete] ‚Üí [continuous]

Linear:       Dense Vector ‚Üí Logits over Vocab
             [continuous] ‚Üí [discrete probabilities]

**Trong unembedding context:**
- Input: Hidden state [embed_dim]
- Weight: [vocab_size, embed_dim]
- Output: Logits [vocab_size]
- Operation: Dot product v·ªõi m·ªói vocab entry

---

## 6. Practical Implications

### 6.1 Memory Efficiency

**C·∫£ hai l·ªõp:**
- Store same-sized weight matrix: `vocab_size √ó embed_dim`
- Memory footprint: Identical
- Example: 50k vocab √ó 768 dim √ó 4 bytes = ~153 MB

### 6.2 Computational Efficiency

**nn.Embedding:**
```python
# Batch lookup: Very efficient

$$

$$

input_ids = torch.randint(0, 5000, (32, 512))  # [batch, seq]

$$

$$

$$
output = e(input_ids)  # [32, 512, 70]
$$

$$
# Operation: Simple indexing, O(batch √ó seq) **nn.Linear:** ```python # Batch matrix multiplication
$$

$$
hidden = torch.randn(32, 512, 70)  # [batch, seq, hidden]
$$

$$

$$

logits = l(hidden)  # [32, 512, 5000]

$$

$$

# Operation: GEMM, O(batch √ó seq √ó hidden √ó vocab)

**Performance consideration:**
- Embedding lookup: Extremely fast
- Linear transformation: Depends on matrix sizes
- Bottleneck in LLMs: Usually the unembedding step

### 6.3 Gradient Flow

**C·∫£ hai support:**
- Automatic differentiation
- Backpropagation through weights
- Gradient accumulation
- Optimizer updates

**Embedding-specific:**
```python
# Sparse gradients: Only update accessed embeddings

$$

$$

optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

$$

$$

# Only rows corresponding to input tokens get gradient updates

**Linear:**
```python
# Dense gradients: All weights potentially updated
# Full matrix receives gradients every forward pass

---

## 7. Best Practices v√† Recommendations

### 7.1 Khi N√†o D√πng nn.Embedding

**Use cases:**
1. Token-to-vector mapping (standard embedding layer)
2. Lookup tables cho discrete entities
3. Khi c·∫ßn padding_idx functionality
4. Khi c·∫ßn sparse gradients
5. Positional embeddings

**V√≠ d·ª•:**
```python

$$

$$

token_embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)

$$

$$

$$
position_embedding = nn.Embedding(max_seq_len, hidden_dim)
$$

$$
### 7.2 Khi N√†o D√πng nn.Linear **Use cases:** 1. Unembedding layer (hidden ‚Üí logits) 2. Feed-forward layers trong transformer 3. Projection layers (Q, K, V trong attention) 4. Khi c·∫ßn bias term 5. B·∫•t k·ª≥ dense transformation n√†o **V√≠ d·ª•:** ```python
$$

$$
unembedding = nn.Linear(hidden_dim, vocab_size, bias=False)
$$

$$

$$

ffn = nn.Linear(hidden_dim, ffn_dim)

$$

$$

$$
query_proj = nn.Linear(hidden_dim, head_dim)
$$

$$
### 7.3 Weight Tying (Li√™n k·∫øt Tr·ªçng S·ªë) **Advanced technique:** ```python class TransformerLM(nn.Module): def __init__(self, vocab_size, hidden_dim): super().__init__()
$$

$$
self.embedding = nn.Embedding(vocab_size, hidden_dim)
$$

$$

$$

self.unembedding = nn.Linear(hidden_dim, vocab_size, bias=False)

$$

$$

        
        # WEIGHT TYING: Share weights

$$

$$

self.unembedding.weight = self.embedding.weight

$$

$$

**L·ª£i √≠ch:**
- Gi·∫£m 50% s·ªë parameters
- Regularization effect
- Embedding v√† unembedding h·ªçc c√πng representation
- ƒê∆∞·ª£c s·ª≠ d·ª•ng trong nhi·ªÅu LLMs (GPT-2, BERT variants)

**L∆∞u √Ω:** Weight tying ch·ª©ng minh r√µ r√†ng r·∫±ng embedding v√† linear weights l√† interchangeable.

---

## 8. Common Pitfalls v√† Troubleshooting

### 8.1 Dimension Ordering Confusion

**Problem:**
```python
# WRONG

$$

$$

e = nn.Embedding(70, 5000)  # Swapped!

$$

$$

$$
l = nn.Linear(5000, 70)      # Swapped!
$$

$$
# CORRECT
$$

$$
e = nn.Embedding(5000, 70)   # (vocab, embed)
$$

$$

$$

l = nn.Linear(70, 5000)      # (in, out)

$$

$$

**Solution:** Always double-check parameter order and verify v·ªõi `.weight.shape`.

### 8.2 Indexing Errors

**Problem:**
```python

$$

$$

l = nn.Linear(70, 5000)

$$

$$

$$
vector = l(14)  # TypeError!
$$

$$
**Solution:** ```python # Option 1: Direct weight access
$$

$$
vector = l.weight[14]
$$

$$
# Option 2: Use as intended
$$

$$
hidden = torch.randn(1, 70)
$$

$$

$$

logits = l(hidden)

$$

$$

### 8.3 Initialization Mismatch

**Problem:**
```python

$$

$$

e = nn.Embedding(5000, 70)  # Normal distribution

$$

$$

$$
l = nn.Linear(70, 5000)     # Uniform distribution
$$

$$
# Different initializations may cause training issues **Solution:** ```python # Standardize initialization
$$

torch.nn.init.normal_(l.weight, mean=0.0, std=1.0)

$$
# Or use Xavier/Kaiming consistently --- ## 9. K·∫øt Lu·∫≠n ### 9.1 T√≥m T·∫Øt Findings **ƒêi·ªÉm gi·ªëng nhau:** 1. C·∫£ hai ƒë·ªÅu wrapper xung quanh `nn.Parameter` 2. Weight matrix c√≥ c√πng k√≠ch th∆∞·ªõc 3. C√≥ th·ªÉ emulate functionality c·ªßa nhau 4. Support automatic differentiation 5. Part of computational graph **ƒêi·ªÉm kh√°c nhau:** | Aspect | nn.Embedding | nn.Linear | |--------|--------------|-----------| | Parameter order | (vocab, embed) | (in, out) | | Direct indexing | ‚úì | ‚úó | | Bias term | ‚úó | ‚úì (optional) | | Default init | Normal(0,1) | Kaiming Uniform | | Primary use | Token lookup | Matrix transformation | | Sparse gradients | ‚úì (possible) | ‚úó | | Padding support | ‚úì | ‚úó | ### 9.2 Practical Wisdom **Key Insight:** > S·ª± kh√°c bi·ªát gi·ªØa `nn.Embedding` v√† `nn.Linear` kh√¥ng ph·∫£i v·ªÅ to√°n h·ªçc hay ki·∫øn tr√∫c c∆° b·∫£n, m√† v·ªÅ **convenience v√† optimization cho specific use cases**. Hi·ªÉu r√µ ƒëi·ªÅu n√†y gi√∫p developer s·ª≠ d·ª•ng ƒë√∫ng tool cho ƒë√∫ng task. ### 9.3 Educational Value Vi·ªác ph√¢n t√≠ch s√¢u v·ªÅ hai l·ªõp n√†y minh h·ªça m·ªôt nguy√™n l√Ω quan tr·ªçng trong deep learning frameworks: **"Under the hood simplicity with surface-level convenience"** PyTorch (v√† c√°c frameworks kh√°c) cung c·∫•p multiple interfaces cho c√πng m·ªôt underlying operation, optimized cho different contexts v√† use patterns. --- ## 10. Directions for Further Study ### 10.1 Advanced Topics 1. **Sparse embeddings**: Hi·ªáu qu·∫£ memory v·ªõi large vocabularies 2. **Quantization**: Gi·∫£m precision ƒë·ªÉ tƒÉng speed 3. **Custom initialization schemes**: Xavier, He, orthogonal 4. **Gradient clipping**: Stabilize training v·ªõi embeddings 5. **Embedding regularization**: L2 penalty, dropout ### 10.2 Related Concepts - **Weight tying** trong language models - **Positional embeddings** (learned vs sinusoidal) - **Subword tokenization** impact l√™n embedding size - **Low-rank factorization** of embedding matrices - **Contextual embeddings** (BERT-style) vs static --- ## T√†i Li·ªáu Tham Kh·∫£o 1. Paszke, A., et al. (2019). "PyTorch: An Imperative Style, High-Performance Deep Learning Library." NeurIPS. 2. He, K., et al. (2015). "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification." ICCV. 3. Glorot, X., & Bengio, Y. (2010). "Understanding the difficulty of training deep feedforward neural networks." AISTATS. 4. Press, O., & Wolf, L. (2017). "Using the Output Embedding to Improve Language Models." EACL. --- ## Ph·ª• L·ª•c: Complete Code Examples ### A.1 Basic Setup ```python import torch import torch.nn as nn
$$

$$
vocab_size = 5000
$$

$$

$$

embed_dim = 70

$$

$$

$$
e = nn.Embedding(vocab_size, embed_dim)
$$

$$

$$

l = nn.Linear(embed_dim, vocab_size)

$$

$$

### A.2 Equivalence Demonstration
```python
# Method 1: Embedding

$$
idx = 14
$$

$$
emb_output = e(torch.tensor([idx]))
$$

$$
# Method 2: Linear with one-hot
$$

$$
one_hot = torch.zeros(vocab_size)
$$

$$

$$

one_hot[idx] = 1.0

$$

$$

$$
lin_output = one_hot @ l.weight
$$

$$
# Method 3: Direct indexing
$$

$$
direct = l.weight[idx]
$$

$$
# All should be equivalent (except shape) ### A.3 Weight Tying Example ```python class TiedModel(nn.Module): def __init__(self, vocab_size, embed_dim): super().__init__()
$$

$$
self.embed = nn.Embedding(vocab_size, embed_dim)
$$

$$

$$

self.linear = nn.Linear(embed_dim, vocab_size, bias=False)

$$

$$

$$
self.linear.weight = self.embed.weight  # Tie weights
$$

$$
def forward(self, input_ids):
$$

$$
embedded = self.embed(input_ids)
$$

$$

$$

logits = self.linear(embedded)

$$

$$

        return logits

---

**T·ª´ kh√≥a:** PyTorch, nn.Embedding, nn.Linear, Weight Matrix, Token Embedding, Unembedding, Parameter Initialization, Kaiming Initialization, Weight Tying, Computational Graph, Automatic Differentiation, Deep Learning Framework
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [M·ªü r·ªông Ki·∫øn tr√∫c GPT: Position Embedding, Layer Normalization, Weight Tying v√† Temperature Scaling](aero_llm_010_posion_embedding.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_010_posion_embedding.md) |
| [Bi·ªÉu di·ªÖn T√≠nh Nh√¢n Qu·∫£ Th·ªùi Gian trong C∆° Ch·∫ø Attention b·∫±ng ƒê·∫°i S·ªë Tuy·∫øn T√≠nh](aero_llm_011_temporal_causality_via_linear_algebra_theory_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_011_temporal_causality_via_linear_algebra_theory_.md) |
| [C∆° Ch·∫ø Trung B√¨nh H√≥a Qu√° Kh·ª© v√† Lo·∫°i B·ªè T∆∞∆°ng Lai trong M√¥ H√¨nh Ng√¥n Ng·ªØ Nh√¢n Qu·∫£](aero_llm_012_averaging_the_past_while_ignoring_the_future.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_012_averaging_the_past_while_ignoring_the_future.md) |
| [Thu·∫≠t To√°n Attention trong M√¥ H√¨nh Transformer: C∆° S·ªü L√Ω Thuy·∫øt, C∆° Ch·∫ø Ho·∫°t ƒê·ªông v√† H√†m √ù ·ª®ng D·ª•ng](aero_llm_013_the_attention_algorithm_theory_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_013_the_attention_algorithm_theory_.md) |
| [Ph√¢n T√≠ch v√† Tri·ªÉn Khai C∆° Ch·∫ø Attention: So S√°nh C√†i ƒê·∫∑t Th·ªß C√¥ng v√† PyTorch T·ªëi ∆Øu](aero_llm_014_codechallenge_code_attention.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_014_codechallenge_code_attention.md) |
| [Ph√¢n T√≠ch Ki·∫øn Tr√∫c M√¥ H√¨nh Ng√¥n Ng·ªØ v·ªõi M·ªôt Attention Head: L√Ω Thuy·∫øt, Tri·ªÉn Khai v√† ƒê√°nh Gi√°](aero_llm_015_model.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_015_model.md) |
| [Ph√¢n T√≠ch C·∫•u Tr√∫c Transformer Block: L√Ω Thuy·∫øt, C∆° Ch·∫ø Bi·ªÉu Di·ªÖn v√† Vai Tr√≤ Trong M√¥ H√¨nh Ng√¥n Ng·ªØ](aero_llm_016_the_transformer_block_theory_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_016_the_transformer_block_theory_.md) |
| [C√†i ƒê·∫∑t Transformer Block B·∫±ng PyTorch: Ph√¢n T√≠ch Ki·∫øn Tr√∫c, Lu·ªìng D·ªØ Li·ªáu v√† T·ªëi ∆Øu H√≥a](aero_llm_017_the_transformer_block_code_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_017_the_transformer_block_code_.md) |
| [M√¥ H√¨nh Nhi·ªÅu Transformer Blocks Trong M·∫°ng Ng√¥n Ng·ªØ: Ki·∫øn Tr√∫c, Ph√¢n C·∫•p Bi·ªÉu Di·ªÖn v√† Kh·∫£ NƒÉng M·ªü R·ªông](aero_llm_018_model_4_multiple_transformer_blocks_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_018_model_4_multiple_transformer_blocks_.md) |
| [aero llm 019 copy 10](aero_llm_019_copy_10.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_019_copy_10.md) |
| [aero llm 019 copy 11](aero_llm_019_copy_11.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_019_copy_11.md) |
| [aero llm 019 copy 12](aero_llm_019_copy_12.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_019_copy_12.md) |
| [aero llm 019 copy 13](aero_llm_019_copy_13.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_019_copy_13.md) |
| [aero llm 019 copy 9](aero_llm_019_copy_9.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_019_copy_9.md) |
| [Multi-Head Attention: C∆° S·ªü L√Ω Thuy·∫øt v√† Tri·ªÉn Khai Th·ª±c Ti·ªÖn](aero_llm_019_multihead_attention_theory_and_implementation.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_019_multihead_attention_theory_and_implementation.md) |
| [aero llm 01 intro](aero_llm_01_intro.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_01_intro.md) |
| [T·ªëi ∆Øu H√≥a Hu·∫•n Luy·ªán M√¥ H√¨nh H·ªçc S√¢u B·∫±ng GPU: Nguy√™n L√Ω v√† Th·ª±c H√†nh](aero_llm_020_working_on_the_gpu.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_020_working_on_the_gpu.md) |
| [Tri·ªÉn Khai M√¥ H√¨nh GPT-2 Ho√†n Ch·ªânh Tr√™n GPU: Ki·∫øn Tr√∫c, T·ªëi ∆Øu H√≥a v√† ƒê√°nh Gi√° Hi·ªáu NƒÉng](aero_llm_021_mo_hinh_gpt_2_hoan_chinh_tren_gpu.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_021_mo_hinh_gpt_2_hoan_chinh_tren_gpu.md) |
| [ƒê√°nh Gi√° Hi·ªáu NƒÉng GPT-2 Tr√™n CPU v√† GPU: Th·ª±c Nghi·ªám Th·ªùi Gian Kh·ªüi T·∫°o, Suy Lu·∫≠n v√† Hu·∫•n Luy·ªán](aero_llm_022_anh_gia_hieu_nang_gpt_2_tren_cpu_va_gpu.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_022_anh_gia_hieu_nang_gpt_2_tren_cpu_va_gpu.md) |
| [Kh·∫£o S√°t M√¥ H√¨nh GPT-2 Ti·ªÅn Hu·∫•n Luy·ªán c·ªßa OpenAI: Ki·∫øn Tr√∫c, Tham S·ªë v√† C∆° Ch·∫ø Sinh VƒÉn B·∫£n](aero_llm_023_inspecting_openai_s_gpt2.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_023_inspecting_openai_s_gpt2.md) |
| [Ki·∫øn Tr√∫c Transformer v√† Tri·ªÉn Khai GPT-2 tr√™n GPU: Ph√¢n T√≠ch To√°n H·ªçc v√† Hi·ªáu NƒÉng T√≠nh To√°n](aero_llm_024_summarizing_gpt_using_equations.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_024_summarizing_gpt_using_equations.md) |
| [Tr·ª±c Quan H√≥a Ki·∫øn Tr√∫c GPT Th√¥ng Qua nano-GPT: Ti·∫øp C·∫≠n Tr·ª±c Quan trong Nghi√™n C·ª©u M√¥ H√¨nh Ng√¥n Ng·ªØ](aero_llm_025_visualizing_nano_gpt.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_025_visualizing_nano_gpt.md) |
| [Ph√¢n T√≠ch S·ªë L∆∞·ª£ng Tham S·ªë Trong M√¥ H√¨nh GPT-2: Ph∆∞∆°ng Ph√°p ƒê·ªãnh L∆∞·ª£ng v√† √ù Nghƒ©a Ki·∫øn Tr√∫c](aero_llm_026_codechallenge_how_many_parameters_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_026_codechallenge_how_many_parameters_part_1_.md) |
| [Ph√¢n B·ªë Tham S·ªë Trong GPT-2: So S√°nh Attention, MLP v√† Layer Normalization](aero_llm_027_codechallenge_how_many_parameters_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_027_codechallenge_how_many_parameters_part_2_.md) |
| [üìò Ph√¢n T√≠ch Ki·∫øn Tr√∫c GPT-2: T·ª´ C∆° Ch·∫ø Multi-Head Attention ƒê·∫øn Hi·ªáu NƒÉng T√≠nh To√°n Tr√™n GPU](aero_llm_028_codechallenge_gpt2_trained_weights_distributions.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_028_codechallenge_gpt2_trained_weights_distributions.md) |
| [üß† Ph√¢n T√≠ch Nh√¢n Qu·∫£ Trong GPT-2: Vai Tr√≤ C·ªßa Ma Tr·∫≠n Query Th√¥ng Qua Can Thi·ªáp Tham S·ªë](aero_llm_029_codechallenge_do_we_really_need_q.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_029_codechallenge_do_we_really_need_q.md) |
| [Ph√¢n T√≠ch Ki·∫øn Tr√∫c v√† C∆° Ch·∫ø Ho·∫°t ƒê·ªông c·ªßa M√¥ H√¨nh Ng√¥n Ng·ªØ Transformer C∆° B·∫£n](aero_llm_02_transformer.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_02_transformer.md) |
| üìå **[Ph√¢n T√≠ch K·ªπ Thu·∫≠t: So S√°nh `nn.Embedding` v√† `nn.Linear` trong PyTorch](aero_llm_03_embedding_linear.md)** | [Xem b√†i vi·∫øt ‚Üí](aero_llm_03_embedding_linear.md) |
| [Ph√¢n T√≠ch So S√°nh H√†m K√≠ch Ho·∫°t GELU v√† ReLU trong M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn: G√≥c Nh√¨n L√Ω Thuy·∫øt v√† Th·ª±c Nghi·ªám](aero_llm_04_gelu_vs_relu_academic_analysis.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_04_gelu_vs_relu_academic_analysis.md) |
| [H√†m Softmax v√† Tham S·ªë Temperature trong M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn: Ph√¢n T√≠ch To√°n H·ªçc v√† Th·ª±c Nghi·ªám](aero_llm_05_softmax_temperature_academic_analysis.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_05_softmax_temperature_academic_analysis.md) |
| [Ph√¢n T√≠ch `torch.multinomial`: L·∫•y M·∫´u X√°c Su·∫•t trong Sinh VƒÉn B·∫£n v·ªõi PyTorch](aero_llm_06_torch_multinomial_academic_analysis.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_06_torch_multinomial_academic_analysis.md) |
| [Ph∆∞∆°ng Ph√°p L·∫•y M·∫´u Token trong Sinh VƒÉn B·∫£n: Ph√¢n T√≠ch So S√°nh Greedy, Top-K, Top-P v√† Multinomial Sampling](aero_llm_07_token_sampling_methods.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_07_token_sampling_methods.md) |
| [Ph√¢n T√≠ch H√†nh Vi C·ªßa H√†m Softmax Trong M√¥ H√¨nh H·ªçc S√¢u: ·∫¢nh H∆∞·ªüng C·ªßa L·∫∑p, Ph·∫°m Vi S·ªë H·ªçc V√† Nhi·ªát ƒê·ªô](aero_llm_08_ham_softbank.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_08_ham_softbank.md) |
| [Ph√¢n T√≠ch Layer Normalization Trong H·ªçc S√¢u: C∆° S·ªü L√Ω Thuy·∫øt, ·ªîn ƒê·ªãnh S·ªë H·ªçc V√† ·ª®ng D·ª•ng Th·ª±c Ti·ªÖn](aero_llm_09_layer_normalization.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_09_layer_normalization.md) |
| [kien truc mo hinh ngon ngu lon](kien_truc_mo_hinh_ngon_ngu_lon.md) | [Xem b√†i vi·∫øt ‚Üí](kien_truc_mo_hinh_ngon_ngu_lon.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [04 buildgpt](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# PhÃ¢n TÃ­ch Ká»¹ Thuáº­t: So SÃ¡nh `nn.Embedding` vÃ  `nn.Linear` trong PyTorch

## TÃ³m táº¯t

BÃ i viáº¿t nÃ y trÃ¬nh bÃ y phÃ¢n tÃ­ch chi tiáº¿t vá» hai lá»›p PyTorch Ä‘Æ°á»£c sá»­ dá»¥ng phá»• biáº¿n trong mÃ´ hÃ¬nh ngÃ´n ngá»¯: `nn.Embedding` vÃ  `nn.Linear`. Máº·c dÃ¹ hai lá»›p nÃ y phá»¥c vá»¥ cÃ¡c má»¥c Ä‘Ã­ch khÃ¡c nhau trong kiáº¿n trÃºc mÃ´ hÃ¬nh (embedding cho token representations vÃ  linear cho unembedding transformations), nghiÃªn cá»©u chá»©ng minh ráº±ng vá» báº£n cháº¥t, chÃºng lÃ  cÃ¡c wrapper khÃ¡c nhau xung quanh cÃ¹ng má»™t cáº¥u trÃºc dá»¯ liá»‡u cÆ¡ báº£n. PhÃ¢n tÃ­ch lÃ m rÃµ sá»± khÃ¡c biá»‡t vá» cÃº phÃ¡p, phÆ°Æ¡ng thá»©c khá»Ÿi táº¡o, vÃ  cÆ¡ cháº¿ truy cáº­p dá»¯ liá»‡u giá»¯a hai lá»›p nÃ y.

---

## 1. Giá»›i Thiá»‡u

### 1.1 Bá»‘i Cáº£nh

Trong kiáº¿n trÃºc mÃ´ hÃ¬nh ngÃ´n ngá»¯ transformer, hai phÃ©p biáº¿n Ä‘á»•i quan trá»ng Ä‘Æ°á»£c thá»±c hiá»‡n:
- **Embedding**: Chuyá»ƒn Ä‘á»•i token indices thÃ nh dense vectors
- **Unembedding**: Ãnh xáº¡ hidden states trá»Ÿ láº¡i vocabulary space

PyTorch cung cáº¥p hai lá»›p riÃªng biá»‡t cho cÃ¡c phÃ©p toÃ¡n nÃ y: `nn.Embedding` vÃ  `nn.Linear`, táº¡o ra sá»± nháº§m láº«n vá» má»‘i quan há»‡ vÃ  sá»± khÃ¡c biá»‡t giá»¯a chÃºng.

### 1.2 Má»¥c TiÃªu NghiÃªn Cá»©u

NghiÃªn cá»©u nÃ y nháº±m:
1. LÃ m rÃµ báº£n cháº¥t cÆ¡ báº£n giá»‘ng nhau cá»§a hai lá»›p
2. PhÃ¢n tÃ­ch cÃ¡c khÃ¡c biá»‡t vá» implementation vÃ  usage
3. Chá»©ng minh tÃ­nh tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá» máº·t toÃ¡n há»c
4. Cung cáº¥p hÆ°á»›ng dáº«n thá»±c tiá»…n cho viá»‡c sá»­ dá»¥ng

---

## 2. CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t

### 2.1 Wrapper Classes trong PyTorch

**Äá»‹nh nghÄ©a:** Wrapper class lÃ  má»™t lá»›p bao bá»c xung quanh má»™t object cÆ¡ báº£n hÆ¡n, cung cáº¥p interface vÃ  functionality bá»• sung.

**NguyÃªn lÃ½ cá»‘t lÃµi:**
> Cáº£ `nn.Embedding` vÃ  `nn.Linear` Ä‘á»u lÃ  wrapper classes xung quanh `nn.Parameter`, má»™t tensor cÃ³ kháº£ nÄƒng tÃ­nh gradient vÃ  tÃ­ch há»£p vÃ o computational graph.

### 2.2 Vai TrÃ² trong Kiáº¿n TrÃºc LLM

Token IDs â†’ [nn.Embedding] â†’ Dense Vectors â†’ [Transformer Blocks] 
          â†’ Hidden States â†’ [nn.Linear] â†’ Logits â†’ Probabilities

**Chá»©c nÄƒng:**
- **nn.Embedding**: Token lookup operation
- **nn.Linear**: Matrix multiplication vá»›i transpose

---

## 3. PhÃ¢n TÃ­ch Ká»¹ Thuáº­t Chi Tiáº¿t

### 3.1 Khai BÃ¡o vÃ  Khá»Ÿi Táº¡o

#### 3.1.1 Äá»‹nh NghÄ©a CÃº PhÃ¡p

**nn.Embedding:**
```python

e = nn.Embedding(num_embeddings=5000, embedding_dim=70)

# CÃº phÃ¡p: (vocab_size, embed_dim)
# Thá»© tá»±: INPUT â†’ OUTPUT

**nn.Linear:**
```python

l = nn.Linear(in_features=70, out_features=5000)

# CÃº phÃ¡p: (embed_dim, vocab_size)
# Thá»© tá»±: INPUT â†’ OUTPUT (nhÆ°ng Ä‘áº£o ngÆ°á»£c so vá»›i Embedding)

**Quan sÃ¡t:**
> Thá»© tá»± tham sá»‘ bá»‹ Ä‘áº£o ngÆ°á»£c giá»¯a hai lá»›p, táº¡o ra nguá»“n gÃ¢y nháº§m láº«n lá»›n. Tuy nhiÃªn, kÃ­ch thÆ°á»›c thá»±c táº¿ cá»§a weight matrix cÆ¡ báº£n lÃ  giá»‘ng nhau.

#### 3.1.2 KÃ­ch ThÆ°á»›c Weight Matrix

**Verification:**
```python
print(e.weight.shape)  # torch.Size([5000, 70])
print(l.weight.shape)  # torch.Size([5000, 70])

**Káº¿t luáº­n:** Cáº£ hai Ä‘á»u lÆ°u trá»¯ ma tráº­n cÃ³ kÃ­ch thÆ°á»›c `[vocab_size, embedding_dim]`.

**Giáº£i thÃ­ch lÃ½ do Ä‘áº£o ngÆ°á»£c:**

1. **nn.Embedding**: 
   - Má»—i token ID mapping Ä‘áº¿n má»™t row trong matrix
   - Row i chá»©a embedding vector cho token i
   - Thá»© tá»± `(vocab, embed)` phÃ¹ há»£p vá»›i semantic cá»§a "lookup table"

2. **nn.Linear**:
   - Thá»±c hiá»‡n phÃ©p toÃ¡n: `y = xW^T + b`
   - Matrix Ä‘Æ°á»£c transpose trong quÃ¡ trÃ¬nh tÃ­nh toÃ¡n
   - Thá»© tá»± `(in, out)` phÃ¹ há»£p vá»›i convention cá»§a linear algebra

---

### 3.2 PhÃ¢n TÃ­ch Attributes vÃ  Methods

#### 3.2.1 Sá»‘ LÆ°á»£ng Attributes

**Thá»‘ng kÃª:**
```python
len(dir(e))  # > 100 attributes
len(dir(l))  # > 100 attributes
# NhÆ°ng khÃ´ng hoÃ n toÃ n giá»‘ng nhau

#### 3.2.2 Unique Attributes

**Chá»‰ cÃ³ trong nn.Embedding:**
- `_fill_padding_idx_with_zero`
- `max_norm`
- `norm_type`
- `scale_grad_by_freq`
- `sparse`
- `padding_idx`

**Chá»‰ cÃ³ trong nn.Linear:**
- `in_features`
- `out_features`
- `bias` (optional parameter)

**PhÃ¢n tÃ­ch:**
- Embedding cÃ³ cÃ¡c features Ä‘áº·c biá»‡t cho text processing (padding, sparsity)
- Linear cÃ³ bias term (optional), embedding khÃ´ng cÃ³

---

### 3.3 CÆ¡ Cháº¿ Truy Cáº­p Dá»¯ Liá»‡u (Indexing)

#### 3.3.1 Direct Indexing vá»›i nn.Embedding

**CÃ¡ch 1: Implicit indexing**
```python
# Láº¥y embedding vector cho token index 14

vector = e(torch.tensor([14]))  # Shape: [1, 70]

**Äáº·c Ä‘iá»ƒm:**
- CÃº phÃ¡p Ä‘Æ¡n giáº£n, trá»±c quan
- PyTorch tá»± Ä‘á»™ng xá»­ lÃ½ indexing
- Tráº£ vá» embedding vector trá»±c tiáº¿p

#### 3.3.2 Indexing vá»›i nn.Linear

**CÃ¡ch 1: Direct weight access (FAILS)**
```python

vector = l(14)  # TypeError: forward() missing required argument

**LÃ½ do:** `nn.Linear` khÃ´ng support direct integer indexing.

**CÃ¡ch 2: Manual weight indexing (WORKS)**
```python

vector = l.weight[14]  # Shape: [70]

**CÃ¡ch 3: One-hot encoding emulation (MATHEMATICALLY EQUIVALENT)**
```python
# Táº¡o one-hot vector

one_hot = torch.zeros(5000)

$$
one_hot[14] = 1.0 # Matrix multiplication vector = one_hot @ l.weight  # Shape: [70] **Giáº£i thÃ­ch toÃ¡n há»c:** PhÆ°Æ¡ng phÃ¡p one-hot emulation mÃ´ phá»ng chÃ­nh xÃ¡c cÃ¡ch `nn.Embedding` hoáº¡t Ä‘á»™ng: \mathbf{v} = \mathbf{e}_i^T \mathbf{W} Trong Ä‘Ã³: - \mathbf{e}_i = one-hot vector vá»›i 1 á»Ÿ vá»‹ trÃ­ i - \mathbf{W} = weight matrix [vocab_size Ã— embed_dim] - \mathbf{v} = embedding vector káº¿t quáº£ #### 3.3.3 Báº£ng So SÃ¡nh PhÆ°Æ¡ng PhÃ¡p Indexing | PhÆ°Æ¡ng phÃ¡p | nn.Embedding | nn.Linear | Hiá»‡u quáº£ | Use case | |-------------|--------------|-----------|----------|----------| | Direct call `(index)` | âœ“ | âœ— | Cao | Standard embedding lookup | | `.weight[index]` | âœ“ | âœ“ | Cao | Manual weight access | | One-hot Ã— weight | âœ“ | âœ“ | Tháº¥p | Educational, debugging | --- ### 3.4 PhÃ¢n Phá»‘i Khá»Ÿi Táº¡o (Initialization Distribution) #### 3.4.1 Default Initialization **nn.Embedding (Normal Distribution):** ```python e = nn.Embedding(5000, 70) # Default: Normal(Î¼=0, Ïƒ=1) **Äáº·c Ä‘iá»ƒm phÃ¢n phá»‘i:** - Mean â‰ˆ 0 - Std â‰ˆ 1.0 - Gaussian/Normal distribution - Symmetric bell curve **nn.Linear (Uniform Distribution):** ```python l = nn.Linear(70, 5000) # Default: Uniform(-k, k) where k = sqrt(1/in_features) **Äáº·c Ä‘iá»ƒm phÃ¢n phá»‘i:** - Kaiming Uniform initialization - Bounds: \pm \sqrt{\frac{1}{\text{in\_features}}} - VÃ­ dá»¥: vá»›i `in_features=70`, bounds â‰ˆ Â±0.119 **Quan sÃ¡t thá»±c nghiá»‡m:** ```python import matplotlib.pyplot as plt
$$

plt.hist(e.weight.flatten().detach().numpy(), bins=50)

$$
# HÃ¬nh dáº¡ng: Bell curve (Gaussian)
$$

plt.hist(l.weight.flatten().detach().numpy(), bins=50)

$$
# HÃ¬nh dáº¡ng: Flat-top (Uniform) #### 3.4.2 Custom Initialization **Má»¥c tiÃªu:** LÃ m cho `nn.Linear` cÃ³ phÃ¢n phá»‘i giá»‘ng `nn.Embedding` **Implementation:** ```python
$$

l2 = nn.Linear(70, 5000)

torch.nn.init.normal_(l2.weight, mean=0.0, std=1.0)

$$
# Verification print(l2.weight.mean())  # â‰ˆ 0.0 print(l2.weight.std())   # â‰ˆ 1.0 **Káº¿t quáº£:** - PhÃ¢n phá»‘i cá»§a `l2.weight` bÃ¢y giá» match vá»›i `e.weight` - Chá»©ng minh tÃ­nh linh hoáº¡t cá»§a initialization #### 3.4.3 Kaiming Initialization Analysis **CÃ´ng thá»©c Kaiming Uniform:** \text{bound} = \sqrt{\frac{6}{(1 + a^2) \times \text{fan\_in}}} Vá»›i ReLU a=0: \text{bound} = \sqrt{\frac{6}{\text{fan\_in}}} **Expected Statistics:** ```python import math k = math.sqrt(1/70)  # k â‰ˆ 0.1195 # Uniform distribution [-k, k] # Expected mean: 0 # Expected std: k/sqrt(3) â‰ˆ 0.069 **Empirical verification:** ```python print(f"Theoretical std: {k/math.sqrt(3):.4f}") print(f"Actual std: {l.weight.std():.4f}") # Output shows close match --- ## 4. PhÃ¢n TÃ­ch Source Code ### 4.1 Underlying Implementation #### 4.1.1 nn.Embedding Source Code **TrÃ­ch xuáº¥t tá»« PyTorch source:** ```python class Embedding(Module): def __init__(self, num_embeddings, embedding_dim, ...): super(Embedding, self).__init__() self.num_embeddings = num_embeddings
$$

self.embedding_dim = embedding_dim

        
        # KEY LINE: Creates weight matrix

self.weight = Parameter(

            torch.empty((num_embeddings, embedding_dim))
        )
        self.reset_parameters()

#### 4.1.2 nn.Linear Source Code

**TrÃ­ch xuáº¥t tá»« PyTorch source:**
```python
class Linear(Module):

def __init__(self, in_features, out_features, bias=True):

        super(Linear, self).__init__()

self.in_features = in_features

$$
self.out_features = out_features # KEY LINE: Creates weight matrix self.weight = Parameter( torch.empty((out_features, in_features)) ) if bias: self.bias = Parameter(torch.empty(out_features)) else: self.register_parameter('bias', None) #### 4.1.3 Common Core: nn.Parameter **Cáº£ hai Ä‘á»u sá»­ dá»¥ng:** ```python nn.Parameter(tensor) **Äá»‹nh nghÄ©a `nn.Parameter`:** - Subclass cá»§a `torch.Tensor` - Automatically registered as model parameter - Included in `model.parameters()` - Tracked for gradient computation - Part of computational graph **Káº¿t luáº­n quan trá»ng:** > Sá»± khÃ¡c biá»‡t giá»¯a `nn.Embedding` vÃ  `nn.Linear` chá»‰ lÃ  "syntactic sugar" - chÃºng Ä‘á»u dá»±a trÃªn cÃ¹ng má»™t underlying mechanism (`nn.Parameter`) vÃ  chá»‰ khÃ¡c nhau vá» interface vÃ  má»™t sá»‘ features Ä‘áº·c biá»‡t. --- ## 5. Computational Equivalence ### 5.1 Forward Pass Comparison #### 5.1.1 nn.Embedding Forward **Pseudocode:** ```python def embedding_forward(input_ids, weight): # input_ids: [batch_size] hoáº·c [batch_size, seq_len] # weight: [vocab_size, embed_dim] output = weight[input_ids]  # Advanced indexing return output **VÃ­ dá»¥:** ```python input_ids = torch.tensor([14, 27, 103])  # 3 tokens
$$

output = e(input_ids)  # Shape: [3, 70]

# Equivalent to: weight[[14, 27, 103], :]

#### 5.1.2 nn.Linear Forward (for unembedding)

**Pseudocode:**
```python

def linear_forward(input, weight, bias=None):

    # input: [batch_size, in_features]
    # weight: [out_features, in_features]
    # output: [batch_size, out_features]
    

output = input @ weight.T  # Matrix multiply with transpose

    if bias is not None:

$$
output += bias
$$

    return output

**VÃ­ dá»¥:**
```python

hidden = torch.randn(3, 70)  # 3 samples, 70 dims

$$
logits = l(hidden)  # Shape: [3, 5000] # Equivalent to: hidden @ l.weight.T + l.bias ### 5.2 Mathematical Operations #### 5.2.1 Embedding as Matrix Multiplication **Embedding operation cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t láº¡i:** ```python # Standard embedding output = e(torch.tensor([14])) # Equivalent one-hot multiplication one_hot = F.one_hot(torch.tensor([14]), num_classes=5000).float()
$$

output_equiv = one_hot @ e.weight

assert torch.allclose(output, output_equiv)

**Complexity analysis:**
- Direct indexing: O(1) lookup
- One-hot multiplication: O(vocab_size Ã— embed_dim)
- **Direct indexing lÃ  tá»‘i Æ°u hÆ¡n ráº¥t nhiá»u**

#### 5.2.2 Linear as Reverse Embedding

**Conceptually:**
Embedding:    Token ID â†’ Dense Vector
             [discrete] â†’ [continuous]

Linear:       Dense Vector â†’ Logits over Vocab
             [continuous] â†’ [discrete probabilities]

**Trong unembedding context:**
- Input: Hidden state [embed_dim]
- Weight: [vocab_size, embed_dim]
- Output: Logits [vocab_size]
- Operation: Dot product vá»›i má»—i vocab entry

---

## 6. Practical Implications

### 6.1 Memory Efficiency

**Cáº£ hai lá»›p:**
- Store same-sized weight matrix: `vocab_size Ã— embed_dim`
- Memory footprint: Identical
- Example: 50k vocab Ã— 768 dim Ã— 4 bytes = ~153 MB

### 6.2 Computational Efficiency

**nn.Embedding:**
```python
# Batch lookup: Very efficient

input_ids = torch.randint(0, 5000, (32, 512))  # [batch, seq]

$$
output = e(input_ids)  # [32, 512, 70] # Operation: Simple indexing, O(batch Ã— seq) **nn.Linear:** ```python # Batch matrix multiplication hidden = torch.randn(32, 512, 70)  # [batch, seq, hidden]
$$

logits = l(hidden)  # [32, 512, 5000]

# Operation: GEMM, O(batch Ã— seq Ã— hidden Ã— vocab)

**Performance consideration:**
- Embedding lookup: Extremely fast
- Linear transformation: Depends on matrix sizes
- Bottleneck in LLMs: Usually the unembedding step

### 6.3 Gradient Flow

**Cáº£ hai support:**
- Automatic differentiation
- Backpropagation through weights
- Gradient accumulation
- Optimizer updates

**Embedding-specific:**
```python
# Sparse gradients: Only update accessed embeddings

optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# Only rows corresponding to input tokens get gradient updates

**Linear:**
```python
# Dense gradients: All weights potentially updated
# Full matrix receives gradients every forward pass

---

## 7. Best Practices vÃ  Recommendations

### 7.1 Khi NÃ o DÃ¹ng nn.Embedding

**Use cases:**
1. Token-to-vector mapping (standard embedding layer)
2. Lookup tables cho discrete entities
3. Khi cáº§n padding_idx functionality
4. Khi cáº§n sparse gradients
5. Positional embeddings

**VÃ­ dá»¥:**
```python

token_embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)

$$
position_embedding = nn.Embedding(max_seq_len, hidden_dim) ### 7.2 Khi NÃ o DÃ¹ng nn.Linear **Use cases:** 1. Unembedding layer (hidden â†’ logits) 2. Feed-forward layers trong transformer 3. Projection layers (Q, K, V trong attention) 4. Khi cáº§n bias term 5. Báº¥t ká»³ dense transformation nÃ o **VÃ­ dá»¥:** ```python unembedding = nn.Linear(hidden_dim, vocab_size, bias=False)
$$

ffn = nn.Linear(hidden_dim, ffn_dim)

$$
query_proj = nn.Linear(hidden_dim, head_dim) ### 7.3 Weight Tying (LiÃªn káº¿t Trá»ng Sá»‘) **Advanced technique:** ```python class TransformerLM(nn.Module): def __init__(self, vocab_size, hidden_dim): super().__init__() self.embedding = nn.Embedding(vocab_size, hidden_dim)
$$

self.unembedding = nn.Linear(hidden_dim, vocab_size, bias=False)

        
        # WEIGHT TYING: Share weights

self.unembedding.weight = self.embedding.weight

**Lá»£i Ã­ch:**
- Giáº£m 50% sá»‘ parameters
- Regularization effect
- Embedding vÃ  unembedding há»c cÃ¹ng representation
- ÄÆ°á»£c sá»­ dá»¥ng trong nhiá»u LLMs (GPT-2, BERT variants)

**LÆ°u Ã½:** Weight tying chá»©ng minh rÃµ rÃ ng ráº±ng embedding vÃ  linear weights lÃ  interchangeable.

---

## 8. Common Pitfalls vÃ  Troubleshooting

### 8.1 Dimension Ordering Confusion

**Problem:**
```python
# WRONG

e = nn.Embedding(70, 5000)  # Swapped!

$$
l = nn.Linear(5000, 70)      # Swapped! # CORRECT e = nn.Embedding(5000, 70)   # (vocab, embed)
$$

l = nn.Linear(70, 5000)      # (in, out)

**Solution:** Always double-check parameter order and verify vá»›i `.weight.shape`.

### 8.2 Indexing Errors

**Problem:**
```python

l = nn.Linear(70, 5000)

$$
vector = l(14)  # TypeError! **Solution:** ```python # Option 1: Direct weight access vector = l.weight[14] # Option 2: Use as intended hidden = torch.randn(1, 70)
$$

logits = l(hidden)

### 8.3 Initialization Mismatch

**Problem:**
```python

e = nn.Embedding(5000, 70)  # Normal distribution

$$
l = nn.Linear(70, 5000)     # Uniform distribution # Different initializations may cause training issues **Solution:** ```python # Standardize initialization
$$

torch.nn.init.normal_(l.weight, mean=0.0, std=1.0)

$$
# Or use Xavier/Kaiming consistently --- ## 9. Káº¿t Luáº­n ### 9.1 TÃ³m Táº¯t Findings **Äiá»ƒm giá»‘ng nhau:** 1. Cáº£ hai Ä‘á»u wrapper xung quanh `nn.Parameter` 2. Weight matrix cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c 3. CÃ³ thá»ƒ emulate functionality cá»§a nhau 4. Support automatic differentiation 5. Part of computational graph **Äiá»ƒm khÃ¡c nhau:** | Aspect | nn.Embedding | nn.Linear | |--------|--------------|-----------| | Parameter order | (vocab, embed) | (in, out) | | Direct indexing | âœ“ | âœ— | | Bias term | âœ— | âœ“ (optional) | | Default init | Normal(0,1) | Kaiming Uniform | | Primary use | Token lookup | Matrix transformation | | Sparse gradients | âœ“ (possible) | âœ— | | Padding support | âœ“ | âœ— | ### 9.2 Practical Wisdom **Key Insight:** > Sá»± khÃ¡c biá»‡t giá»¯a `nn.Embedding` vÃ  `nn.Linear` khÃ´ng pháº£i vá» toÃ¡n há»c hay kiáº¿n trÃºc cÆ¡ báº£n, mÃ  vá» **convenience vÃ  optimization cho specific use cases**. Hiá»ƒu rÃµ Ä‘iá»u nÃ y giÃºp developer sá»­ dá»¥ng Ä‘Ãºng tool cho Ä‘Ãºng task. ### 9.3 Educational Value Viá»‡c phÃ¢n tÃ­ch sÃ¢u vá» hai lá»›p nÃ y minh há»a má»™t nguyÃªn lÃ½ quan trá»ng trong deep learning frameworks: **"Under the hood simplicity with surface-level convenience"** PyTorch (vÃ  cÃ¡c frameworks khÃ¡c) cung cáº¥p multiple interfaces cho cÃ¹ng má»™t underlying operation, optimized cho different contexts vÃ  use patterns. --- ## 10. Directions for Further Study ### 10.1 Advanced Topics 1. **Sparse embeddings**: Hiá»‡u quáº£ memory vá»›i large vocabularies 2. **Quantization**: Giáº£m precision Ä‘á»ƒ tÄƒng speed 3. **Custom initialization schemes**: Xavier, He, orthogonal 4. **Gradient clipping**: Stabilize training vá»›i embeddings 5. **Embedding regularization**: L2 penalty, dropout ### 10.2 Related Concepts - **Weight tying** trong language models - **Positional embeddings** (learned vs sinusoidal) - **Subword tokenization** impact lÃªn embedding size - **Low-rank factorization** of embedding matrices - **Contextual embeddings** (BERT-style) vs static --- ## TÃ i Liá»‡u Tham Kháº£o 1. Paszke, A., et al. (2019). "PyTorch: An Imperative Style, High-Performance Deep Learning Library." NeurIPS. 2. He, K., et al. (2015). "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification." ICCV. 3. Glorot, X., & Bengio, Y. (2010). "Understanding the difficulty of training deep feedforward neural networks." AISTATS. 4. Press, O., & Wolf, L. (2017). "Using the Output Embedding to Improve Language Models." EACL. --- ## Phá»¥ Lá»¥c: Complete Code Examples ### A.1 Basic Setup ```python import torch import torch.nn as nn vocab_size = 5000
$$

embed_dim = 70

$$
e = nn.Embedding(vocab_size, embed_dim)
$$

l = nn.Linear(embed_dim, vocab_size)

### A.2 Equivalence Demonstration
```python
# Method 1: Embedding

$$
idx = 14 emb_output = e(torch.tensor([idx])) # Method 2: Linear with one-hot one_hot = torch.zeros(vocab_size)
$$

one_hot[idx] = 1.0

$$
lin_output = one_hot @ l.weight # Method 3: Direct indexing direct = l.weight[idx] # All should be equivalent (except shape) ### A.3 Weight Tying Example ```python class TiedModel(nn.Module): def __init__(self, vocab_size, embed_dim): super().__init__() self.embed = nn.Embedding(vocab_size, embed_dim)
$$

self.linear = nn.Linear(embed_dim, vocab_size, bias=False)

$$
self.linear.weight = self.embed.weight  # Tie weights def forward(self, input_ids): embedded = self.embed(input_ids)
$$

logits = self.linear(embedded)

        return logits

---

**Tá»« khÃ³a:** PyTorch, nn.Embedding, nn.Linear, Weight Matrix, Token Embedding, Unembedding, Parameter Initialization, Kaiming Initialization, Weight Tying, Computational Graph, Automatic Differentiation, Deep Learning Framework
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Má»Ÿ rá»™ng Kiáº¿n trÃºc GPT: Position Embedding, Layer Normalization, Weight Tying vÃ  Temperature Scaling](aero_llm_010_posion_embedding.md) | [Xem bÃ i viáº¿t â†’](aero_llm_010_posion_embedding.md) |
| [Biá»ƒu diá»…n TÃ­nh NhÃ¢n Quáº£ Thá»i Gian trong CÆ¡ Cháº¿ Attention báº±ng Äáº¡i Sá»‘ Tuyáº¿n TÃ­nh](aero_llm_011_temporal_causality_via_linear_algebra_theory_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_011_temporal_causality_via_linear_algebra_theory_.md) |
| [CÆ¡ Cháº¿ Trung BÃ¬nh HÃ³a QuÃ¡ Khá»© vÃ  Loáº¡i Bá» TÆ°Æ¡ng Lai trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ NhÃ¢n Quáº£](aero_llm_012_averaging_the_past_while_ignoring_the_future.md) | [Xem bÃ i viáº¿t â†’](aero_llm_012_averaging_the_past_while_ignoring_the_future.md) |
| [Thuáº­t ToÃ¡n Attention trong MÃ´ HÃ¬nh Transformer: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, CÆ¡ Cháº¿ Hoáº¡t Äá»™ng vÃ  HÃ m Ã á»¨ng Dá»¥ng](aero_llm_013_the_attention_algorithm_theory_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_013_the_attention_algorithm_theory_.md) |
| [PhÃ¢n TÃ­ch vÃ  Triá»ƒn Khai CÆ¡ Cháº¿ Attention: So SÃ¡nh CÃ i Äáº·t Thá»§ CÃ´ng vÃ  PyTorch Tá»‘i Æ¯u](aero_llm_014_codechallenge_code_attention.md) | [Xem bÃ i viáº¿t â†’](aero_llm_014_codechallenge_code_attention.md) |
| [PhÃ¢n TÃ­ch Kiáº¿n TrÃºc MÃ´ HÃ¬nh NgÃ´n Ngá»¯ vá»›i Má»™t Attention Head: LÃ½ Thuyáº¿t, Triá»ƒn Khai vÃ  ÄÃ¡nh GiÃ¡](aero_llm_015_model.md) | [Xem bÃ i viáº¿t â†’](aero_llm_015_model.md) |
| [PhÃ¢n TÃ­ch Cáº¥u TrÃºc Transformer Block: LÃ½ Thuyáº¿t, CÆ¡ Cháº¿ Biá»ƒu Diá»…n vÃ  Vai TrÃ² Trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_016_the_transformer_block_theory_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_016_the_transformer_block_theory_.md) |
| [CÃ i Äáº·t Transformer Block Báº±ng PyTorch: PhÃ¢n TÃ­ch Kiáº¿n TrÃºc, Luá»“ng Dá»¯ Liá»‡u vÃ  Tá»‘i Æ¯u HÃ³a](aero_llm_017_the_transformer_block_code_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_017_the_transformer_block_code_.md) |
| [MÃ´ HÃ¬nh Nhiá»u Transformer Blocks Trong Máº¡ng NgÃ´n Ngá»¯: Kiáº¿n TrÃºc, PhÃ¢n Cáº¥p Biá»ƒu Diá»…n vÃ  Kháº£ NÄƒng Má»Ÿ Rá»™ng](aero_llm_018_model_4_multiple_transformer_blocks_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_018_model_4_multiple_transformer_blocks_.md) |
| [aero llm 019 copy 10](aero_llm_019_copy_10.md) | [Xem bÃ i viáº¿t â†’](aero_llm_019_copy_10.md) |
| [aero llm 019 copy 11](aero_llm_019_copy_11.md) | [Xem bÃ i viáº¿t â†’](aero_llm_019_copy_11.md) |
| [aero llm 019 copy 12](aero_llm_019_copy_12.md) | [Xem bÃ i viáº¿t â†’](aero_llm_019_copy_12.md) |
| [aero llm 019 copy 13](aero_llm_019_copy_13.md) | [Xem bÃ i viáº¿t â†’](aero_llm_019_copy_13.md) |
| [aero llm 019 copy 9](aero_llm_019_copy_9.md) | [Xem bÃ i viáº¿t â†’](aero_llm_019_copy_9.md) |
| [Multi-Head Attention: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t vÃ  Triá»ƒn Khai Thá»±c Tiá»…n](aero_llm_019_multihead_attention_theory_and_implementation.md) | [Xem bÃ i viáº¿t â†’](aero_llm_019_multihead_attention_theory_and_implementation.md) |
| [aero llm 01 intro](aero_llm_01_intro.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_intro.md) |
| [Tá»‘i Æ¯u HÃ³a Huáº¥n Luyá»‡n MÃ´ HÃ¬nh Há»c SÃ¢u Báº±ng GPU: NguyÃªn LÃ½ vÃ  Thá»±c HÃ nh](aero_llm_020_working_on_the_gpu.md) | [Xem bÃ i viáº¿t â†’](aero_llm_020_working_on_the_gpu.md) |
| [Triá»ƒn Khai MÃ´ HÃ¬nh GPT-2 HoÃ n Chá»‰nh TrÃªn GPU: Kiáº¿n TrÃºc, Tá»‘i Æ¯u HÃ³a vÃ  ÄÃ¡nh GiÃ¡ Hiá»‡u NÄƒng](aero_llm_021_mo_hinh_gpt_2_hoan_chinh_tren_gpu.md) | [Xem bÃ i viáº¿t â†’](aero_llm_021_mo_hinh_gpt_2_hoan_chinh_tren_gpu.md) |
| [ÄÃ¡nh GiÃ¡ Hiá»‡u NÄƒng GPT-2 TrÃªn CPU vÃ  GPU: Thá»±c Nghiá»‡m Thá»i Gian Khá»Ÿi Táº¡o, Suy Luáº­n vÃ  Huáº¥n Luyá»‡n](aero_llm_022_anh_gia_hieu_nang_gpt_2_tren_cpu_va_gpu.md) | [Xem bÃ i viáº¿t â†’](aero_llm_022_anh_gia_hieu_nang_gpt_2_tren_cpu_va_gpu.md) |
| [Kháº£o SÃ¡t MÃ´ HÃ¬nh GPT-2 Tiá»n Huáº¥n Luyá»‡n cá»§a OpenAI: Kiáº¿n TrÃºc, Tham Sá»‘ vÃ  CÆ¡ Cháº¿ Sinh VÄƒn Báº£n](aero_llm_023_inspecting_openai_s_gpt2.md) | [Xem bÃ i viáº¿t â†’](aero_llm_023_inspecting_openai_s_gpt2.md) |
| [Kiáº¿n TrÃºc Transformer vÃ  Triá»ƒn Khai GPT-2 trÃªn GPU: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Hiá»‡u NÄƒng TÃ­nh ToÃ¡n](aero_llm_024_summarizing_gpt_using_equations.md) | [Xem bÃ i viáº¿t â†’](aero_llm_024_summarizing_gpt_using_equations.md) |
| [Trá»±c Quan HÃ³a Kiáº¿n TrÃºc GPT ThÃ´ng Qua nano-GPT: Tiáº¿p Cáº­n Trá»±c Quan trong NghiÃªn Cá»©u MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_llm_025_visualizing_nano_gpt.md) | [Xem bÃ i viáº¿t â†’](aero_llm_025_visualizing_nano_gpt.md) |
| [PhÃ¢n TÃ­ch Sá»‘ LÆ°á»£ng Tham Sá»‘ Trong MÃ´ HÃ¬nh GPT-2: PhÆ°Æ¡ng PhÃ¡p Äá»‹nh LÆ°á»£ng vÃ  Ã NghÄ©a Kiáº¿n TrÃºc](aero_llm_026_codechallenge_how_many_parameters_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_026_codechallenge_how_many_parameters_part_1_.md) |
| [PhÃ¢n Bá»‘ Tham Sá»‘ Trong GPT-2: So SÃ¡nh Attention, MLP vÃ  Layer Normalization](aero_llm_027_codechallenge_how_many_parameters_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_027_codechallenge_how_many_parameters_part_2_.md) |
| [ğŸ“˜ PhÃ¢n TÃ­ch Kiáº¿n TrÃºc GPT-2: Tá»« CÆ¡ Cháº¿ Multi-Head Attention Äáº¿n Hiá»‡u NÄƒng TÃ­nh ToÃ¡n TrÃªn GPU](aero_llm_028_codechallenge_gpt2_trained_weights_distributions.md) | [Xem bÃ i viáº¿t â†’](aero_llm_028_codechallenge_gpt2_trained_weights_distributions.md) |
| [ğŸ§  PhÃ¢n TÃ­ch NhÃ¢n Quáº£ Trong GPT-2: Vai TrÃ² Cá»§a Ma Tráº­n Query ThÃ´ng Qua Can Thiá»‡p Tham Sá»‘](aero_llm_029_codechallenge_do_we_really_need_q.md) | [Xem bÃ i viáº¿t â†’](aero_llm_029_codechallenge_do_we_really_need_q.md) |
| [PhÃ¢n TÃ­ch Kiáº¿n TrÃºc vÃ  CÆ¡ Cháº¿ Hoáº¡t Äá»™ng cá»§a MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Transformer CÆ¡ Báº£n](aero_llm_02_transformer.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_transformer.md) |
| ğŸ“Œ **[PhÃ¢n TÃ­ch Ká»¹ Thuáº­t: So SÃ¡nh `nn.Embedding` vÃ  `nn.Linear` trong PyTorch](aero_llm_03_embedding_linear.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_03_embedding_linear.md) |
| [PhÃ¢n TÃ­ch So SÃ¡nh HÃ m KÃ­ch Hoáº¡t GELU vÃ  ReLU trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: GÃ³c NhÃ¬n LÃ½ Thuyáº¿t vÃ  Thá»±c Nghiá»‡m](aero_llm_04_gelu_vs_relu_academic_analysis.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_gelu_vs_relu_academic_analysis.md) |
| [HÃ m Softmax vÃ  Tham Sá»‘ Temperature trong MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n: PhÃ¢n TÃ­ch ToÃ¡n Há»c vÃ  Thá»±c Nghiá»‡m](aero_llm_05_softmax_temperature_academic_analysis.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_softmax_temperature_academic_analysis.md) |
| [PhÃ¢n TÃ­ch `torch.multinomial`: Láº¥y Máº«u XÃ¡c Suáº¥t trong Sinh VÄƒn Báº£n vá»›i PyTorch](aero_llm_06_torch_multinomial_academic_analysis.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_torch_multinomial_academic_analysis.md) |
| [PhÆ°Æ¡ng PhÃ¡p Láº¥y Máº«u Token trong Sinh VÄƒn Báº£n: PhÃ¢n TÃ­ch So SÃ¡nh Greedy, Top-K, Top-P vÃ  Multinomial Sampling](aero_llm_07_token_sampling_methods.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_token_sampling_methods.md) |
| [PhÃ¢n TÃ­ch HÃ nh Vi Cá»§a HÃ m Softmax Trong MÃ´ HÃ¬nh Há»c SÃ¢u: áº¢nh HÆ°á»Ÿng Cá»§a Láº·p, Pháº¡m Vi Sá»‘ Há»c VÃ  Nhiá»‡t Äá»™](aero_llm_08_ham_softbank.md) | [Xem bÃ i viáº¿t â†’](aero_llm_08_ham_softbank.md) |
| [PhÃ¢n TÃ­ch Layer Normalization Trong Há»c SÃ¢u: CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t, á»”n Äá»‹nh Sá»‘ Há»c VÃ  á»¨ng Dá»¥ng Thá»±c Tiá»…n](aero_llm_09_layer_normalization.md) | [Xem bÃ i viáº¿t â†’](aero_llm_09_layer_normalization.md) |
| [kien truc mo hinh ngon ngu lon](kien_truc_mo_hinh_ngon_ngu_lon.md) | [Xem bÃ i viáº¿t â†’](kien_truc_mo_hinh_ngon_ngu_lon.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

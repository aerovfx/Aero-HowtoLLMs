D∆∞·ªõi ƒë√¢y l√† b√†i vi·∫øt khoa h·ªçc d∆∞·ªõi d·∫°ng **Markdown**, d·ª±a tr√™n n·ªôi dung b·∫°n cung c·∫•p t·ª´ t√†i li·ªáu *‚ÄúMore Softmax Explorations‚Äù* , k·∫øt h·ª£p ph√¢n t√≠ch l√Ω thuy·∫øt v√† tham kh·∫£o h·ªçc thu·∫≠t.

---

# Ph√¢n T√≠ch H√†nh Vi C·ªßa H√†m Softmax Trong M√¥ H√¨nh H·ªçc S√¢u: ·∫¢nh H∆∞·ªüng C·ªßa L·∫∑p, Ph·∫°m Vi S·ªë H·ªçc V√† Nhi·ªát ƒê·ªô

## T√≥m t·∫Øt (Abstract)

H√†m Softmax l√† m·ªôt th√†nh ph·∫ßn c·ªët l√µi trong c√°c m√¥ h√¨nh h·ªçc s√¢u, ƒë·∫∑c bi·ªát trong x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n v√† th·ªã gi√°c m√°y t√≠nh. B√†i vi·∫øt n√†y ph√¢n t√≠ch h√†nh vi c·ªßa Softmax th√¥ng qua hai th√≠ nghi·ªám: (1) √°p d·ª•ng Softmax l·∫∑p nhi·ªÅu l·∫ßn l√™n c√πng m·ªôt ph√¢n ph·ªëi, v√† (2) kh·∫£o s√°t ·∫£nh h∆∞·ªüng c·ªßa ph·∫°m vi gi√° tr·ªã logits v√† tham s·ªë nhi·ªát ƒë·ªô (temperature). K·∫øt qu·∫£ cho th·∫•y Softmax c√≥ xu h∆∞·ªõng l√†m ph·∫≥ng ph√¢n ph·ªëi khi ƒë∆∞·ª£c l·∫∑p l·∫°i, ƒë·ªìng th·ªùi r·∫•t nh·∫°y c·∫£m v·ªõi mi·ªÅn gi√° tr·ªã s·ªë v√† nhi·ªát ƒë·ªô. Nh·ªØng ph√°t hi·ªán n√†y nh·∫•n m·∫°nh vai tr√≤ c·ªßa chu·∫©n h√≥a v√† ki·ªÉm so√°t ƒë·ªô ·ªïn ƒë·ªãnh s·ªë trong c√°c m√¥ h√¨nh h·ªçc s√¢u hi·ªán ƒë·∫°i.

---

## 1. Gi·ªõi thi·ªáu

Trong h·ªçc s√¢u, Softmax th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ chuy·ªÉn ƒë·ªïi vector logits th√†nh ph√¢n ph·ªëi x√°c su·∫•t. Cho vector ƒë·∫ßu v√†o ( x = (x_1, x_2, ..., x_n) ), Softmax ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ sau:

[
\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
]

H√†m n√†y ƒë·∫£m b·∫£o r·∫±ng:

* M·ªói gi√° tr·ªã ƒë·∫ßu ra n·∫±m trong kho·∫£ng (0,1),
* T·ªïng c√°c gi√° tr·ªã b·∫±ng 1.

M·∫∑c d√π c√¥ng th·ª©c ƒë∆°n gi·∫£n, h√†nh vi th·ª±c t·∫ø c·ªßa Softmax trong m√¥i tr∆∞·ªùng s·ªë h·ªçc v√† hu·∫•n luy·ªán m√¥ h√¨nh ph·ª©c t·∫°p h∆°n nhi·ªÅu. T√†i li·ªáu th·ª±c nghi·ªám ƒë∆∞·ª£c cung c·∫•p  cho th·∫•y nhi·ªÅu hi·ªán t∆∞·ª£ng phi tr·ª±c gi√°c, ƒë·∫∑c bi·ªát khi Softmax ƒë∆∞·ª£c √°p d·ª•ng l·∫∑p l·∫°i ho·∫∑c k·∫øt h·ª£p v·ªõi tham s·ªë nhi·ªát ƒë·ªô.

---

## 2. C∆° s·ªü l√Ω thuy·∫øt

### 2.1. Softmax v√† ph√¢n ph·ªëi x√°c su·∫•t

Softmax bi·∫øn ƒë·ªïi c√°c gi√° tr·ªã logits th√†nh x√°c su·∫•t b·∫±ng h√†m m≈©. Do t√≠nh ch·∫•t tƒÉng nhanh c·ªßa h√†m m≈©, nh·ªØng gi√° tr·ªã l·ªõn s·∫Ω ƒë∆∞·ª£c khu·∫øch ƒë·∫°i, trong khi gi√° tr·ªã nh·ªè b·ªã suy gi·∫£m m·∫°nh.

### 2.2. Softmax v·ªõi tham s·ªë nhi·ªát ƒë·ªô

Phi√™n b·∫£n m·ªü r·ªông c·ªßa Softmax c√≥ d·∫°ng:

[
\text{Softmax}*T(x_i) = \frac{e^{x_i/T}}{\sum*{j=1}^{n} e^{x_j/T}}
]

Trong ƒë√≥ (T) l√† nhi·ªát ƒë·ªô:

* (T < 1): Ph√¢n ph·ªëi s·∫Øc n√©t (sharp), t·∫≠p trung v√†o ph·∫ßn t·ª≠ l·ªõn nh·∫•t.
* (T = 1): Softmax chu·∫©n.
* (T > 1): Ph√¢n ph·ªëi ph·∫≥ng (smooth), tƒÉng t√≠nh ƒëa d·∫°ng.

### 2.3. ·ªîn ƒë·ªãnh s·ªë h·ªçc

Vi·ªác t√≠nh to√°n h√†m m≈© tr√™n c√°c gi√° tr·ªã l·ªõn ho·∫∑c nh·ªè c√≥ th·ªÉ g√¢y:

* Tr√†n s·ªë (overflow),
* M·∫•t ƒë·ªô ch√≠nh x√°c (underflow),
* Gradient bi·∫øn m·∫•t ho·∫∑c b√πng n·ªï.

Do ƒë√≥, c√°c k·ªπ thu·∫≠t chu·∫©n h√≥a (normalization) l√† c·∫ßn thi·∫øt trong m·∫°ng s√¢u.

---

## 3. Ph∆∞∆°ng ph√°p nghi√™n c·ª©u

Nghi√™n c·ª©u d·ª±a tr√™n hai th√≠ nghi·ªám ch√≠nh ƒë∆∞·ª£c m√¥ t·∫£ trong t√†i li·ªáu g·ªëc .

### 3.1. Th√≠ nghi·ªám 1: Softmax l·∫∑p

#### M√¥ t·∫£

* T·∫°o 20 s·ªë tuy·∫øn t√≠nh trong kho·∫£ng [0,1].
* √Åp d·ª•ng Softmax.
* L·∫∑p l·∫°i qu√° tr√¨nh Softmax tr√™n ch√≠nh ƒë·∫ßu ra nhi·ªÅu l·∫ßn (8 l·∫ßn).
* T√≠nh ƒë·ªô l·ªách chu·∫©n c·ªßa ph√¢n ph·ªëi sau m·ªói l·∫ßn l·∫∑p.

#### M·ª•c ti√™u

Kh·∫£o s√°t vi·ªác Softmax l·∫∑p c√≥ l√†m ph√¢n ph·ªëi tr·ªü n√™n ‚Äús·∫Øc n√©t‚Äù h∆°n hay kh√¥ng.

---

### 3.2. Th√≠ nghi·ªám 2: Ph·∫°m vi logits v√† nhi·ªát ƒë·ªô

#### M√¥ t·∫£

* Sinh 100 logits trong c√°c kho·∫£ng:

  * [-0.4, 0.4]
  * [-1, 1]
  * [-5, 5]
* Th√™m m·ªôt gi√° tr·ªã ngo·∫°i lai: 6.
* √Åp d·ª•ng Softmax v·ªõi c√°c nhi·ªát ƒë·ªô: 0.5, 1, 3.
* Ph√¢n t√≠ch x√°c su·∫•t ƒë·∫ßu ra.

#### M·ª•c ti√™u

ƒê√°nh gi√° ·∫£nh h∆∞·ªüng c·ªßa:

* Mi·ªÅn gi√° tr·ªã logits,
* Nhi·ªát ƒë·ªô,
* Gi√° tr·ªã ngo·∫°i lai.

---

## 4. K·∫øt qu·∫£ th·ª±c nghi·ªám

### 4.1. Hi·ªáu ·ª©ng c·ªßa Softmax l·∫∑p

K·∫øt qu·∫£ cho th·∫•y:

* Sau v√†i l·∫ßn l·∫∑p, ph√¢n ph·ªëi h·ªôi t·ª• v·ªÅ d·∫°ng g·∫ßn nh∆∞ ƒë·ªìng ƒë·ªÅu.
* V·ªõi 20 ph·∫ßn t·ª≠, m·ªói gi√° tr·ªã ti·∫øn g·∫ßn ƒë·∫øn 0.05.
* ƒê·ªô l·ªách chu·∫©n gi·∫£m nhanh theo c·∫•p s·ªë nh√¢n.

ƒêi·ªÅu n√†y cho th·∫•y Softmax l·∫∑p kh√¥ng l√†m n·ªïi b·∫≠t ph·∫ßn t·ª≠ l·ªõn nh·∫•t, m√† ng∆∞·ª£c l·∫°i l√†m m·∫•t t√≠nh ph√¢n bi·ªát.

### 4.2. Vai tr√≤ c·ªßa s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠

S·ªë l∆∞·ª£ng ph·∫ßn t·ª≠ ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn gi√° tr·ªã trung b√¨nh:

| S·ªë ph·∫ßn t·ª≠ | Gi√° tr·ªã trung b√¨nh |
| ---------- | ------------------ |
| 4          | ‚âà 0.25             |
| 20         | ‚âà 0.05             |
| 100        | ‚âà 0.01             |

C√†ng nhi·ªÅu ph·∫ßn t·ª≠, x√°c su·∫•t ri√™ng l·∫ª c√†ng nh·ªè.

### 4.3. ·∫¢nh h∆∞·ªüng c·ªßa nhi·ªát ƒë·ªô

| Nhi·ªát ƒë·ªô | ƒê·∫∑c ƒëi·ªÉm               |
| -------- | ---------------------- |
| T < 1    | T·∫≠p trung m·∫°nh v√†o max |
| T = 1    | C√¢n b·∫±ng               |
| T > 1    | Ph√¢n t√°n               |

·ªû T = 0.5, ph·∫ßn t·ª≠ c√≥ logit = 6 chi·∫øm g·∫ßn nh∆∞ to√†n b·ªô x√°c su·∫•t.
·ªû T = 3, ph√¢n ph·ªëi tr·ªü n√™n m·ªÅm h∆°n, tƒÉng t√≠nh ng·∫´u nhi√™n.

### 4.4. ·∫¢nh h∆∞·ªüng c·ªßa ph·∫°m vi logits

Khi mi·ªÅn gi√° tr·ªã h·∫πp ([-0.4, 0.4]):

* Gi√° tr·ªã 6 tr·ªü n√™n v∆∞·ª£t tr·ªôi tuy·ªát ƒë·ªëi.

Khi mi·ªÅn r·ªông ([-5, 5]):

* S·ª± kh√°c bi·ªát t∆∞∆°ng ƒë·ªëi gi·∫£m.
* Ph√¢n ph·ªëi c√¢n b·∫±ng h∆°n.

ƒêi·ªÅu n√†y ch·ª©ng minh r·∫±ng Softmax ph·ª• thu·ªôc m·∫°nh v√†o ƒë·ªô ch√™nh l·ªách t∆∞∆°ng ƒë·ªëi, kh√¥ng ch·ªâ gi√° tr·ªã tuy·ªát ƒë·ªëi.

---

## 5. Th·∫£o lu·∫≠n

### 5.1. V√¨ sao Softmax l·∫∑p l√†m ph·∫≥ng ph√¢n ph·ªëi?

Do ƒë·∫ßu ra Softmax ƒë√£ n·∫±m trong [0,1] v√† c√≥ t·ªïng b·∫±ng 1. Khi ti·∫øp t·ª•c √°p d·ª•ng h√†m m≈© tr√™n mi·ªÅn h·∫πp, h√†m m≈© tr·ªü n√™n g·∫ßn tuy·∫øn t√≠nh, l√†m m·∫•t hi·ªáu ·ª©ng khu·∫øch ƒë·∫°i.

### 5.2. H·ªá qu·∫£ trong m√¥ h√¨nh ng√¥n ng·ªØ

Trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn:

* Vocabulary c√≥ th·ªÉ > 100,000 token.
* Softmax s·∫Ω n√©n h·∫ßu h·∫øt x√°c su·∫•t v·ªÅ g·∫ßn 0.
* Ch·ªâ v√†i token chi·∫øm ∆∞u th·∫ø.

Do ƒë√≥:

* Nhi·ªát ƒë·ªô th·∫•p ‚Üí m√¥ h√¨nh l·∫∑p, √≠t s√°ng t·∫°o.
* Nhi·ªát ƒë·ªô cao ‚Üí ƒëa d·∫°ng nh∆∞ng gi·∫£m ƒë·ªô ch√≠nh x√°c.

### 5.3. Li√™n h·ªá v·ªõi chu·∫©n h√≥a

C√°c k·∫øt qu·∫£ cho th·∫•y:

* Logits qu√° nh·ªè ‚Üí Softmax m·∫•t hi·ªáu qu·∫£.
* Logits qu√° l·ªõn ‚Üí m·∫•t ·ªïn ƒë·ªãnh.

V√¨ v·∫≠y, c√°c k·ªπ thu·∫≠t nh∆∞:

* Layer Normalization,
* Batch Normalization,
* Weight Regularization,

l√† c·∫ßn thi·∫øt ƒë·ªÉ duy tr√¨ mi·ªÅn gi√° tr·ªã h·ª£p l√Ω.

---

## 6. ·ª®ng d·ª•ng th·ª±c ti·ªÖn

### 6.1. ƒêi·ªÅu ch·ªânh nhi·ªát ƒë·ªô sinh vƒÉn b·∫£n

| M·ª•c ti√™u  | Nhi·ªát ƒë·ªô  |
| --------- | --------- |
| Ch√≠nh x√°c | 0.2 ‚Äì 0.7 |
| C√¢n b·∫±ng  | ‚âà 1       |
| S√°ng t·∫°o  | 1.2 ‚Äì 2   |

### 6.2. Thi·∫øt k·∫ø m√¥ h√¨nh

* Tr√°nh ƒë·ªÉ logits qu√° h·∫πp.
* √Åp d·ª•ng normalization ph√π h·ª£p.
* Ki·ªÉm so√°t scale khi hu·∫•n luy·ªán.

### 6.3. Debug m√¥ h√¨nh

Hi·ªán t∆∞·ª£ng ph√¢n ph·ªëi ‚Äúch·∫øt‚Äù (collapse) c√≥ th·ªÉ xu·∫•t ph√°t t·ª´:

* Logits b·ªã n√©n,
* Temperature qu√° th·∫•p,
* L·ªói chu·∫©n h√≥a.

---

## 7. K·∫øt lu·∫≠n

B√†i vi·∫øt ƒë√£ ph√¢n t√≠ch chi ti·∫øt h√†nh vi c·ªßa Softmax th√¥ng qua hai th√≠ nghi·ªám th·ª±c nghi·ªám. C√°c k·∫øt lu·∫≠n ch√≠nh bao g·ªìm:

1. Softmax l·∫∑p l√†m ph·∫≥ng ph√¢n ph·ªëi thay v√¨ l√†m s·∫Øc n√©t.
2. S·ªë l∆∞·ª£ng ph·∫ßn t·ª≠ ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn x√°c su·∫•t.
3. Nhi·ªát ƒë·ªô ki·ªÉm so√°t m·ª©c ƒë·ªô t·∫≠p trung.
4. Ph·∫°m vi logits quy·∫øt ƒë·ªãnh m·ª©c ƒë·ªô ph√¢n bi·ªát.
5. Chu·∫©n h√≥a l√† y·∫øu t·ªë s·ªëng c√≤n trong m·∫°ng s√¢u.

Nh·ªØng k·∫øt qu·∫£ n√†y gi√∫p hi·ªÉu r√µ h∆°n v·ªÅ c√°ch Softmax ho·∫°t ƒë·ªông trong c√°c m√¥ h√¨nh h·ªçc s√¢u hi·ªán ƒë·∫°i v√† cung c·∫•p c∆° s·ªü cho vi·ªác ƒëi·ªÅu ch·ªânh m√¥ h√¨nh hi·ªáu qu·∫£.

---

## T√†i li·ªáu tham kh·∫£o

1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.
2. Vaswani, A. et al. (2017). Attention Is All You Need. *NeurIPS*.
3. Bishop, C. (2006). *Pattern Recognition and Machine Learning*. Springer.
4. T√†i li·ªáu th·ª±c nghi·ªám v·ªÅ Softmax 

---
**phi√™n b·∫£n Python/PyTorch chu·∫©n, ƒë·∫ßy ƒë·ªß, c√≥ ch√∫ th√≠ch khoa h·ªçc**, t∆∞∆°ng ·ª©ng tr·ª±c ti·∫øp v·ªõi ph·∫ßn *pseudocode* ·ªü tr√™n. M√£ n√†y c√≥ th·ªÉ d√πng ƒë·ªÉ:

‚úÖ T√°i l·∫≠p th√≠ nghi·ªám
‚úÖ Vi·∫øt b√°o c√°o / paper
‚úÖ L√†m supplementary material
‚úÖ Th·ª±c h√†nh nghi√™n c·ª©u Softmax

B·∫°n c√≥ th·ªÉ copy v√† ch·∫°y tr·ª±c ti·∫øp trong m√¥i tr∆∞·ªùng c√≥ `torch` v√† `matplotlib`.

---

# üìå 1. C√†i ƒë·∫∑t th∆∞ vi·ªán

```python
import torch
import matplotlib.pyplot as plt
import numpy as np
```

---

# üìå 2. H√†m Softmax chu·∫©n (c√≥ nhi·ªát ƒë·ªô)

```python
def softmax_t(x, temperature=1.0):
    """
    Temperature-scaled softmax

    Args:
        x (Tensor): Logits
        temperature (float): Temperature parameter

    Returns:
        Tensor: Probability distribution
    """
    x_scaled = x / temperature
    exp_x = torch.exp(x_scaled - torch.max(x_scaled))  # numerical stability
    return exp_x / torch.sum(exp_x)
```

üëâ Tr·ª´ `max(x)` ƒë·ªÉ tr√°nh overflow (chu·∫©n nghi√™n c·ª©u).

---

# üìå 3. Th√≠ nghi·ªám 1: Softmax l·∫∑p

## 3.1. H√†m th·ª±c nghi·ªám

```python
def iterative_softmax_experiment(
    n_points=20,
    n_iters=8,
    min_val=0.0,
    max_val=1.0
):
    """
    Iterative Softmax Experiment

    Returns:
        probs (list): List of distributions
        stds (list): Standard deviations
    """

    # Generate linear data
    x = torch.linspace(min_val, max_val, n_points)

    probs = []
    stds = []

    # Initial softmax
    p = softmax_t(x)

    probs.append(p.clone())

    for i in range(n_iters):

        std = torch.std(p)
        stds.append(std.item())

        # Apply softmax again
        p = softmax_t(p)

        probs.append(p.clone())

    return probs, stds, x
```

---

## 3.2. Ch·∫°y th√≠ nghi·ªám

```python
probs, stds, x = iterative_softmax_experiment()
```

---

## 3.3. V·∫Ω k·∫øt qu·∫£

### Ph√¢n ph·ªëi theo v√≤ng l·∫∑p

```python
plt.figure(figsize=(8, 6))

for i, p in enumerate(probs):
    plt.scatter(x, p, label=f"Iter {i}", s=30)

plt.xlabel("Input values")
plt.ylabel("Softmax probability")
plt.title("Iterative Softmax Behavior")
plt.legend()
plt.grid(True)
plt.show()
```

---

### Log ƒë·ªô l·ªách chu·∫©n

```python
log_stds = np.log(np.array(stds) + 1e-12)

plt.figure(figsize=(6, 5))

plt.plot(range(len(log_stds)), log_stds, marker='o')

plt.xlabel("Iteration")
plt.ylabel("Log(Standard Deviation)")
plt.title("Convergence of Iterative Softmax")
plt.grid(True)
plt.show()
```

---

# üìå 4. Th√≠ nghi·ªám 2: Ph·∫°m vi logits & Nhi·ªát ƒë·ªô

---

## 4.1. H√†m th·ª±c nghi·ªám

```python
def temperature_range_experiment(
    ranges=[0.4, 1, 5],
    temperatures=[0.5, 1.0, 3.0],
    n_points=100,
    outlier=6.0
):
    """
    Temperature & Logit Range Experiment

    Returns:
        results (dict): Nested results
    """

    results = {}

    for r in ranges:

        # Generate logits
        x = torch.linspace(-r, r, n_points)

        # Append outlier
        out = torch.tensor([outlier])
        logits = torch.cat([x, out])

        results[r] = {}

        for t in temperatures:

            probs = softmax_t(logits, t)

            results[r][t] = {
                "logits": logits,
                "probs": probs
            }

    return results
```

---

## 4.2. Ch·∫°y th√≠ nghi·ªám

```python
results = temperature_range_experiment()
```

---

## 4.3. V·∫Ω k·∫øt qu·∫£

### Zoom-in (x√°c su·∫•t nh·ªè)

```python
def plot_zoomed(results):

    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    for idx, r in enumerate(results):

        ax = axes[idx]

        for t in results[r]:

            data = results[r][t]

            logits = data["logits"][:-1]
            probs = data["probs"][:-1]

            ax.scatter(
                logits.numpy(),
                probs.numpy(),
                label=f"T={t}",
                s=20
            )

        ax.set_title(f"Range [-{r}, {r}]")
        ax.set_ylim(0, 0.03)
        ax.set_xlabel("Logits")
        ax.set_ylabel("Probability")
        ax.legend()
        ax.grid(True)

    plt.tight_layout()
    plt.show()
```

---

### Zoom-out (to√†n b·ªô ph√¢n ph·ªëi)

```python
def plot_full(results):

    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    for idx, r in enumerate(results):

        ax = axes[idx]

        for t in results[r]:

            data = results[r][t]

            logits = data["logits"]
            probs = data["probs"]

            ax.scatter(
                logits.numpy(),
                probs.numpy(),
                label=f"T={t}",
                s=20
            )

        ax.set_title(f"Range [-{r}, {r}]")
        ax.set_ylim(0, 1)
        ax.set_xlabel("Logits")
        ax.set_ylabel("Probability")
        ax.legend()
        ax.grid(True)

    plt.tight_layout()
    plt.show()
```

---

## 4.4. V·∫Ω ƒë·ªì th·ªã

```python
plot_zoomed(results)
plot_full(results)
```

---

# üìå 5. Pipeline t·ªïng h·ª£p (chu·∫©n nghi√™n c·ª©u)

```python
def run_full_pipeline():

    print("Running Iterative Softmax...")
    probs, stds, x = iterative_softmax_experiment()

    print("Running Temperature Experiment...")
    results = temperature_range_experiment()

    plot_zoomed(results)
    plot_full(results)

    return probs, stds, results
```

---

```python
probs, stds, results = run_full_pipeline()
```

---

# üìå 6. ƒê·∫∑c ƒëi·ªÉm chu·∫©n h·ªçc thu·∫≠t c·ªßa m√£ n√†y

M√£ tr√™n ƒë√°p ·ª©ng ti√™u chu·∫©n:

‚úÖ Numerical stability
‚úÖ Reproducibility
‚úÖ Modular design
‚úÖ Research-friendly
‚úÖ D·ªÖ m·ªü r·ªông

√Åp d·ª•ng ƒë∆∞·ª£c cho:

* Paper supplementary
* Replication study
* Thesis
* Research demo

---

# üìå 7. G·ª£i √Ω n√¢ng cao (n·∫øu b·∫°n l√†m nghi√™n c·ª©u s√¢u h∆°n)

B·∫°n c√≥ th·ªÉ m·ªü r·ªông th√™m:

### üîπ Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p

```python
# O(N) per softmax
# O(KN) for iterative
```

### üîπ Seed c·ªë ƒë·ªãnh

```python
torch.manual_seed(42)
np.random.seed(42)
```

### üîπ GPU

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```



<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [08 instruction tuning](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Reinforcement Learning from Human Feedback (RLHF): CÆ¡ sá»Ÿ lÃ½ thuyáº¿t, mÃ´ hÃ¬nh toÃ¡n há»c vÃ  á»©ng dá»¥ng trong huáº¥n luyá»‡n mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n

---

## TÃ³m táº¯t

Reinforcement Learning from Human Feedback (RLHF) lÃ  phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) nháº±m tá»‘i Æ°u hÃ³a Ä‘áº§u ra theo Ä‘Ã¡nh giÃ¡ cá»§a con ngÆ°á»i. BÃ i viáº¿t nÃ y trÃ¬nh bÃ y cÆ¡ sá»Ÿ toÃ¡n há»c cá»§a RLHF, phÃ¢n tÃ­ch tá»«ng giai Ä‘oáº¡n huáº¥n luyá»‡n (Supervised Fine-Tuning, Reward Modeling, Policy Optimization), vÃ  tháº£o luáº­n vai trÃ² cá»§a PPO cÃ¹ng regularization KL-divergence. PhÃ¢n tÃ­ch Ä‘Æ°á»£c Ä‘áº·t trong bá»‘i cáº£nh cÃ¡c mÃ´ hÃ¬nh GPT do OpenAI phÃ¡t triá»ƒn, Ä‘áº·c biá»‡t lÃ  InstructGPT (Ouyang et al., 2022).

---

# 1. Giá»›i thiá»‡u

CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ nhÆ° GPT-2 hay GPT-3 Ä‘Æ°á»£c huáº¥n luyá»‡n theo má»¥c tiÃªu dá»± Ä‘oÃ¡n token káº¿ tiáº¿p:

[
P(x_1, x_2, ..., x_T) = \prod_{t=1}^{T} P(x_t \mid x_{<t})
]

Tuy nhiÃªn, má»¥c tiÃªu tá»‘i Ä‘a hÃ³a likelihood khÃ´ng Ä‘áº£m báº£o mÃ´ hÃ¬nh:

* TuÃ¢n thá»§ chá»‰ thá»‹ (instruction-following)
* Tráº£ lá»i an toÃ n
* PhÃ¹ há»£p vá»›i giÃ¡ trá»‹ con ngÆ°á»i

RLHF Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘á»ƒ giáº£i quyáº¿t khoáº£ng cÃ¡ch giá»¯a tá»‘i Æ°u hÃ³a xÃ¡c suáº¥t vÃ  tá»‘i Æ°u hÃ³a sá»± hÃ i lÃ²ng cá»§a con ngÆ°á»i.

---

# 2. Khung lÃ½ thuyáº¿t Reinforcement Learning

Trong RL cá»• Ä‘iá»ƒn, ta cÃ³:

* Tráº¡ng thÃ¡i: ( s )
* HÃ nh Ä‘á»™ng: ( a )
* ChÃ­nh sÃ¡ch: ( \pi_\theta(a|s) )
* Pháº§n thÆ°á»Ÿng: ( r(s,a) )

Má»¥c tiÃªu tá»‘i Æ°u:

[
\max_\theta \mathbb{E}*{\tau \sim \pi*\theta}
\left[
\sum_{t=0}^{T} \gamma^t r_t
\right]
]

Trong RLHF:

* Tráº¡ng thÃ¡i ( s ): prompt (instruction)
* HÃ nh Ä‘á»™ng ( a ): chuá»—i pháº£n há»“i
* Reward: Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡ tá»« con ngÆ°á»i hoáº·c reward model

---

# 3. Quy trÃ¬nh RLHF

## 3.1. BÆ°á»›c 1 â€“ Supervised Fine-Tuning (SFT)

Huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u cáº·p (instruction, response):

[
\mathcal{L}*{SFT}
= - \sum*{t \in R} \log P_\theta(x_t \mid x_{<t})
]

Má»¥c tiÃªu: Ä‘Æ°a mÃ´ hÃ¬nh vá» phÃ¢n phá»‘i gáº§n vá»›i hÃ nh vi mong muá»‘n.

---

## 3.2. BÆ°á»›c 2 â€“ Huáº¥n luyá»‡n Reward Model

Cho hai pháº£n há»“i ( y_1, y_2 ) vá»›i cÃ¹ng prompt ( x ), con ngÆ°á»i chá»n pháº£n há»“i tá»‘t hÆ¡n.

Reward model ( r_\phi(x,y) ) Ä‘Æ°á»£c huáº¥n luyá»‡n báº±ng loss Bradley-Terry:

[
P(y_1 \succ y_2)
= \frac{e^{r_\phi(x,y_1)}}{e^{r_\phi(x,y_1)} + e^{r_\phi(x,y_2)}}
]

Loss:

[
\mathcal{L}*{RM}
= - \log \sigma(r*\phi(x,y_{chosen}) - r_\phi(x,y_{rejected}))
]

Trong Ä‘Ã³ ( \sigma ) lÃ  sigmoid:

[
\sigma(z) = \frac{1}{1 + e^{-z}}
]

---

## 3.3. BÆ°á»›c 3 â€“ Policy Optimization (PPO)

Sau khi cÃ³ reward model, ta tá»‘i Æ°u policy:

[
\max_\theta
\mathbb{E}*{x \sim \pi*\theta}
\left[
r_\phi(x) - \beta D_{KL}(\pi_\theta | \pi_{ref})
\right]
]

Trong Ä‘Ã³:

* ( \pi_{ref} ): mÃ´ hÃ¬nh SFT ban Ä‘áº§u
* ( D_{KL} ): KL divergence

[
D_{KL}(P|Q) = \sum_x P(x)\log\frac{P(x)}{Q(x)}
]

---

# 4. Proximal Policy Optimization (PPO)

PPO tá»‘i Æ°u hÃ m má»¥c tiÃªu:

[
L^{CLIP}(\theta)
= \mathbb{E}
\left[
\min
\left(
r_t(\theta) A_t,
\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t
\right)
\right]
]

Trong Ä‘Ã³:

[
r_t(\theta)
= \frac{\pi_\theta(a_t|s_t)}
{\pi_{\theta_{old}}(a_t|s_t)}
]

(A_t): advantage estimate.

Clipping giÃºp:

* TrÃ¡nh cáº­p nháº­t quÃ¡ lá»›n
* á»”n Ä‘á»‹nh huáº¥n luyá»‡n

---

# 5. KL Regularization

Náº¿u khÃ´ng cÃ³ KL penalty:

[
\pi_\theta \to \text{mode collapse}
]

Vá»›i KL:

[
\mathcal{L}
= \mathbb{E}[r(x)]

* \beta D_{KL}(\pi_\theta | \pi_{ref})
  ]

KL Ä‘Ã³ng vai trÃ² nhÆ° regularizer:

* Giá»¯ mÃ´ hÃ¬nh gáº§n phÃ¢n phá»‘i gá»‘c
* TrÃ¡nh hÃ nh vi báº¥t thÆ°á»ng

---

# 6. PhÃ¢n tÃ­ch Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n

Giáº£ sá»­:

* (L): sá»‘ layer
* (T): chiá»u dÃ i chuá»—i
* (d): embedding dimension

Self-attention:

[
\mathcal{O}(L \cdot T^2 \cdot d)
]

Trong RLHF:

* Má»—i bÆ°á»›c cáº§n forward nhiá»u máº«u
* TÃ­nh thÃªm reward model
* TÃ­nh KL divergence

Chi phÃ­ tÄƒng gáº¥p 2â€“3 láº§n so vá»›i SFT.

---

# 7. CÃ¡c váº¥n Ä‘á» lÃ½ thuyáº¿t

## 7.1. Reward Hacking

MÃ´ hÃ¬nh cÃ³ thá»ƒ tá»‘i Ä‘a hÃ³a reward model nhÆ°ng khÃ´ng thá»±c sá»± tá»‘t.

Giáº£ sá»­ reward model xáº¥p xá»‰:

[
r_\phi(x) = r_{true}(x) + \epsilon(x)
]

Khi tá»‘i Æ°u:

[
\max_\theta \mathbb{E}[r_\phi(x)]
]

Sai sá»‘ ( \epsilon(x) ) cÃ³ thá»ƒ bá»‹ khai thÃ¡c.

---

## 7.2. Alignment Problem

Ta muá»‘n:

[
\pi_\theta \approx \pi_{human}
]

NhÆ°ng reward chá»‰ lÃ  xáº¥p xá»‰.

ÄÃ¢y lÃ  trung tÃ¢m cá»§a nghiÃªn cá»©u alignment hiá»‡n Ä‘áº¡i.

---

# 8. So sÃ¡nh vá»›i cÃ¡c hÆ°á»›ng tiáº¿p cáº­n khÃ¡c

| PhÆ°Æ¡ng phÃ¡p | Æ¯u Ä‘iá»ƒm                  | NhÆ°á»£c Ä‘iá»ƒm         |
| ----------- | ------------------------ | ------------------ |
| SFT         | ÄÆ¡n giáº£n                 | Phá»¥ thuá»™c dá»¯ liá»‡u  |
| RLHF        | Linh hoáº¡t, alignment tá»‘t | Tá»‘n chi phÃ­        |
| DPO         | KhÃ´ng cáº§n PPO            | Giá»›i háº¡n lÃ½ thuyáº¿t |

---

# 9. Tháº£o luáº­n

RLHF lÃ  cáº§u ná»‘i giá»¯a:

* Há»c cÃ³ giÃ¡m sÃ¡t
* Há»c tÄƒng cÆ°á»ng
* Há»c theo giÃ¡ trá»‹ con ngÆ°á»i

CÃ¡ch tiáº¿p cáº­n nÃ y Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng trong cÃ¡c mÃ´ hÃ¬nh GPT cá»§a OpenAI vÃ  má»Ÿ ra hÆ°á»›ng phÃ¡t triá»ƒn LLM an toÃ n hÆ¡n.

---

# 10. Káº¿t luáº­n

RLHF cho phÃ©p:

1. Tá»‘i Æ°u hÃ³a hÃ nh vi thay vÃ¬ chá»‰ tá»‘i Æ°u xÃ¡c suáº¥t
2. Káº¿t há»£p Ä‘Ã¡nh giÃ¡ con ngÆ°á»i vÃ o vÃ²ng láº·p huáº¥n luyá»‡n
3. Kiá»ƒm soÃ¡t mÃ´ hÃ¬nh thÃ´ng qua KL-regularization

Vá» máº·t toÃ¡n há»c, RLHF lÃ  sá»± káº¿t há»£p giá»¯a:

* Maximum Likelihood Estimation
* Policy Gradient
* Regularized Optimization

PhÆ°Æ¡ng phÃ¡p nÃ y hiá»‡n lÃ  ná»n táº£ng cá»§a cÃ¡c há»‡ thá»‘ng LLM hiá»‡n Ä‘áº¡i.

---

# TÃ i liá»‡u tham kháº£o

1. Ouyang, L. et al. (2022). *Training language models to follow instructions with human feedback*.
2. Schulman, J. et al. (2017). *Proximal Policy Optimization Algorithms*.
3. Sutton, R., Barto, A. (2018). *Reinforcement Learning: An Introduction*.
4. Radford, A. et al. (2019). *Language Models are Unsupervised Multitask Learners*.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Instruction Tuning (Tinh Chá»‰nh Báº±ng Chá»‰ Thá»‹) Trong CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n (LLMs)](aero_llm_01_what_is_instruction_tuning.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_what_is_instruction_tuning.md) |
| [Instruction Tuning trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n](aero_llm_02_some_datasets_for_instruction_tuning.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_some_datasets_for_instruction_tuning.md) |
| [Huáº¥n luyá»‡n Chatbot theo Instruction Tuning vÃ  MÃ´ hÃ¬nh Systemâ€“Userâ€“Assistant](aero_llm_03_training_a_chatbot_with_system_user_assistant.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_training_a_chatbot_with_system_user_assistant.md) |
| [Instruction Tuning vá»›i GPT-2 trong Huáº¥n luyá»‡n MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_llm_04_instruction_tuning_with_gpt2.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_instruction_tuning_with_gpt2.md) |
| [aero llm 05 codechallenge instruction tuning gpt2 large part 1](aero_llm_05_codechallenge_instruction_tuning_gpt2_large_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_codechallenge_instruction_tuning_gpt2_large_part_1_.md) |
| [PhÃ¢n tÃ­ch nÃ¢ng cao quÃ¡ trÃ¬nh Instruction Tuning cho GPT-2 Large: á»”n Ä‘á»‹nh huáº¥n luyá»‡n, Ä‘á»™ng há»c gradient vÃ  tá»‘i Æ°u hoÃ¡ tÃ­nh toÃ¡n](aero_llm_06_codechallenge_instruction_tuning_gpt2_large_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_instruction_tuning_gpt2_large_part_2_.md) |
| ğŸ“Œ **[Reinforcement Learning from Human Feedback (RLHF): CÆ¡ sá»Ÿ lÃ½ thuyáº¿t, mÃ´ hÃ¬nh toÃ¡n há»c vÃ  á»©ng dá»¥ng trong huáº¥n luyá»‡n mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_llm_07_reinforcement_learning_from_human_feedback_rlhf_.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_07_reinforcement_learning_from_human_feedback_rlhf_.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

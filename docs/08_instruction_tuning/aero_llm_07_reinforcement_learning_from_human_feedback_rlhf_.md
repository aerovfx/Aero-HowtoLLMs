
<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [08 instruction tuning](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Reinforcement Learning from Human Feedback (RLHF): C∆° s·ªü l√Ω thuy·∫øt, m√¥ h√¨nh to√°n h·ªçc v√† ·ª©ng d·ª•ng trong hu·∫•n luy·ªán m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn

---

## T√≥m t·∫Øt

Reinforcement Learning from Human Feedback (RLHF) l√† ph∆∞∆°ng ph√°p hu·∫•n luy·ªán m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLMs) nh·∫±m t·ªëi ∆∞u h√≥a ƒë·∫ßu ra theo ƒë√°nh gi√° c·ªßa con ng∆∞·ªùi. B√†i vi·∫øt n√†y tr√¨nh b√†y c∆° s·ªü to√°n h·ªçc c·ªßa RLHF, ph√¢n t√≠ch t·ª´ng giai ƒëo·∫°n hu·∫•n luy·ªán (Supervised Fine-Tuning, Reward Modeling, Policy Optimization), v√† th·∫£o lu·∫≠n vai tr√≤ c·ªßa PPO c√πng regularization KL-divergence. Ph√¢n t√≠ch ƒë∆∞·ª£c ƒë·∫∑t trong b·ªëi c·∫£nh c√°c m√¥ h√¨nh GPT do OpenAI ph√°t tri·ªÉn, ƒë·∫∑c bi·ªát l√† InstructGPT (Ouyang et al., 2022).

---

# 1. Gi·ªõi thi·ªáu

C√°c m√¥ h√¨nh ng√¥n ng·ªØ nh∆∞ GPT-2 hay GPT-3 ƒë∆∞·ª£c hu·∫•n luy·ªán theo m·ª•c ti√™u d·ª± ƒëo√°n token k·∫ø ti·∫øp:

$$
P(x_1, x_2, ..., x_T) = \prod_{t=1}^{T} P(x_t \mid x_{\lt t})
$$

Tuy nhi√™n, m·ª•c ti√™u t·ªëi ƒëa h√≥a likelihood kh√¥ng ƒë·∫£m b·∫£o m√¥ h√¨nh:

* Tu√¢n th·ªß ch·ªâ th·ªã (instruction-following)
* Tr·∫£ l·ªùi an to√†n
* Ph√π h·ª£p v·ªõi gi√° tr·ªã con ng∆∞·ªùi

RLHF ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t ƒë·ªÉ gi·∫£i quy·∫øt kho·∫£ng c√°ch gi·ªØa t·ªëi ∆∞u h√≥a x√°c su·∫•t v√† t·ªëi ∆∞u h√≥a s·ª± h√†i l√≤ng c·ªßa con ng∆∞·ªùi.

---

# 2. Khung l√Ω thuy·∫øt Reinforcement Learning

Trong RL c·ªï ƒëi·ªÉn, ta c√≥:

* Tr·∫°ng th√°i: $s$
* H√†nh ƒë·ªông: $a$
* Ch√≠nh s√°ch: $\pi_\theta(a\mids$ )
* Ph·∫ßn th∆∞·ªüng: ( r(s,a) )

M·ª•c ti√™u t·ªëi ∆∞u:

$$
\max_\theta \mathbb{E}*{\tau \sim \pi*\theta} \left[ \sum_{t=0}^{T} \gamma^t r_t \right]
$$

Trong RLHF:

* Tr·∫°ng th√°i $s$: prompt (instruction)
* H√†nh ƒë·ªông $a$: chu·ªói ph·∫£n h·ªìi
* Reward: ƒëi·ªÉm ƒë√°nh gi√° t·ª´ con ng∆∞·ªùi ho·∫∑c reward model

---

# 3. Quy tr√¨nh RLHF

## 3.1. B∆∞·ªõc 1 ‚Äì Supervised Fine-Tuning (SFT)

Hu·∫•n luy·ªán tr√™n d·ªØ li·ªáu c·∫∑p (instruction, response):

$$
\mathcal{L}*{SFT} = - \sum*{t \in R} \log P_\theta(x_t \mid x_{\lt t})
$$

M·ª•c ti√™u: ƒë∆∞a m√¥ h√¨nh v·ªÅ ph√¢n ph·ªëi g·∫ßn v·ªõi h√†nh vi mong mu·ªën.

---

## 3.2. B∆∞·ªõc 2 ‚Äì Hu·∫•n luy·ªán Reward Model

Cho hai ph·∫£n h·ªìi ( y_1, y_2 ) v·ªõi c√πng prompt $x$, con ng∆∞·ªùi ch·ªçn ph·∫£n h·ªìi t·ªët h∆°n.

Reward model $r_\phi(x,y$ ) ƒë∆∞·ª£c hu·∫•n luy·ªán b·∫±ng loss Bradley-Terry:

$$
P(y_1 \succ y_2) = \frac{e^{r_\phi(x,y_1)}}{e^{r_\phi(x,y_1)} + e^{r_\phi(x,y_2)}}
$$

Loss:

$$
\mathcal{L}*{RM} = - \log \sigma(r*\phi(x,y_{chosen}) - r_\phi(x,y_{rejected}))
$$

Trong ƒë√≥ $\sigma$ l√† sigmoid:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

---

## 3.3. B∆∞·ªõc 3 ‚Äì Policy Optimization (PPO)

Sau khi c√≥ reward model, ta t·ªëi ∆∞u policy:

$$
\max_\theta \mathbb{E}*{x \sim \pi*\theta} \left[ r_\phi(x) - \beta D_{KL}(\pi_\theta | \pi_{ref}) \right]
$$

Trong ƒë√≥:

* $\pi_{ref}$: m√¥ h√¨nh SFT ban ƒë·∫ßu
* $D_{KL}$: KL divergence

$$
D_{KL}(P|Q) = \sum_x P(x)\log\frac{P(x)}{Q(x)}
$$

---

# 4. Proximal Policy Optimization (PPO)

PPO t·ªëi ∆∞u h√†m m·ª•c ti√™u:

$$
L^{CLIP}(\theta) = \mathbb{E} \left[ \min \left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right]
$$

Trong ƒë√≥:

$$
r_t(\theta) = \frac{\pi_\theta(a_t|s_t)} {\pi_{\theta_{old}}(a_t|s_t)}
$$

$A_t$: advantage estimate.

Clipping gi√∫p:

* Tr√°nh c·∫≠p nh·∫≠t qu√° l·ªõn
* ·ªîn ƒë·ªãnh hu·∫•n luy·ªán

---

# 5. KL Regularization

N·∫øu kh√¥ng c√≥ KL penalty:

$$
\pi_\theta \to \text{mode collapse}
$$

V·ªõi KL:

$$
\mathcal{L} = \mathbb{E}[r(x)] * \beta D_{KL}(\pi_\theta | \pi_{ref})
$$

KL ƒë√≥ng vai tr√≤ nh∆∞ regularizer:

* Gi·ªØ m√¥ h√¨nh g·∫ßn ph√¢n ph·ªëi g·ªëc
* Tr√°nh h√†nh vi b·∫•t th∆∞·ªùng

---

# 6. Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n

Gi·∫£ s·ª≠:

* $L$: s·ªë layer
* $T$: chi·ªÅu d√†i chu·ªói
* $d$: embedding dimension

Self-attention:

$$
\mathcal{O}(L \cdot T^2 \cdot d)
$$

Trong RLHF:

* M·ªói b∆∞·ªõc c·∫ßn forward nhi·ªÅu m·∫´u
* T√≠nh th√™m reward model
* T√≠nh KL divergence

Chi ph√≠ tƒÉng g·∫•p 2‚Äì3 l·∫ßn so v·ªõi SFT.

---

# 7. C√°c v·∫•n ƒë·ªÅ l√Ω thuy·∫øt

## 7.1. Reward Hacking

M√¥ h√¨nh c√≥ th·ªÉ t·ªëi ƒëa h√≥a reward model nh∆∞ng kh√¥ng th·ª±c s·ª± t·ªët.

Gi·∫£ s·ª≠ reward model x·∫•p x·ªâ:

$$
r_\phi(x) = r_{true}(x) + \epsilon(x)
$$

Khi t·ªëi ∆∞u:

$$
\max_\theta \mathbb{E}[r_\phi(x)]
$$

Sai s·ªë $\epsilon(x$ ) c√≥ th·ªÉ b·ªã khai th√°c.

---

## 7.2. Alignment Problem

Ta mu·ªën:

$$
\pi_\theta \approx \pi_{human}
$$

Nh∆∞ng reward ch·ªâ l√† x·∫•p x·ªâ.

ƒê√¢y l√† trung t√¢m c·ªßa nghi√™n c·ª©u alignment hi·ªán ƒë·∫°i.

---

# 8. So s√°nh v·ªõi c√°c h∆∞·ªõng ti·∫øp c·∫≠n kh√°c

| Ph∆∞∆°ng ph√°p | ∆Øu ƒëi·ªÉm                  | Nh∆∞·ª£c ƒëi·ªÉm         |
| ----------- | ------------------------ | ------------------ |
| SFT         | ƒê∆°n gi·∫£n                 | Ph·ª• thu·ªôc d·ªØ li·ªáu  |
| RLHF        | Linh ho·∫°t, alignment t·ªët | T·ªën chi ph√≠        |
| DPO         | Kh√¥ng c·∫ßn PPO            | Gi·ªõi h·∫°n l√Ω thuy·∫øt |

---

# 9. Th·∫£o lu·∫≠n

RLHF l√† c·∫ßu n·ªëi gi·ªØa:

* H·ªçc c√≥ gi√°m s√°t
* H·ªçc tƒÉng c∆∞·ªùng
* H·ªçc theo gi√° tr·ªã con ng∆∞·ªùi

C√°ch ti·∫øp c·∫≠n n√†y ƒë√£ ƒë∆∞·ª£c √°p d·ª•ng trong c√°c m√¥ h√¨nh GPT c·ªßa OpenAI v√† m·ªü ra h∆∞·ªõng ph√°t tri·ªÉn LLM an to√†n h∆°n.

---

# 10. K·∫øt lu·∫≠n

RLHF cho ph√©p:

1. T·ªëi ∆∞u h√≥a h√†nh vi thay v√¨ ch·ªâ t·ªëi ∆∞u x√°c su·∫•t
2. K·∫øt h·ª£p ƒë√°nh gi√° con ng∆∞·ªùi v√†o v√≤ng l·∫∑p hu·∫•n luy·ªán
3. Ki·ªÉm so√°t m√¥ h√¨nh th√¥ng qua KL-regularization

V·ªÅ m·∫∑t to√°n h·ªçc, RLHF l√† s·ª± k·∫øt h·ª£p gi·ªØa:

* Maximum Likelihood Estimation
* Policy Gradient
* Regularized Optimization

Ph∆∞∆°ng ph√°p n√†y hi·ªán l√† n·ªÅn t·∫£ng c·ªßa c√°c h·ªá th·ªëng LLM hi·ªán ƒë·∫°i.

---

# T√†i li·ªáu tham kh·∫£o

1. Ouyang, L. et al. (2022). *Training language models to follow instructions with human feedback*.
2. Schulman, J. et al. (2017). *Proximal Policy Optimization Algorithms*.
3. Sutton, R., Barto, A. (2018). *Reinforcement Learning: An Introduction*.
4. Radford, A. et al. (2019). *Language Models are Unsupervised Multitask Learners*.
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [Instruction Tuning (Tinh Ch·ªânh B·∫±ng Ch·ªâ Th·ªã) Trong C√°c M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn (LLMs)](aero_llm_01_what_is_instruction_tuning.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_01_what_is_instruction_tuning.md) |
| [Instruction Tuning trong M√¥ h√¨nh Ng√¥n ng·ªØ L·ªõn](aero_llm_02_some_datasets_for_instruction_tuning.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_02_some_datasets_for_instruction_tuning.md) |
| [Hu·∫•n luy·ªán Chatbot theo Instruction Tuning v√† M√¥ h√¨nh System‚ÄìUser‚ÄìAssistant](aero_llm_03_training_a_chatbot_with_system_user_assistant.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_03_training_a_chatbot_with_system_user_assistant.md) |
| [Instruction Tuning v·ªõi GPT-2 trong Hu·∫•n luy·ªán M√¥ h√¨nh Ng√¥n ng·ªØ](aero_llm_04_instruction_tuning_with_gpt2.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_04_instruction_tuning_with_gpt2.md) |
| [aero llm 05 codechallenge instruction tuning gpt2 large part 1](aero_llm_05_codechallenge_instruction_tuning_gpt2_large_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_05_codechallenge_instruction_tuning_gpt2_large_part_1_.md) |
| [Ph√¢n t√≠ch n√¢ng cao qu√° tr√¨nh Instruction Tuning cho GPT-2 Large: ·ªîn ƒë·ªãnh hu·∫•n luy·ªán, ƒë·ªông h·ªçc gradient v√† t·ªëi ∆∞u ho√° t√≠nh to√°n](aero_llm_06_codechallenge_instruction_tuning_gpt2_large_part_2_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_06_codechallenge_instruction_tuning_gpt2_large_part_2_.md) |
| üìå **[Reinforcement Learning from Human Feedback (RLHF): C∆° s·ªü l√Ω thuy·∫øt, m√¥ h√¨nh to√°n h·ªçc v√† ·ª©ng d·ª•ng trong hu·∫•n luy·ªán m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn](aero_llm_07_reinforcement_learning_from_human_feedback_rlhf_.md)** | [Xem b√†i vi·∫øt ‚Üí](aero_llm_07_reinforcement_learning_from_human_feedback_rlhf_.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

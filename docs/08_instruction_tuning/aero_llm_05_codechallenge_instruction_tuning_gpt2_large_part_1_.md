
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [08 instruction tuning](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
1. Giá»›i thiá»‡u

MÃ´ hÃ¬nh GPT-2 Ä‘Æ°á»£c cÃ´ng bá»‘ bá»Ÿi nhÃ³m nghiÃªn cá»©u táº¡i OpenAI (Radford et al., 2019) dÆ°á»›i sá»± dáº«n dáº¯t cá»§a Alec Radford. GPT-2 dá»±a trÃªn kiáº¿n trÃºc Transformer decoder-only vÃ  Ä‘Æ°á»£c huáº¥n luyá»‡n theo má»¥c tiÃªu mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ tá»± há»“i quy:

$P(x)$ = $\prod$_{t=1}^{T} $P($x_t$ \mid x_{\lt t})$

Trong Ä‘Ã³:

â€¢	x = (x_1, x_2, ..., x_T) lÃ  chuá»—i token

	â€¢	x_{<t} lÃ  cÃ¡c token trÆ°á»›c thá»i Ä‘iá»ƒm t

Instruction tuning má»Ÿ rá»™ng cÃ¡ch tiáº¿p cáº­n nÃ y báº±ng cÃ¡ch huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn dá»¯ liá»‡u gá»“m cáº·p (instruction, response), nháº±m tá»‘i Æ°u kháº£ nÄƒng tuÃ¢n thá»§ yÃªu cáº§u ngÆ°á»i dÃ¹ng.

â¸»

2. Kiáº¿n trÃºc GPT-2 Large

GPT-2 Large cÃ³ khoáº£ng 1.5 tá»· tham sá»‘, vá»›i cáº¥u hÃ¬nh Ä‘iá»ƒn hÃ¬nh:

$$
â€¢	Sá»‘ táº§ng Transformer: L = 36 â€¢	KÃ­ch thÆ°á»›c embedding: d_{model} = 1280 â€¢	Sá»‘ head attention: h = 20 â€¢	KÃ­ch thÆ°á»›c táº§ng MLP trung gian: d_{ff} = 4 \times d_{model} = 5120
$$

2.1. CÆ¡ cháº¿ Self-Attention

Trong má»—i táº§ng Transformer, attention Ä‘Æ°á»£c tÃ­nh theo cÃ´ng thá»©c:

\text{Attention}(Q, K, V) = \text{softmax} \left\frac{QK^T}{\sqrt{d_k}} \rightV

Trong Ä‘Ã³:

â€¢	Q = XW_Q

$$
â€¢	K = XW_K
$$

â€¢	V = XW_V

Multi-head attention Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a:

\text{MultiHead}(Q,K,V) = \text{Concat}(head_1,...,head_h)W_O

2.2. Khá»‘i MLP

Sau attention lÃ  táº§ng feed-forward:

\text{MLP}x = \text{GELU}(xW_1 + b_1)W_2 + b_2

Viá»‡c má»Ÿ rá»™ng chiá»u khÃ´ng gian lÃªn 4 \times d_{model} giÃºp tÄƒng kháº£ nÄƒng biá»ƒu diá»…n phi tuyáº¿n.

â¸»

3. PhÃ¢n tÃ­ch dá»¯ liá»‡u Q&A

Trong bÃ i toÃ¡n instruction tuning, dá»¯ liá»‡u gá»“m:
	â€¢	CÃ¢u há»i (instruction)
	â€¢	CÃ¢u tráº£ lá»i (response)

3.1. Thá»‘ng kÃª Ä‘á»™ dÃ i token

Giáº£ sá»­:
	â€¢	$Q_i$: Ä‘á»™ dÃ i cÃ¢u há»i thá»© i
	â€¢	$A_i$: Ä‘á»™ dÃ i cÃ¢u tráº£ lá»i thá»© i

Tá»•ng sá»‘ token:

N_Q = \sum_{i=1}^{n} Q_i

$$
N_A = \sum_{i=1}^{n} A_i Káº¿t quáº£ quan sÃ¡t thá»±c nghiá»‡m cho tháº¥y: \mathbb{E}[A_i] \gg \mathbb{E}[Q_i] Äiá»u nÃ y dáº«n Ä‘áº¿n máº¥t cÃ¢n báº±ng trong gradient khi tá»‘i Æ°u hÃ³a. â¸» 4. HÃ m máº¥t mÃ¡t vÃ  tá»‘i Æ°u hÃ³a Má»¥c tiÃªu huáº¥n luyá»‡n lÃ  tá»‘i thiá»ƒu hÃ³a cross-entropy:
$$

\mathcal${L} = - $\sum$_{t=1}^{T} $\log$ P_\theta $x_t \mid x_{\lt t}

$$
Trong instruction tuning, ta thÆ°á»ng: â€¢	Ná»‘i instruction vÃ  response thÃ nh má»™t chuá»—i â€¢	Che (mask) loss pháº§n instruction â€¢	Chá»‰ tá»‘i Æ°u pháº§n response Khi Ä‘Ã³:
$$

\mathcal${L}_{response} = - $\sum$_{t \in R} $\log$ P_\theta $x_t \mid x_{\lt t}

$$
vá»›i R lÃ  táº­p token thuá»™c response. â¸» 5. TÃ¡c Ä‘á»™ng cá»§a phÃ¢n bá»‘ token Ä‘áº¿n huáº¥n luyá»‡n 5.1. Máº¥t cÃ¢n báº±ng gradient VÃ¬ response dÃ i hÆ¡n nhiá»u so vá»›i instruction: |R| \gg |Q| Äiá»u nÃ y dáº«n Ä‘áº¿n: â€¢	Gradient chá»§ yáº¿u Ä‘áº¿n tá»« response â€¢	Instruction Ã­t áº£nh hÆ°á»Ÿng náº¿u khÃ´ng masking há»£p lÃ½ 5.2. Giá»›i háº¡n chiá»u dÃ i ngá»¯ cáº£nh Náº¿u Ä‘á»™ dÃ i tá»‘i Ä‘a lÃ  T_{max}: |Q| + |A| \le T_{max} Vá»›i GPT-2:
$$

T_{max} = 1024

$$
Náº¿u cÃ¢u tráº£ lá»i quÃ¡ dÃ i, instruction cÃ³ thá»ƒ bá»‹ cáº¯t ngáº¯n â†’ giáº£m kháº£ nÄƒng hiá»ƒu ngá»¯ cáº£nh. â¸» 6. So sÃ¡nh vá»›i cÃ¡c hÆ°á»›ng tiáº¿p cáº­n hiá»‡n Ä‘áº¡i Instruction tuning sau nÃ y (vÃ­ dá»¥ InstructGPT) bá»• sung: 1.	Supervised fine-tuning (SFT) 2.	Reinforcement Learning from Human Feedback (RLHF) HÃ m má»¥c tiÃªu trong RLHF:
$$

\max_\theta \mathbb{E}_{x \sim \pi_\theta} [ rx ]

$$
Trong Ä‘Ã³ rx lÃ  reward model Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i. â¸» 7. CÃ¡c cÃ¢n nháº¯c thá»±c tiá»…n khi huáº¥n luyá»‡n GPT-2 Large 7.1. Bá»™ nhá»› vÃ  batch size Vá»›i 1.5B tham sá»‘: \text{Memory} \approx 6 - 12 \text{ GB (FP16)} Gradient accumulation thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng:
$$

\text{Effective Batch Size} = \text{Micro Batch} \times \text{Steps}

$$
7.2. Learning rate ThÃ´ng thÆ°á»ng: \eta \in [10^{-5}, 10^{-4}] Vá»›i warmup: \eta_t = \eta_{max} \cdot \frac{t}{T_{warmup}}
$$



<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [08 instruction tuning](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
1. Giá»›i thiá»‡u

MÃ´ hÃ¬nh GPT-2 Ä‘Æ°á»£c cÃ´ng bá»‘ bá»Ÿi nhÃ³m nghiÃªn cá»©u táº¡i OpenAI (Radford et al., 2019) dÆ°á»›i sá»± dáº«n dáº¯t cá»§a Alec Radford. GPT-2 dá»±a trÃªn kiáº¿n trÃºc Transformer decoder-only vÃ  Ä‘Æ°á»£c huáº¥n luyá»‡n theo má»¥c tiÃªu mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ tá»± há»“i quy:

P(x) = \prod_{t=1}^{T} P(x_t \mid x_{<t})

Trong Ä‘Ã³:
	â€¢	x = (x_1, x_2, ..., x_T) lÃ  chuá»—i token
	â€¢	x_{<t} lÃ  cÃ¡c token trÆ°á»›c thá»i Ä‘iá»ƒm t

Instruction tuning má»Ÿ rá»™ng cÃ¡ch tiáº¿p cáº­n nÃ y báº±ng cÃ¡ch huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn dá»¯ liá»‡u gá»“m cáº·p (instruction, response), nháº±m tá»‘i Æ°u kháº£ nÄƒng tuÃ¢n thá»§ yÃªu cáº§u ngÆ°á»i dÃ¹ng.

â¸»

2. Kiáº¿n trÃºc GPT-2 Large

GPT-2 Large cÃ³ khoáº£ng 1.5 tá»· tham sá»‘, vá»›i cáº¥u hÃ¬nh Ä‘iá»ƒn hÃ¬nh:
	â€¢	Sá»‘ táº§ng Transformer: L = 36
	â€¢	KÃ­ch thÆ°á»›c embedding: d_{model} = 1280
	â€¢	Sá»‘ head attention: h = 20
	â€¢	KÃ­ch thÆ°á»›c táº§ng MLP trung gian: d_{ff} = 4 \times d_{model} = 5120

2.1. CÆ¡ cháº¿ Self-Attention

Trong má»—i táº§ng Transformer, attention Ä‘Æ°á»£c tÃ­nh theo cÃ´ng thá»©c:

\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right)V

Trong Ä‘Ã³:
	â€¢	Q = XW_Q
	â€¢	K = XW_K
	â€¢	V = XW_V

Multi-head attention Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a:

\text{MultiHead}(Q,K,V) = \text{Concat}(head_1,...,head_h)W_O

2.2. Khá»‘i MLP

Sau attention lÃ  táº§ng feed-forward:

\text{MLP}(x) = \text{GELU}(xW_1 + b_1)W_2 + b_2

Viá»‡c má»Ÿ rá»™ng chiá»u khÃ´ng gian lÃªn 4 \times d_{model} giÃºp tÄƒng kháº£ nÄƒng biá»ƒu diá»…n phi tuyáº¿n.

â¸»

3. PhÃ¢n tÃ­ch dá»¯ liá»‡u Q&A

Trong bÃ i toÃ¡n instruction tuning, dá»¯ liá»‡u gá»“m:
	â€¢	CÃ¢u há»i (instruction)
	â€¢	CÃ¢u tráº£ lá»i (response)

3.1. Thá»‘ng kÃª Ä‘á»™ dÃ i token

Giáº£ sá»­:
	â€¢	Q_i: Ä‘á»™ dÃ i cÃ¢u há»i thá»© i
	â€¢	A_i: Ä‘á»™ dÃ i cÃ¢u tráº£ lá»i thá»© i

Tá»•ng sá»‘ token:

N_Q = \sum_{i=1}^{n} Q_i

N_A = \sum_{i=1}^{n} A_i

Káº¿t quáº£ quan sÃ¡t thá»±c nghiá»‡m cho tháº¥y:

\mathbb{E}[A_i] \gg \mathbb{E}[Q_i]

Äiá»u nÃ y dáº«n Ä‘áº¿n máº¥t cÃ¢n báº±ng trong gradient khi tá»‘i Æ°u hÃ³a.

â¸»

4. HÃ m máº¥t mÃ¡t vÃ  tá»‘i Æ°u hÃ³a

Má»¥c tiÃªu huáº¥n luyá»‡n lÃ  tá»‘i thiá»ƒu hÃ³a cross-entropy:

\mathcal{L} = - \sum_{t=1}^{T} \log P_\theta (x_t \mid x_{<t})

Trong instruction tuning, ta thÆ°á»ng:
	â€¢	Ná»‘i instruction vÃ  response thÃ nh má»™t chuá»—i
	â€¢	Che (mask) loss pháº§n instruction
	â€¢	Chá»‰ tá»‘i Æ°u pháº§n response

Khi Ä‘Ã³:

\mathcal{L}_{response} = - \sum_{t \in R} \log P_\theta (x_t \mid x_{<t})

vá»›i R lÃ  táº­p token thuá»™c response.

â¸»

5. TÃ¡c Ä‘á»™ng cá»§a phÃ¢n bá»‘ token Ä‘áº¿n huáº¥n luyá»‡n

5.1. Máº¥t cÃ¢n báº±ng gradient

VÃ¬ response dÃ i hÆ¡n nhiá»u so vá»›i instruction:

|R| \gg |Q|

Äiá»u nÃ y dáº«n Ä‘áº¿n:
	â€¢	Gradient chá»§ yáº¿u Ä‘áº¿n tá»« response
	â€¢	Instruction Ã­t áº£nh hÆ°á»Ÿng náº¿u khÃ´ng masking há»£p lÃ½

5.2. Giá»›i háº¡n chiá»u dÃ i ngá»¯ cáº£nh

Náº¿u Ä‘á»™ dÃ i tá»‘i Ä‘a lÃ  T_{max}:

|Q| + |A| \le T_{max}

Vá»›i GPT-2:

T_{max} = 1024

Náº¿u cÃ¢u tráº£ lá»i quÃ¡ dÃ i, instruction cÃ³ thá»ƒ bá»‹ cáº¯t ngáº¯n â†’ giáº£m kháº£ nÄƒng hiá»ƒu ngá»¯ cáº£nh.

â¸»

6. So sÃ¡nh vá»›i cÃ¡c hÆ°á»›ng tiáº¿p cáº­n hiá»‡n Ä‘áº¡i

Instruction tuning sau nÃ y (vÃ­ dá»¥ InstructGPT) bá»• sung:
	1.	Supervised fine-tuning (SFT)
	2.	Reinforcement Learning from Human Feedback (RLHF)

HÃ m má»¥c tiÃªu trong RLHF:

\max_\theta \mathbb{E}_{x \sim \pi_\theta} [ r(x) ]

Trong Ä‘Ã³ r(x) lÃ  reward model Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i.

â¸»

7. CÃ¡c cÃ¢n nháº¯c thá»±c tiá»…n khi huáº¥n luyá»‡n GPT-2 Large

7.1. Bá»™ nhá»› vÃ  batch size

Vá»›i 1.5B tham sá»‘:

\text{Memory} \approx 6 - 12 \text{ GB (FP16)}

Gradient accumulation thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng:

\text{Effective Batch Size} = \text{Micro Batch} \times \text{Steps}

7.2. Learning rate

ThÃ´ng thÆ°á»ng:

\eta \in [10^{-5}, 10^{-4}]

Vá»›i warmup:

\eta_t = \eta_{max} \cdot \frac{t}{T_{warmup}}

â¸»

8. Tháº£o luáº­n

Instruction tuning cho GPT-2 Large cho tháº¥y:
	â€¢	MÃ´ hÃ¬nh lá»›n cÃ³ kháº£ nÄƒng tá»•ng quÃ¡t tá»‘t hÆ¡n
	â€¢	PhÃ¢n bá»‘ token áº£nh hÆ°á»Ÿng máº¡nh Ä‘áº¿n gradient
	â€¢	Masking loss lÃ  quyáº¿t Ä‘á»‹nh thiáº¿t káº¿ quan trá»ng
	â€¢	Chi phÃ­ tÃ­nh toÃ¡n tÄƒng theo:

\mathcal{O}(L \cdot T^2 \cdot d_{model})

Do self-attention cÃ³ Ä‘á»™ phá»©c táº¡p báº­c hai theo chiá»u dÃ i chuá»—i.

â¸»

9. Káº¿t luáº­n

Tinh chá»‰nh GPT-2 Large cho bÃ i toÃ¡n há»iâ€“Ä‘Ã¡p minh há»a rÃµ:
	1.	Táº§m quan trá»ng cá»§a kiáº¿n trÃºc Transformer
	2.	áº¢nh hÆ°á»Ÿng cá»§a phÃ¢n bá»‘ token
	3.	Vai trÃ² cá»§a thiáº¿t káº¿ hÃ m máº¥t mÃ¡t
	4.	CÃ¡c rÃ ng buá»™c thá»±c táº¿ vá» tÃ i nguyÃªn

PhÃ¢n tÃ­ch nÃ y cho tháº¥y instruction tuning khÃ´ng chá»‰ lÃ  fine-tuning thÃ´ng thÆ°á»ng mÃ  lÃ  má»™t quÃ¡ trÃ¬nh thiáº¿t káº¿ cáº©n trá»ng giá»¯a dá»¯ liá»‡u, kiáº¿n trÃºc vÃ  má»¥c tiÃªu tá»‘i Æ°u.

â¸»

TÃ i liá»‡u tham kháº£o
	1.	Radford, A. et al. (2019). Language Models are Unsupervised Multitask Learners. OpenAI.
	2.	Vaswani, A. et al. (2017). Attention Is All You Need.
	3.	Ouyang, L. et al. (2022). Training language models to follow instructions with human feedback.
	4.	Goodfellow, I., Bengio, Y., Courville, A. (2016). Deep Learning. MIT Press.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Instruction Tuning (Tinh Chá»‰nh Báº±ng Chá»‰ Thá»‹) Trong CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n (LLMs)](aero_llm_01_what_is_instruction_tuning.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_what_is_instruction_tuning.md) |
| [Instruction Tuning trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n](aero_llm_02_some_datasets_for_instruction_tuning.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_some_datasets_for_instruction_tuning.md) |
| [Huáº¥n luyá»‡n Chatbot theo Instruction Tuning vÃ  MÃ´ hÃ¬nh Systemâ€“Userâ€“Assistant](aero_llm_03_training_a_chatbot_with_system_user_assistant.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_training_a_chatbot_with_system_user_assistant.md) |
| [Instruction Tuning vá»›i GPT-2 trong Huáº¥n luyá»‡n MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_llm_04_instruction_tuning_with_gpt2.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_instruction_tuning_with_gpt2.md) |
| ğŸ“Œ **[aero llm 05 codechallenge instruction tuning gpt2 large part 1](aero_llm_05_codechallenge_instruction_tuning_gpt2_large_part_1_.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_05_codechallenge_instruction_tuning_gpt2_large_part_1_.md) |
| [PhÃ¢n tÃ­ch nÃ¢ng cao quÃ¡ trÃ¬nh Instruction Tuning cho GPT-2 Large: á»”n Ä‘á»‹nh huáº¥n luyá»‡n, Ä‘á»™ng há»c gradient vÃ  tá»‘i Æ°u hoÃ¡ tÃ­nh toÃ¡n](aero_llm_06_codechallenge_instruction_tuning_gpt2_large_part_2_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_instruction_tuning_gpt2_large_part_2_.md) |
| [Reinforcement Learning from Human Feedback (RLHF): CÆ¡ sá»Ÿ lÃ½ thuyáº¿t, mÃ´ hÃ¬nh toÃ¡n há»c vÃ  á»©ng dá»¥ng trong huáº¥n luyá»‡n mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_llm_07_reinforcement_learning_from_human_feedback_rlhf_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_reinforcement_learning_from_human_feedback_rlhf_.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

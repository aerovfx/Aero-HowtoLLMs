
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [08 instruction tuning](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Instruction Tuning vá»›i GPT-2 trong Huáº¥n luyá»‡n MÃ´ hÃ¬nh NgÃ´n ngá»¯

## TÃ³m táº¯t

BÃ i viáº¿t nÃ y trÃ¬nh bÃ y phÆ°Æ¡ng phÃ¡p Instruction Tuning Ã¡p dá»¥ng cho mÃ´ hÃ¬nh GPT-2 dá»±a trÃªn tÃ i liá»‡u Ä‘Ã­nh kÃ¨m. Ná»™i dung táº­p trung vÃ o quy trÃ¬nh xÃ¢y dá»±ng dá»¯ liá»‡u, ká»¹ thuáº­t tinh chá»‰nh mÃ´ hÃ¬nh, cÆ¡ sá»Ÿ toÃ¡n há»c vÃ  Ä‘Ã¡nh giÃ¡ hiá»‡u nÄƒng. NgoÃ i ra, bÃ i viáº¿t bá»• sung cÃ¡c nguá»“n tham kháº£o há»c thuáº­t nháº±m lÃ m rÃµ vai trÃ² cá»§a Instruction Tuning trong phÃ¡t triá»ƒn chatbot vÃ  trá»£ lÃ½ áº£o hiá»‡n Ä‘áº¡i.

---

## 1. Giá»›i thiá»‡u

MÃ´ hÃ¬nh GPT-2 lÃ  má»™t trong nhá»¯ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ ná»n táº£ng Ä‘áº·t ná»n mÃ³ng cho sá»± phÃ¡t triá»ƒn cá»§a cÃ¡c Large Language Models (LLMs). Tuy nhiÃªn, mÃ´ hÃ¬nh gá»‘c chá»§ yáº¿u Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u vÄƒn báº£n thuáº§n, chÆ°a tá»‘i Æ°u cho viá»‡c lÃ m theo yÃªu cáº§u cá»§a ngÆ°á»i dÃ¹ng. Instruction Tuning ra Ä‘á»i nháº±m kháº¯c phá»¥c háº¡n cháº¿ nÃ y, giÃºp mÃ´ hÃ¬nh pháº£n há»“i chÃ­nh xÃ¡c vÃ  phÃ¹ há»£p hÆ¡n vá»›i ngá»¯ cáº£nh.

TÃ i liá»‡u Ä‘Ã­nh kÃ¨m "Instruction Tuning with GPT-2" cho tháº¥y quÃ¡ trÃ¬nh tinh chá»‰nh GPT-2 cÃ³ thá»ƒ thá»±c hiá»‡n hiá»‡u quáº£ ngay cáº£ vá»›i tÃ i nguyÃªn tÃ­nh toÃ¡n háº¡n cháº¿.

---

## 2. Tá»•ng quan vá» GPT-2

### 2.1. Kiáº¿n trÃºc Transformer

GPT-2 Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn kiáº¿n trÃºc Transformer vá»›i cÆ¡ cháº¿ Self-Attention. Má»—i lá»›p Transformer bao gá»“m:

* Multi-Head Attention
* Feed-Forward Network
* Layer Normalization
* Residual Connection

### 2.2. Biá»ƒu diá»…n chuá»—i Ä‘áº§u vÃ o

Chuá»—i Ä‘áº§u vÃ o Ä‘Æ°á»£c mÃ£ hÃ³a thÃ nh cÃ¡c token:

$$

$$

X = (x_1, x_2, ..., x_T)

$$

$$

vÃ  Ä‘Æ°á»£c Ã¡nh xáº¡ thÃ nh vector nhÃºng:

$$

$$

e_t = E(x_t)

$$

$$

Trong Ä‘Ã³ $E$ lÃ  ma tráº­n embedding.

---

## 3. Instruction Tuning vá»›i GPT-2

### 3.1. Cáº¥u trÃºc dá»¯ liá»‡u huáº¥n luyá»‡n

Dá»¯ liá»‡u Ä‘Æ°á»£c tá»• chá»©c dÆ°á»›i dáº¡ng:

$$

$$

D = {(I_i, Y_i)}_{i=1}^{N}

$$

$$

Trong Ä‘Ã³:

* $I_i$: cÃ¢u lá»‡nh
* $Y_i$: pháº£n há»“i mong muá»‘n
* $N$: sá»‘ lÆ°á»£ng máº«u

VÃ­ dá»¥:

Instruction: TÃ³m táº¯t Ä‘oáº¡n vÄƒn sau
Response: ...

---

### 3.2. Chuáº©n hÃ³a dá»¯ liá»‡u Ä‘áº§u vÃ o

Má»—i máº«u dá»¯ liá»‡u Ä‘Æ°á»£c chuyá»ƒn thÃ nh chuá»—i:

$$

$$

S_i = [BOS, I_i, SEP, Y_i, EOS]

$$

$$

Trong Ä‘Ã³ BOS, SEP, EOS lÃ  cÃ¡c token Ä‘áº·c biá»‡t.

---

## 4. CÆ¡ sá»Ÿ toÃ¡n há»c cá»§a quÃ¡ trÃ¬nh huáº¥n luyá»‡n

### 4.1. MÃ´ hÃ¬nh xÃ¡c suáº¥t ngÃ´n ngá»¯

GPT-2 mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t chuá»—i:

$$

$$

P(X) = \prod_{t=1}^{T} P(x_t  \mid  x_{\lt t})

$$

$$

---

### 4.2. HÃ m máº¥t mÃ¡t Cross-Entropy

HÃ m máº¥t mÃ¡t Ä‘Æ°á»£c sá»­ dá»¥ng lÃ :

$$

$$

$\mathcal${L} = - \frac{1}{T} $\sum$_{t=1}^{T} $y_t$ $\log$(\hat{y}_t)

$$

$$

Trong Ä‘Ã³:

* $y_t$: nhÃ£n tháº­t
* $\hat{y}_t$: xÃ¡c suáº¥t dá»± Ä‘oÃ¡n

---

### 4.3. Thuáº­t toÃ¡n tá»‘i Æ°u Adam

GPT-2 thÆ°á»ng Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i Adam:

$$

$$

m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t

$$

$$

$$

$$

v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2

$$

$$

$$

$$

\theta_t = \theta_{t-1} - \eta \frac{m_t}{\sqrt{v_t}+\epsilon}

$$

$$

---

## 5. Quy trÃ¬nh Instruction Tuning vá»›i GPT-2

Quy trÃ¬nh gá»“m cÃ¡c bÆ°á»›c:

1. Thu tháº­p dá»¯ liá»‡u instruction
2. LÃ m sáº¡ch vÃ  tiá»n xá»­ lÃ½
3. Chuáº©n hÃ³a dá»¯ liá»‡u theo template
4. Fine-tune GPT-2
5. ÄÃ¡nh giÃ¡ vÃ  tinh chá»‰nh

SÆ¡ Ä‘á»“ tá»•ng quÃ¡t:

Dá»¯ liá»‡u â†’ Tokenizer â†’ GPT-2 â†’ Loss â†’ Adam â†’ Cáº­p nháº­t tham sá»‘

---

## 6. ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh

### 6.1. Chá»‰ sá»‘ Perplexity

$$

$$

PP = \exp(\mathcal{L})

$$

$$

### 6.2. Äá»™ chÃ­nh xÃ¡c theo nhiá»‡m vá»¥

MÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trÃªn cÃ¡c táº­p kiá»ƒm thá»­ instruction.

---

## 7. Thá»±c nghiá»‡m minh há»a

$$
Giáº£ sá»­ táº­p huáº¥n luyá»‡n gá»“m N=10.000 máº«u, sau 5 epoch huáº¥n luyá»‡n, hÃ m máº¥t mÃ¡t há»™i tá»¥:
$$

$$
\mathcal{L}_{final} \approx 1.95
$$

$$
TÆ°Æ¡ng á»©ng:
$$

$$
PP \approx e^{1.95} \approx 7.03
$$


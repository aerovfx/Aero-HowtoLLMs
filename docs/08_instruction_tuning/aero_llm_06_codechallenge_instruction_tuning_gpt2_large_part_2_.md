
<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [08 instruction tuning](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Ph√¢n t√≠ch n√¢ng cao qu√° tr√¨nh Instruction Tuning cho GPT-2 Large: ·ªîn ƒë·ªãnh hu·∫•n luy·ªán, ƒë·ªông h·ªçc gradient v√† t·ªëi ∆∞u ho√° t√≠nh to√°n

## T√≥m t·∫Øt

B√†i vi·∫øt n√†y ti·∫øp t·ª•c ph√¢n t√≠ch qu√° tr√¨nh instruction tuning cho GPT-2 Large (1.5B tham s·ªë), t·∫≠p trung v√†o c√°c v·∫•n ƒë·ªÅ n√¢ng cao g·ªìm: ƒë·ªông h·ªçc gradient, ·ªïn ƒë·ªãnh hu·∫•n luy·ªán (training stability), chi·∫øn l∆∞·ª£c t·ªëi ∆∞u ho√° b·ªô nh·ªõ v√† ·∫£nh h∆∞·ªüng c·ªßa ph√¢n b·ªë ƒë·ªô d√†i chu·ªói. Ph√¢n t√≠ch ƒë∆∞·ª£c ƒë·∫∑t tr√™n n·ªÅn t·∫£ng ki·∫øn tr√∫c Transformer c·ªßa Vaswani et al. (2017) v√† m√¥ h√¨nh GPT-2 do OpenAI c√¥ng b·ªë (Radford et al., 2019). ƒê·ªìng th·ªùi, b√†i vi·∫øt li√™n h·ªá v·ªõi h∆∞·ªõng Instruction Tuning v√† RLHF sau n√†y trong InstructGPT (Ouyang et al., 2022).

---

# 1. B·ªëi c·∫£nh l√Ω thuy·∫øt

## 1.1. M√¥ h√¨nh ng√¥n ng·ªØ t·ª± h·ªìi quy

GPT-2 t·ªëi ∆∞u ho√° x√°c su·∫•t chu·ªói:

$$
P(x_1, x_2, ..., x_T) = \prod_{t=1}^{T} P(x_t \mid x_{\lt t})
$$

H√†m m·∫•t m√°t cross-entropy:

$$
\mathcal{L}(\theta) = - \sum_{t=1}^{T} \log P_\theta(x_t \mid x_{\lt t})
$$

Trong instruction tuning, chu·ªói ƒë·∫ßu v√†o c√≥ c·∫•u tr√∫c:

$$
x = [\text{Instruction}; \text{Response}]
$$

V√† loss ch·ªâ t√≠nh tr√™n ph·∫ßn response:

$$
\mathcal{L}*{SFT} = - \sum*{t \in R} \log P_\theta(x_t \mid x_{\lt t})
$$

---

# 2. ƒê·ªông h·ªçc Gradient trong Instruction Tuning

## 2.1. Ph√¢n b·ªë ƒë·ªô d√†i token

Gi·∫£ s·ª≠:

* ( L_Q = \mathbb{E}[|Q|] )
* ( L_A = \mathbb{E}[|A|] )

Th·ª±c nghi·ªám cho th·∫•y:

$$
L_A \gg L_Q
$$

Gradient k·ª≥ v·ªçng:

$$
\mathbb{E}[\nabla_\theta \mathcal{L}] = - \mathbb{E} \left[ \sum_{t \in R} \nabla_\theta \log P_\theta(x_t \mid x_{\lt t}) \right]
$$

ƒêi·ªÅu n√†y d·∫´n t·ªõi hi·ªán t∆∞·ª£ng:

* Ph·∫ßn response chi ph·ªëi to√†n b·ªô c·∫≠p nh·∫≠t tham s·ªë
* Instruction ƒë√≥ng vai tr√≤ ƒëi·ªÅu ki·ªán nh∆∞ng √≠t ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp

---

## 2.2. Ph∆∞∆°ng sai gradient

Ph∆∞∆°ng sai gradient t·ªâ l·ªá v·ªõi ƒë·ªô d√†i chu·ªói:

$$
Var(\nabla_\theta \mathcal{L}) \propto T
$$

Khi c√¢u tr·∫£ l·ªùi d√†i, ta c√≥:

$$
Var \uparrow \Rightarrow \text{training instability}
$$

Bi·ªán ph√°p:

* Gradient clipping:

$$
g \leftarrow \frac{g}{\max(1, \frac{|g|}{c})}
$$

* Mixed precision $FP16/BF16$
* Gradient accumulation

---

# 3. Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n

Self-attention c√≥ ƒë·ªô ph·ª©c t·∫°p:

$$
\mathcal{O}(T^2 d)
$$

V·ªõi:

* $T$: chi·ªÅu d√†i chu·ªói
* $d$: embedding dimension

T·ªïng chi ph√≠ cho to√†n m√¥ h√¨nh:

$$
\mathcal{O}(L \cdot T^2 \cdot d)
$$

Trong ƒë√≥:

* $L = 36$ (s·ªë layer GPT-2 Large)
* $d = 1280$

N·∫øu tƒÉng chi·ªÅu d√†i chu·ªói t·ª´ 512 l√™n 1024:

$$
\text{Compute} \approx 4 \times
$$

Do ph·ª• thu·ªôc b·∫≠c hai theo $T$.

---

# 4. ·ªîn ƒë·ªãnh hu·∫•n luy·ªán (Training Stability)

## 4.1. Learning rate schedule

Warmup tuy·∫øn t√≠nh:

$$
\eta_t = \eta_{max} \cdot \frac{t}{T_{warmup}}
$$

Sau warmup, th∆∞·ªùng d√πng cosine decay:

$$
\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min}) \left(1 + \cos \frac{t\pi}{T}\right)
$$

---

## 4.2. Adam Optimizer

GPT-2 th∆∞·ªùng d√πng Adam:

$$
m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t
$$

$$
v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2
$$

C·∫≠p nh·∫≠t tham s·ªë:

$$
\theta_t = \theta_{t-1} - \eta \frac{\hat m_t}{\sqrt{\hat v_t} + \epsilon}
$$

Adam gi√∫p ·ªïn ƒë·ªãnh khi gradient dao ƒë·ªông m·∫°nh do chu·ªói d√†i.

---

# 5. ·∫¢nh h∆∞·ªüng c·ªßa Masking Loss

N·∫øu kh√¥ng mask instruction:

$$
\mathcal{L}*{total} = \mathcal{L}*{instruction} + \mathcal{L}_{response}
$$

Khi ƒë√≥ m√¥ h√¨nh s·∫Ω h·ªçc:

* Sao ch√©p instruction
* T·ªëi ∆∞u ph√¢n ph·ªëi token kh√¥ng mong mu·ªën

Masking ƒë·∫£m b·∫£o:

$$
\mathcal{L}_{instruction} = 0
$$

Gi√∫p m√¥ h√¨nh t·∫≠p trung v√†o sinh response.

---

# 6. So s√°nh v·ªõi RLHF

Trong InstructGPT (Ouyang et al., 2022), qu√° tr√¨nh g·ªìm:

1. Supervised Fine-Tuning
2. Reward Model
3. Proximal Policy Optimization (PPO)

M·ª•c ti√™u PPO:

$$
\max_\theta \mathbb{E}*{x \sim \pi*\theta} \left[ r(x) - \beta D_{KL}(\pi_\theta | \pi_{ref}) \right]
$$

Trong ƒë√≥:

* ( r(x) ): reward t·ª´ m√¥ h√¨nh ƒë√°nh gi√°
* $D_{KL}$: KL divergence

$$
D_{KL}(P|Q) = \sum_x P(x)\log\frac{P(x)}{Q(x)}
$$

KL gi√∫p gi·ªØ m√¥ h√¨nh kh√¥ng l·ªách qu√° xa m√¥ h√¨nh g·ªëc.

---

# 7. V·∫•n ƒë·ªÅ b·ªô nh·ªõ GPU

B·ªô nh·ªõ c·∫ßn thi·∫øt:

$$
Memory \approx \text{Parameters} + \text{Gradients} + \text{Optimizer States}
$$

V·ªõi 1.5B tham s·ªë:

* FP16: ~6GB
* Adam states: ~12GB

T·ªïng c√≥ th·ªÉ v∆∞·ª£t 20GB.

Gi·∫£i ph√°p:

* ZeRO optimization
* Gradient checkpointing
* Offloading

---

# 8. ƒê·ªông h·ªçc t·ªïng qu√°t h√≥a (Generalization Dynamics)

Theo l√Ω thuy·∫øt bias-variance:

$$
\mathbb{E}[(y - \hat y)^2] = Bias^2 + Variance + Noise
$$

Instruction tuning l√†m:

* Gi·∫£m bias v·ªõi t√°c v·ª• h·ªèi-ƒë√°p
* C√≥ th·ªÉ tƒÉng variance n·∫øu dataset nh·ªè

Do ƒë√≥ c·∫ßn:

$$
n \gg \frac{d}{\epsilon}
$$

Trong ƒë√≥:

* $n$: s·ªë m·∫´u
* $d$: s·ªë tham s·ªë hi·ªáu d·ª•ng
* $\epsilon$: sai s·ªë mong mu·ªën

---

# 9. Th·∫£o lu·∫≠n

Ph·∫ßn 2 c·ªßa qu√° tr√¨nh instruction tuning cho th·∫•y:

* ƒê·ªô d√†i response chi ph·ªëi gradient
* Attention t·∫°o chi ph√≠ b·∫≠c hai theo chi·ªÅu d√†i
* Masking l√† quy·∫øt ƒë·ªãnh thi·∫øt k·∫ø quan tr·ªçng
* ·ªîn ƒë·ªãnh hu·∫•n luy·ªán ph·ª• thu·ªôc m·∫°nh v√†o LR schedule v√† optimizer

GPT-2 Large, d√π kh√¥ng ƒë∆∞·ª£c thi·∫øt k·∫ø ban ƒë·∫ßu cho chatbot, v·∫´n c√≥ th·ªÉ ƒë·∫°t hi·ªáu qu·∫£ cao sau instruction tuning nh·ªù kh·∫£ nƒÉng bi·ªÉu di·ªÖn l·ªõn.

---

# 10. K·∫øt lu·∫≠n

Instruction tuning cho GPT-2 Large minh h·ªça:

1. M·ªëi quan h·ªá gi·ªØa ki·∫øn tr√∫c Transformer v√† ƒë·ªông h·ªçc gradient
2. ·∫¢nh h∆∞·ªüng c·ªßa ph√¢n b·ªë token ƒë·∫øn t·ªëi ∆∞u h√≥a
3. Vai tr√≤ c·ªßa c√°c k·ªπ thu·∫≠t ·ªïn ƒë·ªãnh hu·∫•n luy·ªán
4. Gi·ªõi h·∫°n t√≠nh to√°n do attention b·∫≠c hai

Nh·ªØng ph√¢n t√≠ch n√†y l√† n·ªÅn t·∫£ng cho c√°c m√¥ h√¨nh l·ªõn h∆°n v√† c√°c ph∆∞∆°ng ph√°p hu·∫•n luy·ªán n√¢ng cao nh∆∞ RLHF.

---

# T√†i li·ªáu tham kh·∫£o

1. Radford, A. et al. (2019). *Language Models are Unsupervised Multitask Learners*. OpenAI.
2. Vaswani, A. et al. (2017). *Attention Is All You Need*.
3. Ouyang, L. et al. (2022). *Training language models to follow instructions with human feedback*.
4. Goodfellow, I., Bengio, Y., Courville, A. (2016). *Deep Learning*. MIT Press.
5. Kingma, D., Ba (2014). *Adam: A Method for Stochastic Optimization*.

---
<!-- Aero-Footer-Start -->

## üìÑ T√†i li·ªáu c√πng chuy√™n m·ª•c
| B√†i h·ªçc | Li√™n k·∫øt |
| :--- | :--- |
| [Instruction Tuning (Tinh Ch·ªânh B·∫±ng Ch·ªâ Th·ªã) Trong C√°c M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn (LLMs)](aero_llm_01_what_is_instruction_tuning.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_01_what_is_instruction_tuning.md) |
| [Instruction Tuning trong M√¥ h√¨nh Ng√¥n ng·ªØ L·ªõn](aero_llm_02_some_datasets_for_instruction_tuning.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_02_some_datasets_for_instruction_tuning.md) |
| [Hu·∫•n luy·ªán Chatbot theo Instruction Tuning v√† M√¥ h√¨nh System‚ÄìUser‚ÄìAssistant](aero_llm_03_training_a_chatbot_with_system_user_assistant.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_03_training_a_chatbot_with_system_user_assistant.md) |
| [Instruction Tuning v·ªõi GPT-2 trong Hu·∫•n luy·ªán M√¥ h√¨nh Ng√¥n ng·ªØ](aero_llm_04_instruction_tuning_with_gpt2.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_04_instruction_tuning_with_gpt2.md) |
| [aero llm 05 codechallenge instruction tuning gpt2 large part 1](aero_llm_05_codechallenge_instruction_tuning_gpt2_large_part_1_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_05_codechallenge_instruction_tuning_gpt2_large_part_1_.md) |
| üìå **[Ph√¢n t√≠ch n√¢ng cao qu√° tr√¨nh Instruction Tuning cho GPT-2 Large: ·ªîn ƒë·ªãnh hu·∫•n luy·ªán, ƒë·ªông h·ªçc gradient v√† t·ªëi ∆∞u ho√° t√≠nh to√°n](aero_llm_06_codechallenge_instruction_tuning_gpt2_large_part_2_.md)** | [Xem b√†i vi·∫øt ‚Üí](aero_llm_06_codechallenge_instruction_tuning_gpt2_large_part_2_.md) |
| [Reinforcement Learning from Human Feedback (RLHF): C∆° s·ªü l√Ω thuy·∫øt, m√¥ h√¨nh to√°n h·ªçc v√† ·ª©ng d·ª•ng trong hu·∫•n luy·ªán m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn](aero_llm_07_reinforcement_learning_from_human_feedback_rlhf_.md) | [Xem b√†i vi·∫øt ‚Üí](aero_llm_07_reinforcement_learning_from_human_feedback_rlhf_.md) |

---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

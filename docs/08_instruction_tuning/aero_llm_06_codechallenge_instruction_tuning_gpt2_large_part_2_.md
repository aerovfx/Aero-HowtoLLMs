
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [08 instruction tuning](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# PhÃ¢n tÃ­ch nÃ¢ng cao quÃ¡ trÃ¬nh Instruction Tuning cho GPT-2 Large: á»”n Ä‘á»‹nh huáº¥n luyá»‡n, Ä‘á»™ng há»c gradient vÃ  tá»‘i Æ°u hoÃ¡ tÃ­nh toÃ¡n

## TÃ³m táº¯t

BÃ i viáº¿t nÃ y tiáº¿p tá»¥c phÃ¢n tÃ­ch quÃ¡ trÃ¬nh instruction tuning cho GPT-2 Large (1.5B tham sá»‘), táº­p trung vÃ o cÃ¡c váº¥n Ä‘á» nÃ¢ng cao gá»“m: Ä‘á»™ng há»c gradient, á»•n Ä‘á»‹nh huáº¥n luyá»‡n (training stability), chiáº¿n lÆ°á»£c tá»‘i Æ°u hoÃ¡ bá»™ nhá»› vÃ  áº£nh hÆ°á»Ÿng cá»§a phÃ¢n bá»‘ Ä‘á»™ dÃ i chuá»—i. PhÃ¢n tÃ­ch Ä‘Æ°á»£c Ä‘áº·t trÃªn ná»n táº£ng kiáº¿n trÃºc Transformer cá»§a Vaswani et al. (2017) vÃ  mÃ´ hÃ¬nh GPT-2 do OpenAI cÃ´ng bá»‘ (Radford et al., 2019). Äá»“ng thá»i, bÃ i viáº¿t liÃªn há»‡ vá»›i hÆ°á»›ng Instruction Tuning vÃ  RLHF sau nÃ y trong InstructGPT (Ouyang et al., 2022).

---

# 1. Bá»‘i cáº£nh lÃ½ thuyáº¿t

## 1.1. MÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»± há»“i quy

GPT-2 tá»‘i Æ°u hoÃ¡ xÃ¡c suáº¥t chuá»—i:

$$
P(x_1, x_2, ..., x_T) = \prod_{t=1}^{T} P(x_t \mid x_{<t})
$$

HÃ m máº¥t mÃ¡t cross-entropy:

$$
\mathcal{L}(\theta) = - \sum_{t=1}^{T} \log P_\theta(x_t \mid x_{<t})
$$

Trong instruction tuning, chuá»—i Ä‘áº§u vÃ o cÃ³ cáº¥u trÃºc:

$$
x = [\text{Instruction}; \text{Response}]
$$

VÃ  loss chá»‰ tÃ­nh trÃªn pháº§n response:

$$
\mathcal{L}*{SFT} = - \sum*{t \in R} \log P_\theta(x_t \mid x_{<t})
$$

---

# 2. Äá»™ng há»c Gradient trong Instruction Tuning

## 2.1. PhÃ¢n bá»‘ Ä‘á»™ dÃ i token

Giáº£ sá»­:

* ( L_Q = \mathbb{E}[|Q|] )
* ( L_A = \mathbb{E}[|A|] )

Thá»±c nghiá»‡m cho tháº¥y:

$$
L_A \gg L_Q
$$

Gradient ká»³ vá»ng:

$$
\mathbb{E}[\nabla_\theta \mathcal{L}]
= - \mathbb{E} \left[ \sum_{t \in R} \nabla_\theta \log P_\theta(x_t \mid x_{<t}) \right]
$$

Äiá»u nÃ y dáº«n tá»›i hiá»‡n tÆ°á»£ng:

* Pháº§n response chi phá»‘i toÃ n bá»™ cáº­p nháº­t tham sá»‘
* Instruction Ä‘Ã³ng vai trÃ² Ä‘iá»u kiá»‡n nhÆ°ng Ã­t áº£nh hÆ°á»Ÿng trá»±c tiáº¿p

---

## 2.2. PhÆ°Æ¡ng sai gradient

PhÆ°Æ¡ng sai gradient tá»‰ lá»‡ vá»›i Ä‘á»™ dÃ i chuá»—i:

$$
Var(\nabla_\theta \mathcal{L}) \propto T
$$

Khi cÃ¢u tráº£ lá»i dÃ i, ta cÃ³:

$$
Var \uparrow \Rightarrow \text{training instability}
$$

Biá»‡n phÃ¡p:

* Gradient clipping:

$$
g \leftarrow \frac{g}{\max(1, \frac{|g|}{c})}
$$

* Mixed precision $FP16/BF16$
* Gradient accumulation

---

# 3. PhÃ¢n tÃ­ch Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n

Self-attention cÃ³ Ä‘á»™ phá»©c táº¡p:

$$
\mathcal{O}(T^2 d)
$$

Vá»›i:

* $T$: chiá»u dÃ i chuá»—i
* $d$: embedding dimension

Tá»•ng chi phÃ­ cho toÃ n mÃ´ hÃ¬nh:

$$
\mathcal{O}(L \cdot T^2 \cdot d)
$$

Trong Ä‘Ã³:

* $L = 36$ (sá»‘ layer GPT-2 Large)
* $d = 1280$

Náº¿u tÄƒng chiá»u dÃ i chuá»—i tá»« 512 lÃªn 1024:

$$
\text{Compute} \approx 4 \times
$$

Do phá»¥ thuá»™c báº­c hai theo $T$.

---

# 4. á»”n Ä‘á»‹nh huáº¥n luyá»‡n (Training Stability)

## 4.1. Learning rate schedule

Warmup tuyáº¿n tÃ­nh:

$$
\eta_t = \eta_{max} \cdot \frac{t}{T_{warmup}}
$$

Sau warmup, thÆ°á»ng dÃ¹ng cosine decay:

$$
\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min}) \left(1 + \cos \frac{t\pi}{T}\right)
$$

---

## 4.2. Adam Optimizer

GPT-2 thÆ°á»ng dÃ¹ng Adam:

$$
m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t
$$

$$
v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2
$$

Cáº­p nháº­t tham sá»‘:

$$
\theta_t = \theta_{t-1} - \eta \frac{\hat m_t}{\sqrt{\hat v_t} + \epsilon}
$$

Adam giÃºp á»•n Ä‘á»‹nh khi gradient dao Ä‘á»™ng máº¡nh do chuá»—i dÃ i.

---

# 5. áº¢nh hÆ°á»Ÿng cá»§a Masking Loss

Náº¿u khÃ´ng mask instruction:

$$
\mathcal{L}*{total} = \mathcal{L}*{instruction} + \mathcal{L}_{response}
$$

Khi Ä‘Ã³ mÃ´ hÃ¬nh sáº½ há»c:

* Sao chÃ©p instruction
* Tá»‘i Æ°u phÃ¢n phá»‘i token khÃ´ng mong muá»‘n

Masking Ä‘áº£m báº£o:

$$
\mathcal{L}_{instruction} = 0
$$

GiÃºp mÃ´ hÃ¬nh táº­p trung vÃ o sinh response.

---

# 6. So sÃ¡nh vá»›i RLHF

Trong InstructGPT (Ouyang et al., 2022), quÃ¡ trÃ¬nh gá»“m:

1. Supervised Fine-Tuning
2. Reward Model
3. Proximal Policy Optimization (PPO)

Má»¥c tiÃªu PPO:

$$
\max_\theta \mathbb{E}*{x \sim \pi*\theta}
\left[
r(x) - \beta D_{KL}(\pi_\theta | \pi_{ref})
\right]
$$

Trong Ä‘Ã³:

* ( r(x) ): reward tá»« mÃ´ hÃ¬nh Ä‘Ã¡nh giÃ¡
* $D_{KL}$: KL divergence

$$
D_{KL}(P|Q) = \sum_x P(x)\log\frac{P(x)}{Q(x)}
$$

KL giÃºp giá»¯ mÃ´ hÃ¬nh khÃ´ng lá»‡ch quÃ¡ xa mÃ´ hÃ¬nh gá»‘c.

---

# 7. Váº¥n Ä‘á» bá»™ nhá»› GPU

Bá»™ nhá»› cáº§n thiáº¿t:

$$
Memory \approx
\text{Parameters} +
\text{Gradients} +
\text{Optimizer States}
$$

Vá»›i 1.5B tham sá»‘:

* FP16: ~6GB
* Adam states: ~12GB

Tá»•ng cÃ³ thá»ƒ vÆ°á»£t 20GB.

Giáº£i phÃ¡p:

* ZeRO optimization
* Gradient checkpointing
* Offloading

---

# 8. Äá»™ng há»c tá»•ng quÃ¡t hÃ³a (Generalization Dynamics)

Theo lÃ½ thuyáº¿t bias-variance:

$$
\mathbb{E}[(y - \hat y)^2] = Bias^2 + Variance + Noise
$$

Instruction tuning lÃ m:

* Giáº£m bias vá»›i tÃ¡c vá»¥ há»i-Ä‘Ã¡p
* CÃ³ thá»ƒ tÄƒng variance náº¿u dataset nhá»

Do Ä‘Ã³ cáº§n:

$$
n \gg \frac{d}{\epsilon}
$$

Trong Ä‘Ã³:

* $n$: sá»‘ máº«u
* $d$: sá»‘ tham sá»‘ hiá»‡u dá»¥ng
* $\epsilon$: sai sá»‘ mong muá»‘n

---

# 9. Tháº£o luáº­n

Pháº§n 2 cá»§a quÃ¡ trÃ¬nh instruction tuning cho tháº¥y:

* Äá»™ dÃ i response chi phá»‘i gradient
* Attention táº¡o chi phÃ­ báº­c hai theo chiá»u dÃ i
* Masking lÃ  quyáº¿t Ä‘á»‹nh thiáº¿t káº¿ quan trá»ng
* á»”n Ä‘á»‹nh huáº¥n luyá»‡n phá»¥ thuá»™c máº¡nh vÃ o LR schedule vÃ  optimizer

GPT-2 Large, dÃ¹ khÃ´ng Ä‘Æ°á»£c thiáº¿t káº¿ ban Ä‘áº§u cho chatbot, váº«n cÃ³ thá»ƒ Ä‘áº¡t hiá»‡u quáº£ cao sau instruction tuning nhá» kháº£ nÄƒng biá»ƒu diá»…n lá»›n.

---

# 10. Káº¿t luáº­n

Instruction tuning cho GPT-2 Large minh há»a:

1. Má»‘i quan há»‡ giá»¯a kiáº¿n trÃºc Transformer vÃ  Ä‘á»™ng há»c gradient
2. áº¢nh hÆ°á»Ÿng cá»§a phÃ¢n bá»‘ token Ä‘áº¿n tá»‘i Æ°u hÃ³a
3. Vai trÃ² cá»§a cÃ¡c ká»¹ thuáº­t á»•n Ä‘á»‹nh huáº¥n luyá»‡n
4. Giá»›i háº¡n tÃ­nh toÃ¡n do attention báº­c hai

Nhá»¯ng phÃ¢n tÃ­ch nÃ y lÃ  ná»n táº£ng cho cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n nÃ¢ng cao nhÆ° RLHF.

---

# TÃ i liá»‡u tham kháº£o

1. Radford, A. et al. (2019). *Language Models are Unsupervised Multitask Learners*. OpenAI.
2. Vaswani, A. et al. (2017). *Attention Is All You Need*.
3. Ouyang, L. et al. (2022). *Training language models to follow instructions with human feedback*.
4. Goodfellow, I., Bengio, Y., Courville, A. (2016). *Deep Learning*. MIT Press.
5. Kingma, D., Ba (2014). *Adam: A Method for Stochastic Optimization*.

---
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Instruction Tuning (Tinh Chá»‰nh Báº±ng Chá»‰ Thá»‹) Trong CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n (LLMs)](aero_llm_01_what_is_instruction_tuning.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_what_is_instruction_tuning.md) |
| [Instruction Tuning trong MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n](aero_llm_02_some_datasets_for_instruction_tuning.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_some_datasets_for_instruction_tuning.md) |
| [Huáº¥n luyá»‡n Chatbot theo Instruction Tuning vÃ  MÃ´ hÃ¬nh Systemâ€“Userâ€“Assistant](aero_llm_03_training_a_chatbot_with_system_user_assistant.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_training_a_chatbot_with_system_user_assistant.md) |
| [Instruction Tuning vá»›i GPT-2 trong Huáº¥n luyá»‡n MÃ´ hÃ¬nh NgÃ´n ngá»¯](aero_llm_04_instruction_tuning_with_gpt2.md) | [Xem bÃ i viáº¿t â†’](aero_llm_04_instruction_tuning_with_gpt2.md) |
| [aero llm 05 codechallenge instruction tuning gpt2 large part 1](aero_llm_05_codechallenge_instruction_tuning_gpt2_large_part_1_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_05_codechallenge_instruction_tuning_gpt2_large_part_1_.md) |
| ğŸ“Œ **[PhÃ¢n tÃ­ch nÃ¢ng cao quÃ¡ trÃ¬nh Instruction Tuning cho GPT-2 Large: á»”n Ä‘á»‹nh huáº¥n luyá»‡n, Ä‘á»™ng há»c gradient vÃ  tá»‘i Æ°u hoÃ¡ tÃ­nh toÃ¡n](aero_llm_06_codechallenge_instruction_tuning_gpt2_large_part_2_.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_06_codechallenge_instruction_tuning_gpt2_large_part_2_.md) |
| [Reinforcement Learning from Human Feedback (RLHF): CÆ¡ sá»Ÿ lÃ½ thuyáº¿t, mÃ´ hÃ¬nh toÃ¡n há»c vÃ  á»©ng dá»¥ng trong huáº¥n luyá»‡n mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n](aero_llm_07_reinforcement_learning_from_human_feedback_rlhf_.md) | [Xem bÃ i viáº¿t â†’](aero_llm_07_reinforcement_learning_from_human_feedback_rlhf_.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[üè† Home](../index.md) > [08 instruction tuning](index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../index.md)
- [üìö Module 01: LLM Course](../01_llm_course/index.md)
- [üî¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04_buildgpt/index.md)
- [üéØ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [üîç Module 19: AI Safety](../19_ai_safety/index.md)
- [üêç Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Ph√¢n t√≠ch n√¢ng cao qu√° tr√¨nh Instruction Tuning cho GPT-2 Large: ·ªîn ƒë·ªãnh hu·∫•n luy·ªán, ƒë·ªông h·ªçc gradient v√† t·ªëi ∆∞u ho√° t√≠nh to√°n

## T√≥m t·∫Øt

B√†i vi·∫øt n√†y ti·∫øp t·ª•c ph√¢n t√≠ch qu√° tr√¨nh instruction tuning cho GPT-2 Large (1.5B tham s·ªë), t·∫≠p trung v√†o c√°c v·∫•n ƒë·ªÅ n√¢ng cao g·ªìm: ƒë·ªông h·ªçc gradient, ·ªïn ƒë·ªãnh hu·∫•n luy·ªán (training stability), chi·∫øn l∆∞·ª£c t·ªëi ∆∞u ho√° b·ªô nh·ªõ v√† ·∫£nh h∆∞·ªüng c·ªßa ph√¢n b·ªë ƒë·ªô d√†i chu·ªói. Ph√¢n t√≠ch ƒë∆∞·ª£c ƒë·∫∑t tr√™n n·ªÅn t·∫£ng ki·∫øn tr√∫c Transformer c·ªßa Vaswani et al. (2017) v√† m√¥ h√¨nh GPT-2 do OpenAI c√¥ng b·ªë (Radford et al., 2019). ƒê·ªìng th·ªùi, b√†i vi·∫øt li√™n h·ªá v·ªõi h∆∞·ªõng Instruction Tuning v√† RLHF sau n√†y trong InstructGPT (Ouyang et al., 2022).

---

# 1. B·ªëi c·∫£nh l√Ω thuy·∫øt

## 1.1. M√¥ h√¨nh ng√¥n ng·ªØ t·ª± h·ªìi quy

GPT-2 t·ªëi ∆∞u ho√° x√°c su·∫•t chu·ªói:

$$

$$

P(x_1, x_2, ..., x_T) = \prod_{t=1}^{T} P(x_t \mid x_{\lt t})

$$

$$

H√†m m·∫•t m√°t cross-entropy:

$$

$$

$\mathcal${L}(\theta) = - $\sum$_{t=1}^{T} $\log$ P_\theta($x_t$ \mid x_{\lt t})

$$

$$

Trong instruction tuning, chu·ªói ƒë·∫ßu v√†o c√≥ c·∫•u tr√∫c:

$$

$$

x = [\text{Instruction}; \text{Response}]

$$

$$

V√† loss ch·ªâ t√≠nh tr√™n ph·∫ßn response:

$$

$$

$\mathcal${L}*{SFT} = - $\sum$*{t \in R} $\log$ P_\theta($x_t$ \mid x_{\lt t})

$$

$$

---

# 2. ƒê·ªông h·ªçc Gradient trong Instruction Tuning

## 2.1. Ph√¢n b·ªë ƒë·ªô d√†i token

Gi·∫£ s·ª≠:

$$

$$

* ( L_Q = \mathbb{E}[|Q|] )

$$

$$

$$
* ( L_A = \mathbb{E}[|A|] )
$$

$$
Th·ª±c nghi·ªám cho th·∫•y:
$$

L_A$ \gg $L_Q

$$
Gradient k·ª≥ v·ªçng:
$$

$\mathbb${E}[$\nabla$_\theta $\mathcal${L}] = - $\mathbb${E} $\le$ft[ $\sum$_{t \in R} $\nabla$_\theta $\log$ P_\theta($x_t$ \mid x_{\lt t}) \right]

$$
ƒêi·ªÅu n√†y d·∫´n t·ªõi hi·ªán t∆∞·ª£ng: * Ph·∫ßn response chi ph·ªëi to√†n b·ªô c·∫≠p nh·∫≠t tham s·ªë * Instruction ƒë√≥ng vai tr√≤ ƒëi·ªÅu ki·ªán nh∆∞ng √≠t ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp --- ## 2.2. Ph∆∞∆°ng sai gradient Ph∆∞∆°ng sai gradient t·ªâ l·ªá v·ªõi ƒë·ªô d√†i chu·ªói:
$$

$$
Var(\nabla_\theta \mathcal{L}) \propto T
$$

$$
Khi c√¢u tr·∫£ l·ªùi d√†i, ta c√≥:
$$

Var \uparrow \Rightarrow \text{training instability}

$$
Bi·ªán ph√°p: * Gradient clipping:
$$

$$
g \leftarrow \frac{g}{\max(1, \frac{|g|}{c})}
$$

$$
* Mixed precision FP16/BF16 * Gradient accumulation --- # 3. Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n Self-attention c√≥ ƒë·ªô ph·ª©c t·∫°p:
$$

$\mathcal${O}(T^2 d)

$$
V·ªõi: * T: chi·ªÅu d√†i chu·ªói * d: embedding dimension T·ªïng chi ph√≠ cho to√†n m√¥ h√¨nh:
$$

$\mathcal${O}(L \cdot T^2 \cdot d)

$$
Trong ƒë√≥:
$$

* L = 36 (s·ªë layer GPT-2 Large)

$$

$$

* d = 1280

$$
N·∫øu tƒÉng chi·ªÅu d√†i chu·ªói t·ª´ 512 l√™n 1024:
$$

$$
\text{Compute} \approx 4 \times
$$

$$
Do ph·ª• thu·ªôc b·∫≠c hai theo T. --- # 4. ·ªîn ƒë·ªãnh hu·∫•n luy·ªán (Training Stability) ## 4.1. Learning rate schedule Warmup tuy·∫øn t√≠nh:
$$

$$
\eta_t = \eta_{max} \cdot \frac{t}{T_{warmup}}
$$

$$
Sau warmup, th∆∞·ªùng d√πng cosine decay:
$$

$$
\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min}) \left(1 + \cos \frac{t\pi}{T}\right)
$$

$$
--- ## 4.2. Adam Optimizer GPT-2 th∆∞·ªùng d√πng Adam:
$$

$$
m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t
$$

$$

$$

$$
v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2
$$

$$
C·∫≠p nh·∫≠t tham s·ªë:
$$

$$
\theta_t = \theta_{t-1} - \eta \frac{\hat m_t}{\sqrt{\hat v_t} + \epsilon}
$$

$$
Adam gi√∫p ·ªïn ƒë·ªãnh khi gradient dao ƒë·ªông m·∫°nh do chu·ªói d√†i. --- # 5. ·∫¢nh h∆∞·ªüng c·ªßa Masking Loss N·∫øu kh√¥ng mask instruction:
$$

$$
\mathcal{L}*{total} = \mathcal{L}*{instruction} + \mathcal{L}_{response}
$$

$$
Khi ƒë√≥ m√¥ h√¨nh s·∫Ω h·ªçc: * Sao ch√©p instruction * T·ªëi ∆∞u ph√¢n ph·ªëi token kh√¥ng mong mu·ªën Masking ƒë·∫£m b·∫£o:
$$

$$
\mathcal{L}_{instruction} = 0
$$

$$
Gi√∫p m√¥ h√¨nh t·∫≠p trung v√†o sinh response. --- # 6. So s√°nh v·ªõi RLHF Trong InstructGPT (Ouyang et al., 2022), qu√° tr√¨nh g·ªìm: 1. Supervised Fine-Tuning 2. Reward Model 3. Proximal Policy Optimization (PPO) M·ª•c ti√™u PPO:
$$

$$
\max_\theta \mathbb{E}*{x \sim \pi*\theta} \left[ r(x) - \beta D_{KL}(\pi_\theta | \pi_{ref}) \right]
$$

$$
Trong ƒë√≥: * ( r(x) ): reward t·ª´ m√¥ h√¨nh ƒë√°nh gi√° * D_{KL}: KL divergence
$$

$$
D_{KL}(P|Q) = \sum_x P(x)\log\frac{P(x)}{Q(x)}
$$

$$
KL gi√∫p gi·ªØ m√¥ h√¨nh kh√¥ng l·ªách qu√° xa m√¥ h√¨nh g·ªëc. --- # 7. V·∫•n ƒë·ªÅ b·ªô nh·ªõ GPU B·ªô nh·ªõ c·∫ßn thi·∫øt:
$$

$$
Memory \approx \text{Parameters} + \text{Gradients} + \text{Optimizer States}
$$

$$
V·ªõi 1.5B tham s·ªë: * FP16: ~6GB * Adam states: ~12GB T·ªïng c√≥ th·ªÉ v∆∞·ª£t 20GB. Gi·∫£i ph√°p: * ZeRO optimization * Gradient checkpointing * Offloading --- # 8. ƒê·ªông h·ªçc t·ªïng qu√°t h√≥a (Generalization Dynamics) Theo l√Ω thuy·∫øt bias-variance:
$$

$$
\mathbb{E}[(y - \hat y)^2] = Bias^2 + Variance + Noise
$$

$$
Instruction tuning l√†m: * Gi·∫£m bias v·ªõi t√°c v·ª• h·ªèi-ƒë√°p * C√≥ th·ªÉ tƒÉng variance n·∫øu dataset nh·ªè Do ƒë√≥ c·∫ßn:
$$

n \gg \frac{d}{\epsilon}
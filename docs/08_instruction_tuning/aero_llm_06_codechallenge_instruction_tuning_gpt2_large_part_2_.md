
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [08 instruction tuning](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# PhÃ¢n tÃ­ch nÃ¢ng cao quÃ¡ trÃ¬nh Instruction Tuning cho GPT-2 Large: á»”n Ä‘á»‹nh huáº¥n luyá»‡n, Ä‘á»™ng há»c gradient vÃ  tá»‘i Æ°u hoÃ¡ tÃ­nh toÃ¡n

## TÃ³m táº¯t

BÃ i viáº¿t nÃ y tiáº¿p tá»¥c phÃ¢n tÃ­ch quÃ¡ trÃ¬nh instruction tuning cho GPT-2 Large (1.5B tham sá»‘), táº­p trung vÃ o cÃ¡c váº¥n Ä‘á» nÃ¢ng cao gá»“m: Ä‘á»™ng há»c gradient, á»•n Ä‘á»‹nh huáº¥n luyá»‡n (training stability), chiáº¿n lÆ°á»£c tá»‘i Æ°u hoÃ¡ bá»™ nhá»› vÃ  áº£nh hÆ°á»Ÿng cá»§a phÃ¢n bá»‘ Ä‘á»™ dÃ i chuá»—i. PhÃ¢n tÃ­ch Ä‘Æ°á»£c Ä‘áº·t trÃªn ná»n táº£ng kiáº¿n trÃºc Transformer cá»§a Vaswani et al. (2017) vÃ  mÃ´ hÃ¬nh GPT-2 do OpenAI cÃ´ng bá»‘ (Radford et al., 2019). Äá»“ng thá»i, bÃ i viáº¿t liÃªn há»‡ vá»›i hÆ°á»›ng Instruction Tuning vÃ  RLHF sau nÃ y trong InstructGPT (Ouyang et al., 2022).

---

# 1. Bá»‘i cáº£nh lÃ½ thuyáº¿t

## 1.1. MÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»± há»“i quy

GPT-2 tá»‘i Æ°u hoÃ¡ xÃ¡c suáº¥t chuá»—i:

P(x_1, x_2, ..., x_T) = \prod_{t=1}^{T} P(x_t \mid x_{\lt t})

HÃ m máº¥t mÃ¡t cross-entropy:

$\mathcal${L}(\theta) = - $\sum$_{t=1}^{T} $\log$ P_\theta($x_t$ \mid x_{\lt t})

Trong instruction tuning, chuá»—i Ä‘áº§u vÃ o cÃ³ cáº¥u trÃºc:

x = [\text{Instruction}; \text{Response}]

VÃ  loss chá»‰ tÃ­nh trÃªn pháº§n response:

$\mathcal${L}*{SFT} = - $\sum$*{t \in R} $\log$ P_\theta($x_t$ \mid x_{\lt t})

---

# 2. Äá»™ng há»c Gradient trong Instruction Tuning

## 2.1. PhÃ¢n bá»‘ Ä‘á»™ dÃ i token

Giáº£ sá»­:

* ( L_Q = \mathbb{E}[|Q|] )

$$
* ( L_A = \mathbb{E}[|A|] ) Thá»±c nghiá»‡m cho tháº¥y:
$$

L_A$ \gg $L_Q

$$
Gradient ká»³ vá»ng:
$$

$\mathbb${E}[$\nabla$_\theta $\mathcal${L}] = - $\mathbb${E} $\le$ft[ $\sum$_{t \in R} $\nabla$_\theta $\log$ P_\theta($x_t$ \mid x_{\lt t}) \right]

$$
Äiá»u nÃ y dáº«n tá»›i hiá»‡n tÆ°á»£ng: * Pháº§n response chi phá»‘i toÃ n bá»™ cáº­p nháº­t tham sá»‘ * Instruction Ä‘Ã³ng vai trÃ² Ä‘iá»u kiá»‡n nhÆ°ng Ã­t áº£nh hÆ°á»Ÿng trá»±c tiáº¿p --- ## 2.2. PhÆ°Æ¡ng sai gradient PhÆ°Æ¡ng sai gradient tá»‰ lá»‡ vá»›i Ä‘á»™ dÃ i chuá»—i: Var(\nabla_\theta \mathcal{L}) \propto T Khi cÃ¢u tráº£ lá»i dÃ i, ta cÃ³:
$$

Var \uparrow \Rightarrow \text{training instability}

$$
Biá»‡n phÃ¡p: * Gradient clipping: g \leftarrow \frac{g}{\max(1, \frac{|g|}{c})} * Mixed precision FP16/BF16 * Gradient accumulation --- # 3. PhÃ¢n tÃ­ch Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n Self-attention cÃ³ Ä‘á»™ phá»©c táº¡p:
$$

$\mathcal${O}(T^2 d)

$$
Vá»›i: * T: chiá»u dÃ i chuá»—i * d: embedding dimension Tá»•ng chi phÃ­ cho toÃ n mÃ´ hÃ¬nh:
$$

$\mathcal${O}(L \cdot T^2 \cdot d)

$$
Trong Ä‘Ã³:
$$

* L = 36 (sá»‘ layer GPT-2 Large)

* d = 1280

$$
Náº¿u tÄƒng chiá»u dÃ i chuá»—i tá»« 512 lÃªn 1024: \text{Compute} \approx 4 \times Do phá»¥ thuá»™c báº­c hai theo T. --- # 4. á»”n Ä‘á»‹nh huáº¥n luyá»‡n (Training Stability) ## 4.1. Learning rate schedule Warmup tuyáº¿n tÃ­nh: \eta_t = \eta_{max} \cdot \frac{t}{T_{warmup}} Sau warmup, thÆ°á»ng dÃ¹ng cosine decay: \eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min}) \left(1 + \cos \frac{t\pi}{T}\right) --- ## 4.2. Adam Optimizer GPT-2 thÆ°á»ng dÃ¹ng Adam: m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2 Cáº­p nháº­t tham sá»‘: \theta_t = \theta_{t-1} - \eta \frac{\hat m_t}{\sqrt{\hat v_t} + \epsilon} Adam giÃºp á»•n Ä‘á»‹nh khi gradient dao Ä‘á»™ng máº¡nh do chuá»—i dÃ i. --- # 5. áº¢nh hÆ°á»Ÿng cá»§a Masking Loss Náº¿u khÃ´ng mask instruction: \mathcal{L}*{total} = \mathcal{L}*{instruction} + \mathcal{L}_{response} Khi Ä‘Ã³ mÃ´ hÃ¬nh sáº½ há»c: * Sao chÃ©p instruction * Tá»‘i Æ°u phÃ¢n phá»‘i token khÃ´ng mong muá»‘n Masking Ä‘áº£m báº£o: \mathcal{L}_{instruction} = 0 GiÃºp mÃ´ hÃ¬nh táº­p trung vÃ o sinh response. --- # 6. So sÃ¡nh vá»›i RLHF Trong InstructGPT (Ouyang et al., 2022), quÃ¡ trÃ¬nh gá»“m: 1. Supervised Fine-Tuning 2. Reward Model 3. Proximal Policy Optimization (PPO) Má»¥c tiÃªu PPO: \max_\theta \mathbb{E}*{x \sim \pi*\theta} \left[ r(x) - \beta D_{KL}(\pi_\theta | \pi_{ref}) \right] Trong Ä‘Ã³: * ( r(x) ): reward tá»« mÃ´ hÃ¬nh Ä‘Ã¡nh giÃ¡ * D_{KL}: KL divergence D_{KL}(P|Q) = \sum_x P(x)\log\frac{P(x)}{Q(x)} KL giÃºp giá»¯ mÃ´ hÃ¬nh khÃ´ng lá»‡ch quÃ¡ xa mÃ´ hÃ¬nh gá»‘c. --- # 7. Váº¥n Ä‘á» bá»™ nhá»› GPU Bá»™ nhá»› cáº§n thiáº¿t: Memory \approx \text{Parameters} + \text{Gradients} + \text{Optimizer States} Vá»›i 1.5B tham sá»‘: * FP16: ~6GB * Adam states: ~12GB Tá»•ng cÃ³ thá»ƒ vÆ°á»£t 20GB. Giáº£i phÃ¡p: * ZeRO optimization * Gradient checkpointing * Offloading --- # 8. Äá»™ng há»c tá»•ng quÃ¡t hÃ³a (Generalization Dynamics) Theo lÃ½ thuyáº¿t bias-variance: \mathbb{E}[(y - \hat y)^2] = Bias^2 + Variance + Noise Instruction tuning lÃ m: * Giáº£m bias vá»›i tÃ¡c vá»¥ há»i-Ä‘Ã¡p * CÃ³ thá»ƒ tÄƒng variance náº¿u dataset nhá» Do Ä‘Ã³ cáº§n:
$$

n \gg \frac{d}{\epsilon}
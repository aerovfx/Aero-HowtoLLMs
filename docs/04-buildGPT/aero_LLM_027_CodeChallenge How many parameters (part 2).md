
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [04 buildGPT](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# PhÃ¢n Bá»‘ Tham Sá»‘ Trong GPT-2: So SÃ¡nh Attention, MLP vÃ  Layer Normalization

## TÃ³m táº¯t (Abstract)

Trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n dá»±a trÃªn Transformer, viá»‡c phÃ¢n bá»‘ tham sá»‘ giá»¯a cÃ¡c thÃ nh pháº§n kiáº¿n trÃºc cÃ³ áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n nÄƒng lá»±c biá»ƒu diá»…n vÃ  hiá»‡u suáº¥t tÃ­nh toÃ¡n. BÃ i viáº¿t nÃ y trÃ¬nh bÃ y phÆ°Æ¡ng phÃ¡p Ä‘á»‹nh lÆ°á»£ng vÃ  so sÃ¡nh sá»‘ lÆ°á»£ng tham sá»‘ trong cÃ¡c lá»›p Attention, MLP vÃ  Layer Normalization cá»§a cÃ¡c phiÃªn báº£n GPT-2. ThÃ´ng qua phÃ¢n tÃ­ch thá»‘ng kÃª vÃ  trá»±c quan hÃ³a báº±ng biá»ƒu Ä‘á»“, nghiÃªn cá»©u cho tháº¥y cÃ¡c lá»›p MLP chiáº¿m tá»· lá»‡ tham sá»‘ cao gáº¥p khoáº£ng hai láº§n Attention, trong khi Layer Normalization chá»‰ chiáº¿m má»™t pháº§n ráº¥t nhá». Káº¿t quáº£ nÃ y pháº£n Ã¡nh chiáº¿n lÆ°á»£c thiáº¿t káº¿ tá»‘i Æ°u cá»§a GPT-2 trong viá»‡c má»Ÿ rá»™ng quy mÃ´ mÃ´ hÃ¬nh.

---

## 1. Giá»›i thiá»‡u

CÃ¡c mÃ´ hÃ¬nh GPT-2 Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn kiáº¿n trÃºc Transformer gá»“m nhiá»u khá»‘i láº·p láº¡i, má»—i khá»‘i bao gá»“m Attention, MLP vÃ  Layer Normalization. Máº·c dÃ¹ tá»•ng sá»‘ tham sá»‘ thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ quy mÃ´ mÃ´ hÃ¬nh, nhÆ°ng viá»‡c phÃ¢n tÃ­ch chi tiáº¿t sá»± phÃ¢n bá»‘ tham sá»‘ giá»¯a cÃ¡c thÃ nh pháº§n giÃºp hiá»ƒu rÃµ hÆ¡n vá» vai trÃ² cá»§a tá»«ng khá»‘i chá»©c nÄƒng.

TÃ i liá»‡u â€œCodeChallenge: How Many Parameters (Part 2)â€ tiáº¿p ná»‘i pháº§n trÆ°á»›c, táº­p trung vÃ o viá»‡c so sÃ¡nh sá»‘ lÆ°á»£ng tham sá»‘ giá»¯a Attention vÃ  MLP, cÅ©ng nhÆ° phÃ¢n tÃ­ch cÃ¡c tham sá»‘ trong Layer Normalization 

---

## 2. Má»¥c tiÃªu nghiÃªn cá»©u

NghiÃªn cá»©u nÃ y hÆ°á»›ng tá»›i cÃ¡c má»¥c tiÃªu chÃ­nh:

1. So sÃ¡nh sá»‘ lÆ°á»£ng tham sá»‘ giá»¯a Attention vÃ  MLP.
2. Trá»±c quan hÃ³a tá»· lá»‡ pháº§n trÄƒm tham sá»‘ trong tá»«ng thÃ nh pháº§n.
3. PhÃ¢n tÃ­ch tham sá»‘ cá»§a Layer Normalization.
4. Giáº£i thÃ­ch nguyÃªn nhÃ¢n kiáº¿n trÃºc dáº«n Ä‘áº¿n sá»± phÃ¢n bá»‘ nÃ y.

CÃ¡c má»¥c tiÃªu trÃªn gÃ³p pháº§n lÃ m rÃµ cÆ¡ cháº¿ thiáº¿t káº¿ cá»§a GPT-2 

---

## 3. PhÆ°Æ¡ng phÃ¡p nghiÃªn cá»©u

### 3.1. TrÃ­ch xuáº¥t tham sá»‘ theo tÃªn

Má»—i tham sá»‘ trong mÃ´ hÃ¬nh PyTorch Ä‘á»u cÃ³ tÃªn Ä‘á»‹nh danh. PhÆ°Æ¡ng phÃ¡p nghiÃªn cá»©u dá»±a trÃªn viá»‡c lá»c cÃ¡c tham sá»‘ cÃ³ chá»©a chuá»—i:

* `"attn"` hoáº·c `"attention"` â†’ Attention
* `"mlp"` hoáº·c `"fc"` â†’ MLP
* `"ln"` â†’ Layer Normalization

CÃ¡ch tiáº¿p cáº­n nÃ y cho phÃ©p phÃ¢n loáº¡i chÃ­nh xÃ¡c cÃ¡c ma tráº­n trá»ng sá»‘ theo chá»©c nÄƒng 

---

### 3.2. Äáº¿m sá»‘ lÆ°á»£ng tham sá»‘

Sá»‘ tham sá»‘ trong má»—i tensor Ä‘Æ°á»£c tÃ­nh báº±ng:

```python
p.numel()
```

Tá»•ng sá»‘ tham sá»‘ cho tá»«ng nhÃ³m Ä‘Æ°á»£c cá»™ng dá»“n trong quÃ¡ trÃ¬nh láº·p qua `named_parameters()` 

---

### 3.3. Trá»±c quan hÃ³a báº±ng biá»ƒu Ä‘á»“

Do sá»‘ lÆ°á»£ng tham sá»‘ tuyá»‡t Ä‘á»‘i giá»¯a cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau ráº¥t lá»›n, nghiÃªn cá»©u sá»­ dá»¥ng tá»· lá»‡ pháº§n trÄƒm Ä‘á»ƒ biá»ƒu diá»…n:

[
\text{Percentage} = \frac{\text{Parameters of sublayer}}{\text{Total parameters}} \times 100%
]

Káº¿t quáº£ Ä‘Æ°á»£c thá»ƒ hiá»‡n báº±ng biá»ƒu Ä‘á»“ cá»™t (bar plot) Ä‘á»ƒ so sÃ¡nh trá»±c quan 

---

## 4. PhÃ¢n tÃ­ch Tham sá»‘ Attention vÃ  MLP

### 4.1. ThÃ nh pháº§n cá»§a Attention

Trong GPT-2, lá»›p Attention bao gá»“m:

* Ma tráº­n QKV gá»™p (C_attn),
* Ma tráº­n chiáº¿u Ä‘áº§u ra (C_proj).

CÃ¡c ma tráº­n nÃ y chá»‹u trÃ¡ch nhiá»‡m há»c quan há»‡ phá»¥ thuá»™c ngá»¯ cáº£nh giá»¯a cÃ¡c token 

---

### 4.2. ThÃ nh pháº§n cá»§a MLP

MLP bao gá»“m hai lá»›p tuyáº¿n tÃ­nh:

1. Lá»›p má»Ÿ rá»™ng chiá»u (FC / W1),
2. Lá»›p thu háº¹p chiá»u (Projection / W2).

Cáº¥u trÃºc nÃ y táº¡o ra sá»± má»Ÿ rá»™ng khÃ´ng gian Ä‘áº·c trÆ°ng, dáº«n Ä‘áº¿n sá»‘ lÆ°á»£ng tham sá»‘ lá»›n 

---

### 4.3. Káº¿t quáº£ so sÃ¡nh

Káº¿t quáº£ thá»‘ng kÃª cho tháº¥y:

| PhiÃªn báº£n | Attention (%) | MLP (%) |
| --------- | ------------- | ------- |
| Small     | ~22           | ~45     |
| Medium    | ~28           | ~56     |
| Large     | ~30           | ~60     |
| XL        | ~31           | ~63     |

MLP cÃ³ sá»‘ tham sá»‘ xáº¥p xá»‰ gáº¥p Ä‘Ã´i Attention trong táº¥t cáº£ cÃ¡c phiÃªn báº£n 

---

### 4.4. Giáº£i thÃ­ch kiáº¿n trÃºc

Sá»± chÃªnh lá»‡ch nÃ y xuáº¥t phÃ¡t tá»«:

* MLP sá»­ dá»¥ng táº§ng má»Ÿ rá»™ng chiá»u lá»›n,
* Attention giá»›i háº¡n trong khÃ´ng gian embedding ban Ä‘áº§u.

Do Ä‘Ã³, MLP trá»Ÿ thÃ nh thÃ nh pháº§n tiÃªu thá»¥ tham sá»‘ lá»›n nháº¥t trong GPT-2 

---

## 5. Xu hÆ°á»›ng TÄƒng Tá»· lá»‡ Theo Quy MÃ´ MÃ´ HÃ¬nh

### 5.1. Vai trÃ² cá»§a Embedding

CÃ¡c ma tráº­n embedding (token vÃ  position) chá»‰ xuáº¥t hiá»‡n má»™t láº§n á»Ÿ Ä‘áº§u mÃ´ hÃ¬nh vÃ  cÃ³ kÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh cho má»i phiÃªn báº£n GPT-2.

NgÆ°á»£c láº¡i, sá»‘ lÆ°á»£ng Transformer block tÄƒng theo quy mÃ´ mÃ´ hÃ¬nh 

---

### 5.2. áº¢nh hÆ°á»Ÿng Ä‘áº¿n tá»· lá»‡ tham sá»‘

Khi sá»‘ block tÄƒng:

* Tham sá»‘ Attention vÃ  MLP tÄƒng tuyáº¿n tÃ­nh,
* Tham sá»‘ embedding giá»¯ nguyÃªn.

Do Ä‘Ã³, tá»· lá»‡ pháº§n trÄƒm cá»§a Attention vÃ  MLP trong tá»•ng mÃ´ hÃ¬nh ngÃ y cÃ ng lá»›n 

---

## 6. PhÃ¢n TÃ­ch Tham Sá»‘ Layer Normalization

### 6.1. Cáº¥u trÃºc Layer Norm

Layer Normalization sá»­ dá»¥ng hai tham sá»‘ chÃ­nh:

* Tham sá»‘ scale ((\gamma)),
* Tham sá»‘ shift ((\beta)).

CÃ´ng thá»©c:

[
y = \gamma \frac{x - \mu}{\sigma} + \beta
]



---

### 6.2. Káº¿t quáº£ thá»‘ng kÃª

Káº¿t quáº£ cho tháº¥y:

* Sá»‘ tham sá»‘ Layer Norm: vÃ i chá»¥c nghÃ¬n Ä‘áº¿n ~100.000,
* Tá»· lá»‡: < 0.01% tá»•ng mÃ´ hÃ¬nh.

Con sá»‘ nÃ y ráº¥t nhá» so vá»›i hÃ ng trÄƒm triá»‡u hoáº·c hÃ ng tá»· tham sá»‘ tá»•ng thá»ƒ 

---

### 6.3. So sÃ¡nh Weight vÃ  Bias

KhÃ¡c vá»›i toÃ n mÃ´ hÃ¬nh, trong Layer Norm:

* Sá»‘ weight = sá»‘ bias.

NguyÃªn nhÃ¢n lÃ  má»—i chiá»u embedding cÃ³ Ä‘Ãºng má»™t tham sá»‘ scale vÃ  má»™t tham sá»‘ shift 

---

## 7. Tháº£o luáº­n

### 7.1. Ã nghÄ©a Ä‘á»‘i vá»›i thiáº¿t káº¿ mÃ´ hÃ¬nh

Káº¿t quáº£ cho tháº¥y GPT-2 Æ°u tiÃªn:

* Dung lÆ°á»£ng lá»›n cho MLP,
* Duy trÃ¬ Attention á»Ÿ má»©c vá»«a pháº£i,
* Tá»‘i giáº£n Layer Norm.

CÃ¡ch thiáº¿t káº¿ nÃ y giÃºp cÃ¢n báº±ng giá»¯a kháº£ nÄƒng biá»ƒu diá»…n vÃ  chi phÃ­ tÃ­nh toÃ¡n.

---

### 7.2. GiÃ¡ trá»‹ Ä‘á»‘i vá»›i ngÆ°á»i há»c

BÃ i táº­p giÃºp ngÆ°á»i há»c:

* Hiá»ƒu cáº¥u trÃºc tham sá»‘ thá»±c táº¿,
* RÃ¨n luyá»‡n ká»¹ nÄƒng Ä‘á»c mÃ´ hÃ¬nh,
* Chuáº©n bá»‹ cho nghiÃªn cá»©u interpretability.



---

### 7.3. Háº¡n cháº¿

Má»™t sá»‘ háº¡n cháº¿ cá»§a nghiÃªn cá»©u:

* Chá»‰ phÃ¢n tÃ­ch sá»‘ lÆ°á»£ng, chÆ°a xÃ©t phÃ¢n bá»‘ giÃ¡ trá»‹,
* KhÃ´ng liÃªn há»‡ trá»±c tiáº¿p vá»›i hiá»‡u nÄƒng,
* ChÆ°a má»Ÿ rá»™ng sang cÃ¡c mÃ´ hÃ¬nh má»›i hÆ¡n.

Do Ä‘Ã³, cáº§n káº¿t há»£p vá»›i phÃ¢n tÃ­ch thá»±c nghiá»‡m trong tÆ°Æ¡ng lai.

---

## 8. Káº¿t luáº­n

BÃ i viáº¿t Ä‘Ã£ phÃ¢n tÃ­ch sá»± phÃ¢n bá»‘ tham sá»‘ trong GPT-2 theo ba thÃ nh pháº§n chÃ­nh: Attention, MLP vÃ  Layer Normalization. CÃ¡c káº¿t luáº­n chÃ­nh bao gá»“m:

1. MLP chiáº¿m khoáº£ng gáº¥p Ä‘Ã´i sá»‘ tham sá»‘ cá»§a Attention.
2. Tá»· lá»‡ Attention vÃ  MLP tÄƒng theo quy mÃ´ mÃ´ hÃ¬nh.
3. Layer Normalization chá»‰ chiáº¿m tá»· lá»‡ ráº¥t nhá».
4. Weight vÃ  bias trong Layer Norm cÃ³ sá»‘ lÆ°á»£ng báº±ng nhau.

Nhá»¯ng káº¿t quáº£ nÃ y pháº£n Ã¡nh chiáº¿n lÆ°á»£c thiáº¿t káº¿ tá»‘i Æ°u cá»§a GPT-2 vÃ  cung cáº¥p ná»n táº£ng cho nghiÃªn cá»©u sÃ¢u hÆ¡n vá» kiáº¿n trÃºc LLM.

---

## TÃ i liá»‡u tham kháº£o

[1] CodeChallenge: How Many Parameters (Part 2), Lecture Transcript. 

-
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

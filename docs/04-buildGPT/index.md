# üìÇ Index ‚Äî 04-buildGPT

[Home](../README.md) > **04-buildGPT**

---

### üß≠ Quick Navigation

- [üè† C·ªïng t√†i li·ªáu](../README.md)
- [üìö Module 01: LLM Course](../01-LLM_Course/index.md)
- [üî¢ Module 02: Tokenization](../02-Words-to-tokens-to-numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../04-buildGPT/index.md)
- [üéØ Module 07: Fine-tuning](../07-Fine-tune-pretrained-models/index.md)
- [üîç Module 19: AI Safety](../19-AI-safety/index.md)

---

## üìÑ T√†i li·ªáu trong m·ª•c n√†y

- [Kien_truc_mo_hinh_ngon_ngu_lon.md](Kien_truc_mo_hinh_ngon_ngu_lon.md)
- [M·ªü r·ªông Ki·∫øn tr√∫c GPT: Position Embedding, Layer Normalization, Weight Tying v√† Temperature Scaling](aero_LLM_010_Posion_Embedding.md)
- [Bi·ªÉu di·ªÖn T√≠nh Nh√¢n Qu·∫£ Th·ªùi Gian trong C∆° Ch·∫ø Attention b·∫±ng ƒê·∫°i S·ªë Tuy·∫øn T√≠nh](aero_LLM_011_Temporal causality via linear algebra (theory).md)
- [C∆° Ch·∫ø Trung B√¨nh H√≥a Qu√° Kh·ª© v√† Lo·∫°i B·ªè T∆∞∆°ng Lai trong M√¥ H√¨nh Ng√¥n Ng·ªØ Nh√¢n Qu·∫£](aero_LLM_012_Averaging the past while ignoring the future.md)
- [Thu·∫≠t To√°n Attention trong M√¥ H√¨nh Transformer: C∆° S·ªü L√Ω Thuy·∫øt, C∆° Ch·∫ø Ho·∫°t ƒê·ªông v√† H√†m √ù ·ª®ng D·ª•ng](aero_LLM_013_The attention algorithm (theory).md)
- [Ph√¢n T√≠ch v√† Tri·ªÉn Khai C∆° Ch·∫ø Attention: So S√°nh C√†i ƒê·∫∑t Th·ªß C√¥ng v√† PyTorch T·ªëi ∆Øu](aero_LLM_014_CodeChallenge Code Attention.md)
- [Ph√¢n T√≠ch Ki·∫øn Tr√∫c M√¥ H√¨nh Ng√¥n Ng·ªØ v·ªõi M·ªôt Attention Head: L√Ω Thuy·∫øt, Tri·ªÉn Khai v√† ƒê√°nh Gi√°](aero_LLM_015_Model.md)
- [Ph√¢n T√≠ch C·∫•u Tr√∫c Transformer Block: L√Ω Thuy·∫øt, C∆° Ch·∫ø Bi·ªÉu Di·ªÖn v√† Vai Tr√≤ Trong M√¥ H√¨nh Ng√¥n Ng·ªØ](aero_LLM_016_The Transformer block (theory).md)
- [C√†i ƒê·∫∑t Transformer Block B·∫±ng PyTorch: Ph√¢n T√≠ch Ki·∫øn Tr√∫c, Lu·ªìng D·ªØ Li·ªáu v√† T·ªëi ∆Øu H√≥a](aero_LLM_017_The Transformer block (code).md)
- [M√¥ H√¨nh Nhi·ªÅu Transformer Blocks Trong M·∫°ng Ng√¥n Ng·ªØ: Ki·∫øn Tr√∫c, Ph√¢n C·∫•p Bi·ªÉu Di·ªÖn v√† Kh·∫£ NƒÉng M·ªü R·ªông](aero_LLM_018_Model 4 Multiple Transformer blocks..md)
- [aero_LLM_019 copy 10.md](aero_LLM_019 copy 10.md)
- [aero_LLM_019 copy 11.md](aero_LLM_019 copy 11.md)
- [aero_LLM_019 copy 12.md](aero_LLM_019 copy 12.md)
- [aero_LLM_019 copy 13.md](aero_LLM_019 copy 13.md)
- [aero_LLM_019 copy 9.md](aero_LLM_019 copy 9.md)
- [Multi-Head Attention: C∆° S·ªü L√Ω Thuy·∫øt v√† Tri·ªÉn Khai Th·ª±c Ti·ªÖn](aero_LLM_019_Multihead attention theory and implementation.md)
- [aero_LLM_01_Intro.md](aero_LLM_01_Intro.md)
- [T·ªëi ∆Øu H√≥a Hu·∫•n Luy·ªán M√¥ H√¨nh H·ªçc S√¢u B·∫±ng GPU: Nguy√™n L√Ω v√† Th·ª±c H√†nh](aero_LLM_020_Working on the GPU.md)
- [Tri·ªÉn Khai M√¥ H√¨nh GPT-2 Ho√†n Ch·ªânh Tr√™n GPU: Ki·∫øn Tr√∫c, T·ªëi ∆Øu H√≥a v√† ƒê√°nh Gi√° Hi·ªáu NƒÉng](aero_LLM_021_M√¥ H√¨nh GPT-2 Ho√†n Ch·ªânh Tr√™n GPU.md)
- [ƒê√°nh Gi√° Hi·ªáu NƒÉng GPT-2 Tr√™n CPU v√† GPU: Th·ª±c Nghi·ªám Th·ªùi Gian Kh·ªüi T·∫°o, Suy Lu·∫≠n v√† Hu·∫•n Luy·ªán](aero_LLM_022_ƒê√°nh Gi√° Hi·ªáu NƒÉng GPT-2 Tr√™n CPU v√† GPU.md)
- [Kh·∫£o S√°t M√¥ H√¨nh GPT-2 Ti·ªÅn Hu·∫•n Luy·ªán c·ªßa OpenAI: Ki·∫øn Tr√∫c, Tham S·ªë v√† C∆° Ch·∫ø Sinh VƒÉn B·∫£n](aero_LLM_023_Inspecting OpenAI's GPT2.md)
- [Ki·∫øn Tr√∫c Transformer v√† Tri·ªÉn Khai GPT-2 tr√™n GPU: Ph√¢n T√≠ch To√°n H·ªçc v√† Hi·ªáu NƒÉng T√≠nh To√°n](aero_LLM_024_Summarizing GPT using equations.md)
- [Tr·ª±c Quan H√≥a Ki·∫øn Tr√∫c GPT Th√¥ng Qua nano-GPT: Ti·∫øp C·∫≠n Tr·ª±c Quan trong Nghi√™n C·ª©u M√¥ H√¨nh Ng√¥n Ng·ªØ](aero_LLM_025_Visualizing nano-GPT.md)
- [Ph√¢n T√≠ch S·ªë L∆∞·ª£ng Tham S·ªë Trong M√¥ H√¨nh GPT-2: Ph∆∞∆°ng Ph√°p ƒê·ªãnh L∆∞·ª£ng v√† √ù Nghƒ©a Ki·∫øn Tr√∫c](aero_LLM_026_CodeChallenge How many parameters (part 1).md)
- [Ph√¢n B·ªë Tham S·ªë Trong GPT-2: So S√°nh Attention, MLP v√† Layer Normalization](aero_LLM_027_CodeChallenge How many parameters (part 2).md)
- [üìò Ph√¢n T√≠ch Ki·∫øn Tr√∫c GPT-2: T·ª´ C∆° Ch·∫ø Multi-Head Attention ƒê·∫øn Hi·ªáu NƒÉng T√≠nh To√°n Tr√™n GPU](aero_LLM_028_CodeChallenge GPT2 trained weights distributions.md)
- [üß† Ph√¢n T√≠ch Nh√¢n Qu·∫£ Trong GPT-2: Vai Tr√≤ C·ªßa Ma Tr·∫≠n Query Th√¥ng Qua Can Thi·ªáp Tham S·ªë](aero_LLM_029_CodeChallenge Do we really need Q.md)
- [Ph√¢n T√≠ch Ki·∫øn Tr√∫c v√† C∆° Ch·∫ø Ho·∫°t ƒê·ªông c·ªßa M√¥ H√¨nh Ng√¥n Ng·ªØ Transformer C∆° B·∫£n](aero_LLM_02_Transformer.md)
- [Ph√¢n T√≠ch K·ªπ Thu·∫≠t: So S√°nh `nn.Embedding` v√† `nn.Linear` trong PyTorch](aero_LLM_03_embedding_Linear.md)
- [Ph√¢n T√≠ch So S√°nh H√†m K√≠ch Ho·∫°t GELU v√† ReLU trong M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn: G√≥c Nh√¨n L√Ω Thuy·∫øt v√† Th·ª±c Nghi·ªám](aero_LLM_04_GELU_vs_ReLU_Academic_Analysis.md)
- [H√†m Softmax v√† Tham S·ªë Temperature trong M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn: Ph√¢n T√≠ch To√°n H·ªçc v√† Th·ª±c Nghi·ªám](aero_LLM_05_Softmax temperature academic analysis.md)
- [Ph√¢n T√≠ch `torch.multinomial`: L·∫•y M·∫´u X√°c Su·∫•t trong Sinh VƒÉn B·∫£n v·ªõi PyTorch](aero_LLM_06_Torch multinomial academic analysis.md)
- [Ph∆∞∆°ng Ph√°p L·∫•y M·∫´u Token trong Sinh VƒÉn B·∫£n: Ph√¢n T√≠ch So S√°nh Greedy, Top-K, Top-P v√† Multinomial Sampling](aero_LLM_07_Token_Sampling_methods.md)
- [Ph√¢n T√≠ch H√†nh Vi C·ªßa H√†m Softmax Trong M√¥ H√¨nh H·ªçc S√¢u: ·∫¢nh H∆∞·ªüng C·ªßa L·∫∑p, Ph·∫°m Vi S·ªë H·ªçc V√† Nhi·ªát ƒê·ªô](aero_LLM_08_Ham_Softbank.md)
- [Ph√¢n T√≠ch Layer Normalization Trong H·ªçc S√¢u: C∆° S·ªü L√Ω Thuy·∫øt, ·ªîn ƒê·ªãnh S·ªë H·ªçc V√† ·ª®ng D·ª•ng Th·ª±c Ti·ªÖn](aero_LLM_09_Layer_Normalization.md)

---
*T·ª± ƒë·ªông c·∫≠p nh·∫≠t b·ªüi Aero-Indexer*
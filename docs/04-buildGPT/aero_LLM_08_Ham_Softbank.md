
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [04 buildGPT](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
DÆ°á»›i Ä‘Ã¢y lÃ  bÃ i viáº¿t khoa há»c dÆ°á»›i dáº¡ng **Markdown**, dá»±a trÃªn ná»™i dung báº¡n cung cáº¥p tá»« tÃ i liá»‡u *â€œMore Softmax Explorationsâ€* , káº¿t há»£p phÃ¢n tÃ­ch lÃ½ thuyáº¿t vÃ  tham kháº£o há»c thuáº­t.

---

# PhÃ¢n TÃ­ch HÃ nh Vi Cá»§a HÃ m Softmax Trong MÃ´ HÃ¬nh Há»c SÃ¢u: áº¢nh HÆ°á»Ÿng Cá»§a Láº·p, Pháº¡m Vi Sá»‘ Há»c VÃ  Nhiá»‡t Äá»™

## TÃ³m táº¯t (Abstract)

HÃ m Softmax lÃ  má»™t thÃ nh pháº§n cá»‘t lÃµi trong cÃ¡c mÃ´ hÃ¬nh há»c sÃ¢u, Ä‘áº·c biá»‡t trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  thá»‹ giÃ¡c mÃ¡y tÃ­nh. BÃ i viáº¿t nÃ y phÃ¢n tÃ­ch hÃ nh vi cá»§a Softmax thÃ´ng qua hai thÃ­ nghiá»‡m: (1) Ã¡p dá»¥ng Softmax láº·p nhiá»u láº§n lÃªn cÃ¹ng má»™t phÃ¢n phá»‘i, vÃ  (2) kháº£o sÃ¡t áº£nh hÆ°á»Ÿng cá»§a pháº¡m vi giÃ¡ trá»‹ logits vÃ  tham sá»‘ nhiá»‡t Ä‘á»™ (temperature). Káº¿t quáº£ cho tháº¥y Softmax cÃ³ xu hÆ°á»›ng lÃ m pháº³ng phÃ¢n phá»‘i khi Ä‘Æ°á»£c láº·p láº¡i, Ä‘á»“ng thá»i ráº¥t nháº¡y cáº£m vá»›i miá»n giÃ¡ trá»‹ sá»‘ vÃ  nhiá»‡t Ä‘á»™. Nhá»¯ng phÃ¡t hiá»‡n nÃ y nháº¥n máº¡nh vai trÃ² cá»§a chuáº©n hÃ³a vÃ  kiá»ƒm soÃ¡t Ä‘á»™ á»•n Ä‘á»‹nh sá»‘ trong cÃ¡c mÃ´ hÃ¬nh há»c sÃ¢u hiá»‡n Ä‘áº¡i.

---

## 1. Giá»›i thiá»‡u

Trong há»c sÃ¢u, Softmax thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ chuyá»ƒn Ä‘á»•i vector logits thÃ nh phÃ¢n phá»‘i xÃ¡c suáº¥t. Cho vector Ä‘áº§u vÃ o ( x = (x_1, x_2, ..., x_n) ), Softmax Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° sau:

[
\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
]

HÃ m nÃ y Ä‘áº£m báº£o ráº±ng:

* Má»—i giÃ¡ trá»‹ Ä‘áº§u ra náº±m trong khoáº£ng (0,1),
* Tá»•ng cÃ¡c giÃ¡ trá»‹ báº±ng 1.

Máº·c dÃ¹ cÃ´ng thá»©c Ä‘Æ¡n giáº£n, hÃ nh vi thá»±c táº¿ cá»§a Softmax trong mÃ´i trÆ°á»ng sá»‘ há»c vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh phá»©c táº¡p hÆ¡n nhiá»u. TÃ i liá»‡u thá»±c nghiá»‡m Ä‘Æ°á»£c cung cáº¥p  cho tháº¥y nhiá»u hiá»‡n tÆ°á»£ng phi trá»±c giÃ¡c, Ä‘áº·c biá»‡t khi Softmax Ä‘Æ°á»£c Ã¡p dá»¥ng láº·p láº¡i hoáº·c káº¿t há»£p vá»›i tham sá»‘ nhiá»‡t Ä‘á»™.

---

## 2. CÆ¡ sá»Ÿ lÃ½ thuyáº¿t

### 2.1. Softmax vÃ  phÃ¢n phá»‘i xÃ¡c suáº¥t

Softmax biáº¿n Ä‘á»•i cÃ¡c giÃ¡ trá»‹ logits thÃ nh xÃ¡c suáº¥t báº±ng hÃ m mÅ©. Do tÃ­nh cháº¥t tÄƒng nhanh cá»§a hÃ m mÅ©, nhá»¯ng giÃ¡ trá»‹ lá»›n sáº½ Ä‘Æ°á»£c khuáº¿ch Ä‘áº¡i, trong khi giÃ¡ trá»‹ nhá» bá»‹ suy giáº£m máº¡nh.

### 2.2. Softmax vá»›i tham sá»‘ nhiá»‡t Ä‘á»™

PhiÃªn báº£n má»Ÿ rá»™ng cá»§a Softmax cÃ³ dáº¡ng:

[
\text{Softmax}*T(x_i) = \frac{e^{x_i/T}}{\sum*{j=1}^{n} e^{x_j/T}}
]

Trong Ä‘Ã³ (T) lÃ  nhiá»‡t Ä‘á»™:

* (T < 1): PhÃ¢n phá»‘i sáº¯c nÃ©t (sharp), táº­p trung vÃ o pháº§n tá»­ lá»›n nháº¥t.
* (T = 1): Softmax chuáº©n.
* (T > 1): PhÃ¢n phá»‘i pháº³ng (smooth), tÄƒng tÃ­nh Ä‘a dáº¡ng.

### 2.3. á»”n Ä‘á»‹nh sá»‘ há»c

Viá»‡c tÃ­nh toÃ¡n hÃ m mÅ© trÃªn cÃ¡c giÃ¡ trá»‹ lá»›n hoáº·c nhá» cÃ³ thá»ƒ gÃ¢y:

* TrÃ n sá»‘ (overflow),
* Máº¥t Ä‘á»™ chÃ­nh xÃ¡c (underflow),
* Gradient biáº¿n máº¥t hoáº·c bÃ¹ng ná»•.

Do Ä‘Ã³, cÃ¡c ká»¹ thuáº­t chuáº©n hÃ³a (normalization) lÃ  cáº§n thiáº¿t trong máº¡ng sÃ¢u.

---

## 3. PhÆ°Æ¡ng phÃ¡p nghiÃªn cá»©u

NghiÃªn cá»©u dá»±a trÃªn hai thÃ­ nghiá»‡m chÃ­nh Ä‘Æ°á»£c mÃ´ táº£ trong tÃ i liá»‡u gá»‘c .

### 3.1. ThÃ­ nghiá»‡m 1: Softmax láº·p

#### MÃ´ táº£

* Táº¡o 20 sá»‘ tuyáº¿n tÃ­nh trong khoáº£ng [0,1].
* Ãp dá»¥ng Softmax.
* Láº·p láº¡i quÃ¡ trÃ¬nh Softmax trÃªn chÃ­nh Ä‘áº§u ra nhiá»u láº§n (8 láº§n).
* TÃ­nh Ä‘á»™ lá»‡ch chuáº©n cá»§a phÃ¢n phá»‘i sau má»—i láº§n láº·p.

#### Má»¥c tiÃªu

Kháº£o sÃ¡t viá»‡c Softmax láº·p cÃ³ lÃ m phÃ¢n phá»‘i trá»Ÿ nÃªn â€œsáº¯c nÃ©tâ€ hÆ¡n hay khÃ´ng.

---

### 3.2. ThÃ­ nghiá»‡m 2: Pháº¡m vi logits vÃ  nhiá»‡t Ä‘á»™

#### MÃ´ táº£

* Sinh 100 logits trong cÃ¡c khoáº£ng:

  * [-0.4, 0.4]
  * [-1, 1]
  * [-5, 5]
* ThÃªm má»™t giÃ¡ trá»‹ ngoáº¡i lai: 6.
* Ãp dá»¥ng Softmax vá»›i cÃ¡c nhiá»‡t Ä‘á»™: 0.5, 1, 3.
* PhÃ¢n tÃ­ch xÃ¡c suáº¥t Ä‘áº§u ra.

#### Má»¥c tiÃªu

ÄÃ¡nh giÃ¡ áº£nh hÆ°á»Ÿng cá»§a:

* Miá»n giÃ¡ trá»‹ logits,
* Nhiá»‡t Ä‘á»™,
* GiÃ¡ trá»‹ ngoáº¡i lai.

---

## 4. Káº¿t quáº£ thá»±c nghiá»‡m

### 4.1. Hiá»‡u á»©ng cá»§a Softmax láº·p

Káº¿t quáº£ cho tháº¥y:

* Sau vÃ i láº§n láº·p, phÃ¢n phá»‘i há»™i tá»¥ vá» dáº¡ng gáº§n nhÆ° Ä‘á»“ng Ä‘á»u.
* Vá»›i 20 pháº§n tá»­, má»—i giÃ¡ trá»‹ tiáº¿n gáº§n Ä‘áº¿n 0.05.
* Äá»™ lá»‡ch chuáº©n giáº£m nhanh theo cáº¥p sá»‘ nhÃ¢n.

Äiá»u nÃ y cho tháº¥y Softmax láº·p khÃ´ng lÃ m ná»•i báº­t pháº§n tá»­ lá»›n nháº¥t, mÃ  ngÆ°á»£c láº¡i lÃ m máº¥t tÃ­nh phÃ¢n biá»‡t.

### 4.2. Vai trÃ² cá»§a sá»‘ lÆ°á»£ng pháº§n tá»­

Sá»‘ lÆ°á»£ng pháº§n tá»­ áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n giÃ¡ trá»‹ trung bÃ¬nh:

| Sá»‘ pháº§n tá»­ | GiÃ¡ trá»‹ trung bÃ¬nh |
| ---------- | ------------------ |
| 4          | â‰ˆ 0.25             |
| 20         | â‰ˆ 0.05             |
| 100        | â‰ˆ 0.01             |

CÃ ng nhiá»u pháº§n tá»­, xÃ¡c suáº¥t riÃªng láº» cÃ ng nhá».

### 4.3. áº¢nh hÆ°á»Ÿng cá»§a nhiá»‡t Ä‘á»™

| Nhiá»‡t Ä‘á»™ | Äáº·c Ä‘iá»ƒm               |
| -------- | ---------------------- |
| T < 1    | Táº­p trung máº¡nh vÃ o max |
| T = 1    | CÃ¢n báº±ng               |
| T > 1    | PhÃ¢n tÃ¡n               |

á» T = 0.5, pháº§n tá»­ cÃ³ logit = 6 chiáº¿m gáº§n nhÆ° toÃ n bá»™ xÃ¡c suáº¥t.
á» T = 3, phÃ¢n phá»‘i trá»Ÿ nÃªn má»m hÆ¡n, tÄƒng tÃ­nh ngáº«u nhiÃªn.

### 4.4. áº¢nh hÆ°á»Ÿng cá»§a pháº¡m vi logits

Khi miá»n giÃ¡ trá»‹ háº¹p ([-0.4, 0.4]):

* GiÃ¡ trá»‹ 6 trá»Ÿ nÃªn vÆ°á»£t trá»™i tuyá»‡t Ä‘á»‘i.

Khi miá»n rá»™ng ([-5, 5]):

* Sá»± khÃ¡c biá»‡t tÆ°Æ¡ng Ä‘á»‘i giáº£m.
* PhÃ¢n phá»‘i cÃ¢n báº±ng hÆ¡n.

Äiá»u nÃ y chá»©ng minh ráº±ng Softmax phá»¥ thuá»™c máº¡nh vÃ o Ä‘á»™ chÃªnh lá»‡ch tÆ°Æ¡ng Ä‘á»‘i, khÃ´ng chá»‰ giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i.

---

## 5. Tháº£o luáº­n

### 5.1. VÃ¬ sao Softmax láº·p lÃ m pháº³ng phÃ¢n phá»‘i?

Do Ä‘áº§u ra Softmax Ä‘Ã£ náº±m trong [0,1] vÃ  cÃ³ tá»•ng báº±ng 1. Khi tiáº¿p tá»¥c Ã¡p dá»¥ng hÃ m mÅ© trÃªn miá»n háº¹p, hÃ m mÅ© trá»Ÿ nÃªn gáº§n tuyáº¿n tÃ­nh, lÃ m máº¥t hiá»‡u á»©ng khuáº¿ch Ä‘áº¡i.

### 5.2. Há»‡ quáº£ trong mÃ´ hÃ¬nh ngÃ´n ngá»¯

Trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n:

* Vocabulary cÃ³ thá»ƒ > 100,000 token.
* Softmax sáº½ nÃ©n háº§u háº¿t xÃ¡c suáº¥t vá» gáº§n 0.
* Chá»‰ vÃ i token chiáº¿m Æ°u tháº¿.

Do Ä‘Ã³:

* Nhiá»‡t Ä‘á»™ tháº¥p â†’ mÃ´ hÃ¬nh láº·p, Ã­t sÃ¡ng táº¡o.
* Nhiá»‡t Ä‘á»™ cao â†’ Ä‘a dáº¡ng nhÆ°ng giáº£m Ä‘á»™ chÃ­nh xÃ¡c.

### 5.3. LiÃªn há»‡ vá»›i chuáº©n hÃ³a

CÃ¡c káº¿t quáº£ cho tháº¥y:

* Logits quÃ¡ nhá» â†’ Softmax máº¥t hiá»‡u quáº£.
* Logits quÃ¡ lá»›n â†’ máº¥t á»•n Ä‘á»‹nh.

VÃ¬ váº­y, cÃ¡c ká»¹ thuáº­t nhÆ°:

* Layer Normalization,
* Batch Normalization,
* Weight Regularization,

lÃ  cáº§n thiáº¿t Ä‘á»ƒ duy trÃ¬ miá»n giÃ¡ trá»‹ há»£p lÃ½.

---

## 6. á»¨ng dá»¥ng thá»±c tiá»…n

### 6.1. Äiá»u chá»‰nh nhiá»‡t Ä‘á»™ sinh vÄƒn báº£n

| Má»¥c tiÃªu  | Nhiá»‡t Ä‘á»™  |
| --------- | --------- |
| ChÃ­nh xÃ¡c | 0.2 â€“ 0.7 |
| CÃ¢n báº±ng  | â‰ˆ 1       |
| SÃ¡ng táº¡o  | 1.2 â€“ 2   |

### 6.2. Thiáº¿t káº¿ mÃ´ hÃ¬nh

* TrÃ¡nh Ä‘á»ƒ logits quÃ¡ háº¹p.
* Ãp dá»¥ng normalization phÃ¹ há»£p.
* Kiá»ƒm soÃ¡t scale khi huáº¥n luyá»‡n.

### 6.3. Debug mÃ´ hÃ¬nh

Hiá»‡n tÆ°á»£ng phÃ¢n phá»‘i â€œcháº¿tâ€ (collapse) cÃ³ thá»ƒ xuáº¥t phÃ¡t tá»«:

* Logits bá»‹ nÃ©n,
* Temperature quÃ¡ tháº¥p,
* Lá»—i chuáº©n hÃ³a.

---

## 7. Káº¿t luáº­n

BÃ i viáº¿t Ä‘Ã£ phÃ¢n tÃ­ch chi tiáº¿t hÃ nh vi cá»§a Softmax thÃ´ng qua hai thÃ­ nghiá»‡m thá»±c nghiá»‡m. CÃ¡c káº¿t luáº­n chÃ­nh bao gá»“m:

1. Softmax láº·p lÃ m pháº³ng phÃ¢n phá»‘i thay vÃ¬ lÃ m sáº¯c nÃ©t.
2. Sá»‘ lÆ°á»£ng pháº§n tá»­ áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n xÃ¡c suáº¥t.
3. Nhiá»‡t Ä‘á»™ kiá»ƒm soÃ¡t má»©c Ä‘á»™ táº­p trung.
4. Pháº¡m vi logits quyáº¿t Ä‘á»‹nh má»©c Ä‘á»™ phÃ¢n biá»‡t.
5. Chuáº©n hÃ³a lÃ  yáº¿u tá»‘ sá»‘ng cÃ²n trong máº¡ng sÃ¢u.

Nhá»¯ng káº¿t quáº£ nÃ y giÃºp hiá»ƒu rÃµ hÆ¡n vá» cÃ¡ch Softmax hoáº¡t Ä‘á»™ng trong cÃ¡c mÃ´ hÃ¬nh há»c sÃ¢u hiá»‡n Ä‘áº¡i vÃ  cung cáº¥p cÆ¡ sá»Ÿ cho viá»‡c Ä‘iá»u chá»‰nh mÃ´ hÃ¬nh hiá»‡u quáº£.

---

## TÃ i liá»‡u tham kháº£o

1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.
2. Vaswani, A. et al. (2017). Attention Is All You Need. *NeurIPS*.
3. Bishop, C. (2006). *Pattern Recognition and Machine Learning*. Springer.
4. TÃ i liá»‡u thá»±c nghiá»‡m vá» Softmax 

---
**phiÃªn báº£n Python/PyTorch chuáº©n, Ä‘áº§y Ä‘á»§, cÃ³ chÃº thÃ­ch khoa há»c**, tÆ°Æ¡ng á»©ng trá»±c tiáº¿p vá»›i pháº§n *pseudocode* á»Ÿ trÃªn. MÃ£ nÃ y cÃ³ thá»ƒ dÃ¹ng Ä‘á»ƒ:

âœ… TÃ¡i láº­p thÃ­ nghiá»‡m
âœ… Viáº¿t bÃ¡o cÃ¡o / paper
âœ… LÃ m supplementary material
âœ… Thá»±c hÃ nh nghiÃªn cá»©u Softmax

Báº¡n cÃ³ thá»ƒ copy vÃ  cháº¡y trá»±c tiáº¿p trong mÃ´i trÆ°á»ng cÃ³ `torch` vÃ  `matplotlib`.

---

# ğŸ“Œ 1. CÃ i Ä‘áº·t thÆ° viá»‡n

```python
import torch
import matplotlib.pyplot as plt
import numpy as np
```

---

# ğŸ“Œ 2. HÃ m Softmax chuáº©n (cÃ³ nhiá»‡t Ä‘á»™)

```python
def softmax_t(x, temperature=1.0):
    """
    Temperature-scaled softmax

    Args:
        x (Tensor): Logits
        temperature (float): Temperature parameter

    Returns:
        Tensor: Probability distribution
    """
    x_scaled = x / temperature
    exp_x = torch.exp(x_scaled - torch.max(x_scaled))  # numerical stability
    return exp_x / torch.sum(exp_x)
```

ğŸ‘‰ Trá»« `max(x)` Ä‘á»ƒ trÃ¡nh overflow (chuáº©n nghiÃªn cá»©u).

---

# ğŸ“Œ 3. ThÃ­ nghiá»‡m 1: Softmax láº·p

## 3.1. HÃ m thá»±c nghiá»‡m

```python
def iterative_softmax_experiment(
    n_points=20,
    n_iters=8,
    min_val=0.0,
    max_val=1.0
):
    """
    Iterative Softmax Experiment

    Returns:
        probs (list): List of distributions
        stds (list): Standard deviations
    """

    # Generate linear data
    x = torch.linspace(min_val, max_val, n_points)

    probs = []
    stds = []

    # Initial softmax
    p = softmax_t(x)

    probs.append(p.clone())

    for i in range(n_iters):

        std = torch.std(p)
        stds.append(std.item())

        # Apply softmax again
        p = softmax_t(p)

        probs.append(p.clone())

    return probs, stds, x
```

---

## 3.2. Cháº¡y thÃ­ nghiá»‡m

```python
probs, stds, x = iterative_softmax_experiment()
```

---

## 3.3. Váº½ káº¿t quáº£

### PhÃ¢n phá»‘i theo vÃ²ng láº·p

```python
plt.figure(figsize=(8, 6))

for i, p in enumerate(probs):
    plt.scatter(x, p, label=f"Iter {i}", s=30)

plt.xlabel("Input values")
plt.ylabel("Softmax probability")
plt.title("Iterative Softmax Behavior")
plt.legend()
plt.grid(True)
plt.show()
```

---

### Log Ä‘á»™ lá»‡ch chuáº©n

```python
log_stds = np.log(np.array(stds) + 1e-12)

plt.figure(figsize=(6, 5))

plt.plot(range(len(log_stds)), log_stds, marker='o')

plt.xlabel("Iteration")
plt.ylabel("Log(Standard Deviation)")
plt.title("Convergence of Iterative Softmax")
plt.grid(True)
plt.show()
```

---

# ğŸ“Œ 4. ThÃ­ nghiá»‡m 2: Pháº¡m vi logits & Nhiá»‡t Ä‘á»™

---

## 4.1. HÃ m thá»±c nghiá»‡m

```python
def temperature_range_experiment(
    ranges=[0.4, 1, 5],
    temperatures=[0.5, 1.0, 3.0],
    n_points=100,
    outlier=6.0
):
    """
    Temperature & Logit Range Experiment

    Returns:
        results (dict): Nested results
    """

    results = {}

    for r in ranges:

        # Generate logits
        x = torch.linspace(-r, r, n_points)

        # Append outlier
        out = torch.tensor([outlier])
        logits = torch.cat([x, out])

        results[r] = {}

        for t in temperatures:

            probs = softmax_t(logits, t)

            results[r][t] = {
                "logits": logits,
                "probs": probs
            }

    return results
```

---

## 4.2. Cháº¡y thÃ­ nghiá»‡m

```python
results = temperature_range_experiment()
```

---

## 4.3. Váº½ káº¿t quáº£

### Zoom-in (xÃ¡c suáº¥t nhá»)

```python
def plot_zoomed(results):

    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    for idx, r in enumerate(results):

        ax = axes[idx]

        for t in results[r]:

            data = results[r][t]

            logits = data["logits"][:-1]
            probs = data["probs"][:-1]

            ax.scatter(
                logits.numpy(),
                probs.numpy(),
                label=f"T={t}",
                s=20
            )

        ax.set_title(f"Range [-{r}, {r}]")
        ax.set_ylim(0, 0.03)
        ax.set_xlabel("Logits")
        ax.set_ylabel("Probability")
        ax.legend()
        ax.grid(True)

    plt.tight_layout()
    plt.show()
```

---

### Zoom-out (toÃ n bá»™ phÃ¢n phá»‘i)

```python
def plot_full(results):

    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    for idx, r in enumerate(results):

        ax = axes[idx]

        for t in results[r]:

            data = results[r][t]

            logits = data["logits"]
            probs = data["probs"]

            ax.scatter(
                logits.numpy(),
                probs.numpy(),
                label=f"T={t}",
                s=20
            )

        ax.set_title(f"Range [-{r}, {r}]")
        ax.set_ylim(0, 1)
        ax.set_xlabel("Logits")
        ax.set_ylabel("Probability")
        ax.legend()
        ax.grid(True)

    plt.tight_layout()
    plt.show()
```

---

## 4.4. Váº½ Ä‘á»“ thá»‹

```python
plot_zoomed(results)
plot_full(results)
```

---

# ğŸ“Œ 5. Pipeline tá»•ng há»£p (chuáº©n nghiÃªn cá»©u)

```python
def run_full_pipeline():

    print("Running Iterative Softmax...")
    probs, stds, x = iterative_softmax_experiment()

    print("Running Temperature Experiment...")
    results = temperature_range_experiment()

    plot_zoomed(results)
    plot_full(results)

    return probs, stds, results
```

---

```python
probs, stds, results = run_full_pipeline()
```

---

# ğŸ“Œ 6. Äáº·c Ä‘iá»ƒm chuáº©n há»c thuáº­t cá»§a mÃ£ nÃ y

MÃ£ trÃªn Ä‘Ã¡p á»©ng tiÃªu chuáº©n:

âœ… Numerical stability
âœ… Reproducibility
âœ… Modular design
âœ… Research-friendly
âœ… Dá»… má»Ÿ rá»™ng

Ãp dá»¥ng Ä‘Æ°á»£c cho:

* Paper supplementary
* Replication study
* Thesis
* Research demo

---

# ğŸ“Œ 7. Gá»£i Ã½ nÃ¢ng cao (náº¿u báº¡n lÃ m nghiÃªn cá»©u sÃ¢u hÆ¡n)

Báº¡n cÃ³ thá»ƒ má»Ÿ rá»™ng thÃªm:

### ğŸ”¹ PhÃ¢n tÃ­ch Ä‘á»™ phá»©c táº¡p

```python
# O(N) per softmax
# O(KN) for iterative
```

### ğŸ”¹ Seed cá»‘ Ä‘á»‹nh

```python
torch.manual_seed(42)
np.random.seed(42)
```

### ğŸ”¹ GPU

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

**FULL TEMPLATE: Local RAG vá»›i Ollama + Qdrant + FastAPI (Cháº¡y 100% Offline)** ğŸš€
PhÃ¹ há»£p: Privacy cao, Intranet, doanh nghiá»‡p khÃ´ng dÃ¹ng API cloud.

# ğŸ—ï¸ LOCAL RAG STACK

```
FastAPI + Ollama (LLM Local) + Qdrant (Vector DB) + Embedding Local
```

KhÃ´ng cáº§n OpenAI â€“ khÃ´ng gá»­i dá»¯ liá»‡u ra ngoÃ i.

---

## ğŸ”§ TECH STACK

| Layer     | Tool                 |
| --------- | -------------------- |
| LLM       | **Ollama**           |
| Vector DB | **Qdrant**           |
| API       | FastAPI              |
| Embedding | SentenceTransformers |
| Infra     | Docker               |

---

# ğŸ“ PROJECT STRUCTURE

```
local-rag/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ ingest.py
â”‚   â”œâ”€â”€ rag.py
â”‚   â”œâ”€â”€ vector.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ data/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env
```

---

# 1ï¸âƒ£ CÃ€I OLLAMA (LOCAL LLM)

### Mac / Linux

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Windows

Táº£i táº¡i: ollama.com

---

## Pull Model (Khuyáº¿n nghá»‹)

```bash
ollama pull llama3
ollama pull mistral
ollama pull qwen2
```

Test:

```bash
ollama run llama3
```

---

# 2ï¸âƒ£ docker-compose.yml (Qdrant)

```yaml
version: "3.9"

services:
  qdrant:
    image: qdrant/qdrant
    ports:
      - "6333:6333"
    volumes:
      - ./qdrant_data:/qdrant/storage
```

Run:

```bash
docker compose up -d
```

---

# 3ï¸âƒ£ requirements.txt

```txt
fastapi
uvicorn
qdrant-client
sentence-transformers
python-dotenv
pypdf
tiktoken
requests
```

---

# 4ï¸âƒ£ .env

```env
QDRANT_URL=http://localhost:6333
COLLECTION_NAME=local_rag
OLLAMA_URL=http://localhost:11434
LLM_MODEL=llama3
```

---

# 5ï¸âƒ£ app/config.py

```python
import os
from dotenv import load_dotenv

load_dotenv()

QDRANT_URL = os.getenv("QDRANT_URL")
COLLECTION = os.getenv("COLLECTION_NAME")

OLLAMA_URL = os.getenv("OLLAMA_URL")
LLM_MODEL = os.getenv("LLM_MODEL")

CHUNK_SIZE = 500
OVERLAP = 80
TOP_K = 5
```

---

# 6ï¸âƒ£ app/vector.py

```python
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance
from .config import QDRANT_URL, COLLECTION


client = QdrantClient(url=QDRANT_URL)


def init_collection(dim):

    names = [c.name for c in client.get_collections().collections]

    if COLLECTION not in names:
        client.create_collection(
            collection_name=COLLECTION,
            vectors_config=VectorParams(
                size=dim,
                distance=Distance.COSINE
            )
        )


def upsert(vectors, payloads, ids):

    client.upsert(
        collection_name=COLLECTION,
        points=[
            {
                "id": ids[i],
                "vector": vectors[i],
                "payload": payloads[i]
            }
            for i in range(len(vectors))
        ]
    )


def search(qvec, limit):

    return client.search(
        collection_name=COLLECTION,
        query_vector=qvec,
        limit=limit
    )
```

---

# 7ï¸âƒ£ app/utils.py (Chunking)

```python
import uuid
import tiktoken
from pypdf import PdfReader
from .config import CHUNK_SIZE, OVERLAP


tokenizer = tiktoken.get_encoding("cl100k_base")


def load_pdf(path):

    reader = PdfReader(path)
    text = ""

    for p in reader.pages:
        text += p.extract_text() + "\n"

    return text


def chunk_text(text):

    tokens = tokenizer.encode(text)

    chunks = []

    for i in range(0, len(tokens), CHUNK_SIZE - OVERLAP):
        chunk = tokens[i:i + CHUNK_SIZE]
        chunks.append(tokenizer.decode(chunk))

    return chunks


def gen_ids(n):

    return [str(uuid.uuid4()) for _ in range(n)]
```

---

# 8ï¸âƒ£ app/ingest.py (Embedding + Index)

```python
from sentence_transformers import SentenceTransformer

from .utils import load_pdf, chunk_text, gen_ids
from .vector import init_collection, upsert


model = SentenceTransformer("all-MiniLM-L6-v2")
EMBED_DIM = 384


def embed(texts):

    return model.encode(texts).tolist()


def ingest_pdf(path, metadata={}):

    text = load_pdf(path)

    chunks = chunk_text(text)

    vectors = embed(chunks)

    payloads = [
        {
            "text": chunks[i],
            **metadata
        }
        for i in range(len(chunks))
    ]

    ids = gen_ids(len(chunks))

    init_collection(EMBED_DIM)

    upsert(vectors, payloads, ids)

    return len(chunks)
```

---

# 9ï¸âƒ£ app/rag.py (Ollama Integration)

```python
import requests

from .vector import search
from .config import TOP_K, OLLAMA_URL, LLM_MODEL
from sentence_transformers import SentenceTransformer


embed_model = SentenceTransformer("all-MiniLM-L6-v2")


def embed_query(q):

    return embed_model.encode([q])[0].tolist()


def call_ollama(prompt):

    res = requests.post(
        f"{OLLAMA_URL}/api/generate",
        json={
            "model": LLM_MODEL,
            "prompt": prompt,
            "stream": False
        }
    )

    return res.json()["response"]


def ask(question):

    qvec = embed_query(question)

    docs = search(qvec, TOP_K)

    context = "\n".join(
        [d.payload["text"] for d in docs]
    )

    prompt = f"""
You are an internal assistant.
Only use the context below.

Context:
{context}

Question:
{question}

Answer:
"""

    answer = call_ollama(prompt)

    sources = [d.id for d in docs]

    return {
        "answer": answer,
        "sources": sources
    }
```

---

# ğŸ”Ÿ app/main.py (API)

```python
from fastapi import FastAPI, UploadFile, File
import shutil

from .ingest import ingest_pdf
from .rag import ask


app = FastAPI(title="Local RAG System")


@app.post("/upload")
async def upload(file: UploadFile = File(...)):

    path = f"data/{file.filename}"

    with open(path, "wb") as f:
        shutil.copyfileobj(file.file, f)

    n = ingest_pdf(path)

    return {"indexed_chunks": n}


@app.post("/ask")
async def query(q: str):

    return ask(q)
```

---

# ğŸš€ CHáº Y Há»† THá»NG

### 1ï¸âƒ£ Start Ollama

```bash
ollama serve
```

### 2ï¸âƒ£ Start Qdrant

```bash
docker compose up -d
```

### 3ï¸âƒ£ Install Python

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run API

```bash
uvicorn app.main:app --reload
```

### 5ï¸âƒ£ Open Swagger

```
http://localhost:8000/docs
```

---

# ğŸ§ª TEST FLOW

### Upload tÃ i liá»‡u

```
POST /upload
```

### Há»i AI

```
POST /ask?q=Quy trÃ¬nh hoÃ n tiá»n nÄƒm 2024?
```

---

# ğŸ“Š PERFORMANCE THá»°C Táº¾ (LOCAL)

| Model      | RAM  | Speed | Quality |
| ---------- | ---- | ----- | ------- |
| llama3:8b  | 16GB | âš¡âš¡âš¡   | â­â­â­â­    |
| mistral:7b | 12GB | âš¡âš¡    | â­â­â­     |
| qwen2:7b   | 16GB | âš¡âš¡âš¡   | â­â­â­â­    |

ğŸ‘‰ Khuyáº¿n nghá»‹: llama3 8B cho production ná»™i bá»™

---

# ğŸ” Æ¯U ÄIá»‚M LOCAL RAG

âœ… 100% private
âœ… KhÃ´ng tá»‘n API cost
âœ… KhÃ´ng lo leak data
âœ… Custom model thoáº£i mÃ¡i

---

# âš ï¸ Háº N CHáº¾

âŒ Cáº§n GPU/RAM
âŒ Scale khÃ³ hÆ¡n cloud
âŒ Rerank yáº¿u náº¿u khÃ´ng thÃªm module

---

# ğŸš€ NÃ‚NG Cáº¤P CHO DOANH NGHIá»†P

Khi dÃ¹ng tháº­t:

âœ… GPU Server (A10/A100)
âœ… Redis Cache
âœ… Reranker local (bge-reranker)
âœ… RBAC
âœ… Audit Log

---

# ğŸ’ ARCHITECTURE PROD

```
User
 â†“
Gateway
 â†“
FastAPI
 â†“
Vector DB â†’ Ollama
 â†“
Answer
```
 **Cáº¤U HÃŒNH Tá»I THIá»‚U Ä‘á»ƒ cháº¡y Local RAG vá»›i Ollama (Mac & PC)**

DÃ¹ng tá»‘t cho: cÃ¡ nhÃ¢n, dev, team nhá», há»‡ thá»‘ng ná»™i bá»™.

---

# ğŸ§  LOCAL RAG = Ollama + Vector DB + Embedding

LLM local cháº¡y báº±ng **Ollama**
â†’ KhÃ´ng gá»­i dá»¯ liá»‡u ra ngoÃ i â†’ An toÃ n & tiáº¿t kiá»‡m chi phÃ­.

---

# ğŸ Cáº¤U HÃŒNH Tá»I THIá»‚U â€“ MAC (Apple Silicon & Intel)

Ãp dá»¥ng cho Mac cá»§a **Apple**

---

## âœ… Má»¨C 1: Tá»I THIá»‚U CHáº Y ÄÆ¯á»¢C (Há»c â€“ Test â€“ MVP)

| ThÃ nh pháº§n | YÃªu cáº§u            |
| ---------- | ------------------ |
| CPU        | M1 / M2 / Intel i5 |
| RAM        | **8 GB** (minimum) |
| á»” cá»©ng     | 20 GB trá»‘ng        |
| macOS      | 12+                |
| GPU        | KhÃ´ng báº¯t buá»™c     |

### ğŸ‘‰ Cháº¡y Ä‘Æ°á»£c model:

```
mistral:7b (quantized)
qwen2:3b
phi-3
```

âš ï¸ Tá»‘c Ä‘á»™: cháº­m â€“ trung bÃ¬nh

---

## âœ… Má»¨C 2: KHUYáº¾N NGHá»Š (DÃ¹ng Tháº­t)

| ThÃ nh pháº§n | YÃªu cáº§u             |
| ---------- | ------------------- |
| CPU        | M1 Pro / M2 / M3    |
| RAM        | **16 GB+**          |
| SSD        | 50 GB               |
| GPU        | Apple Neural Engine |

### ğŸ‘‰ Cháº¡y tá»‘t:

```
llama3:8b
qwen2:7b
mistral:7b
```

âš¡ Tá»‘c Ä‘á»™: mÆ°á»£t

ğŸ‘‰ ÄÃ¢y lÃ  má»©c â€œngon â€“ bá»n â€“ á»•n Ä‘á»‹nhâ€ nháº¥t cho dev.

---

## âœ… Má»¨C 3: CAO Cáº¤P (Heavy RAG)

| ThÃ nh pháº§n | YÃªu cáº§u            |
| ---------- | ------------------ |
| Chip       | M2 Pro / M3 Max    |
| RAM        | 32 GB+             |
| SSD        | 100 GB             |
| GPU        | Full Apple Silicon |

ğŸ‘‰ Cháº¡y Ä‘Æ°á»£c:

```
llama3:13b
mixtral
```

---

# ğŸ’» Cáº¤U HÃŒNH Tá»I THIá»‚U â€“ PC / WINDOWS / LINUX

Ãp dá»¥ng cho mÃ¡y dÃ¹ng **Microsoft** Windows / Linux PC

---

## âœ… Má»¨C 1: CPU-ONLY (Ráº» â€“ Phá»• thÃ´ng)

| ThÃ nh pháº§n | YÃªu cáº§u              |
| ---------- | -------------------- |
| CPU        | i5 Gen 9 / Ryzen 5   |
| RAM        | **16 GB** (báº¯t buá»™c) |
| SSD        | 50 GB                |
| GPU        | KhÃ´ng cáº§n            |

### ğŸ‘‰ Cháº¡y Ä‘Æ°á»£c:

```
mistral:7b
qwen2:3b
phi-3
```

âš ï¸ Cháº­m hÆ¡n Mac M1

ğŸ‘‰ Chá»‰ nÃªn dÃ¹ng Ä‘á»ƒ test.

---

## âœ… Má»¨C 2: GPU PHá»” THÃ”NG (KHUYáº¾N NGHá»Š)

| ThÃ nh pháº§n | YÃªu cáº§u              |
| ---------- | -------------------- |
| CPU        | i7 / Ryzen 7         |
| RAM        | 16â€“32 GB             |
| GPU        | RTX 3060 (12GB VRAM) |
| SSD        | NVMe 100 GB          |

### ğŸ‘‰ Cháº¡y mÆ°á»£t:

```
llama3:8b
qwen2:7b
mistral
```

âš¡ Ráº¥t á»•n cho production nhá».

---

## âœ… Má»¨C 3: GPU Máº NH (ENTERPRISE)

| ThÃ nh pháº§n | YÃªu cáº§u         |
| ---------- | --------------- |
| CPU        | Xeon / Ryzen 9  |
| RAM        | 64 GB           |
| GPU        | RTX 4090 / A100 |
| SSD        | 1TB NVMe        |

ğŸ‘‰ Cháº¡y:

```
llama3:70b
mixtral
```

(Chá»‰ cáº§n khi lÃ m doanh nghiá»‡p lá»›n)

---

# ğŸ“Š SO SÃNH NHANH MAC vs PC

| TiÃªu chÃ­ | Mac M1/M2  | PC RTX 3060 |
| -------- | ---------- | ----------- |
| Setup    | â­â­â­â­â­      | â­â­â­         |
| á»”n Ä‘á»‹nh  | â­â­â­â­â­      | â­â­â­â­        |
| GiÃ¡      | Cao        | Tá»‘t         |
| Scale    | Trung bÃ¬nh | Ráº¥t tá»‘t     |
| GPU      | Integrated | Dedicated   |

ğŸ‘‰ CÃ¡ nhÃ¢n/dev â†’ Mac
ğŸ‘‰ Server/team â†’ PC GPU

---

# ğŸ§© Cáº¤U HÃŒNH RAG Tá»I THIá»‚U (SOFTWARE)

DÃ¹ Mac hay PC, báº¡n cáº§n tá»‘i thiá»ƒu:

### âœ… Pháº§n má»m

```
Ollama
Python 3.10+
Qdrant (Docker)
FastAPI
```

### âœ… RAM PhÃ¢n bá»• tá»‘i thiá»ƒu

| ThÃ nh pháº§n | RAM    |
| ---------- | ------ |
| Ollama     | 6â€“10GB |
| Embedding  | 2GB    |
| Qdrant     | 1GB    |
| API        | 1GB    |

â¡ï¸ Tá»•ng: ~10â€“14GB

ğŸ‘‰ VÃ¬ váº­y 8GB RAM chá»‰ Ä‘á»§ test.

---

# ğŸš€ Cáº¤U HÃŒNH â€œQUá»C DÃ‚Nâ€ KHUYáº¾N NGHá»Š 2026

Náº¿u báº¡n mua mÃ¡y má»›i:

### ğŸ† MAC BEST CHOICE

```
Mac M2 / M3
16GB RAM
512GB SSD
```

### ğŸ† PC BEST CHOICE

```
Ryzen 7
32GB RAM
RTX 3060 12GB
1TB NVMe
```

â†’ Cháº¡y RAG mÆ°á»£t 3â€“5 nÄƒm.

---

# âš ï¸ Lá»–I THÆ¯á»œNG Gáº¶P DO Cáº¤U HÃŒNH Yáº¾U

| Lá»—i            | NguyÃªn nhÃ¢n |
| -------------- | ----------- |
| Treo mÃ¡y       | Thiáº¿u RAM   |
| Load model lÃ¢u | SSD cháº­m    |
| Tráº£ lá»i cháº­m   | KhÃ´ng GPU   |
| Crash          | VRAM thiáº¿u  |

ğŸ‘‰ 80% lá»—i = thiáº¿u RAM.

---

# ğŸ’ Gá»¢I Ã MODEL THEO Cáº¤U HÃŒNH

| RAM   | Model      |
| ----- | ---------- |
| 8GB   | qwen2:3b   |
| 16GB  | llama3:8b  |
| 32GB  | llama3:13b |
| 64GB+ | llama3:70b |

# Lecture 4: LLM Training - Pre-training üèãÔ∏è

> **T√≥m t·∫Øt t·ª´ kh√≥a h·ªçc Stanford CME 295: Transformers & Large Language Models.**
> B√†i gi·∫£ng n√†y t·∫≠p trung v√†o quy tr√¨nh hu·∫•n luy·ªán LLM, t·ª´ Pre-training (Ti·ªÅn hu·∫•n luy·ªán) ƒë·∫øn c√°c k·ªπ thu·∫≠t t·ªëi ∆∞u h√≥a ph·∫ßn c·ª©ng (Parallelism, FlashAttention).

---

## üìö M·ª•c L·ª•c
1. [Quy tr√¨nh hu·∫•n luy·ªán LLM](#1-quy-tr√¨nh-hu·∫•n-luy·ªán-llm)
2. [Pre-training (Ti·ªÅn hu·∫•n luy·ªán)](#2-pre-training-ti·ªÅn-hu·∫•n-luy·ªán)
3. [Lu·∫≠t m·ªü r·ªông (Scaling Laws)](#3-lu·∫≠t-m·ªü-r·ªông-scaling-laws)
4. [T·ªëi ∆∞u h√≥a ph·∫ßn c·ª©ng & B·ªô nh·ªõ](#4-t·ªëi-∆∞u-h√≥a-ph·∫ßn-c·ª©ng--b·ªô-nh·ªõ)
5. [FlashAttention ‚ö°](#5-flashattention-‚ö°)
6. [Quantization & Mixed Precision Training](#6-quantization--mixed-precision-training)

---

## 1. Quy tr√¨nh hu·∫•n luy·ªán LLM
Hu·∫•n luy·ªán LLM kh√¥ng ph·∫£i l√† m·ªôt b∆∞·ªõc duy nh·∫•t m√† l√† m·ªôt quy tr√¨nh nhi·ªÅu giai ƒëo·∫°n (Multi-stage training):

1.  **Pre-training (Ti·ªÅn hu·∫•n luy·ªán):** H·ªçc ki·∫øn th·ª©c t·ªïng qu√°t t·ª´ d·ªØ li·ªáu kh·ªïng l·ªì (Internet, s√°ch, code).
    *   *M·ª•c ti√™u:* D·ª± ƒëo√°n t·ª´ ti·∫øp theo (Next token prediction).
    *   *K·∫øt qu·∫£:* Base model (M√¥ h√¨nh n·ªÅn t·∫£ng) - bi·∫øt nhi·ªÅu nh∆∞ng ch∆∞a bi·∫øt l√†m tr·ª£ l√Ω.
2.  **Fine-tuning (Tinh ch·ªânh):**
    *   *SFT (Supervised Fine-Tuning):* D·∫°y m√¥ h√¨nh l√†m theo h∆∞·ªõng d·∫´n (Instruction Following).
    *   *RLHF/RLAIF:* CƒÉn ch·ªânh m√¥ h√¨nh theo s·ªü th√≠ch con ng∆∞·ªùi (Alignment).

---

## 2. Pre-training (Ti·ªÅn hu·∫•n luy·ªán)
ƒê√¢y l√† giai ƒëo·∫°n t·ªën k√©m nh·∫•t (h√†ng tri·ªáu USD).

*   **D·ªØ li·ªáu (Data):** Common Crawl (Internet), Wikipedia, Reddit, GitHub (Code).
    *   *Quy m√¥:* H√†ng ngh√¨n t·ª∑ tokens (Trillions of tokens).
    *   *V√≠ d·ª•:* Llama 3 ƒë∆∞·ª£c train tr√™n 15T tokens.
*   **Th√°ch th·ª©c:**
    *   **Cost (Chi ph√≠):** R·∫•t l·ªõn v·ªÅ ti·ªÅn b·∫°c v√† nƒÉng l∆∞·ª£ng.
    *   **Knowledge Cutoff (Gi·ªõi h·∫°n ki·∫øn th·ª©c):** M√¥ h√¨nh ch·ªâ bi·∫øt nh·ªØng g√¨ x·∫£y ra tr∆∞·ªõc th·ªùi ƒëi·ªÉm thu th·∫≠p d·ªØ li·ªáu.
    *   **Hallucination (·∫¢o gi√°c):** C√≥ th·ªÉ b·ªãa ƒë·∫∑t th√¥ng tin.

---

## 3. Lu·∫≠t m·ªü r·ªông (Scaling Laws)
L√†m sao ƒë·ªÉ m√¥ h√¨nh th√¥ng minh h∆°n?

*   **Kaplan et al. (2020):** Hi·ªáu nƒÉng m√¥ h√¨nh tƒÉng theo h√†m m≈© (power law) khi tƒÉng:
    1.  K√≠ch th∆∞·ªõc m√¥ h√¨nh (Parameters).
    2.  K√≠ch th∆∞·ªõc d·ªØ li·ªáu (Dataset size).
    3.  L∆∞·ª£ng t√≠nh to√°n (Compute).
*   **Chinchilla Scaling Law (Hoffmann et al., 2022):**
    *   ƒê·ªÉ t·ªëi ∆∞u h√≥a chi ph√≠ t√≠nh to√°n, khi tƒÉng g·∫•p ƒë√¥i k√≠ch th∆∞·ªõc m√¥ h√¨nh, c·∫ßn tƒÉng g·∫•p ƒë√¥i l∆∞·ª£ng d·ªØ li·ªáu.
    *   **C√¥ng th·ª©c v√†ng:** S·ªë l∆∞·ª£ng tokens hu·∫•n luy·ªán n√™n g·∫•p kho·∫£ng **20 l·∫ßn** s·ªë l∆∞·ª£ng tham s·ªë m√¥ h√¨nh.
    *   *H·ªá qu·∫£:* Nhi·ªÅu m√¥ h√¨nh tr∆∞·ªõc ƒë√≥ (nh∆∞ GPT-3) l√† "under-trained" (hu·∫•n luy·ªán ch∆∞a ƒë·ªß). C√°c m√¥ h√¨nh hi·ªán ƒë·∫°i (Llama) c√≥ xu h∆∞·ªõng nh·ªè h∆°n nh∆∞ng train l√¢u h∆°n (tr√™n nhi·ªÅu d·ªØ li·ªáu h∆°n).

---

## 4. T·ªëi ∆∞u h√≥a ph·∫ßn c·ª©ng & B·ªô nh·ªõ
Hu·∫•n luy·ªán LLM ƒë√≤i h·ªèi b·ªô nh·ªõ VRAM kh·ªïng l·ªì, v∆∞·ª£t xa kh·∫£ nƒÉng c·ªßa m·ªôt GPU ƒë∆°n l·∫ª (v√≠ d·ª• H100 80GB).

### C√°c k·ªπ thu·∫≠t song song h√≥a (Parallelism):
1.  **Data Parallelism (DP):**
    *   Copy m√¥ h√¨nh ra nhi·ªÅu GPU.
    *   M·ªói GPU x·ª≠ l√Ω m·ªôt ph·∫ßn d·ªØ li·ªáu (batch) kh√°c nhau.
    *   ƒê·ªìng b·ªô h√≥a gradient sau m·ªói b∆∞·ªõc.
2.  **Model Parallelism (MP):**
    *   **Tensor Parallelism (TP):** Chia nh·ªè c√°c ma tr·∫≠n tr·ªçng s·ªë (weight matrices) ƒë·ªÉ t√≠nh to√°n song song tr√™n nhi·ªÅu GPU.
    *   **Pipeline Parallelism (PP):** Chia c√°c l·ªõp (layers) c·ªßa m√¥ h√¨nh cho c√°c GPU kh√°c nhau (GPU 1 l√†m l·ªõp 1-10, GPU 2 l√†m l·ªõp 11-20...).
3.  **ZeRO (Zero Redundancy Optimizer):**
    *   T·ªëi ∆∞u h√≥a Data Parallelism b·∫±ng c√°ch chia nh·ªè Optimizer States, Gradients, v√† Parameters ra c√°c GPU thay v√¨ m·ªói GPU ph·∫£i gi·ªØ m·ªôt b·∫£n copy ƒë·∫ßy ƒë·ªß.

---

## 5. FlashAttention ‚ö°
**V·∫•n ƒë·ªÅ:** Attention ti√™u chu·∫©n c√≥ ƒë·ªô ph·ª©c t·∫°p $O(N^2)$ v√† t·ªën r·∫•t nhi·ªÅu thao t√°c ƒë·ªçc/ghi b·ªô nh·ªõ (Memory IO) gi·ªØa HBM (b·ªô nh·ªõ ch·∫≠m, l·ªõn) v√† SRAM (b·ªô nh·ªõ nhanh, nh·ªè).

**Gi·∫£i ph√°p (FlashAttention - Dao et al., 2022):**
*   **Tiling:** Chia ma tr·∫≠n Attention th√†nh c√°c kh·ªëi nh·ªè (tiles) ƒë·ªÉ t√≠nh to√°n ho√†n to√†n trong SRAM t·ªëc ƒë·ªô cao.
*   **Recomputation:** Ch·∫•p nh·∫≠n t√≠nh to√°n l·∫°i m·ªôt s·ªë gi√° tr·ªã trong qu√° tr√¨nh backward pass thay v√¨ l∆∞u tr·ªØ t·∫•t c·∫£ (ti·∫øt ki·ªám VRAM, gi·∫£m IO).
*   **K·∫øt qu·∫£:** TƒÉng t·ªëc ƒë·ªô hu·∫•n luy·ªán 2-4 l·∫ßn, gi·∫£m b·ªô nh·ªõ s·ª≠ d·ª•ng 10-20 l·∫ßn, cho ph√©p train v·ªõi context length d√†i h∆°n.

---

## 6. Quantization & Mixed Precision Training
Gi·∫£m ƒë·ªô ch√≠nh x√°c c·ªßa s·ªë h·ªçc ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ v√† tƒÉng t·ªëc ƒë·ªô.

*   **FP32 (Full Precision - 32 bit):** Chu·∫©n c≈©, r·∫•t t·ªën k√©m.
*   **FP16 / BF16 (Half Precision - 16 bit):** Ti√™u chu·∫©n hi·ªán nay cho training.
    *   **BF16 (Brain Float 16):** Gi·ªØ nguy√™n d·∫£i gi√° tr·ªã (range) c·ªßa FP32 nh∆∞ng gi·∫£m ƒë·ªô ch√≠nh x√°c ph·∫ßn th·∫≠p ph√¢n (mantissa). ·ªîn ƒë·ªãnh h∆°n FP16 cho training LLM.
*   **Mixed Precision Training:**
    *   L∆∞u tr·ªØ tr·ªçng s·ªë ch√≠nh (Master weights) ·ªü FP32.
    *   T√≠nh to√°n Forward/Backward ·ªü FP16/BF16.
    *   C·∫≠p nh·∫≠t tr·ªçng s·ªë ·ªü FP32.

---
*Bi√™n so·∫°n b·ªüi Pixiboss - D·ª±a tr√™n Stanford CME 295.*


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [15 Editing hidden states](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ thÃ¡ch Láº­p trÃ¬nh: Thay Ä‘á»•i Quy mÃ´ Hidden State vÃ  Tá»•n tháº¥t Token

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y trÃ¬nh bÃ y káº¿t quáº£ thá»­ thÃ¡ch láº­p trÃ¬nh vá» tÃ¡c Ä‘á»™ng cá»§a viá»‡c can thiá»‡p Hidden State Ä‘á»‘i vá»›i Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh (Token selection) vÃ  giÃ¡ trá»‹ Log Softmax. Sá»­ dá»¥ng mÃ´ hÃ¬nh GPT-2 Medium, nghiÃªn cá»©u thá»±c hiá»‡n cÃ¡c phÃ©p thay Ä‘á»•i quy mÃ´ nÄƒng Ä‘á»™ng thÃ´ng qua Dictionary-based Hooks. ThÃ­ nghiá»‡m láº¥y danh ngÃ´n cá»§a Einstein lÃ m máº«u thá»­ Ä‘á»ƒ quan sÃ¡t sá»± biáº¿n thiÃªn cá»§a Logits vÃ  Loss khi má»™t lá»›p cá»¥ thá»ƒ bá»‹ suy giáº£m tÃ­n hiá»‡u. Káº¿t quáº£ gÃ¢y ngáº¡c nhiÃªn cho tháº¥y viá»‡c giáº£m quy mÃ´ (Scale 0.6) táº¡i má»™t sá»‘ táº§ng cÃ³ thá»ƒ lÃ m "sáº¯c bÃ©n" phÃ¢n phá»‘i xÃ¡c suáº¥t, dáº«n Ä‘áº¿n viá»‡c giáº£m Loss cho token má»¥c tiÃªu, tÆ°Æ¡ng tá»± nhÆ° hiá»‡u á»©ng giáº£m nhiá»‡t Ä‘á»™ (Temperature) trong hÃ m Softmax.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Má»¥c tiÃªu cá»‘t lÃµi cá»§a Diá»…n giáº£i há»c (Interpretability) khÃ´ng chá»‰ dá»«ng láº¡i á»Ÿ viá»‡c quan sÃ¡t cÃ¡c vi máº¡ch ná»™i táº¡i mÃ  pháº£i káº¿t ná»‘i Ä‘Æ°á»£c cÃ¡c biáº¿n Ä‘á»™ng Ä‘Ã³ vá»›i hÃ nh vi Ä‘áº§u ra thá»±c táº¿ cá»§a mÃ´ hÃ¬nh (sinh tá»«). Thá»­ thÃ¡ch nÃ y táº­p trung vÃ o viá»‡c Ä‘á»‹nh lÆ°á»£ng sá»± thay Ä‘á»•i cá»§a Logits toÃ n vocab khi ta "bÃ³p" tÃ­n hiá»‡u táº¡i má»™t Transformer Block báº¥t ká»³. ChÃºng ta sáº½ kiá»ƒm chá»©ng liá»‡u mÃ´ hÃ¬nh cÃ³ cÃ²n giá»¯ Ä‘Æ°á»£c kháº£ nÄƒng dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c token káº¿ tiáº¿p sau khi bá»‹ can thiá»‡p nhÃ¢n quáº£ hay khÃ´ng.

---

## 2. Tiáº¿t Thiáº¿t Láº­p Thá»­ ThÃ¡ch (Methodology)

### 2.1. Cáº¥u trÃºc Dict-based Hook Linh hoáº¡t
Thay vÃ¬ hard-code má»™t layer duy nháº¥t, chÃºng ta xÃ¢y dá»±ng há»‡ thá»‘ng Hook tham chiáº¿u Ä‘áº¿n má»™t `scaling_dict`.
- **CÆ¡ cháº¿:** `if layer_num in scaling_dict.keys(): output = output[0] * scaling_dict[layer_num]`.
- **Lá»£i Ã­ch:** Cho phÃ©p kiá»ƒm thá»­ Ä‘Æ¡n láº» hoáº·c Ä‘á»“ng thá»i nhiá»u lá»›p vá»›i cÃ¡c há»‡ sá»‘ scale khÃ¡c nhau chá»‰ báº±ng cÃ¡ch cáº­p nháº­t Dictionary mÃ  khÃ´ng cáº§n gá»¡/cÃ i láº¡i Hook.

### 2.2. Dá»¯ liá»‡u Thá»­ nghiá»‡m vÃ  Baseline
- **Prompt:** "I have no special talents. I am only passionately" (TrÃ­ch Einstein).
- **Target Token:** " curious" (Token ID: 11040).
- **Baseline:** Cháº¡y mÃ´ hÃ¬nh á»Ÿ tráº¡ng thÃ¡i nguyÃªn báº£n (`pure_logits`) Ä‘á»ƒ lÃ m má»‘c Ä‘á»‘i chá»©ng cho xÃ¡c suáº¥t vÃ  Loss.

---

## 3. Káº¿t Quáº£ Thá»±c Nghiá»‡m (Results & Analysis)

### 3.1. Sá»± TÄ©nh Láº·ng ToÃ n Cá»¥c (Global Suppression)
Khi scale Layer 2 vá»›i há»‡ sá»‘ 0.6, Ä‘á»“ thá»‹ Logits cho tháº¥y má»™t sá»± sá»¥t giáº£m biÃªn Ä‘á»™ Ä‘á»“ng loáº¡t (Global downward shift) trÃªn toÃ n bá»™ dáº£i tá»« vá»±ng. Máº·c dÃ¹ cÆ°á»ng Ä‘á»™ tÃ­n hiá»‡u giáº£m máº¡nh, má»‘i tÆ°Æ¡ng quan (Correlation) giá»¯a Logits sáº¡ch vÃ  Logits bá»‹ can thiá»‡p váº«n duy trÃ¬ á»Ÿ má»©c cá»±c cao ($r \approx 0.995$). Äiá»u nÃ y chá»©ng tá» cáº¥u trÃºc tÆ°Æ¡ng Ä‘á»‘i giá»¯a cÃ¡c tá»« váº«n Ä‘Æ°á»£c báº£o toÃ n.

### 3.2. Nghá»‹ch lÃ½ Giáº£m Loss (The Loss Paradox)
Má»™t phÃ¡t hiá»‡n thÃº vá»‹ lÃ  khi scale lá»›p sá»›m, vá»‹ trÃ­ cá»§a token " curious" trong danh sÃ¡ch Top-10 dá»± Ä‘oÃ¡n láº¡i tÄƒng lÃªn so vá»›i mÃ´ hÃ¬nh gá»‘c. 
- **Giáº£i thÃ­ch:** Viá»‡c giáº£m quy mÃ´ Hidden State tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i viá»‡c "lÃ m láº¡nh" (decreasing temperature) há»‡ thá»‘ng. NÃ³ giÃºp loáº¡i bá» bá»›t cÃ¡c nhiá»…u ná»n vÃ  lÃ m cho phÃ¢n phá»‘i xÃ¡c suáº¥t táº­p trung hÆ¡n vÃ o cÃ¡c á»©ng viÃªn hÃ ng Ä‘áº§u. Trong trÆ°á»ng há»£p nÃ y, sá»± can thiá»‡p nhÃ¢n quáº£ vÃ´ tÃ¬nh láº¡i mang láº¡i káº¿t quáº£ "tá»‘t hÆ¡n" vá» máº·t toÃ¡n há»c (Loss tháº¥p hÆ¡n).

### 3.3. QuÃ©t ToÃ n Bá»™ CÃ¡c Lá»›p (Layer Sweep)
Thá»±c hiá»‡n láº·p qua 24 lá»›p cá»§a GPT-2 Medium:
- **TÃ­nh á»•n Ä‘á»‹nh:** Háº§u háº¿t cÃ¡c lá»›p khi bá»‹ scale 0.6 Ä‘á»u dáº«n Ä‘áº¿n viá»‡c giáº£m Loss cho token má»¥c tiÃªu.
- **Xu hÆ°á»›ng:** Loss cÃ³ xu hÆ°á»›ng tÄƒng dáº§n (mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n kÃ©m Ä‘i) khi can thiá»‡p xáº£y ra á»Ÿ cÃ¡c lá»›p cÃ ng sÃ¢u vá» phÃ­a cuá»‘i. Äiá»u nÃ y cá»§ng cá»‘ giáº£ thuyáº¿t ráº±ng cÃ¡c lá»›p cuá»‘i cÃ¹ng Ä‘Ã³ng vai trÃ² quyáº¿t Ä‘á»‹nh trá»±c tiáº¿p hÆ¡n Ä‘áº¿n viá»‡c tinh chá»‰nh xÃ¡c suáº¥t Ä‘áº§u ra.

---

## 4. Káº¿t Luáº­n
Can thiá»‡p nhÃ¢n quáº£ báº±ng cÃ¡ch thay Ä‘á»•i quy mÃ´ Hidden State tiáº¿t lá»™ ráº±ng mÃ´ hÃ¬nh cÃ³ tÃ­nh á»•n Ä‘á»‹nh cao vá» máº·t cáº¥u trÃºc tÆ°Æ¡ng quan Logits. Tuy nhiÃªn, cÆ°á»ng Ä‘á»™ tÃ­n hiá»‡u cÃ³ áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n Ä‘á»™ "sáº¯c" cá»§a softmax. Viá»‡c giáº£m nÄƒng lÆ°á»£ng tÃ­n hiá»‡u (Scaling down) cÃ³ thá»ƒ lÃ m giáº£m tÃ­nh ngáº«u nhiÃªn (Stochasticity) cá»§a mÃ´ hÃ¬nh. BÃ i há»c rÃºt ra lÃ : khi nghiÃªn cá»©u ná»™i Ä‘á»™ng lá»±c cá»§a mÃ´ hÃ¬nh, luÃ´n cáº§n liÃªn káº¿t chÃºng vá»›i lá»±a chá»n Token cuá»‘i cÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ tÃ¡c Ä‘á»™ng thá»±c tiá»…n.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. Thá»­ thÃ¡ch Hidden-state scaling trÃªn GPT-2 Medium dá»±a trÃªn tÃ i liá»‡u `aero_LLM_02_CodeChallenge Hidden-state scaling and token loss.md`. PhÃ¢n tÃ­ch sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a Scaling vÃ  Softmax Temperature.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [TÃ¡c Ä‘á»™ng Háº¡ nguá»“n cá»§a viá»‡c Thay Ä‘á»•i Quy mÃ´ Lá»›p sá»›m (Downstream Impact of Early Layer Scaling)](aero_LLM_01_Downstream impact of early layer scaling.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_Downstream impact of early layer scaling.md) |
| ğŸ“Œ **[Thá»­ thÃ¡ch Láº­p trÃ¬nh: Thay Ä‘á»•i Quy mÃ´ Hidden State vÃ  Tá»•n tháº¥t Token](aero_LLM_02_CodeChallenge Hidden-state scaling and token loss.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_02_CodeChallenge Hidden-state scaling and token loss.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Dá»± Ä‘oÃ¡n BERT vá»›i Nhiá»…u vÃ  HoÃ¡n vá»‹ (Noisy and Shuffled BERT Predictions)](aero_LLM_03_CodeChallenge Noisy and shuffled BERT predictions.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Noisy and shuffled BERT predictions.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Äo lÆ°á»ng vÃ  Hiá»‡u chá»‰nh Äá»‹nh kiáº¿n Giá»›i trong BERT](aero_LLM_04_CodeChallenge Measure and correct BERT's bias.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Measure and correct BERT's bias.md) |
| [VÃ¡ Hoáº¡t hÃ³a vÃ  TÃ¡c vá»¥ Nháº­n diá»‡n TÃ¢n ngá»¯ GiÃ¡n tiáº¿p (Activation Patching and Indirect Object Identification)](aero_LLM_05_Activation patching with indirect object identification.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_Activation patching with indirect object identification.md) |
| [Bá» qua má»™t Táº§ng Transformer (Skip a Layer)](aero_LLM_06_Skip a layer.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_Skip a layer.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

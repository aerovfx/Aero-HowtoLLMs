WEBVTT

00:02.000 --> 00:09.600
I hope that from the previous few videos, you now have a pretty solid intuition for how gradient descent

00:09.600 --> 00:17.200
works, why gradient descent works, and a geometric picture of it working in one dimensional functions.

00:17.200 --> 00:19.960
So functions that can be graphed as a line.

00:20.640 --> 00:25.160
In this video we are going to double the dimensionality of gradient descent.

00:25.160 --> 00:29.560
We're going to go from one dimensional functions to two dimensional functions.

00:29.680 --> 00:33.040
And you will see that gradient descent is the same.

00:33.040 --> 00:36.120
The concept is the same, the algorithm is the same.

00:36.280 --> 00:42.120
The even the code is basically the same in two dimensions as it is in one dimension.

00:42.360 --> 00:49.400
We're also going to use this as an opportunity to continue exploring the possible difficulties of gradient

00:49.400 --> 00:52.000
descent for finding global minima.

00:52.600 --> 00:56.520
Okay, but first I want to start with a little bit of terminology.

00:56.560 --> 01:02.100
This is actually some terminology that I have mentioned before about the derivative and the gradient.

01:02.100 --> 01:05.220
But here I'm going to make it a little bit more explicit.

01:06.020 --> 01:11.580
So you know that if we have a function that looks something like this, we can compute the derivative

01:11.700 --> 01:15.300
which is the slope of the function at any given point.

01:15.500 --> 01:18.060
So this thing here is called the derivative.

01:18.540 --> 01:26.220
Now when we move to two dimensional functions there is a derivative along each dimension or each axis.

01:26.220 --> 01:29.580
So there's a derivative with respect to this dimension.

01:29.580 --> 01:31.660
Maybe we call this dimension x.

01:32.100 --> 01:37.460
And there is another derivative of this function at each point with respect to this dimension.

01:37.460 --> 01:38.660
Maybe we'll call this y.

01:39.300 --> 01:44.420
So the at any given point we can compute what's called the partial derivative.

01:44.420 --> 01:48.140
So the partial derivatives are the derivatives of the function.

01:48.540 --> 01:52.820
Ignoring one dimension and only focusing on the other dimension.

01:52.820 --> 01:59.680
So the partial derivative with respect to x means that we ignore y and we just see how is the function

01:59.680 --> 02:01.960
changing over x.

02:02.320 --> 02:03.760
And similar for y.

02:04.080 --> 02:05.560
So we have two dimensions.

02:05.560 --> 02:08.000
So that means we have two partial derivatives.

02:08.080 --> 02:12.480
In this case you know you could call this a partial derivative with respect to x.

02:12.480 --> 02:15.840
But there's no other variable to be partial to.

02:15.880 --> 02:17.600
So we just call this the derivative.

02:18.240 --> 02:18.480
Okay.

02:18.520 --> 02:19.840
So we have two dimensions.

02:19.840 --> 02:21.880
So that means we have two partial derivatives.

02:21.880 --> 02:26.800
And then what we can do is take both of those partial derivatives.

02:26.800 --> 02:30.320
Take those two partial derivatives and just put them in a list.

02:30.360 --> 02:35.640
It would just be a two element vector, an array of partial derivatives.

02:35.640 --> 02:38.200
And that is what we call the gradient.

02:38.400 --> 02:45.080
So the gradient of the function at any given point is just a collection of all the partial derivatives

02:45.080 --> 02:48.680
with respect to all the dimensions of that function.

02:49.080 --> 02:53.960
So again that means that gradient is basically the same thing as a derivative.

02:53.960 --> 02:58.940
It's just involving a little bit more, you know, more numbers because we have higher dimensionality.

02:59.540 --> 02:59.980
Okay.

03:00.020 --> 03:02.940
A little bit more about notations and terminology.

03:03.260 --> 03:04.380
Here we have the derivative.

03:04.420 --> 03:07.020
A couple of different ways of indicating the derivative.

03:07.220 --> 03:11.860
And all three of these notations you've already seen before.

03:12.100 --> 03:17.700
Now you know there are historical interesting reasons for these different notations to exist.

03:17.860 --> 03:23.620
They come from the different mathematicians who developed the theory of calculus, but that is way beyond

03:23.620 --> 03:26.340
the scope of what you need to know for this course.

03:26.700 --> 03:32.020
Partial derivatives are indicated by this kind of funny looking curvy D symbol.

03:32.020 --> 03:33.100
This is called a del.

03:34.260 --> 03:36.460
Again, the notation doesn't really matter.

03:36.460 --> 03:38.140
I don't want to get hung up on that.

03:38.140 --> 03:41.020
This one is important though, because this you will see often.

03:41.140 --> 03:44.980
So the gradient is indicated by this upside down triangle.

03:45.140 --> 03:48.180
It's called a nabla nabla.

03:48.220 --> 03:49.220
So it's nabla.

03:49.660 --> 03:56.260
And it's just a collection of all the partial derivatives with respect to all of the different directions

03:56.260 --> 03:58.440
or dimensions of the data.

03:58.960 --> 03:59.280
Okay.

03:59.320 --> 04:06.560
So with that notation and terminology aside, now let's get back to this crazy looking function.

04:06.560 --> 04:11.640
This is the function that we are going to be working with in this video.

04:12.240 --> 04:13.640
It's a pretty long involved function.

04:13.640 --> 04:16.400
You can see it's a function of two variables.

04:16.400 --> 04:19.680
So this is a two dimensional function x and y.

04:20.360 --> 04:27.600
Now we can plot the value of this function at every location x and y every coordinate x and y.

04:27.880 --> 04:29.720
And it's going to look something like this.

04:29.760 --> 04:31.400
Pretty interesting looking function.

04:31.960 --> 04:34.480
The yellow colors correspond to the peaks.

04:34.640 --> 04:37.400
Blue colors correspond to the valleys.

04:37.600 --> 04:45.360
Now what we are going to do is use gradient descent to find the minimum of this function.

04:45.520 --> 04:47.040
So the global minimum is.

04:47.040 --> 04:51.680
Here you can see we also have two local minima one here.

04:51.680 --> 04:54.560
And this is an even lighter local minimum here.

04:54.880 --> 04:55.720
And these are the peaks.

04:55.720 --> 04:57.740
So this would be the global maximum.

04:57.740 --> 04:59.100
And to local maxima.

04:59.100 --> 05:01.860
And out here this greenish color this is basically zero.

05:02.060 --> 05:06.900
It's essentially just a plane going out to infinity in all directions.

05:07.340 --> 05:07.700
Okay.

05:07.740 --> 05:15.140
So we will see that, you know if you start here, if you start the algorithm here, the initial guess,

05:15.140 --> 05:20.180
then gradient descent is going to do a great job and and get us to the true local minimum.

05:20.540 --> 05:25.540
If we start up here, you know this will be an interesting case to start in the the peak up here.

05:25.660 --> 05:31.100
In fact, you know, we we have the benefit of looking at this entire function from the top.

05:31.140 --> 05:32.660
You know, we have the bird's eye view.

05:33.140 --> 05:37.700
We know where the, the, the local, the global minimum is.

05:37.900 --> 05:40.140
But gradient descent doesn't actually know that.

05:40.140 --> 05:45.380
So gradient descent if we start here gradient descent will actually go out here to the plane.

05:45.420 --> 05:50.340
Now that's the correct thing to do from the perspective of gradient descent.

05:50.380 --> 05:57.880
Gradient descent is doing is implementing the correct Algorithm, but if it has an unfortunate starting

05:57.880 --> 06:01.680
point, it's not going to end up with a good solution.

06:01.680 --> 06:05.440
That's something we will explore in the code in a little bit.

06:05.880 --> 06:08.000
Okay, so let me make this a little more concrete.

06:08.680 --> 06:10.880
Here's what we are going to do in a moment.

06:10.880 --> 06:13.280
We're going to compute the gradient of the function.

06:13.280 --> 06:17.000
We're going to compute the partial derivatives using SymPy.

06:17.800 --> 06:21.920
And then we are going to repeat the gradient descent algorithm.

06:21.920 --> 06:22.600
That loop.

06:22.600 --> 06:26.120
It's exactly the same as in the previous couple of videos.

06:26.600 --> 06:28.920
And there's a couple of minor adjustments.

06:28.920 --> 06:34.720
Like now the local minimum is a two element list or a tuple or an array.

06:34.720 --> 06:38.440
I forget exactly what data type I used here, but we will soon discover.

06:38.440 --> 06:44.360
But anyway, it's two numbers for the local minimum x and y, not just one value x.

06:44.520 --> 06:46.000
And then we're going to visualize.

06:46.120 --> 06:48.960
And we're going to produce a graph that looks something like this.

06:49.280 --> 06:51.720
This is a random starting location.

06:51.720 --> 06:54.280
In this case it happened to start over here.

06:54.580 --> 06:57.660
And this is the trajectory that gradient descent took.

06:57.700 --> 06:59.620
And in this case it worked out super well.

06:59.620 --> 07:01.660
We got exactly the right result.

07:01.980 --> 07:02.300
All right.

07:02.300 --> 07:07.340
So now we switch to Python and we will end up with a graph that looks something like this.

07:08.900 --> 07:12.780
So we begin by importing the libraries that we need.

07:13.620 --> 07:20.700
Here is a function that defines the large function that that I showed you the formula for this.

07:20.700 --> 07:22.260
The name of this function is peaks.

07:22.260 --> 07:24.180
It's often used in Matlab.

07:24.180 --> 07:27.260
In fact it's part of the Matlab logo.

07:27.260 --> 07:28.700
Is this function.

07:28.980 --> 07:31.500
Now I haven't computed the derivative here.

07:31.540 --> 07:38.340
Remember in the previous videos we I had two functions here one to define the function and another Python

07:38.340 --> 07:42.580
function to define the derivative of the mathematical function.

07:42.900 --> 07:47.860
Here I'm going to let sympy compute the derivative for me because this is a small nightmare.

07:47.900 --> 07:54.030
If you, you know, if you're if you like suffering in calculus, then go for it and compute the derivative

07:54.030 --> 07:54.790
of this by hand.

07:54.830 --> 07:57.270
I'm going to let SymPy do the hard work for me.

07:57.910 --> 07:58.230
Okay.

07:58.270 --> 08:01.870
So let's look at this function first.

08:01.990 --> 08:08.350
So here I create a a grid of coordinates x and y go from minus three to plus three.

08:08.550 --> 08:10.630
I'm going to define this function.

08:10.630 --> 08:11.830
Create this function.

08:11.830 --> 08:15.710
And we use IMHO to have a look at this function.

08:16.070 --> 08:21.670
Now this is nothing new because you just saw exactly what this function looks like in the slides a moment

08:21.670 --> 08:22.110
ago.

08:22.670 --> 08:22.870
Okay.

08:22.910 --> 08:28.110
So here we create the the partial derivatives using SymPy.

08:29.030 --> 08:35.910
So here I had to recreate this function because up here this is a function in numpy.

08:36.150 --> 08:42.910
And here we need a SymPy function because these are symbolic variables x and y.

08:43.710 --> 08:49.870
Now because this is a little bit more complicated, I had to go through a little bit more SymPy code

08:50.070 --> 08:53.890
in order to get the function that creates the derivative.

08:54.210 --> 08:56.890
So you still recognize this simdiff.

08:57.170 --> 09:02.810
This is the function we used in previous videos to get the to compute the derivative.

09:02.850 --> 09:07.010
You saw that in the the videos about derivatives in the previous section.

09:07.490 --> 09:13.010
Here we have to specify which variable we want to compute the derivative with respect to.

09:13.370 --> 09:16.330
So this is the derivative of z.

09:16.370 --> 09:21.450
This symbolic function z with respect to x and with respect to y.

09:22.010 --> 09:27.770
Now from here this is actually going to give me a symbolic object of the derivative.

09:27.930 --> 09:34.970
I want to convert that symbolic function into a numpy function that I can then call.

09:35.010 --> 09:37.690
And that's what lambdify does.

09:37.690 --> 09:45.810
So sim dot lambdify will transform a SymPy object into a function that we can call, just like just

09:45.810 --> 09:48.050
like this function up here.

09:48.730 --> 09:53.270
Now, if everything I just said is weird and confusing and incomprehensible.

09:53.310 --> 09:54.230
Don't worry about it.

09:54.230 --> 09:58.950
As I mentioned before, we're only using SymPy in this course.

09:59.150 --> 10:05.670
In this section, we're going to stop using SymPy pretty soon, so we're only using SymPy in the next

10:05.670 --> 10:06.550
couple of videos.

10:06.550 --> 10:13.150
You don't have to worry about all the details of the mechanics and all the functions in SymPy.

10:13.550 --> 10:13.750
Okay.

10:13.790 --> 10:20.630
And then this is just an example that's going to compute the partial derivative of the function with

10:20.630 --> 10:25.150
respect to x at location one comma one.

10:25.550 --> 10:27.070
So how do we interpret this value.

10:27.070 --> 10:34.470
So the partial derivative of the function with respect to x at the .11 is minus one.

10:34.470 --> 10:38.510
Let's see if we can make sense of that looking back at the function here.

10:38.510 --> 10:42.070
So the .11 is somewhere around here.

10:42.470 --> 10:49.790
And the gradient or the partial derivative with respect to x is minus one at this point.

10:49.830 --> 10:54.450
And that means that the function is going down to the right as we go here to the right.

10:54.490 --> 10:58.610
Now that's pretty clearly visible from the color scheme in this plot here.

10:59.050 --> 11:06.050
Of course, in higher dimensions you can't just visualize this, but for 1D and 2D functions, we can

11:06.050 --> 11:12.570
just look at the function and see how that relates to the sign of the derivative at each point in the

11:12.570 --> 11:13.250
function.

11:13.890 --> 11:14.290
All right.

11:14.290 --> 11:19.250
So now we have a Python function that computes the the function.

11:19.290 --> 11:21.250
The mathematical function peaks.

11:21.250 --> 11:25.810
And we have SymPy functions that compute the partial derivatives.

11:25.970 --> 11:28.770
And the collection of these two is going to be the gradient.

11:29.170 --> 11:29.450
Okay.

11:29.490 --> 11:32.290
So now we get to this cell of code.

11:32.330 --> 11:33.610
Now this should look familiar.

11:33.650 --> 11:40.010
This is pretty much exactly the same as the gradient descent code in the previous two videos.

11:40.610 --> 11:43.290
There's just a couple of small modifications.

11:43.450 --> 11:46.490
So we we define an initial starting point.

11:46.490 --> 11:52.510
This is a random number I'm defining a random number somewhere between minus two and plus two.

11:52.550 --> 11:59.830
And here we have two random numbers, because we have to have a starting x value and a starting y value.

12:00.430 --> 12:03.510
Here's our two key parameters that we've learned a little bit about.

12:03.670 --> 12:05.910
We'll talk a lot more about these later.

12:05.950 --> 12:08.830
The learning rate and the number of training epochs.

12:09.190 --> 12:13.390
Here we are looping through the number of training epochs.

12:13.710 --> 12:16.630
And then here you can see we're computing the gradient.

12:16.630 --> 12:21.710
So this is I define this to be a numpy array which contains two elements.

12:21.710 --> 12:29.870
The first element is the the partial derivative with respect to x at our local minimum x y, and then

12:29.870 --> 12:35.990
the partial derivative of the function with respect to y at exactly that same location.

12:36.070 --> 12:38.270
And this is the gradient right here.

12:38.310 --> 12:41.710
This these two elements organized in a numpy array.

12:42.350 --> 12:49.190
And then here we update the local minimum according to itself minus the gradient times the learning

12:49.290 --> 12:49.890
rate.

12:49.890 --> 12:52.530
And then a little bit of extra code here because I'm.

12:52.570 --> 12:53.770
Storing the trajectory.

12:53.770 --> 12:55.130
That's for the visualization.

12:55.410 --> 12:55.850
Okay.

12:56.010 --> 13:03.330
So let's run this and we can see this is the final local minimum and the starting point.

13:03.370 --> 13:06.170
Now I don't know whether these are the correct values.

13:06.170 --> 13:08.170
So we are going to visualize this.

13:08.170 --> 13:11.210
So we make an image of the function z.

13:11.250 --> 13:12.290
This peaks function.

13:12.570 --> 13:18.410
And then I'm going to plot dots according to the start location and the end location.

13:18.570 --> 13:24.570
And then I'm going to plot the entire trajectory, which is basically showing how the model moved the

13:24.570 --> 13:29.170
data point, the estimate of the local minimum over training epochs.

13:29.570 --> 13:30.690
Okay, so this is pretty neat.

13:30.690 --> 13:33.490
We started here and we got to this local minimum.

13:33.770 --> 13:37.970
This is actually like the the least interesting local minimum.

13:37.970 --> 13:42.090
This is the worst of the three local minima in this function.

13:42.090 --> 13:44.570
So it's pretty interesting okay I'm going to run this again.

13:45.250 --> 13:50.590
So run this one more time again starting with the different random location, so hopefully it will be

13:50.590 --> 13:54.030
somewhere else, but it's unpredictable where it's going to start.

13:54.870 --> 13:55.310
All right.

13:55.350 --> 13:58.230
So it actually started on this peak and it went back.

13:58.270 --> 14:04.430
It went back to the same local minimum, the same final estimate where it started at before.

14:05.030 --> 14:05.270
Okay.

14:05.310 --> 14:09.270
Now I encourage you to spend some time playing around with this code.

14:09.270 --> 14:13.190
You can have a look through the additional explorations here and here.

14:13.190 --> 14:19.550
I'm already suggesting to to force the initial starting guess to be an interesting location.

14:21.110 --> 14:27.510
So in this video you have seen that gradient descent in two dimensions is basically the same as it is

14:27.510 --> 14:28.750
in one dimension.

14:28.750 --> 14:31.190
There's a little bit more housekeeping that's involved.

14:31.190 --> 14:36.270
The code gets a little bit more complicated, but the principle is exactly the same.

14:36.390 --> 14:43.190
And that also means that going from two dimensions up to a million dimensions is also exactly the same

14:43.190 --> 14:45.870
thing as going from two dimensions to one dimension.

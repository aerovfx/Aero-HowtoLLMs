WEBVTT

00:02.040 --> 00:09.560
There are many algorithms that are used in deep learning, but it is no understatement to claim that

00:09.560 --> 00:17.280
the gradient descent algorithm is the single most important, most fundamental algorithm in all of deep

00:17.280 --> 00:17.840
learning.

00:18.200 --> 00:25.280
In fact, without gradient descent, deep learning simply would not exist, and deep learning is also

00:25.280 --> 00:30.960
important in other areas of machine learning and AI and optimization.

00:31.560 --> 00:38.640
Fortunately, deep learning algorithm is actually not that complicated, and it doesn't involve a lot

00:38.640 --> 00:40.200
of really intense math.

00:40.400 --> 00:46.400
In fact, it's really just using derivatives and a little bit of algebra.

00:46.840 --> 00:52.200
And so in this video I'm going to motivate why we need something like gradient descent.

00:52.200 --> 00:56.080
And then I will explain the gradient descent algorithm.

00:56.480 --> 01:03.340
I'm going to hold off on illustrating gradient descent in Python for now because I want to make sure

01:03.340 --> 01:07.340
that the concepts are clear before we switch to code.

01:07.740 --> 01:10.780
So let me start by providing a motivation.

01:11.180 --> 01:17.780
Here is I'm going to show you three steps or state the three steps of how deep learning models actually

01:17.780 --> 01:18.380
learn.

01:18.940 --> 01:22.100
Step one is to just guess a solution.

01:22.100 --> 01:24.060
We guess a solution randomly.

01:24.740 --> 01:31.780
Step two is to compute the error, which is the mistakes, or whether the solution, whether the model

01:31.780 --> 01:35.980
guess was correct or incorrect and how incorrect it was.

01:36.580 --> 01:39.780
So for example, let's imagine we show the model.

01:39.780 --> 01:41.460
We have some deep learning model.

01:41.460 --> 01:45.740
We give it a picture of a cat, and the model guesses a solution.

01:45.740 --> 01:50.140
And it says, you know, I think that's a picture of a peanut butter and jelly sandwich.

01:50.660 --> 01:52.380
Well, you know, that's that's wrong.

01:52.500 --> 01:54.660
So we have to figure out how to compute the error.

01:54.700 --> 01:57.820
The model made a mistake and we compute the error.

01:57.940 --> 02:02.020
Now this these two steps here are actually pretty trivial.

02:02.060 --> 02:04.590
Step one is is really very simple.

02:04.790 --> 02:07.230
Step two is also fairly straightforward.

02:07.230 --> 02:08.390
It's kind of trivial.

02:08.750 --> 02:12.830
The problem and where this starts to get difficult is step three.

02:12.870 --> 02:19.390
And step three is where we learn from our mistakes, or the model learns from its mistakes to modify

02:19.390 --> 02:20.390
the parameters.

02:20.390 --> 02:28.830
And the goal of modifying the parameters is to give a better guess of a solution the next time around.

02:29.150 --> 02:36.070
Now what we need to do is is figure out how to convert this statement, which you know, this should

02:36.070 --> 02:36.710
make sense.

02:36.710 --> 02:42.590
Step three this makes sense to us, but we need to convert this from a statement that makes sense to

02:42.630 --> 02:50.110
us humans conceptually into a mathematical framework that a model can actually implement and compute.

02:50.550 --> 02:57.470
So we need a mathematical description of the error, the landscape of the errors of the problem.

02:57.750 --> 03:04.210
And we need a way to we need to discover a way to find the minimum of that error landscape.

03:04.210 --> 03:10.810
So we need to be able to tell the model how to make fewer errors, how to learn from the mistakes,

03:10.810 --> 03:17.650
how to modify the parameters so that the errors are smaller in the future so that the guesses are better.

03:18.010 --> 03:23.570
And this is important, of course, because the smaller the errors, the fewer mistakes we make, the

03:23.610 --> 03:29.490
better the model is going to guess at a solution, the more accurate the model is going to be.

03:29.850 --> 03:30.170
Okay.

03:30.210 --> 03:34.930
So and then how do we create these error descriptions and how do we learn from our mistakes.

03:35.050 --> 03:37.730
Well this is where calculus comes to the rescue.

03:37.890 --> 03:40.090
And in particular derivatives.

03:41.010 --> 03:44.530
So this is the core idea of gradient descent.

03:44.730 --> 03:51.290
The core idea of gradient descent is that we figure out a way to represent the errors, the mistakes

03:51.290 --> 03:54.930
that the model is making as a mathematical function.

03:55.250 --> 03:58.770
And then we find the minimum of that function.

03:58.770 --> 04:04.530
So the minimum of the function, the minimum of the error function means that the model is making the

04:04.710 --> 04:06.350
fewest mistakes possible.

04:06.870 --> 04:12.390
Now, I showed you a plot that looks something like this several videos ago where I showed x squared.

04:12.390 --> 04:14.830
So this is a function that would look like x squared.

04:15.350 --> 04:23.670
There I labeled this axis x and I labeled this axis f of x or y equals f of x equals x squared.

04:24.150 --> 04:24.990
The same concept.

04:24.990 --> 04:27.750
Here we are using the parameter name w.

04:27.790 --> 04:30.630
This stands for weights, but it's just another parameter.

04:30.630 --> 04:32.990
You can think of it as a variable like x.

04:33.430 --> 04:37.430
This is the function in gradient descent.

04:37.470 --> 04:43.750
It's it's common to use j of w which indicates the loss function or the error function.

04:43.870 --> 04:50.430
And then our goal is to get the model to have the smallest errors as possible, which means finding

04:50.430 --> 04:52.990
the minimum of this function.

04:53.190 --> 04:55.670
And this is what we do with gradient descent.

04:55.710 --> 04:57.190
Now what does this word even mean?

04:57.190 --> 04:58.270
Gradient descent.

04:58.270 --> 05:02.710
Well, the term gradient is exactly the same thing as a derivative.

05:03.070 --> 05:09.210
People typically use the word derivative For, you know, like one dimensional functions like this that

05:09.210 --> 05:15.890
you can plot on a two dimensional axis when you have a larger number of dimensions, two dimensions,

05:15.890 --> 05:19.570
four dimensions, a billion dimensions, lots and lots of parameters.

05:19.810 --> 05:23.650
Then we start using the term gradient instead of derivative.

05:23.650 --> 05:26.050
But the concept is exactly the same.

05:26.250 --> 05:31.170
Everything you've ever learned about derivatives you apply to gradients, okay.

05:31.210 --> 05:33.130
And descent of course, just means to go down.

05:33.170 --> 05:37.290
Like the descent into madness means they go down into craziness.

05:37.570 --> 05:44.410
So gradient descent is simply following the derivative down to get to the function minimum.

05:44.850 --> 05:48.770
So this is what a one dimensional function might look like.

05:48.890 --> 05:52.250
Here is an example of a two dimensional function.

05:52.330 --> 05:54.490
So now we start off at some random point.

05:54.610 --> 05:58.850
Here we're starting off at some random initial weights initial point, initial guess.

05:59.090 --> 06:02.890
And then we slowly start moving down towards the minimum.

06:03.170 --> 06:10.540
Now, in one dimension and in two dimensions, it's possible to visualize the process of gradient descent

06:10.740 --> 06:11.940
in higher dimensions.

06:11.940 --> 06:13.540
It's totally impossible.

06:13.540 --> 06:16.300
We just have to trust the algebra and the math.

06:16.420 --> 06:21.300
Of course, the math is the same regardless of the number of dimensions.

06:21.860 --> 06:25.300
Okay, so let's work through a numerical example.

06:25.540 --> 06:26.780
Here is a function.

06:26.780 --> 06:28.460
This is a polynomial function.

06:28.940 --> 06:31.380
So we need to compute its derivative.

06:31.780 --> 06:37.260
If you would like to pause the video and compute the derivative of this function, then I think that

06:37.260 --> 06:40.620
would be a pretty nice way to spend 10s of your life.

06:41.060 --> 06:42.420
Okay, so anyway, here's the function.

06:42.420 --> 06:43.540
Here's the derivative.

06:43.740 --> 06:47.220
It is six x because we bring the two down, multiply the three.

06:47.420 --> 06:50.700
Then the two disappears because implicitly there's a one here.

06:51.580 --> 06:53.940
This x has a one in the exponent.

06:53.940 --> 06:56.380
So that multiplies the three.

06:56.380 --> 07:01.460
And then the x here turns into x to the power of zero which is just one.

07:01.460 --> 07:02.900
So that also disappears.

07:02.900 --> 07:04.980
And here this is a term without a variable.

07:04.980 --> 07:06.660
So that just disappears as well.

07:07.560 --> 07:10.440
Okay, now we can make a plot of this function.

07:10.440 --> 07:12.760
Let's imagine this is our error function.

07:12.760 --> 07:14.920
This is the thing we want to minimize.

07:15.040 --> 07:17.680
And here is its derivative in orange.

07:17.680 --> 07:20.200
So we see the blue plot and the orange plot.

07:20.560 --> 07:25.000
So now our goal is to find the minimum of this function.

07:25.000 --> 07:28.320
Where is this function at its smallest value.

07:28.840 --> 07:34.760
Well I mean obviously you know, visually in this one dimensional case we can just look at the graph

07:34.760 --> 07:35.360
and see.

07:35.800 --> 07:42.600
But the minimum of this function is also going to correspond to the where the derivative equals zero.

07:42.800 --> 07:45.920
Now remember we don't want to minimize the derivative.

07:45.920 --> 07:47.920
We want to minimize the function.

07:48.080 --> 07:52.480
And the function is minimized where the derivative is zero.

07:53.040 --> 07:53.360
All right.

07:53.360 --> 07:57.640
And so how do we find the function minimum using gradient descent.

07:58.080 --> 08:01.040
So here is the gradient descent algorithm.

08:01.040 --> 08:03.360
You can see it's only a couple of steps.

08:03.720 --> 08:10.340
So the first step is to initialize some random guess of a minimum we just randomly guess where's the

08:10.340 --> 08:11.900
minimum of this function?

08:11.940 --> 08:12.660
Totally at random.

08:12.660 --> 08:15.420
Maybe we're going to initialize our guess to minus two.

08:15.460 --> 08:17.140
That's you know, it's kind of far away.

08:17.340 --> 08:20.180
Maybe, you know, our initial guess will be 0.6.

08:20.220 --> 08:21.620
In which case we're pretty close.

08:22.700 --> 08:27.780
So we initialize a totally random guess, and then we have a loop over here.

08:27.780 --> 08:28.660
This is a for loop.

08:28.660 --> 08:32.100
We loop over some number of iterations.

08:32.100 --> 08:34.260
And how many iterations do we loop over.

08:34.540 --> 08:36.180
Well that's a parameter of learning.

08:36.180 --> 08:37.980
We will discuss that in the future.

08:38.500 --> 08:45.220
So the first thing we do inside this loop is to compute the derivative at the the initial guess at our

08:45.220 --> 08:47.580
guess of the function minimum.

08:48.740 --> 08:51.980
So let's say that we guessed -0.5.

08:51.980 --> 08:54.140
So that was our initial random guess.

08:54.580 --> 08:57.860
Now the function value itself is like six.

08:57.860 --> 09:01.620
But we don't actually care about that value for this step.

09:01.620 --> 09:04.580
What we care about is the derivative at that value.

09:04.980 --> 09:09.020
So the derivative at -0.5 is like minus six.

09:09.710 --> 09:16.030
Okay, so now that we have our derivative of minus six, we go on to the second step inside this loop.

09:16.390 --> 09:20.630
And that is to update our initial our guess of the minimum.

09:20.790 --> 09:25.990
So the updated guess of the minimum is itself and itself is -0.5.

09:26.030 --> 09:32.790
That was our guess minus the derivative scaled by the learning rate or times the learning rate.

09:32.790 --> 09:38.110
Learning rate is a small number that we use to make sure that we're taking small steps so we don't miss.

09:38.110 --> 09:42.550
The minimum learning rate is a super important parameter in deep learning.

09:42.550 --> 09:46.510
I will have much more to say about the learning rate in later videos.

09:46.510 --> 09:48.630
For now, let's just, you know, just imagine this.

09:48.630 --> 09:49.950
This is some small number.

09:49.990 --> 09:51.110
Maybe it's 0.01.

09:51.390 --> 09:51.750
Okay.

09:51.790 --> 09:56.670
So we update the the guess as itself minus the derivative.

09:56.830 --> 09:59.310
So the derivative here is negative.

09:59.590 --> 10:03.230
So that means we multiply the derivative by minus one.

10:03.230 --> 10:05.790
So that becomes positive and it's positive.

10:05.790 --> 10:07.470
So that means we go to the right.

10:07.470 --> 10:09.310
So we would move up here.

10:09.310 --> 10:10.930
We move up the derivative.

10:10.930 --> 10:16.330
And that gets us closer to the solution, which is where the derivative equals zero.

10:17.450 --> 10:21.610
Now let's imagine we randomly guessed a number over here like 1.5.

10:21.650 --> 10:23.850
This was our initial random guess.

10:24.330 --> 10:28.890
So then we we go through here, we we compute the derivative at 1.5.

10:29.210 --> 10:33.570
And we find that the derivative here is around five a little over five.

10:34.530 --> 10:39.050
So then we update the guess as itself minus the derivative.

10:39.050 --> 10:41.250
So the derivative here is positive.

10:41.250 --> 10:42.370
It's above zero.

10:42.650 --> 10:45.610
So therefore we turn around and we go to the left.

10:45.890 --> 10:50.930
We go in the direction that of the negative derivative.

10:50.930 --> 10:53.290
So that's going to get us in this direction.

10:53.530 --> 10:59.690
So you can see that if we start off being too low we're going to move up towards the function minimum.

10:59.690 --> 11:04.490
If we start off being too high, we're going to go down towards the function minimum.

11:04.490 --> 11:09.970
And eventually we are going to get closer and closer and closer to the function minimum, which is right

11:09.970 --> 11:11.490
here at 0.5.

11:11.490 --> 11:11.590
Five.

11:12.510 --> 11:12.830
All right.

11:12.870 --> 11:14.110
And then it looks something like this.

11:14.110 --> 11:20.350
This is actually a screenshot of the Python code that you will see in a couple videos from now.

11:20.350 --> 11:27.270
So we are going to actually perform gradient descent to find the local minimum of this function f of

11:27.270 --> 11:27.670
x.

11:27.950 --> 11:32.550
So we can see here that the minimum is 0.495, blah blah blah.

11:32.550 --> 11:34.510
Some really, you know, very long number here.

11:34.950 --> 11:37.110
Now is this the correct answer?

11:37.590 --> 11:40.710
It turns out that this is not the correct answer.

11:40.710 --> 11:46.710
And to prove that this is the incorrect answer, we can actually compute the the real minimum of this

11:46.710 --> 11:48.430
function analytically.

11:48.830 --> 11:53.950
And the way we do that is by computing the derivative which we already have, we set the derivative

11:53.950 --> 11:54.830
to zero.

11:54.990 --> 12:03.470
And then we solve for x, and we find that the true minimum of this function is x equals 0.5.

12:03.870 --> 12:08.510
So so gradient descent actually gave us an answer that was close.

12:08.550 --> 12:13.490
You know I'm not saying that it was totally, totally wrong, but it is definitely not accurate.

12:13.530 --> 12:18.050
This is not the correct solution to the minimum of this function.

12:18.450 --> 12:24.050
So there's a couple of points that I want to make here, which I'm going to expand upon quite a bit

12:24.090 --> 12:25.690
throughout the next several sections.

12:25.690 --> 12:27.130
But just to give you a heads up.

12:27.610 --> 12:32.650
Gradient descent is not guaranteed to give us the correct solution.

12:32.650 --> 12:37.010
It's not guaranteed to give us the optimal, the best solution.

12:37.170 --> 12:43.810
There are several circumstances that can make gradient descent algorithm either go completely wrong

12:43.810 --> 12:47.210
or just something like this, where we're pretty close, but we're not actually correct.

12:47.450 --> 12:54.570
One is using a poor choice of model parameters, so we could actually get closer to the correct solution

12:54.570 --> 12:56.690
here by changing the model parameters.

12:57.290 --> 13:00.450
Two is we can get stuck in local minimum.

13:00.810 --> 13:03.050
I'll talk more about that in the next video.

13:03.690 --> 13:10.130
A third issue is we can run into some problems like vanishing gradients or exploding gradients.

13:10.490 --> 13:14.020
And I think that's actually probably all the things that can go wrong.

13:14.140 --> 13:17.140
So gradient descent is a great algorithm.

13:17.140 --> 13:19.300
It's simple and it's very powerful.

13:19.540 --> 13:25.180
Gradient descent is guaranteed to move us towards the direction of a good solution.

13:25.340 --> 13:30.140
It is not necessarily guaranteed to give us the exact correct solution.

13:30.500 --> 13:37.980
Now, fortunately, many of the problems of gradient descent have been overcome in the past decade or

13:37.980 --> 13:41.820
so or two decades of research and development in deep learning.

13:42.140 --> 13:48.500
So there are quite a few tricks that we use in deep learning that will get around some of the problems

13:48.500 --> 13:49.940
of gradient descent.

13:50.420 --> 13:54.500
So therefore the conclusion is that gradient descent is a great algorithm.

13:54.500 --> 13:56.940
It basically always works very well.

13:57.060 --> 13:59.460
It is not guaranteed to work perfectly.

14:00.020 --> 14:02.620
And that's pretty much how gradient descent works.

14:02.620 --> 14:04.900
There isn't really a lot more to it.

14:05.020 --> 14:06.460
There are some tricks and tips.

14:06.460 --> 14:13.140
There's some parameters and implementation details, but you now understand the gradient descent algorithm.

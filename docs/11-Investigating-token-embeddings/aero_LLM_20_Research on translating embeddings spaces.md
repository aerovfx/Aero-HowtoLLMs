
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [11 Investigating token embeddings](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Sá»± Dá»‹ch Chuyá»ƒn VÃ  Äá»“ng Tá»“n Biá»ƒu Diá»…n Giá»¯a CÃ¡c KhÃ´ng Gian NhÃºng

## TÃ³m táº¯t

BÃ i bÃ¡o khoa há»c nÃ y nÃªu báº­t má»™t trong nhá»¯ng luá»“ng suy nghÄ© tham vá»ng nháº¥t cá»§a Giá»›i trÃ­ tuá»‡ nhÃ¢n táº¡o há»c thuáº­t: Liá»‡u sá»± khÃ¡c biá»‡t cá»§a hÃ ng loáº¡t cÃ¡c bá»™ nÃ£o LLMs (nhÆ° Word2Vec, GloVe, BERT hay GPT) chá»‰ lÃ  káº¿t quáº£ cá»§a sá»± xÃ´ lá»‡ch trá»¥c tá»a Ä‘á»™? Liá»‡u cÃ³ tá»“n táº¡i má»™t KhÃ´ng gian biá»ƒu diá»…n phá»• quÃ¡t (Universal Platonic Space) vÃ  cÃ¡c ma tráº­n phÃ¢n lá»›p tá»« nhÃºng cá»§a má»—i máº¡ng lÆ°á»›i nÆ¡-ron thá»±c cháº¥t hoÃ n toÃ n cÃ³ thá»ƒ Ä‘Æ°á»£c "biÃªn dá»‹ch chÃ©o" láº«n nhau? 

---

## 1. Giáº£ Thuyáº¿t KhÃ´ng Gian NgÃ´n Ngá»¯ Phá»• QuÃ¡t (Platonic Embedding Space)

Hiá»‡n táº¡i, viá»‡c khai thÃ¡c cáº¥u trÃºc ma tráº­n nhÃºng cá»§a hai mÃ´ hÃ¬nh $M_1$ (vÃ­ dá»¥: Word2Vec) vÃ  $M_2$ (vÃ­ dá»¥: GPT-2) luÃ´n cho tháº¥y cÃ¡c phÆ°Æ¡ng sai chiá»u khÃ´ng há» tuyáº¿n tÃ­nh Ä‘Ã¨ lÃªn nhau. KhÃ´ng cÃ³ hai ma tráº­n embeddings nÃ o hoÃ n toÃ n khÃ­t láº¡i do sá»± chÃªnh lá»‡ch hÃ m má»¥c tiÃªu tá»‘i Æ°u lÃºc Ä‘Ã o táº¡o (Objective function optimization).

DÃ¹ váº­y, má»™t luá»“ng triáº¿t há»c vÃ  kiáº¿n trÃºc há»c thuyáº¿t (Alignment Hypothesis) Ä‘Æ°a ra Ã½ tÆ°á»Ÿng ráº±ng cÃ³ má»™t chiá»u khÃ´ng gian siÃªu viá»‡t vÃ  vÃ´ hÆ°á»›ng (Platonic space) $\mathbb{U}$ quy tá»¥ toÃ n bá»™ Ä‘áº·c tÃ­nh vÃ  khá»‘i tÆ°Æ¡ng quan ngÃ´n ngá»¯ loÃ i ngÆ°á»i. CÃ¡c ma tráº­n $E_{\text{w2v}}$ vÃ  $E_{\text{gpt}}$ hiá»‡n chá»‰ coi lÃ  cÃ¡c chÃ¹m tia sÃ¡ng (Projections layer) mang báº£n chá»¥p tÄ©nh cá»§a khá»‘i lÆ°á»£ng tÆ° duy áº¥y.

### 1.1 TÃ¬m PhÃ©p Biáº¿n Äá»•i VÃ´ HÆ°á»›ng BiÃªn Dá»‹ch ChÃ©o (Cross-lingual / Cross-model Mapping)
Náº¿u há»‡ há»c cá»§a hai mÃ´ hÃ¬nh lÃ  chung quy luáº­t, thÃ¬ vá» máº·t lÃ½ thuyáº¿t thuáº§n tÃºy ToÃ¡n HÃ¬nh Há»c, cÃ³ thá»ƒ Ã¡nh xáº¡ (map) tá»« vá»±ng khÃ´ng gian nÃ y sang khÃ´ng gian kia (Translation Mapping) báº±ng bá»™ khung quy táº¯c bao gá»“m ma tráº­n xoay (Rotation $W$) vÃ  co dÃ£n chiá»u (Scaling matrix $S$):
$$
E_2 \approx E_1 \cdot W + b 
$$
Viá»‡c dá»‹ch chuyá»ƒn nÃ y thÆ°á»ng Ä‘Æ°á»£c ná»— lá»±c Ä‘áº¡t thÃ´ng qua CÄƒn chá»‰nh Procrustes Trá»±c giao (Orthogonal Procrustes problem), má»™t bÃ i toÃ¡n tÃ¬m ma tráº­n trá»±c giao tá»‘i Æ°u Ä‘á»ƒ chá»“ng khÃ­t hai khá»‘i vector mÃ  khÃ´ng sá»­ dá»¥ng sá»± uá»‘n náº¯n phi tuyáº¿n. Trá»ng Ä‘iá»ƒm chi phÃ­ máº¥t mÃ¡t:
$$ 
\text{Loss} = \| E_1 W - E_2 \|_F^2 \quad \text{vá»›i Ä‘iá»u kiá»‡n } W^\top W = I
$$

---

## 2. ThÃ¡ch Thá»©c Sá»± Chuyá»ƒn HÃ³a Cá»§a Äá»“ Thá»‹ NgÃ´n Ngá»¯

Viá»‡c thiáº¿t láº­p nhá»¯ng hÃ m biÃªn dá»‹ch Ä‘á»“ng quy mÃ´ cho mÃ´ hÃ¬nh Embeddings gáº·p pháº£i rÃ o cáº£n chÃ­ máº¡ng lÃ  "Sá»± Di Äá»™ng" (Dynamism) cá»§a mÃ´ hÃ¬nh hÃ³a. 

### RÃ o cáº£n Kiáº¿n trÃºc Attention so vá»›i Tá»« vá»±ng tÄ©nh
- **MÃ´ HÃ¬nh TÄ©nh (Word2Vec / GloVe):** Sá»Ÿ há»¯u káº¿t cáº¥u lÆ°á»›i má»™t-Ä‘á»‘i-má»™t cá»©ng ráº¯n, "TrÃ¡i tÃ¡o" mÃ£i mÃ£i lÃ  1 Ä‘iá»ƒm áº£nh Euclidean khÃ´ng Ä‘á»•i á»Ÿ tá»a Ä‘á»™ tuyá»‡t Ä‘á»‘i.
- **MÃ´ HÃ¬nh Äá»™ng Theo Ngá»¯ Cáº£nh (Transformer / GPT / BERT):** "TrÃ¡i tÃ¡o" khi káº¿t há»£p cÃ¹ng chuá»—i há»™i thoáº¡i vá» "Apple M2" vÃ  "Apple Pie" sáº½ bá»‹ báº» cong thÃ nh cÃ¡c ma tráº­n nhÃºng biáº¿n dá»‹ dá»±a trÃªn ma tráº­n tá»· trá»ng lÆ°á»›i lÆ°u Ã½ (Attention weights remapping). 

Do Ä‘Ã³, vector nhÃºng trong Transformer khÃ´ng bao giá» lÃ  báº¥t di báº¥t dá»‹ch, chÃºng sáº½ trÆ°á»£t Ä‘i, uá»‘n lÆ°á»£n táº¡i dÃ²ng Residual Stream Ä‘á»ƒ láº¥p Ä‘áº§y sá»± nhiá»…u loáº¡n ngáº«u nhiÃªn cá»§a cÃ¡c nÃºt Sampling cÃ³ nhiá»‡t Ä‘á»™ (Softmax Sampling with Temperature T).

---

## 3. Khá»Ÿi Äiá»ƒm Há»‡ NghiÃªn Cá»©u Má»›i

Sá»± ná»— lá»±c cá»§a toÃ¡n há»c Ä‘á»ƒ biáº¿n biÃªn dá»‹ch Vector Matrix Translation tuy chá»©a Ä‘á»±ng sá»± báº¥p bÃªnh Ä‘á»‘i vá»›i Ä‘á»™ sÃ¢u phá»©c táº¡p, nhÆ°ng Ä‘Ã³ng vai trÃ² cá»±c ká»³ quan trá»ng Ä‘á»‘i vá»›i kháº£ nÄƒng diá»…n giáº£i cÆ¡ cháº¿ (Mech Interp). Sá»± Ä‘Ã o sÃ¢u vá» tÃ­nh báº¥t toÃ n cá»§a cÃ¡c phÃ©p trá»±c giao Procrustes giÃºp cá»§ng cá»‘ báº£n cháº¥t thá»±c sá»± cá»§a phÆ°Æ¡ng trÃ¬nh Transformer: Sá»± khÃ´n ngoan cá»§a mÃ¡y mÃ³c khÃ´ng tá»›i tá»« tá»a Ä‘á»™ lÆ°u tá»« Ä‘iá»ƒn, mÃ  tá»« vÃ²ng láº·p cá»™ng nhá»“i vector cá»§a cÃ¡c Layer phi tuyáº¿n vá»›i sá»± nhiá»…u tÃ­n há»c (Randomness Token distribution).

---

## TÃ i liá»‡u tham kháº£o

1. **Smith, S., et al. (2017).** *Offline bilingual word vectors, orthogonal transformations and the inverted softmax.* ICLR. (Chá»‰ ra sá»± Ã¡nh xáº¡ 2 khÃ´ng gian embeddings dá»‹ch thuáº­t Procrustes).
2. **Conneau, A., et al. (2018).** *Word Translation Without Parallel Data*. ICLR.
3. TÃ i liá»‡u Ä‘á»‹nh hÆ°á»›ng bÃ i giáº£ng *Investigating token embeddings - Translating Embeddings Spaces*.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero_LLM_01_CodeChallenge Cosine similarity (advanced) (part 1).md](aero_LLM_01_CodeChallenge Cosine similarity (advanced) (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_CodeChallenge Cosine similarity (advanced) (part 1).md) |
| [aero_LLM_02_CodeChallenge Cosine similarity (advanced) (part 2).md](aero_LLM_02_CodeChallenge Cosine similarity (advanced) (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_CodeChallenge Cosine similarity (advanced) (part 2).md) |
| [Theo DÃµi DÃ²ng Cháº£y Cosine Similarity TrÃªn Trá»¥c VÄƒn Báº£n ChuyÃªn Tuáº§n Tá»± (Word Sequences)](aero_LLM_03_CodeChallenge Cosine similarity in word sequences.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Cosine similarity in word sequences.md) |
| [Nghá»‡ Thuáº­t Váº½ Báº£n Äá»“ Nhiá»‡t Ma Tráº­n NhÃºng Báº±ng CÆ°á»ng Äá»™ Tá»« (Coloring Cosine Similarity)](aero_LLM_04_CodeChallenge Coloring cosine similarity.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Coloring cosine similarity.md) |
| [áº¢o áº¢nh Cá»§a TrÃ­ Tuá»‡ ToÃ¡n Há»c Trong NgÃ´n Ngá»¯: Sá»©c Máº¡nh Cá»§a Random Embeddings](aero_LLM_05_CodeChallenge Can random embeddings be interpreted.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_CodeChallenge Can random embeddings be interpreted.md) |
| [PhÆ°Æ¡ng PhÃ¡p T-SNE VÃ  Thuáº­t ToÃ¡n PhÃ¢n Cá»¥m DBSCAN: Chiáº¿u KhÃ´ng Gian Äa Chiá»u Cho LLMs](aero_LLM_06_T-SNE projection and DBSCAN clustering (theory).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_T-SNE projection and DBSCAN clustering (theory).md) |
| [PhÃ¢n Cá»¥m Ngá»¯ NghÄ©a Qua PhÃ©p Chiáº¿u t-SNE & Máº­t Äá»™ DBSCAN (Python)](aero_LLM_07_T-SNE projection and DBSCAN clustering (Python).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_T-SNE projection and DBSCAN clustering (Python).md) |
| [ThÃ¡ch Thá»©c Code: TÃ¬m Lá»— Há»•ng PhÃ¢n Cá»¥m Báº±ng Bá»™ Lá»c Báº£ng Chá»¯ CÃ¡i Chá»¯ X](aero_LLM_08_CodeChallenge cluster the x terms.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge cluster the x terms.md) |
| [PhÃ¢n RÃ£ Token, NhÃºng VÃ  PhÃ¢n Cá»¥m Biá»ƒu TÆ°á»£ng Emojis Báº±ng Äá»“ Thá»‹ Máº­t Äá»™](aero_LLM_09_CodeChallenge Tokenize, embed, and cluster happy emojis.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Tokenize, embed, and cluster happy emojis.md) |
| [PhÃ¢n TÃ­ch RSA (Representational Similarity Analysis) Giá»¯a CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_10_RSA (representational similarity analysis).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_RSA (representational similarity analysis).md) |
| [PhÃ¢n TÃ­ch Äá»™ Lá»‡ch RSA (Part 1): So SÃ¡nh Sá»± Báº¥t Äá»“ng Giá»¯a KhÃ´ng Gian GloVe 50D vÃ  300D](aero_LLM_11_CodeChallenge Compare embeddings with RSA (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Compare embeddings with RSA (part 1).md) |
| [PhÃ¢n TÃ­ch Äá»™ Lá»‡ch RSA (Part 2): Äá»‘i Chiáº¿u TÆ°Æ¡ng Quan Pearson Cho Khoáº£ng CÃ¡ch Cosine](aero_LLM_12_CodeChallenge Compare embeddings with RSA (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_CodeChallenge Compare embeddings with RSA (part 2).md) |
| [So SÃ¡nh KhÃ´ng Gian NhÃºng: Word2Vec VÃ  GPT-2 Qua PhÃ¢n TÃ­ch RSA](aero_LLM_13_CodeChallenge Word2vec vs. GPT2.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_CodeChallenge Word2vec vs. GPT2.md) |
| [Bá»‘ Cá»¥c Äá»“ Thá»‹ Máº¡ng (Network Graph) ThÃ´ng Qua Ma Tráº­n Cosine Similarity](aero_LLM_14_CodeChallenge Graph representation of cosine similarities.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_CodeChallenge Graph representation of cosine similarities.md) |
| [Sá»‘ Há»c Tuyáº¿n TÃ­nh vÃ  RÃºt TrÃ­ch TÆ°Æ¡ng Äá»“ng Giá»¯a CÃ¡c Tá»« NhÃºng (Word Embeddings Analogies)](aero_LLM_15_Embeddings arithmetic and analogies.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_Embeddings arithmetic and analogies.md) |
| [Vá»¡ Má»™ng Vá» Sá»‘ Há»c Vector TÆ°Æ¡ng ÄÆ°Æ¡ng (Soft-Coded Analogies) TrÃªn Word2Vec](aero_LLM_16_CodeChallenge soft-coded analogies in word2vec.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_CodeChallenge soft-coded analogies in word2vec.md) |
| [Thiáº¿t Láº­p VÃ  Diá»…n Giáº£i Trá»¥c Ngá»¯ NghÄ©a Tuyáº¿n TÃ­nh (Linear Semantic Axes)](aero_LLM_17_Creating and interpreting linear semantic axes.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_Creating and interpreting linear semantic axes.md) |
| [Khai ThÃ¡c Thuáº­t ToÃ¡n k-NN Cho TÃ¬m Kiáº¿m Tá»« Äá»“ng NghÄ©a TrÃªn BERT](aero_LLM_18_kNN for synonym-searching in BERT.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_kNN for synonym-searching in BERT.md) |
| [Cáº¡nh Tranh TÃ¬m Tá»« Äá»“ng NghÄ©a BERT vs GPT: CÆ¡ Cháº¿ Tokenization Äa KÃ½ Tá»±](aero_LLM_19_CodeChallenge BERT v GPT kNN kompetition.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_19_CodeChallenge BERT v GPT kNN kompetition.md) |
| ğŸ“Œ **[Sá»± Dá»‹ch Chuyá»ƒn VÃ  Äá»“ng Tá»“n Biá»ƒu Diá»…n Giá»¯a CÃ¡c KhÃ´ng Gian NhÃºng](aero_LLM_20_Research on translating embeddings spaces.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_20_Research on translating embeddings spaces.md) |
| [PhÃ¢n TÃ­ch ChÃ¹m Quang Phá»• Suy Biáº¿n (Singular Value Spectrum) Cá»§a KhÃ´ng Gian NhÃºng](aero_LLM_21_Singular value spectrum of embeddings submatrices.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_21_Singular value spectrum of embeddings submatrices.md) |
| [Ãnh Xáº¡ SVD CÃ¡c Dáº£i Äiá»ƒm NhÃºng CÃ³ Quan Há»‡ ChÃ©o](aero_LLM_22_CodeChallenge SVD projections of related embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_22_CodeChallenge SVD projections of related embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

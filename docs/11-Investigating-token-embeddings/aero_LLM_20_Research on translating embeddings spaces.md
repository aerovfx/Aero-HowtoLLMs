
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [11 Investigating token embeddings](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Sá»± Dá»‹ch Chuyá»ƒn VÃ  Äá»“ng Tá»“n Biá»ƒu Diá»…n Giá»¯a CÃ¡c KhÃ´ng Gian NhÃºng

## TÃ³m táº¯t

BÃ i bÃ¡o khoa há»c nÃ y nÃªu báº­t má»™t trong nhá»¯ng luá»“ng suy nghÄ© tham vá»ng nháº¥t cá»§a Giá»›i trÃ­ tuá»‡ nhÃ¢n táº¡o há»c thuáº­t: Liá»‡u sá»± khÃ¡c biá»‡t cá»§a hÃ ng loáº¡t cÃ¡c bá»™ nÃ£o LLMs (nhÆ° Word2Vec, GloVe, BERT hay GPT) chá»‰ lÃ  káº¿t quáº£ cá»§a sá»± xÃ´ lá»‡ch trá»¥c tá»a Ä‘á»™? Liá»‡u cÃ³ tá»“n táº¡i má»™t KhÃ´ng gian biá»ƒu diá»…n phá»• quÃ¡t (Universal Platonic Space) vÃ  cÃ¡c ma tráº­n phÃ¢n lá»›p tá»« nhÃºng cá»§a má»—i máº¡ng lÆ°á»›i nÆ¡-ron thá»±c cháº¥t hoÃ n toÃ n cÃ³ thá»ƒ Ä‘Æ°á»£c "biÃªn dá»‹ch chÃ©o" láº«n nhau? 

---

## 1. Giáº£ Thuyáº¿t KhÃ´ng Gian NgÃ´n Ngá»¯ Phá»• QuÃ¡t (Platonic Embedding Space)

Hiá»‡n táº¡i, viá»‡c khai thÃ¡c cáº¥u trÃºc ma tráº­n nhÃºng cá»§a hai mÃ´ hÃ¬nh $M_1$ (vÃ­ dá»¥: Word2Vec) vÃ  $M_2$ (vÃ­ dá»¥: GPT-2) luÃ´n cho tháº¥y cÃ¡c phÆ°Æ¡ng sai chiá»u khÃ´ng há» tuyáº¿n tÃ­nh Ä‘Ã¨ lÃªn nhau. KhÃ´ng cÃ³ hai ma tráº­n embeddings nÃ o hoÃ n toÃ n khÃ­t láº¡i do sá»± chÃªnh lá»‡ch hÃ m má»¥c tiÃªu tá»‘i Æ°u lÃºc Ä‘Ã o táº¡o (Objective function optimization).

DÃ¹ váº­y, má»™t luá»“ng triáº¿t há»c vÃ  kiáº¿n trÃºc há»c thuyáº¿t (Alignment Hypothesis) Ä‘Æ°a ra Ã½ tÆ°á»Ÿng ráº±ng cÃ³ má»™t chiá»u khÃ´ng gian siÃªu viá»‡t vÃ  vÃ´ hÆ°á»›ng (Platonic space) $\mathbb{U}$ quy tá»¥ toÃ n bá»™ Ä‘áº·c tÃ­nh vÃ  khá»‘i tÆ°Æ¡ng quan ngÃ´n ngá»¯ loÃ i ngÆ°á»i. CÃ¡c ma tráº­n $E_{\text{w2v}}$ vÃ  $E_{\text{gpt}}$ hiá»‡n chá»‰ coi lÃ  cÃ¡c chÃ¹m tia sÃ¡ng (Projections layer) mang báº£n chá»¥p tÄ©nh cá»§a khá»‘i lÆ°á»£ng tÆ° duy áº¥y.

### 1.1 TÃ¬m PhÃ©p Biáº¿n Äá»•i VÃ´ HÆ°á»›ng BiÃªn Dá»‹ch ChÃ©o (Cross-lingual / Cross-model Mapping)
Náº¿u há»‡ há»c cá»§a hai mÃ´ hÃ¬nh lÃ  chung quy luáº­t, thÃ¬ vá» máº·t lÃ½ thuyáº¿t thuáº§n tÃºy ToÃ¡n HÃ¬nh Há»c, cÃ³ thá»ƒ Ã¡nh xáº¡ (map) tá»« vá»±ng khÃ´ng gian nÃ y sang khÃ´ng gian kia (Translation Mapping) báº±ng bá»™ khung quy táº¯c bao gá»“m ma tráº­n xoay (Rotation $W$) vÃ  co dÃ£n chiá»u (Scaling matrix $S$):
$$
E_2 \approx E_1 \cdot W + b 
$$
Viá»‡c dá»‹ch chuyá»ƒn nÃ y thÆ°á»ng Ä‘Æ°á»£c ná»— lá»±c Ä‘áº¡t thÃ´ng qua CÄƒn chá»‰nh Procrustes Trá»±c giao (Orthogonal Procrustes problem), má»™t bÃ i toÃ¡n tÃ¬m ma tráº­n trá»±c giao tá»‘i Æ°u Ä‘á»ƒ chá»“ng khÃ­t hai khá»‘i vector mÃ  khÃ´ng sá»­ dá»¥ng sá»± uá»‘n náº¯n phi tuyáº¿n. Trá»ng Ä‘iá»ƒm chi phÃ­ máº¥t mÃ¡t:
$$ 
\text{Loss} = \| E_1 W - E_2 \|_F^2 \quad \text{vá»›i Ä‘iá»u kiá»‡n } W^\top W = I
$$

---

## 2. ThÃ¡ch Thá»©c Sá»± Chuyá»ƒn HÃ³a Cá»§a Äá»“ Thá»‹ NgÃ´n Ngá»¯

Viá»‡c thiáº¿t láº­p nhá»¯ng hÃ m biÃªn dá»‹ch Ä‘á»“ng quy mÃ´ cho mÃ´ hÃ¬nh Embeddings gáº·p pháº£i rÃ o cáº£n chÃ­ máº¡ng lÃ  "Sá»± Di Äá»™ng" (Dynamism) cá»§a mÃ´ hÃ¬nh hÃ³a. 

### RÃ o cáº£n Kiáº¿n trÃºc Attention so vá»›i Tá»« vá»±ng tÄ©nh
- **MÃ´ HÃ¬nh TÄ©nh (Word2Vec / GloVe):** Sá»Ÿ há»¯u káº¿t cáº¥u lÆ°á»›i má»™t-Ä‘á»‘i-má»™t cá»©ng ráº¯n, "TrÃ¡i tÃ¡o" mÃ£i mÃ£i lÃ  1 Ä‘iá»ƒm áº£nh Euclidean khÃ´ng Ä‘á»•i á»Ÿ tá»a Ä‘á»™ tuyá»‡t Ä‘á»‘i.
- **MÃ´ HÃ¬nh Äá»™ng Theo Ngá»¯ Cáº£nh (Transformer / GPT / BERT):** "TrÃ¡i tÃ¡o" khi káº¿t há»£p cÃ¹ng chuá»—i há»™i thoáº¡i vá» "Apple M2" vÃ  "Apple Pie" sáº½ bá»‹ báº» cong thÃ nh cÃ¡c ma tráº­n nhÃºng biáº¿n dá»‹ dá»±a trÃªn ma tráº­n tá»· trá»ng lÆ°á»›i lÆ°u Ã½ (Attention weights remapping). 

Do Ä‘Ã³, vector nhÃºng trong Transformer khÃ´ng bao giá» lÃ  báº¥t di báº¥t dá»‹ch, chÃºng sáº½ trÆ°á»£t Ä‘i, uá»‘n lÆ°á»£n táº¡i dÃ²ng Residual Stream Ä‘á»ƒ láº¥p Ä‘áº§y sá»± nhiá»…u loáº¡n ngáº«u nhiÃªn cá»§a cÃ¡c nÃºt Sampling cÃ³ nhiá»‡t Ä‘á»™ (Softmax Sampling with Temperature T).

---

## 3. Khá»Ÿi Äiá»ƒm Há»‡ NghiÃªn Cá»©u Má»›i

Sá»± ná»— lá»±c cá»§a toÃ¡n há»c Ä‘á»ƒ biáº¿n biÃªn dá»‹ch Vector Matrix Translation tuy chá»©a Ä‘á»±ng sá»± báº¥p bÃªnh Ä‘á»‘i vá»›i Ä‘á»™ sÃ¢u phá»©c táº¡p, nhÆ°ng Ä‘Ã³ng vai trÃ² cá»±c ká»³ quan trá»ng Ä‘á»‘i vá»›i kháº£ nÄƒng diá»…n giáº£i cÆ¡ cháº¿ (Mech Interp). Sá»± Ä‘Ã o sÃ¢u vá» tÃ­nh báº¥t toÃ n cá»§a cÃ¡c phÃ©p trá»±c giao Procrustes giÃºp cá»§ng cá»‘ báº£n cháº¥t thá»±c sá»± cá»§a phÆ°Æ¡ng trÃ¬nh Transformer: Sá»± khÃ´n ngoan cá»§a mÃ¡y mÃ³c khÃ´ng tá»›i tá»« tá»a Ä‘á»™ lÆ°u tá»« Ä‘iá»ƒn, mÃ  tá»« vÃ²ng láº·p cá»™ng nhá»“i vector cá»§a cÃ¡c Layer phi tuyáº¿n vá»›i sá»± nhiá»…u tÃ­n há»c (Randomness Token distribution).

---

## TÃ i liá»‡u tham kháº£o

1. **Smith, S., et al. (2017).** *Offline bilingual word vectors, orthogonal transformations and the inverted softmax.* ICLR. (Chá»‰ ra sá»± Ã¡nh xáº¡ 2 khÃ´ng gian embeddings dá»‹ch thuáº­t Procrustes).
2. **Conneau, A., et al. (2018).** *Word Translation Without Parallel Data*. ICLR.
3. TÃ i liá»‡u Ä‘á»‹nh hÆ°á»›ng bÃ i giáº£ng *Investigating token embeddings - Translating Embeddings Spaces*.
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

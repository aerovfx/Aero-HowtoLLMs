
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [11 Investigating token embeddings](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Vá»¡ Má»™ng Vá» Sá»‘ Há»c Vector TÆ°Æ¡ng ÄÆ°Æ¡ng (Soft-Coded Analogies) TrÃªn Word2Vec

## TÃ³m táº¯t

CÃ¡c tiÃªu Ä‘á» bÃ¡o chÃ­ khoa há»c Ä‘áº¡i chÃºng thÆ°á»ng sá»­ dá»¥ng má»™t cÃ´ng thá»©c vÃ ng gÃ¢y áº¥n tÆ°á»£ng cá»§a mÃ´ hÃ¬nh Word Embeddings: `King - Man + Woman = Queen`. PhÆ°Æ¡ng trÃ¬nh vector há»c nÃ y táº¡o ra niá»m tin ráº±ng Máº¡ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n hoáº¡t Ä‘á»™ng thuáº§n tÃºy trÃªn cÃ´ng thá»©c toÃ¡n há»c khÃ¡i niá»‡m. BÃ¡o cÃ¡o Ä‘Ã¡nh giÃ¡ Ä‘á»™c láº­p nÃ y má»• sáº» sá»± cháº¯p vÃ¡ vÃ  tÃ­nh báº¥t toÃ n cá»§a thuáº­t toÃ¡n há»c phÃ©p loáº¡i suy khoáº£ng cÃ¡ch (Word analogies), phÃ¢n tÃ­ch sá»± há»¥t háº«ng khi váº­n dá»¥ng "Soft-coding" trÃªn Word2Vec so vá»›i thá»±c táº¡i cá»§a lÃ½ thuyáº¿t hÃ¬nh há»c.

---

## 1. Giá»›i Tuyáº¿n Cá»§a GloVe VÃ  TÃ­nh Sáº¯c BÃ©n Cá»§a Word2Vec

Hai ká»³ phÃ¹ng Ä‘á»‹ch thá»§ thá»i tiá»n-Transformer lÃ  *GloVe* vÃ  *Word2Vec* náº¯m giá»¯ hai cÆ¡ cháº¿ trÃ­ch xuáº¥t ma tráº­n (Factorization) khÃ¡c biá»‡t. 
- **GloVe (Global Vectors):** Thiáº¿t láº­p máº¡ng lÆ°á»›i phÃ¢n giáº£i ma tráº­n Ä‘áº¿m sá»‘ láº§n quy táº©m cáº­n ká» táº§n suáº¥t tá»« vá»±ng (Co-occurrence text mapping). NÃ³ náº¯m trong tay cáº¥u trÃºc vÄ© mÃ´ toÃ n thá»ƒ tÃ i liá»‡u.
- **Word2Vec (CBoW / Skip-gram):** Thiáº¿t láº­p mÃ´ hÃ¬nh há»“i quy trá»ng sá»‘ nháº¯m vÃ o viá»‡c Ä‘iá»n tá»« cÃ²n thiáº¿u giá»¯a bá»™ vi mÃ´ khung cá»­a lÆ°á»›i (Context windows prediction). Viá»‡c mÃ´ phá»ng chuá»—i há»c tÆ°Æ¡ng tá»± quy luáº­t Neural Networks hiá»‡n Ä‘áº¡i giÃºp Word2Vec bÃ©n nháº¡y triá»‡t Ä‘á»ƒ vá»›i cÃ¡c quy luáº­t giao thoa ngá»¯ nghÄ©a há»c (Semantic relationships). 

Theo luáº­n thuyáº¿t trÃªn, kháº£ nÄƒng thao tÃºng phÃ©p TÆ°Æ¡ng Ä‘á»“ng Loáº¡i suy ToÃ¡n há»c (Math analogies) cá»§a Word2Vec 300D Ä‘Æ°á»£c ká»³ vá»ng phÃ¡ vá»¡ ngÆ°á»¡ng cá»±c háº¡n mÃ  cÃ´ng cá»¥ GloVe 50D Ä‘á»ƒ láº¡i.

---

## 2. Kiá»ƒm Äá»‹nh Tháº¥t Báº¡i Vá»›i HÃ m Khai Khai KhÃ¡i Niá»‡m Tá»± Äá»™ng (Soft-Coded Function)

Báº±ng viá»‡c gÃ³i gÃ©m cáº¥u hÃ¬nh hÃ m Soft-coded nháº­n vÃ o Ä‘áº§u vÃ o linh hoáº¡t:
$$ 
\mathbf{V}_{\text{Analogy}} = \mathbf{V}_{\text{Word1}} - \mathbf{V}_{\text{Word2}} + \mathbf{V}_{\text{Word3}} 
$$
Thuáº­t toÃ¡n phÃ³ng chiáº¿u mÅ©i tÃªn $V_{\text{Analogy}}$ rÃ  quÃ©t qua táº­p 400.000 lÆ°á»£ng tá»« Ä‘iá»ƒn cá»§a Word2Vec thÃ´ng qua Cosine Similarity Ä‘á»ƒ xuáº¥t kho Top 10 á»©ng cá»­ viÃªn gáº§n nháº¥t.

**Kiá»ƒm Ä‘á»‹nh 1 - Sá»± tháº§n thÃ¡nh hÃ³a:**
Lá»‡nh: `Tree` so vá»›i `Leaf`  $\approx$ `?` so vá»›i `Petal`. Trá»±c giÃ¡c sinh há»c con ngÆ°á»i dá»… dÃ ng xuáº¥t kho tá»« `Flower`.
Äá»™i ngÅ© mÃ¡y há»c tráº£ vá» káº¿t quáº£ má» má»‹t: Top á»©ng cá»­ viÃªn lá»™n xá»™n cÃ¡c tá»« `Willow Tree` (CÃ¢y Liá»…u).

**Kiá»ƒm Ä‘á»‹nh 2 - Äáº£o chiá»u trá»¥c:**
Lá»‡nh: `Leaf` so vá»›i `Tree` $\approx$ `Petal` so vá»›i `Flower`.
BiÃªn Ä‘á»™ dá»± bÃ¡o cá»§a máº¡ng lÆ°á»›i tá»« vá»±ng trÆ°á»£t dá»‘c. KhÃ´ng cÃ³ báº¥t ká»³ bÃ³ng dÃ¡ng má»™t Ä‘áº¡i lÆ°á»£ng tá»« vá»±ng nÃ o náº±m trong Top 10 cháº¡m tá»›i logic Ã½ niá»‡m. 

**Kiá»ƒm Ä‘á»‹nh 3 - Logic Giáº£i Pháº«u NgÆ°á»i:**
Lá»‡nh: `Finger` so vá»›i `Hand` $\approx$ `?` so vá»›i `Foot`. ÄÃ¡p Ã¡n chuáº©n hÃ³a lÃ  `Toe` (NgÃ³n chÃ¢n).
MÃ´ hÃ¬nh toÃ¡n há»c má»›m láº¡i tá»« `Pinky` (NgÃ³n Ãºt) trÃ´i ná»•i trong khÃ´ng gian nhiá»…u vector.

---

## 3. Báº£n Cháº¥t Cá»§a Ká»¹ Thuáº­t Cá»™ng Trá»« NhÃºng

Sá»± ráº¡n ná»©t giá»¯a huyá»n thoáº¡i `King-Man+Woman` vÃ  sá»± tÃ n báº¡o cá»§a cÃ¡c phÃ©p thá»­ tá»± do ngoÃ i lá» Ä‘Ã¨ báº¹p ká»³ vá»ng cá»§a giá»›i nghiÃªn cá»©u XAI vá» kháº£ nÄƒng suy diá»…n quy náº¡p cá»§a Machine Learning chá»‰ dá»±a trÃªn má»™t Vector Ä‘Æ¡n hÆ°á»›ng.
CÃ¡c phÃ©p phÃ¢n tÃ­ch trá»« - cá»™ng Vector Analogies thá»±c cháº¥t lÃ  má»™t sá»± lÃ£ng máº¡n hÃ³a há»c thuáº­t. Sá»± diá»‡u ká»³ toÃ¡n há»c nÃ y thÆ°á»ng chá»‰ váº­n hÃ nh nhá»‹p nhÃ ng Ä‘á»‘i vá»›i nhá»¯ng táº­p tá»« ngá»¯ phá»• quÃ¡t cá»±c máº¡nh (VD: Giá»›i tÃ­nh, vÆ°Æ¡ng quyá»n, quá»‘c gia - thá»§ Ä‘Ã´) Ä‘Ã£ Ä‘Æ°á»£c cá» xÃ¡t hÃ ng trÄƒm triá»‡u láº§n trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n táº¡o thÃ nh má»™t "DÃ²ng cháº£y trá»ng tÃ¢m" cá»©ng vá»¯ng cháº¯c á»Ÿ ma tráº­n $E$. Vá»›i nhá»¯ng há»‡ thá»‘ng cáº¥u trÃºc tÆ°Æ¡ng quan nhá» vÃ  há»‘c bÃºa hÆ¡n, cÃ¡c ma tráº­n Vector thÆ°á»ng bá»‹ xÃ© rÃ£o (Vector entanglement) vÃ  khÃ´ng tuÃ¢n theo luáº­t chÆ¡i Tá»‹nh tiáº¿n Ä‘á»™ dÃ i tam giÃ¡c.

Tuy váº­y, nhá»¯ng phÃ©p tÃ­nh Vector cÄƒn nguyÃªn nháº¥t nÃ y khÃ´ng há» vá»©t Ä‘i. ChÃºng lÃ  báº£n nguyÃªn ná»n mÃ³ng Ä‘á»ƒ phÃ¡t triá»ƒn lÃªn há»‡ quy chiáº¿u siÃªu tinh vi Transformer. Táº¡i kiáº¿n trÃºc ChatGPT hiá»‡n Ä‘áº¡i, nhá»¯ng phÃ©p ma tráº­n nhÃºng cá»™ng trá»« (Vector adjustments) khÃ´ng xáº£y ra má»™t láº§n, mÃ  bá»‹ giáº±ng xÃ© nhÃ o náº·n qua 96 vÃ²ng quy há»“i Attention phi tuyáº¿n nháº±m Ä‘Ãºc ra má»™t luá»“ng suy nghÄ© sáº¯c láº¹m thay vÃ¬ chá»‰ lÃ  bá» máº·t cá»§a Vector TÄ©nh.

---

## TÃ i liá»‡u tham kháº£o

1. **Mikolov, T., et al. (2013).** *Distributed Representations of Words and Phrases and their Compositionality.* NIPS. (Khai sinh ká»¹ thuáº­t Word2Vec vÃ  phÃ©p loáº¡i suy King-Queen).
2. **Levy, O., & Goldberg, Y. (2014).** *Linguistic Regularities in Sparse and Explicit Word Representations.* CoNLL. (Chá»‰ trÃ­ch lá»— há»•ng toÃ¡n há»c vector truyá»n thá»‘ng).
3. TÃ i liá»‡u thá»±c hÃ nh láº­p trÃ¬nh *CodeChallenge soft-coded analogies in word2vec*.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero_LLM_01_CodeChallenge Cosine similarity (advanced) (part 1).md](aero_LLM_01_CodeChallenge Cosine similarity (advanced) (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_CodeChallenge Cosine similarity (advanced) (part 1).md) |
| [aero_LLM_02_CodeChallenge Cosine similarity (advanced) (part 2).md](aero_LLM_02_CodeChallenge Cosine similarity (advanced) (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_CodeChallenge Cosine similarity (advanced) (part 2).md) |
| [Theo DÃµi DÃ²ng Cháº£y Cosine Similarity TrÃªn Trá»¥c VÄƒn Báº£n ChuyÃªn Tuáº§n Tá»± (Word Sequences)](aero_LLM_03_CodeChallenge Cosine similarity in word sequences.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Cosine similarity in word sequences.md) |
| [Nghá»‡ Thuáº­t Váº½ Báº£n Äá»“ Nhiá»‡t Ma Tráº­n NhÃºng Báº±ng CÆ°á»ng Äá»™ Tá»« (Coloring Cosine Similarity)](aero_LLM_04_CodeChallenge Coloring cosine similarity.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Coloring cosine similarity.md) |
| [áº¢o áº¢nh Cá»§a TrÃ­ Tuá»‡ ToÃ¡n Há»c Trong NgÃ´n Ngá»¯: Sá»©c Máº¡nh Cá»§a Random Embeddings](aero_LLM_05_CodeChallenge Can random embeddings be interpreted.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_CodeChallenge Can random embeddings be interpreted.md) |
| [PhÆ°Æ¡ng PhÃ¡p T-SNE VÃ  Thuáº­t ToÃ¡n PhÃ¢n Cá»¥m DBSCAN: Chiáº¿u KhÃ´ng Gian Äa Chiá»u Cho LLMs](aero_LLM_06_T-SNE projection and DBSCAN clustering (theory).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_T-SNE projection and DBSCAN clustering (theory).md) |
| [PhÃ¢n Cá»¥m Ngá»¯ NghÄ©a Qua PhÃ©p Chiáº¿u t-SNE & Máº­t Äá»™ DBSCAN (Python)](aero_LLM_07_T-SNE projection and DBSCAN clustering (Python).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_T-SNE projection and DBSCAN clustering (Python).md) |
| [ThÃ¡ch Thá»©c Code: TÃ¬m Lá»— Há»•ng PhÃ¢n Cá»¥m Báº±ng Bá»™ Lá»c Báº£ng Chá»¯ CÃ¡i Chá»¯ X](aero_LLM_08_CodeChallenge cluster the x terms.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge cluster the x terms.md) |
| [PhÃ¢n RÃ£ Token, NhÃºng VÃ  PhÃ¢n Cá»¥m Biá»ƒu TÆ°á»£ng Emojis Báº±ng Äá»“ Thá»‹ Máº­t Äá»™](aero_LLM_09_CodeChallenge Tokenize, embed, and cluster happy emojis.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Tokenize, embed, and cluster happy emojis.md) |
| [PhÃ¢n TÃ­ch RSA (Representational Similarity Analysis) Giá»¯a CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_10_RSA (representational similarity analysis).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_RSA (representational similarity analysis).md) |
| [PhÃ¢n TÃ­ch Äá»™ Lá»‡ch RSA (Part 1): So SÃ¡nh Sá»± Báº¥t Äá»“ng Giá»¯a KhÃ´ng Gian GloVe 50D vÃ  300D](aero_LLM_11_CodeChallenge Compare embeddings with RSA (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Compare embeddings with RSA (part 1).md) |
| [PhÃ¢n TÃ­ch Äá»™ Lá»‡ch RSA (Part 2): Äá»‘i Chiáº¿u TÆ°Æ¡ng Quan Pearson Cho Khoáº£ng CÃ¡ch Cosine](aero_LLM_12_CodeChallenge Compare embeddings with RSA (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_CodeChallenge Compare embeddings with RSA (part 2).md) |
| [So SÃ¡nh KhÃ´ng Gian NhÃºng: Word2Vec VÃ  GPT-2 Qua PhÃ¢n TÃ­ch RSA](aero_LLM_13_CodeChallenge Word2vec vs. GPT2.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_13_CodeChallenge Word2vec vs. GPT2.md) |
| [Bá»‘ Cá»¥c Äá»“ Thá»‹ Máº¡ng (Network Graph) ThÃ´ng Qua Ma Tráº­n Cosine Similarity](aero_LLM_14_CodeChallenge Graph representation of cosine similarities.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_CodeChallenge Graph representation of cosine similarities.md) |
| [Sá»‘ Há»c Tuyáº¿n TÃ­nh vÃ  RÃºt TrÃ­ch TÆ°Æ¡ng Äá»“ng Giá»¯a CÃ¡c Tá»« NhÃºng (Word Embeddings Analogies)](aero_LLM_15_Embeddings arithmetic and analogies.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_Embeddings arithmetic and analogies.md) |
| ğŸ“Œ **[Vá»¡ Má»™ng Vá» Sá»‘ Há»c Vector TÆ°Æ¡ng ÄÆ°Æ¡ng (Soft-Coded Analogies) TrÃªn Word2Vec](aero_LLM_16_CodeChallenge soft-coded analogies in word2vec.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_16_CodeChallenge soft-coded analogies in word2vec.md) |
| [Thiáº¿t Láº­p VÃ  Diá»…n Giáº£i Trá»¥c Ngá»¯ NghÄ©a Tuyáº¿n TÃ­nh (Linear Semantic Axes)](aero_LLM_17_Creating and interpreting linear semantic axes.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_Creating and interpreting linear semantic axes.md) |
| [Khai ThÃ¡c Thuáº­t ToÃ¡n k-NN Cho TÃ¬m Kiáº¿m Tá»« Äá»“ng NghÄ©a TrÃªn BERT](aero_LLM_18_kNN for synonym-searching in BERT.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_kNN for synonym-searching in BERT.md) |
| [Cáº¡nh Tranh TÃ¬m Tá»« Äá»“ng NghÄ©a BERT vs GPT: CÆ¡ Cháº¿ Tokenization Äa KÃ½ Tá»±](aero_LLM_19_CodeChallenge BERT v GPT kNN kompetition.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_19_CodeChallenge BERT v GPT kNN kompetition.md) |
| [Sá»± Dá»‹ch Chuyá»ƒn VÃ  Äá»“ng Tá»“n Biá»ƒu Diá»…n Giá»¯a CÃ¡c KhÃ´ng Gian NhÃºng](aero_LLM_20_Research on translating embeddings spaces.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_20_Research on translating embeddings spaces.md) |
| [PhÃ¢n TÃ­ch ChÃ¹m Quang Phá»• Suy Biáº¿n (Singular Value Spectrum) Cá»§a KhÃ´ng Gian NhÃºng](aero_LLM_21_Singular value spectrum of embeddings submatrices.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_21_Singular value spectrum of embeddings submatrices.md) |
| [Ãnh Xáº¡ SVD CÃ¡c Dáº£i Äiá»ƒm NhÃºng CÃ³ Quan Há»‡ ChÃ©o](aero_LLM_22_CodeChallenge SVD projections of related embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_22_CodeChallenge SVD projections of related embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

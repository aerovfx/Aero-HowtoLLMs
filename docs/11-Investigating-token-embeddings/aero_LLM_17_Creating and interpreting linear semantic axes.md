
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [11 Investigating token embeddings](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thiáº¿t Láº­p VÃ  Diá»…n Giáº£i Trá»¥c Ngá»¯ NghÄ©a Tuyáº¿n TÃ­nh (Linear Semantic Axes)

## TÃ³m táº¯t

CÃ¡c chiá»u trong khÃ´ng gian nhÃºng cá»§a há»‡ mÃ´ hÃ¬nh tá»± há»“i quy lá»›n (Autoregressive LLMs) thÆ°á»ng Ä‘Æ°á»£c gÃ¡n cho má»™t tÃ­nh cháº¥t tháº§n bÃ­ khi mÃ  cÃ¡c nhÃ  khoa há»c cÃ³ thá»ƒ cá»™ng trá»« cÃ¡c Ä‘áº¡i lÆ°á»£ng Ä‘á»‹nh danh Ä‘á»ƒ tÃ¬m cÃ¡c gÃ³c Ä‘á»™ ngá»¯ phÃ¡p (VD: Vector TÆ°Æ¡ng lai - Vector QuÃ¡ khá»© = Trá»¥c thá»i gian). BÃ i bÃ¡o khoa há»c nÃ y chá»©ng minh táº§m áº£nh hÆ°á»Ÿng cá»§a thao tÃ¡c Chuáº©n hÃ³a hÃ¬nh há»c (Normalization) Ä‘á»‘i chiáº¿u kháº£ nÄƒng sÃ ng lá»c tÃ­n hiá»‡u vá»›i má»™t bá»™ lá»c thÃ´ sÆ¡ trong quÃ¡ trÃ¬nh lÃ m nÃ©t "Trá»¥c ngá»¯ nghÄ©a tuyáº¿n tÃ­nh" cá»§a cá»¥m nhÃºng.

---

## 1. Trá»¥c Ngá»¯ NghÄ©a: ÄÆ°á»ng Váº½ Logic Cháº¡y XuyÃªn Ma Tráº­n

KhÃ´ng gian tá»« vá»±ng Word2Vec, theo lÃ½ thuyáº¿t, chá»©a kháº£ nÄƒng biá»ƒu diá»…n nhá»¯ng khÃ¡i niá»‡m tÆ°Æ¡ng pháº£n á»Ÿ hai phÃ­a cá»§a má»™t Ä‘Æ°á»ng tháº³ng. Giáº£ sá»­ ta muá»‘n xÃ¡c láº­p má»™t **Trá»¥c Thá»i Gian (Time Axis)**, phÃ©p tÃ­nh láº¥y Ä‘iá»ƒm nÃºt (anchor points) lÃ  hai tá»a Ä‘á»™ Ä‘áº·c trÆ°ng Ä‘á»‘i láº­p "Past" vÃ  "Future":
$$
\vec{v}_{\text{TimeAxis}} = \vec{v}_{\text{future}} - \vec{v}_{\text{past}}
$$
Má»™t khi Ä‘Ã£ xÃ¡c láº­p Ä‘Æ°á»£c $\vec{v}_{\text{TimeAxis}}$, má»i vector nhÃºng $\vec{w}$ báº¥t ká»³ khi chiáº¿u (project) lÃªn trá»¥c nÃ y sáº½ tráº£ vá» há»‡ sá»‘ (projection scalar) dá»± Ä‘oÃ¡n má»©c Ä‘á»™ "thuá»™c vá» tÆ°Æ¡ng lai" hay "hoÃ i niá»‡m quÃ¡ khá»©" thÃ´ng qua phÃ©p TÃ­ch vÃ´ hÆ°á»›ng (Dot product).

---

## 2. Tiá»n Xá»­ LÃ½ HÃ¬nh Há»c (Geometric Pre-Normalization)

Cáº¡m báº«y tiá»m áº©n cá»§a viá»‡c trá»« Ä‘i hai tá»a Ä‘á»™ thÃ´ náº±m á»Ÿ "Sá»©c náº·ng vi phÃ¢n" cá»§a má»—i token. Nhá»¯ng tá»« vá»±ng thÃ´ng thÆ°á»ng nhÆ°ng vÃ´ nghÄ©a (stop words nhÆ° "the", "an", "is") chá»©a vector embeddings má» vá»›i chiá»u dÃ i chuáº©n ngáº¯n (low $L2-norm$). NgÆ°á»£c láº¡i cÃ¡c tá»« ngá»¯ cáº£nh trá»ng Ä‘iá»ƒm sáº½ cÃ³ $\vec{v}$ vá»›i chiá»u dÃ i cá»±c Ä‘áº¡i Ä‘Ã¢m xa khá»i gá»‘c tá»a Ä‘á»™ $0$.

Náº¿u ta láº¥y $\vec{v}_{\text{future}} - \vec{v}_{\text{the}}$, Ä‘Ã¡p Ã¡n sáº½ bá»‹ nghiÃªng lá»‡ch (bias) khá»•ng lá»“ vá» phÃ­a Ä‘áº§u Ä‘iá»ƒm "future" khiáº¿n cho trá»¥c khÃ´ng gian thÃ nh pháº©m bá»‹ trÆ°á»£t gÃ³c máº¥t tÃ­nh Ä‘á»‘i xá»©ng tÆ°Æ¡ng sinh. Váº¥n Ä‘á» Ä‘Æ°á»£c giáº£i quyáº¿t báº±ng viá»‡c báº¯t buá»™t **Chuáº©n HÃ³a (Normalization)** Ä‘á»™ dÃ i tá»«ng thÃ nh pháº§n trÆ°á»›c khi thá»±c hiá»‡n quy Ä‘á»•i trá»¥c:
$$
\hat{v}_{\text{future}} = \frac{\vec{v}_{\text{future}}}{\|\vec{v}_{\text{future}}\|}
$$
$$
\hat{v}_{\text{past}} = \frac{\vec{v}_{\text{past}}}{\|\vec{v}_{\text{past}}\|}
$$
Trá»¥c ngá»¯ nghÄ©a thá»±c thá»¥ (Normalized Axis) pháº£i Ä‘Æ°á»£c thiáº¿t láº­p trÃªn hai vector chuáº©n quy cÃ³ Ä‘á»™ dÃ i giá»›i háº¡n trong vÃ²ng viá»n cáº§u báº±ng $1$:
$$ 
\vec{v}_{\text{TimeAxisNorm}} = \hat{v}_{\text{future}} - \hat{v}_{\text{past}} 
$$
TÃ­nh khÆ°á»¡ng bá»©c khÃ´ng gian nÃ y tÆ°á»›c Ä‘i áº£o áº£nh phÆ°Æ¡ng sai tá»« Ä‘á»™ lá»›n module, khiáº¿n há»‡ quy chiáº¿u chá»‰ táº­p trung vÃ o khÃ¡c biá»‡t phÆ°Æ¡ng hÆ°á»›ng gÃ³c (Cosine direction divergence).

---

## 3. Há»‡ Quáº£ Tá»« Nhá»¯ng Bá»™ Lá»c CÄƒn Báº£n (Tokens Filtering Rule)

Khi tiáº¿n hÃ nh cháº¥m Ä‘iá»ƒm (Cosine similarity mapping) má»™t "Trá»¥c ranh giá»›i thá»i gian" vá»›i má»™t bá»™ tá»« Ä‘iá»ƒn lÃªn Ä‘áº¿n hÃ ng triá»‡u tá»« vá»±ng cáº¯t ra tá»« Wikipedia, má»™t sá»‘ káº¿t quáº£ láº¡ láº«m Ã¢m cá»±c cÃ³ thá»ƒ ná»• ra (nhá»¯ng liÃªn káº¿t token nhiá»…u nhÆ° Ä‘á»‹a chá»‰ URL, kÃ½ tá»± lá»—i, chá»¯ Latin viáº¿t táº¯t trá»™n láº«n Ä‘iá»ƒm ngáº«u nhiÃªn). Äá»ƒ khá»­ cÃ¡c yáº¿u tá»‘ nhiá»…u nÃ y, logic Lá»c nhÃ£n tá»« vá»±ng (Filters) Ä‘Æ°á»£c bá»• sung:
- **Chuáº©n Cá»± ly Chá»¯ cÃ¡i:** Tá»« vá»±ng yÃªu cáº§u $> 2$  letters.
- **Biá»ƒu thá»©c chÃ­nh quy (Alphanumeric Mask):** Chá»‰ nhá»¯ng máº£ng kÃ­ tá»± hoÃ n toÃ n cáº¥u táº¡o tá»« chá»¯ há»‡ ABC.

Bá»™ lá»c nÃ y quÃ©t qua ma tráº­n tá»« nhÃºng $E \in \mathbb{R}^{V \times D}$ (vá»›i $V=3.000.000$). Káº¿t quáº£ loáº¡i bá» Ä‘áº¿n $70\%$ khá»‘i lÆ°á»£ng vocab cá»§a Word2vec chá»‰ chá»©a rÃ¡c thÃ´ng tin (noise artifacts). Máº£ng rÃºt lÃµi giÃºp tÄƒng tá»‘c Ä‘áº¡o hÃ m bá»™ nhá»› trÃªn $E_{filtered}$, cho phÃ©p $\text{CosSim}(\vec{v}_{\text{axis}}, E_{filtered}^T)$ tÃ¬m trÃºng nhá»¯ng váº¡ch Ä‘Ã­ch tá»« vá»±ng ("pass performance", "yesterday") náº±m Ä‘Ãºng phá»• Ã¢m cá»§a trá»¥c mÃ  khÃ´ng dÃ­nh báº«y ngáº«u nhiÃªn (false positive correlation).

---

## 4. Káº¿t luáº­n

CÃ¡c thá»±c thá»ƒ LLMs vá»›i biá»ƒu Ä‘á»“ Attention khÃ´ng hoáº¡t Ä‘á»™ng báº±ng phÃ¡p thuáº­t - chÃºng lÃ  tá»• há»£p bÄƒm rÃ£ nhá»¯ng lá»›p Norm (Normalization) xáº¿p chá»“ng nhau vÃ  nhá»¯ng thá»§ thuáº­t mÃ ng lá»c nhá» cáº¥u thÃ nh má»™t kiáº¿n trÃºc phi tuyáº¿n ká»³ vÄ©. TÃ­nh toÃ¡n cÃ¡c Ä‘iá»ƒm lÃ¢n cáº­n ngá»¯ nghÄ©a trÃªn máº¡ng NÆ¡-ron yÃªu cáº§u sá»± nghiÃªm ngáº·t Ä‘á»‹nh chuáº©n (vector normalizations) nháº±m khÃ´ng Ä‘á»ƒ cho tÃ­nh Ä‘a dáº¡ng ngáº«u nhiÃªn cá»§a khá»‘i lÆ°á»£ng tá»a Ä‘á»™ chiá»…m lÄ©nh Ä‘á»™ cÃ¢n báº±ng tuyáº¿n tÃ­nh cá»§a hÃ¬nh há»c ngÃ´n ngá»¯ há»c.

---

## TÃ i liá»‡u tham kháº£o

1. **Bolukbasi, T., et al. (2016).** *Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings*. NeurIPS. (Ãp dá»¥ng trá»¥c ngá»¯ nghÄ©a khá»­ thiÃªn vá»‹ giá»›i tÃ­nh).
2. **Li, Y., et al. (2015).** *Representation Learning for Semantic Composition*. ACL.
3. TÃ i liá»‡u bÃ i giáº£ng *Investigating token embeddings - Creating and interpreting linear semantic axes*.
<!-- Aero-Footer-Start -->
---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

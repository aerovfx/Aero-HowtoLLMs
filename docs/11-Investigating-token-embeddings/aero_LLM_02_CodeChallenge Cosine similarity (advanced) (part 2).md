
<!-- Aero-Navigation-Start -->
[üè† Home](../../index.md) > [11 Investigating token embeddings](../index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../index.md)
- [üìö Module 01: LLM Course](../../01-LLM_Course/index.md)
- [üî¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../04-buildGPT/index.md)
- [üéØ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [üîç Module 19: AI Safety](../../19-AI-safety/index.md)
- [üêç Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Cosine Similarity n√¢ng cao (Ph·∫ßn 2):

Ph√¢n t√≠ch h√¨nh h·ªçc x√°c su·∫•t, anisotropy v√† t·ªëi ∆∞u ho√° trong kh√¥ng gian embedding chi·ªÅu cao

‚∏ª

T√≥m t·∫Øt

Ti·∫øp n·ªëi ph·∫ßn tr∆∞·ªõc v·ªÅ Cosine Similarity, b√†i vi·∫øt n√†y m·ªü r·ªông ph√¢n t√≠ch sang c√°c v·∫•n ƒë·ªÅ n√¢ng cao bao g·ªìm: hi·ªán t∆∞·ª£ng anisotropy trong embedding space, ph√¢n ph·ªëi g√≥c trong kh√¥ng gian chi·ªÅu cao, ·∫£nh h∆∞·ªüng c·ªßa chu·∫©n h√≥a (normalization), whitening transformation, v√† vai tr√≤ c·ªßa cosine similarity trong contrastive learning v√† retrieval hi·ªán ƒë·∫°i. C√°c c√¥ng th·ª©c to√°n h·ªçc ƒë∆∞·ª£c tr√¨nh b√†y nh·∫±m l√†m r√µ b·∫£n ch·∫•t h√¨nh h·ªçc ‚Äì x√°c su·∫•t c·ªßa c√°c embedding ƒë∆∞·ª£c hu·∫•n luy·ªán b·ªüi m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLMs).

‚∏ª

1. Gi·ªõi thi·ªáu

Embedding kh√¥ng c√≤n l√† vector ng·∫´u nhi√™n ƒë∆°n gi·∫£n; ch√∫ng ƒë∆∞·ª£c hu·∫•n luy·ªán th√¥ng qua t·ªëi ∆∞u h√≥a gradient, d·∫´n ƒë·∫øn c·∫•u tr√∫c h√¨nh h·ªçc ƒë·∫∑c bi·ªát. C√°c t·ªï ch·ª©c nh∆∞:
	‚Ä¢	OpenAI
	‚Ä¢	Google Research
	‚Ä¢	Meta AI

ƒë√£ ·ª©ng d·ª•ng cosine similarity l√†m l√µi cho:
	‚Ä¢	Semantic search
	‚Ä¢	Retrieval-Augmented Generation (RAG)
	‚Ä¢	Vector database indexing

Tuy nhi√™n, embedding th·ª±c t·∫ø kh√¥ng ph√¢n b·ªë ƒë·ªÅu trong kh√¥ng gian \mathbb{R}^d.

‚∏ª

2. Ph√¢n ph·ªëi g√≥c trong kh√¥ng gian chi·ªÅu cao

Gi·∫£ s·ª≠:

\mathbf{x}, \mathbf{y} \sim \mathcal{N}(0, I_d)

Sau chu·∫©n h√≥a:

\tilde{\mathbf{x}} = \frac{\mathbf{x}}{\|\mathbf{x}\|}

Ph√¢n ph·ªëi c·ªßa:

\cos \theta = \tilde{\mathbf{x}} \cdot \tilde{\mathbf{y}}

Khi d \to \infty:

\cos \theta \xrightarrow{p} 0

V√† ph∆∞∆°ng sai:

Var(\cos \theta) \approx \frac{1}{d}

ƒêi·ªÅu n√†y gi·∫£i th√≠ch v√¨ sao trong embedding dimension l·ªõn (512‚Äì4096), c√°c vector ng·∫´u nhi√™n g·∫ßn nh∆∞ tr·ª±c giao.

‚∏ª

3. Hi·ªán t∆∞·ª£ng Anisotropy

3.1 ƒê·ªãnh nghƒ©a

Anisotropy x·∫£y ra khi embedding t·∫≠p trung quanh m·ªôt h∆∞·ªõng ∆∞u th·∫ø.

Gi·∫£ s·ª≠ trung b√¨nh embedding:

\mu = \mathbb{E}[\mathbf{x}]

N·∫øu:

\|\mu\| \gg 0

‚Üí embedding l·ªách h∆∞·ªõng.

‚∏ª

3.2 H·ªá qu·∫£

Cosine similarity gi·ªØa hai vector b·∫•t k·ª≥:

\cos(\mathbf{x}, \mathbf{y})

b·ªã chi ph·ªëi b·ªüi th√†nh ph·∫ßn chung theo h∆∞·ªõng \mu.

‚∏ª

4. Centering v√† Whitening

4.1 Centering

Lo·∫°i b·ªè trung b√¨nh:

\mathbf{x}' = \mathbf{x} - \mu

‚∏ª

4.2 Whitening Transformation

Cho ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai:

\Sigma = \mathbb{E}[(\mathbf{x}-\mu)(\mathbf{x}-\mu)^T]

Whitening:

\mathbf{x}_{white} = \Sigma^{-1/2}(\mathbf{x}-\mu)

Khi ƒë√≥:

Cov(\mathbf{x}_{white}) = I

ƒêi·ªÅu n√†y gi√∫p ph√¢n ph·ªëi ƒë·ªìng ƒë·ªÅu h∆°n trong kh√¥ng gian.

‚∏ª

5. Cosine Similarity v√† Contrastive Learning

Trong contrastive loss:

\mathcal{L}_i =
- \log
\frac{\exp(\cos(\mathbf{z}_i,\mathbf{z}_j)/\tau)}
{\sum_k \exp(\cos(\mathbf{z}_i,\mathbf{z}_k)/\tau)}

Trong ƒë√≥:
	‚Ä¢	\tau: temperature
	‚Ä¢	\mathbf{z}: embedding ƒë√£ chu·∫©n h√≥a

Khi \tau \to 0:

\exp(\cos/\tau)

khu·∫øch ƒë·∫°i s·ª± kh√°c bi·ªát g√≥c nh·ªè.

‚∏ª

6. Cosine Similarity v√† Maximum Likelihood

Gi·∫£ s·ª≠ embedding query q v√† document d:

P(d|q) =
\frac{\exp(\alpha \cos(q,d))}
{\sum_j \exp(\alpha \cos(q,d_j))}

ƒê√¢y ch√≠nh l√† softmax over cosine scores.

H√†m log-likelihood:

\mathcal{L} =
\sum_i \log P(d_i|q_i)

‚∏ª

7. Ph√¢n t√≠ch Gradient trong Kh√¥ng gian Chu·∫©n h√≥a

Cho:

S = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\|\|\mathbf{y}\|}

Gradient theo \mathbf{x}:

\frac{\partial S}{\partial \mathbf{x}} =
\frac{\mathbf{y}}{\|\mathbf{x}\|\|\mathbf{y}\|}
-
\frac{(\mathbf{x}\cdot\mathbf{y})\mathbf{x}}
{\|\mathbf{x}\|^3\|\mathbf{y}\|}

Gradient n√†y g·ªìm hai th√†nh ph·∫ßn:
	1.	H∆∞·ªõng v·ªÅ ph√≠a \mathbf{y}
	2.	Th√†nh ph·∫ßn ƒëi·ªÅu ch·ªânh ƒë·ªô l·ªõn

ƒêi·ªÅu n√†y l√†m embedding t·ª± ƒë·ªông chu·∫©n h√≥a h∆∞·ªõng thay v√¨ ƒë·ªô l·ªõn.

‚∏ª

8. Li√™n h·ªá v·ªõi Information Geometry

Theo Pattern Recognition and Machine Learning:

Kho·∫£ng c√°ch Bregman v·ªõi entropy:

D_\phi(p,q) =
\phi(p) - \phi(q) - \nabla\phi(q)^T(p-q)

Cosine similarity kh√¥ng ph·∫£i metric Bregman nh∆∞ng c√≥ th·ªÉ xem nh∆∞ metric g√≥c tr√™n hypersphere:

S^{d-1} =
\{\mathbf{x} \in \mathbb{R}^d : \|\mathbf{x}\| = 1\}

‚∏ª

9. ·ª®ng d·ª•ng trong Vector Database

C√°c h·ªá th·ªëng nh∆∞:
	‚Ä¢	FAISS (Meta AI)
	‚Ä¢	ScaNN (Google Research)

s·ª≠ d·ª•ng cosine similarity ho·∫∑c inner product.

N·∫øu vector chu·∫©n h√≥a:

\mathbf{x} \cdot \mathbf{y}
=
\cos(\mathbf{x},\mathbf{y})

‚Üí t·ªëi ∆∞u t√≠nh to√°n b·∫±ng Approximate Nearest Neighbor.

‚∏ª

10. So s√°nh v·ªõi c√°c metric kh√°c

Metric	C√¥ng th·ª©c	Nh·∫°y ƒë·ªô l·ªõn	Ph√π h·ª£p NLP
Euclidean	\|\mathbf{x}-\mathbf{y}\|	C√≥	Trung b√¨nh
Dot Product	x \cdot y	C√≥	Cao
Cosine	\frac{x\cdot y}{\|x\|\|y\|}	Kh√¥ng	R·∫•t cao


‚∏ª

11. Th·∫£o lu·∫≠n

∆Øu ƒëi·ªÉm
	‚Ä¢	B·∫•t bi·∫øn theo scale
	‚Ä¢	Ph√π h·ª£p cho embedding chu·∫©n h√≥a
	‚Ä¢	T·ªëi ∆∞u cho retrieval

H·∫°n ch·∫ø
	‚Ä¢	Kh√¥ng x·ª≠ l√Ω t·ªët anisotropy
	‚Ä¢	Kh√¥ng ƒëo quan h·ªá phi tuy·∫øn
	‚Ä¢	D·ªÖ b·ªã cluster collapse n·∫øu kh√¥ng regularize

‚∏ª

12. K·∫øt lu·∫≠n

Cosine similarity trong embedding hi·ªán ƒë·∫°i kh√¥ng ch·ªâ l√† ph√©p ƒëo h√¨nh h·ªçc ƒë∆°n gi·∫£n m√† l√†:
	‚Ä¢	Metric tr√™n hypersphere
	‚Ä¢	Th√†nh ph·∫ßn c·ªët l√µi c·ªßa contrastive learning
	‚Ä¢	C∆° s·ªü cho retrieval v√† vector database

Hi·ªÉu r√µ ph√¢n ph·ªëi g√≥c, anisotropy v√† whitening gi√∫p c·∫£i thi·ªán ƒë√°ng k·ªÉ ch·∫•t l∆∞·ª£ng embedding trong LLM.

‚∏ª

T√†i li·ªáu tham kh·∫£o
	1.	Bishop (2006). Pattern Recognition and Machine Learning.
	2.	Cover & Thomas (2006). Elements of Information Theory.
	3.	Mikolov et al. (2013). Word2Vec.
	4.	Chen et al. (2020). SimCLR.
	5.	Reimers & Gurevych (2019). Sentence-BERT.
<!-- Aero-Footer-Start -->
---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

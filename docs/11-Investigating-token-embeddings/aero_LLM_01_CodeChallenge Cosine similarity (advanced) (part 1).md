
<!-- Aero-Navigation-Start -->
[üè† Home](../../index.md) > [11 Investigating token embeddings](../index.md)

---
### üß≠ ƒêi·ªÅu h∆∞·ªõng nhanh

- [üè† C·ªïng t√†i li·ªáu](../../index.md)
- [üìö Module 01: LLM Course](../../01-LLM_Course/index.md)
- [üî¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [üèóÔ∏è Module 04: Build GPT](../../04-buildGPT/index.md)
- [üéØ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [üîç Module 19: AI Safety](../../19-AI-safety/index.md)
- [üêç Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
Ph√¢n t√≠ch Cosine Similarity n√¢ng cao trong kh√¥ng gian embedding

C∆° s·ªü to√°n h·ªçc, h√¨nh h·ªçc vector v√† ·ª©ng d·ª•ng trong m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn

‚∏ª

T√≥m t·∫Øt

Cosine Similarity l√† m·ªôt trong nh·ªØng th∆∞·ªõc ƒëo c·ªët l√µi trong x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP), ƒë·∫∑c bi·ªát khi l√†m vi·ªác v·ªõi vector embedding c√≥ chi·ªÅu cao. B√†i vi·∫øt n√†y tr√¨nh b√†y n·ªÅn t·∫£ng to√°n h·ªçc c·ªßa Cosine Similarity, m·ªü r·ªông sang c√°c ph√¢n t√≠ch h√¨nh h·ªçc trong kh√¥ng gian Hilbert, m·ªëi li√™n h·ªá v·ªõi chu·∫©n h√≥a vector, ph√¢n ph·ªëi x√°c su·∫•t trong embedding space, v√† ·ª©ng d·ª•ng trong retrieval, semantic search v√† ƒë√°nh gi√° m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLMs). Ngo√†i ra, b√†i vi·∫øt b·ªï sung c√°c c√¥ng th·ª©c minh h·ªça v√† li√™n h·ªá v·ªõi l√Ω thuy·∫øt th√¥ng tin.

‚∏ª

1. Gi·ªõi thi·ªáu

Trong NLP hi·ªán ƒë·∫°i, vƒÉn b·∫£n ƒë∆∞·ª£c √°nh x·∫° sang vector trong kh√¥ng gian \mathbb{R}^d th√¥ng qua embedding models. C√°c t·ªï ch·ª©c nh∆∞:
	‚Ä¢	OpenAI
	‚Ä¢	Google Research
	‚Ä¢	Meta AI

ƒë√£ ph√°t tri·ªÉn c√°c h·ªá embedding cho:
	‚Ä¢	Semantic search
	‚Ä¢	Retrieval-augmented generation (RAG)
	‚Ä¢	Clustering
	‚Ä¢	Similarity detection

Trong c√°c h·ªá n√†y, Cosine Similarity l√† th∆∞·ªõc ƒëo chu·∫©n ƒë·ªÉ so s√°nh hai vector.

‚∏ª

2. ƒê·ªãnh nghƒ©a Cosine Similarity

Cho hai vector \mathbf{x}, \mathbf{y} \in \mathbb{R}^d:

\text{cosine\_sim}(\mathbf{x}, \mathbf{y}) =
\frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \|\mathbf{y}\|}

Trong ƒë√≥:
	‚Ä¢	T√≠ch v√¥ h∆∞·ªõng:

\mathbf{x} \cdot \mathbf{y} = \sum_{i=1}^{d} x_i y_i
	‚Ä¢	Chu·∫©n Euclid:

\|\mathbf{x}\| = \sqrt{\sum_{i=1}^{d} x_i^2}

‚∏ª

3. Di·ªÖn gi·∫£i h√¨nh h·ªçc

Cosine similarity ƒëo cos c·ªßa g√≥c gi·ªØa hai vector:

\cos \theta = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\|\|\mathbf{y}\|}

Gi√° tr·ªã:
	‚Ä¢	1 ‚Üí c√πng h∆∞·ªõng
	‚Ä¢	0 ‚Üí tr·ª±c giao
	‚Ä¢	-1 ‚Üí ng∆∞·ª£c h∆∞·ªõng

Trong embedding NLP, vector th∆∞·ªùng ƒë∆∞·ª£c chu·∫©n h√≥a:

\tilde{\mathbf{x}} = \frac{\mathbf{x}}{\|\mathbf{x}\|}

Khi ƒë√≥:

\text{cosine\_sim}(\mathbf{x}, \mathbf{y}) =
\tilde{\mathbf{x}} \cdot \tilde{\mathbf{y}}

‚∏ª

4. Kh√¥ng gian chi·ªÅu cao v√† hi·ªán t∆∞·ª£ng t·∫≠p trung

Trong kh√¥ng gian chi·ªÅu cao d \gg 1:
	‚Ä¢	C√°c vector ng·∫´u nhi√™n c√≥ xu h∆∞·ªõng g·∫ßn tr·ª±c giao
	‚Ä¢	G√≥c gi·ªØa hai vector ng·∫´u nhi√™n ti·ªám c·∫≠n 90^\circ

Theo l√Ω thuy·∫øt x√°c su·∫•t:

N·∫øu x_i, y_i \sim \mathcal{N}(0,1)

\mathbb{E}[\mathbf{x} \cdot \mathbf{y}] = 0

Var(\mathbf{x} \cdot \mathbf{y}) = d

Sau chu·∫©n h√≥a:

\mathbb{E}[\cos \theta] \approx 0

Hi·ªán t∆∞·ª£ng n√†y g·ªçi l√† concentration of measure.

‚∏ª

5. Quan h·ªá v·ªõi kho·∫£ng c√°ch Euclid

Kho·∫£ng c√°ch Euclid:

\|\mathbf{x} - \mathbf{y}\|^2 =
\|\mathbf{x}\|^2 + \|\mathbf{y}\|^2 - 2\mathbf{x}\cdot\mathbf{y}

N·∫øu chu·∫©n h√≥a:

\|\tilde{\mathbf{x}} - \tilde{\mathbf{y}}\|^2 =
2 - 2\cos \theta

Do ƒë√≥:

\cos \theta = 1 - \frac{1}{2}\|\tilde{\mathbf{x}} - \tilde{\mathbf{y}}\|^2

‚Üí Cosine similarity t∆∞∆°ng ƒë∆∞∆°ng v·ªõi Euclidean distance trong kh√¥ng gian chu·∫©n h√≥a.

‚∏ª

6. Cosine Similarity trong embedding x√°c su·∫•t

M·ªôt embedding model √°nh x·∫° vƒÉn b·∫£n t th√†nh vector:

f_\theta(t) \in \mathbb{R}^d

X√°c su·∫•t ch·ªçn t√†i li·ªáu d_i trong retrieval:

P(d_i|q) =
\frac{\exp(\alpha \cdot \cos(f(q), f(d_i)))}
{\sum_j \exp(\alpha \cdot \cos(f(q), f(d_j)))}

Trong ƒë√≥:
	‚Ä¢	\alpha l√† temperature scaling

‚∏ª

7. Li√™n h·ªá v·ªõi Information Theory

Theo Elements of Information Theory:

Mutual information gi·ªØa hai vector embedding:

I(X;Y) =
\mathbb{E}\left[
\log \frac{P(X,Y)}{P(X)P(Y)}
\right]

Cosine similarity c√≥ th·ªÉ xem nh∆∞ x·∫•p x·ªâ th√¥ c·ªßa s·ª± ph·ª• thu·ªôc tuy·∫øn t√≠nh gi·ªØa hai bi·∫øn.

‚∏ª

8. Cosine Similarity v√† Loss Function

Trong contrastive learning (v√≠ d·ª• SimCLR):

\mathcal{L} =
- \log
\frac{\exp(\cos(\mathbf{x}_i,\mathbf{x}_j)/\tau)}
{\sum_k \exp(\cos(\mathbf{x}_i,\mathbf{x}_k)/\tau)}

Trong ƒë√≥:
	‚Ä¢	\tau l√† temperature
	‚Ä¢	(\mathbf{x}_i,\mathbf{x}_j) l√† positive pair

‚∏ª

9. Ph√¢n t√≠ch gradient

Gi·∫£ s·ª≠:

S = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\|\|\mathbf{y}\|}

Gradient theo \mathbf{x}:

\frac{\partial S}{\partial \mathbf{x}} =
\frac{\mathbf{y}}{\|\mathbf{x}\|\|\mathbf{y}\|}
-
\frac{(\mathbf{x}\cdot\mathbf{y})\mathbf{x}}
{\|\mathbf{x}\|^3\|\mathbf{y}\|}

ƒêi·ªÅu n√†y cho th·∫•y qu√° tr√¨nh t·ªëi ∆∞u s·∫Ω:
	‚Ä¢	K√©o vector c√πng h∆∞·ªõng l·∫°i g·∫ßn
	‚Ä¢	ƒê·∫©y vector kh√°c h∆∞·ªõng ra xa

‚∏ª

10. ·ª®ng d·ª•ng trong LLM

C√°c ·ª©ng d·ª•ng th·ª±c t·∫ø:
	‚Ä¢	Semantic Search
	‚Ä¢	Retrieval-Augmented Generation
	‚Ä¢	Clustering c√¢u h·ªèi
	‚Ä¢	Detect duplicate content

C√°c t·ªï ch·ª©c nh∆∞ Stanford University v√† MIT ƒë√£ s·ª≠ d·ª•ng cosine similarity trong c√°c h·ªá th·ªëng IR v√† NLP hi·ªán ƒë·∫°i.

‚∏ª

11. H·∫°n ch·∫ø
	1.	Kh√¥ng nh·∫°y v·ªõi ƒë·ªô l·ªõn vector
	2.	Kh√¥ng n·∫Øm b·∫Øt quan h·ªá phi tuy·∫øn
	3.	B·ªã ·∫£nh h∆∞·ªüng b·ªüi anisotropy trong embedding space

M·ªôt s·ªë nghi√™n c·ª©u ƒë·ªÅ xu·∫•t:
	‚Ä¢	Whitening transformation
	‚Ä¢	Centering embeddings
	‚Ä¢	Angular margin loss

‚∏ª

12. K·∫øt lu·∫≠n

Cosine Similarity l√† th∆∞·ªõc ƒëo h√¨nh h·ªçc c∆° b·∫£n nh∆∞ng c·ª±c k·ª≥ hi·ªáu qu·∫£ trong NLP hi·ªán ƒë·∫°i. Trong kh√¥ng gian embedding chi·ªÅu cao, n√≥:
	‚Ä¢	·ªîn ƒë·ªãnh
	‚Ä¢	D·ªÖ t√≠nh to√°n
	‚Ä¢	Ph√π h·ª£p cho retrieval

Tuy nhi√™n, c·∫ßn k·∫øt h·ª£p v·ªõi chu·∫©n h√≥a v√† k·ªπ thu·∫≠t regularization ƒë·ªÉ ƒë·∫°t hi·ªáu nƒÉng t·ªëi ∆∞u.

‚∏ª

T√†i li·ªáu tham kh·∫£o
	1.	Cover & Thomas (2006). Elements of Information Theory.
	2.	Bishop (2006). Pattern Recognition and Machine Learning.
	3.	Chen et al. (2020). SimCLR: A Simple Framework for Contrastive Learning.
	4.	Mikolov et al. (2013). Word2Vec.
	5.	Reimers & Gurevych (2019). Sentence-BERT.
<!-- Aero-Footer-Start -->
---
## ü§ù Li√™n h·ªá & ƒê√≥ng g√≥p
D·ª± √°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **Pixibox**. M·ªçi ƒë√≥ng g√≥p v·ªÅ n·ªôi dung v√† m√£ ngu·ªìn ƒë·ªÅu ƒë∆∞·ª£c ch√†o ƒë√≥n.

> *"Ki·∫øn th·ª©c l√† ƒë·ªÉ chia s·∫ª. H√£y c√πng nhau x√¢y d·ª±ng c·ªông ƒë·ªìng AI v·ªØng m·∫°nh!"* üöÄ

*C·∫≠p nh·∫≠t t·ª± ƒë·ªông b·ªüi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

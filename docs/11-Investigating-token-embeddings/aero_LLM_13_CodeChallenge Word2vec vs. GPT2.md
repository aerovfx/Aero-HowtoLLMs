
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../../index.md) > [11 Investigating token embeddings](../index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../../index.md)
- [ğŸ“š Module 01: LLM Course](../../01-LLM_Course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../../02-Words-to-tokens-to-numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../../04-buildGPT/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../../07-Fine-tune-pretrained-models/index.md)
- [ğŸ” Module 19: AI Safety](../../19-AI-safety/index.md)
- [ğŸ Module 20: Python for AI](../../20-Python-Colab-notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# So SÃ¡nh KhÃ´ng Gian NhÃºng: Word2Vec VÃ  GPT-2 Qua PhÃ¢n TÃ­ch RSA

## TÃ³m táº¯t

Má»™t rÃ o cáº£n chÃ­ máº¡ng trong NghiÃªn cá»©u Dá»¯ liá»‡u VÄƒn báº£n (NLP) lÃ  viá»‡c xÃ¡c nháº­n cháº¥t lÆ°á»£ng tÆ°Æ¡ng quan giá»¯a hai cá»— mÃ¡y sá»Ÿ há»¯u chiá»u kÃ­ch nhÃºng (Embeddings dimension size) Ä‘Ã´i Ä‘Å©a lá»‡ch. Ma tráº­n cá»§a mÃ´ hÃ¬nh Ä‘áº¡i cÆ°Æ¡ng Word2Vec cÃ³ 300 chiá»u áº©n (300D), trong khi GPT-2 náº·ng ná» sá»Ÿ há»¯u 768 chiá»u nhÃºng (768D). LÃ m tháº¿ nÃ o Ä‘á»ƒ giáº£i pháº«u vÃ  cháº©n Ä‘oÃ¡n liá»‡u GPT-2 vÃ  Word2Vec cÃ³ chia sáº» chung má»™t "triáº¿t há»c toÃ¡n há»c" ngÃ´n tá»« hay khÃ´ng? BÃ i bÃ¡o nÃ y sáº½ váº­n dá»¥ng má»™t Ä‘Æ°á»ng vÃ²ng báº±ng giáº£i tÃ­ch khÃ´ng gian thÃ´ng qua thá»§ thuáº­t káº¿t chiáº¿u Há»‡ sá»‘ TÆ°Æ¡ng Quan Pearson ná»™i hÃ m, ná»n táº£ng cá»§a phÆ°Æ¡ng thá»©c **RSA (Representational Similarity Analysis)**.

---

## 1. Thiáº¿t Láº­p Äiá»ƒm Khá»›p Giao (Intersection Point Matching)

Sá»± so sÃ¡nh hai Ä‘a hÃ¬nh há»c khÃ´ng gian báº¯t buá»™c pháº£i Ä‘Æ°á»£c gáº¯n káº¿t trÃªn má»™t táº­p Ä‘á»‘i tÆ°á»£ng con neo Ä‘áº­u duy nháº¥t. PhÃ©p lá»c Ä‘Æ°á»£c thiáº¿t láº­p thÃ´ng qua phÃ¢n tÃ¡ch danh sÃ¡ch tá»« khÃ³a á»Ÿ cáº£ hai tá»‡p tá»« Ä‘iá»ƒn (Vocab Arrays) cá»§a hai Tokenizer (Word2vec token list vÃ  GPT tokenizer vocab). Giáº£ sá»­ há»‡ thá»‘ng thiáº¿t láº­p bá»™ Ä‘áº¿m quÃ©t trÃ­ch lá»c (iteration filtering) 100 tá»« cÃ³ sá»‘ lÆ°á»£ng kÃ½ tá»± chÃ­nh xÃ¡c báº±ng 6 ($length = 6$ letters).

Thuáº­t toÃ¡n dÃ² ngÆ°á»£c Try-Catch exception sáº½ táº¡o Ä‘Æ°á»£c má»™t táº­p ma tráº­n trung gian gá»“m sá»‘ lÆ°á»£ng $N=100$ chá»¯ khá»›p lá»‡nh cÃ³ máº·t trong cáº£ 2 tá»« Ä‘iá»ƒn báº¥t cháº¥p sá»± lá»‡ch pha cá»§a chá»‰ má»¥c Index, triá»‡t tiÃªu má»i biáº¿n dá»‹ cÃ¡c pháº§n phá»¥ kiá»‡n token lá»—i hoáº·c khoáº£ng trá»‘ng áº£o (Spaces issues).

---

## 2. Kiáº¿n Thiáº¿t Khá»‘i PhÃ¢n Giáº£i Cá»¥c Bá»™

Báº¥t lá»±c hoÃ n toÃ n trÆ°á»›c phÃ©p trá»« hoáº·c cá»™ng tuyáº¿n tÃ­nh giá»¯a má»™t vector 300D vÃ  768D, phÆ°Æ¡ng phÃ¡p láº¥y Ä‘áº¡o chÃ©o báº¯t Ä‘áº§u vá»›i tÃ­nh Ä‘á»™c láº­p tá»«ng phe khÃ´ng gian má»™t.
TrÃ­ch láº¥y cá»¥m thÃ´ng tin vector cá»§a $N=100$ token trong hai há»™p khÃ´ng gian, Ã¡p dá»¥ng ma tráº­n tÃ­ch vÃ´ hÆ°á»›ng khoáº£ng cÃ¡ch chÃ©o Cosine Similarity:
$$ 
S_{W2V} = \text{CosineSim}(E_{\text{w2v-100}}) \in \mathbb{R}^{100 \times 100}
$$
$$ 
S_{GPT2} = \text{CosineSim}(E_{\text{gpt2-100}}) \in \mathbb{R}^{100 \times 100}
$$

**Cháº¯t Cáº¥t Äáº¡i LÆ°á»£ng (Upper Triangular Tiling):** 
Dá»c theo chÃ©o chÃ­nh (Diagonal elements), táº¥t cáº£ cÃ¡c thÃ´ng sá»‘ Ä‘á»u vÃ´ nghÄ©a vÃ¬ chÃºng luÃ´n $\equiv 1.0$ (Tá»± soi chiáº¿u gÆ°Æ¡ng). TÆ°Æ¡ng tá»±, máº·t Ä‘á»‘i xá»©ng chÃ©o dÆ°á»›i (Lower triangular) cÅ©ng lÃ  thÃ´ng tin vi pháº¡m lá»—i dÆ° thá»«a. Do Ä‘Ã³, chá»‰ mÃ´t máº£nh tam giÃ¡c trÃªn cÃ¹ng (Upper components extract) cÃ³ trá»‹ sá»‘ vÃ´ hÆ°á»›ng $\frac{100 \times 99}{2} = 4950$ Ä‘iá»ƒm dá»¯ liá»‡u thÃ´ Ä‘Æ°á»£c dÃ n pháº³ng thÃ nh vector dÃ¢y má»™t chiá»u $v_{w2v}$ vÃ  $v_{gpt2}$.

---

## 3. Pearson Correlation LÃªn NgÃ´i Cá»§a Sá»± Phi Tuyáº¿n

ÄÃ¢y chÃ­nh lÃ  Ä‘iá»ƒm giao mÃ¹a cá»§a phÃ¢n tÃ­ch. Liá»‡u chÃºng ta cÃ³ nÃªn lÃ m má»™t phÃ©p Ä‘o khoáº£ng cÃ¡ch Cosine Similarity giá»¯a $v_{w2v}$ vÃ  $v_{gpt2}$ Ä‘á»ƒ cho RSA Score khÃ´ng? Cáº¥u trÃºc cá»§a máº¡ng nÆ¡-ron há»“i Ä‘Ã¡p: **KhÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng Cosine Similarity cho cáº¥u hÃ¬nh RSA so sÃ¡nh, Ä‘iá»ƒm sá»‘ nÃ y luÃ´n luÃ´n pháº£i cháº¡y báº±ng chuáº©n Pearson Correlation.**

Äiá»u nÃ y xáº£y ra do Ä‘á»‹nh dáº¡ng KhÃ´ng gian Dá»‹ch tÃ¢m Dá»‹ hÆ°á»›ng (Distribution offsets deviation): 
Quang phá»• Cosine cá»§a Word2Vec luÃ´n Ä‘Æ°á»£c chuáº©n hÃ³a rá»™ng rÃ£i náº±m giá»¯a khu vá»±c khoáº£ng $[ -0.2 , 0.5 ]$. Trong khi Ä‘Ã³, tÃ­nh cháº¥t khá»‘i lÆ°á»£ng Ä‘á»“ thá»‹ há»c máº¡ng biáº¿n Ã¡p tá»± há»“i quy (Autoregressive Transformers networks) nhÆ° GPT mang Ä‘áº¿n hiá»‡u á»©ng chÃ¹m Ä‘iá»ƒm tá»¥ lÃµi nÃ³n - táº¥t cáº£ má»i Cosine Similarities cá»§a GPT-2 lÆ¡ lá»­ng á»Ÿ Ä‘á»‰nh dÆ° dÆ°Æ¡ng luÃ´n lá»›n hÆ¡n $0$, loanh quanh khoáº£ng $[ 0.3 , 0.8 ]$.

Náº¿u giáº£ tÆ°á»Ÿng ta Ã©p ma tráº­n Word2vec tá»‹nh tiáº¿n xuá»‘ng trá»« Ä‘i $-1$ trá»‹ sá»‘ (Mean Offset subtract 1), chá»‰ sá»‘ Cosine Similarity Ä‘á»™t ngá»™t nháº£y vá»±c thay Ä‘á»•i phÆ°Æ¡ng hÆ°á»›ng Ä‘á»“ thá»‹ toÃ n táº­p. NhÆ°ng tÃ­nh cháº¥t **Há»‡ sá»‘ Pearson ($\rho$) khÃ´ng bao giá» gÃ£y Ä‘á»•**:
$$
\rho = \frac{\text{Cov}(v_{w2v}, v_{gpt2})}{\sigma_{\text{w2v}} \sigma_{\text{gpt2}}}
$$
Luáº­t tÃ­nh hiá»‡p phÆ°Æ¡ng sai chia chuáº©n Ä‘á»™ lá»‡ch $Cov(X,Y)$ tá»± Ä‘á»™ng loáº¡i bá» má»i Ä‘á»™ lá»‡ch trung bÃ¬nh tÃ¢m (global mean offsets shift), khiáº¿n Pearson Correlation chá»‰ xÃ©t dá»±a trÃªn tÃ­nh cháº¥t "*ChÃºng nháº£y nhÃ³t lÃªn vÃ  xuá»‘ng cÃ¹ng má»™t biÃªn Ä‘á»™ hay khÃ´ng*". 

Káº¿t cá»¥c cá»§a Ä‘iá»ƒm $\rho$ tÃ­nh Ä‘Æ°á»£c RSA Score cung cáº¥p má»™t chá»‰ sá»‘ cao áº¥n tÆ°á»£ng, thá»«a nháº­n viá»‡c mÃ¡y há»c dá»± Ä‘oÃ¡n ngÃ´n ngá»¯ GPT-2 trÃªn Transformer hay mÃ´ hÃ¬nh cá»­a sá»• bá»‘i cáº£nh nhá» Continuous Bag-of-Words nhÆ° Word2vec, sá»± kiáº¿n thiáº¿t thÃ´ng triá»‡t cá»§a ngÃ´n ngá»¯ loÃ i ngÆ°á»i á»Ÿ má»©c sÃ¢u nháº¥t trong AI lÃ  tÆ°Æ¡ng Ä‘á»“ng Ä‘Ã¡ng kinh ngáº¡c.

---

## TÃ i liá»‡u tham kháº£o

1. **Abnar, S., et al. (2019).** *Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models.* Proceedings of the 2019 ACL Workshop BlackboxNLP.
2. **Kriegeskorte, N., et al. (2008).** *Representational similarity analysis.* 
3. TÃ i liá»‡u Ä‘Ã o táº¡o nÃ¢ng cao *Investigating embeddings - CodeChallenge Word2vec vs. GPT2*.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [aero_LLM_01_CodeChallenge Cosine similarity (advanced) (part 1).md](aero_LLM_01_CodeChallenge Cosine similarity (advanced) (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_01_CodeChallenge Cosine similarity (advanced) (part 1).md) |
| [aero_LLM_02_CodeChallenge Cosine similarity (advanced) (part 2).md](aero_LLM_02_CodeChallenge Cosine similarity (advanced) (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_02_CodeChallenge Cosine similarity (advanced) (part 2).md) |
| [Theo DÃµi DÃ²ng Cháº£y Cosine Similarity TrÃªn Trá»¥c VÄƒn Báº£n ChuyÃªn Tuáº§n Tá»± (Word Sequences)](aero_LLM_03_CodeChallenge Cosine similarity in word sequences.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_03_CodeChallenge Cosine similarity in word sequences.md) |
| [Nghá»‡ Thuáº­t Váº½ Báº£n Äá»“ Nhiá»‡t Ma Tráº­n NhÃºng Báº±ng CÆ°á»ng Äá»™ Tá»« (Coloring Cosine Similarity)](aero_LLM_04_CodeChallenge Coloring cosine similarity.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_04_CodeChallenge Coloring cosine similarity.md) |
| [áº¢o áº¢nh Cá»§a TrÃ­ Tuá»‡ ToÃ¡n Há»c Trong NgÃ´n Ngá»¯: Sá»©c Máº¡nh Cá»§a Random Embeddings](aero_LLM_05_CodeChallenge Can random embeddings be interpreted.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_05_CodeChallenge Can random embeddings be interpreted.md) |
| [PhÆ°Æ¡ng PhÃ¡p T-SNE VÃ  Thuáº­t ToÃ¡n PhÃ¢n Cá»¥m DBSCAN: Chiáº¿u KhÃ´ng Gian Äa Chiá»u Cho LLMs](aero_LLM_06_T-SNE projection and DBSCAN clustering (theory).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_06_T-SNE projection and DBSCAN clustering (theory).md) |
| [PhÃ¢n Cá»¥m Ngá»¯ NghÄ©a Qua PhÃ©p Chiáº¿u t-SNE & Máº­t Äá»™ DBSCAN (Python)](aero_LLM_07_T-SNE projection and DBSCAN clustering (Python).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_07_T-SNE projection and DBSCAN clustering (Python).md) |
| [ThÃ¡ch Thá»©c Code: TÃ¬m Lá»— Há»•ng PhÃ¢n Cá»¥m Báº±ng Bá»™ Lá»c Báº£ng Chá»¯ CÃ¡i Chá»¯ X](aero_LLM_08_CodeChallenge cluster the x terms.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_08_CodeChallenge cluster the x terms.md) |
| [PhÃ¢n RÃ£ Token, NhÃºng VÃ  PhÃ¢n Cá»¥m Biá»ƒu TÆ°á»£ng Emojis Báº±ng Äá»“ Thá»‹ Máº­t Äá»™](aero_LLM_09_CodeChallenge Tokenize, embed, and cluster happy emojis.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_09_CodeChallenge Tokenize, embed, and cluster happy emojis.md) |
| [PhÃ¢n TÃ­ch RSA (Representational Similarity Analysis) Giá»¯a CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯](aero_LLM_10_RSA (representational similarity analysis).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_10_RSA (representational similarity analysis).md) |
| [PhÃ¢n TÃ­ch Äá»™ Lá»‡ch RSA (Part 1): So SÃ¡nh Sá»± Báº¥t Äá»“ng Giá»¯a KhÃ´ng Gian GloVe 50D vÃ  300D](aero_LLM_11_CodeChallenge Compare embeddings with RSA (part 1).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_11_CodeChallenge Compare embeddings with RSA (part 1).md) |
| [PhÃ¢n TÃ­ch Äá»™ Lá»‡ch RSA (Part 2): Äá»‘i Chiáº¿u TÆ°Æ¡ng Quan Pearson Cho Khoáº£ng CÃ¡ch Cosine](aero_LLM_12_CodeChallenge Compare embeddings with RSA (part 2).md) | [Xem bÃ i viáº¿t â†’](aero_LLM_12_CodeChallenge Compare embeddings with RSA (part 2).md) |
| ğŸ“Œ **[So SÃ¡nh KhÃ´ng Gian NhÃºng: Word2Vec VÃ  GPT-2 Qua PhÃ¢n TÃ­ch RSA](aero_LLM_13_CodeChallenge Word2vec vs. GPT2.md)** | [Xem bÃ i viáº¿t â†’](aero_LLM_13_CodeChallenge Word2vec vs. GPT2.md) |
| [Bá»‘ Cá»¥c Äá»“ Thá»‹ Máº¡ng (Network Graph) ThÃ´ng Qua Ma Tráº­n Cosine Similarity](aero_LLM_14_CodeChallenge Graph representation of cosine similarities.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_14_CodeChallenge Graph representation of cosine similarities.md) |
| [Sá»‘ Há»c Tuyáº¿n TÃ­nh vÃ  RÃºt TrÃ­ch TÆ°Æ¡ng Äá»“ng Giá»¯a CÃ¡c Tá»« NhÃºng (Word Embeddings Analogies)](aero_LLM_15_Embeddings arithmetic and analogies.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_15_Embeddings arithmetic and analogies.md) |
| [Vá»¡ Má»™ng Vá» Sá»‘ Há»c Vector TÆ°Æ¡ng ÄÆ°Æ¡ng (Soft-Coded Analogies) TrÃªn Word2Vec](aero_LLM_16_CodeChallenge soft-coded analogies in word2vec.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_16_CodeChallenge soft-coded analogies in word2vec.md) |
| [Thiáº¿t Láº­p VÃ  Diá»…n Giáº£i Trá»¥c Ngá»¯ NghÄ©a Tuyáº¿n TÃ­nh (Linear Semantic Axes)](aero_LLM_17_Creating and interpreting linear semantic axes.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_17_Creating and interpreting linear semantic axes.md) |
| [Khai ThÃ¡c Thuáº­t ToÃ¡n k-NN Cho TÃ¬m Kiáº¿m Tá»« Äá»“ng NghÄ©a TrÃªn BERT](aero_LLM_18_kNN for synonym-searching in BERT.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_18_kNN for synonym-searching in BERT.md) |
| [Cáº¡nh Tranh TÃ¬m Tá»« Äá»“ng NghÄ©a BERT vs GPT: CÆ¡ Cháº¿ Tokenization Äa KÃ½ Tá»±](aero_LLM_19_CodeChallenge BERT v GPT kNN kompetition.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_19_CodeChallenge BERT v GPT kNN kompetition.md) |
| [Sá»± Dá»‹ch Chuyá»ƒn VÃ  Äá»“ng Tá»“n Biá»ƒu Diá»…n Giá»¯a CÃ¡c KhÃ´ng Gian NhÃºng](aero_LLM_20_Research on translating embeddings spaces.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_20_Research on translating embeddings spaces.md) |
| [PhÃ¢n TÃ­ch ChÃ¹m Quang Phá»• Suy Biáº¿n (Singular Value Spectrum) Cá»§a KhÃ´ng Gian NhÃºng](aero_LLM_21_Singular value spectrum of embeddings submatrices.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_21_Singular value spectrum of embeddings submatrices.md) |
| [Ãnh Xáº¡ SVD CÃ¡c Dáº£i Äiá»ƒm NhÃºng CÃ³ Quan Há»‡ ChÃ©o](aero_LLM_22_CodeChallenge SVD projections of related embeddings.md) | [Xem bÃ i viáº¿t â†’](aero_LLM_22_CodeChallenge SVD projections of related embeddings.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->


<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [14 modify activations](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# CÃ¡c Cháº¿ Äá»™ Sá»­a Äá»•i Hoáº¡t HÃ³a CÆ¡ Há»c (Activation Editing Implementations)

## TÃ³m táº¯t (Abstract)
BÃ¡o cÃ¡o nÃ y táº­p trung vÃ o kÄ© thuáº­t mÃ£ nguá»“n thá»±c hÃ nh Thiáº¿t Láº­p Ma Tráº­n Äiá»u HÆ°á»›ng (Activation Editing) thÃ´ng qua cÃ´ng cá»¥ Forward Hook trÃªn PyTorch. CÃ¡c phÆ°Æ¡ng Ã¡n tiáº¿p cáº­n dao Ä‘á»™ng tá»« can thiá»‡p ghi Ä‘Ã¨ cá»©ng (Hard-coded overwrite) Ä‘áº¿n má»™t há»‡ thá»‘ng Dictionary má»m dáº»o cho phÃ©p ngáº¯t má»Ÿ luá»“ng TÃ­n hiá»‡u tuá»³ Ã½ theo Layer trong quÃ¡ trÃ¬nh Suy luáº­n (Inference). Äáº·c biá»‡t, thÃ´ng qua chuá»—i 6 thÃ­ nghiá»‡m, bÃ i viáº¿t lÃ½ giáº£i vá» giá»›i háº¡n Cáº¯t VÃ¹ng Tensor (Tensor View) cá»§a PyTorch, tá»« Ä‘Ã³ Ä‘á»‹nh hÃ¬nh cÃº phÃ¡p ÄÃ³ng gÃ³i NhÃ¢n Báº£n (`clone()`) vÃ  KhÃ¢u Ná»‘i (`cat()`) nháº±m táº¡o ra MÃ ng Lá»c TÃ­n Hiá»‡u NhÃ¢n Quáº£ khÃ´ng sá»¥p Ä‘á»• bá»™ nhá»›.

---

## 1. Má»Ÿ Äáº§u (Introduction)
NhÆ° Ä‘Ã£ Ä‘á» cáº­p á»Ÿ Pháº§n Dáº«n nháº­p, náº¿u Cáº¥u trÃºc Äá»c Quan sÃ¡t (Observational) chá»‰ cáº§n Ä‘Äƒng kÃ½ Hook Ä‘á»ƒ lÆ°u giÃ¡ trá»‹ $\text{tensor}$, thÃ¬ Can Thiá»‡p NhÃ¢n Quáº£ (Causal Manipulation) Ä‘Ã²i há»i Hook pháº£i TRáº¢ Vá»€ (Return) má»™t Ma tráº­n Output má»›i Ã©p Ä‘Ã¨ lÃªn luá»“ng cháº£y cá»§a Thuáº­t ToÃ¡n. 

CÃº phÃ¡p tá»•ng quÃ¡t cá»§a Pháº«u thuáº­t Forward Hook:
```python
def my_hook(module, inputs, output):

$$
modified_output = output.clone()
$$

    # [Inject, Zero-out, hay Scale tÃ¹y Ã½]
    return modified_output
Khi Pytorch quÃ©t tháº¥y Hook nÃ©m má»™t Object má»›i vá», nÃ³ sáº½ Ã©p Ä‘Ã¨ lÃªn Biáº¿n $\text{output}$ nguyÃªn máº«u náº¿u hai Object nÃ y cÃ³ Dimensions tuyá»‡t Ä‘á»‘i khá»›p nhau. Sá»± Ã©p Ä‘Ã¨ nÃ y má»Ÿ ra quyá»n nÄƒng sinh-sÃ¡t Ä‘á»‘i vá»›i báº¥t ká»³ Token hay Máº¡ch Activation nÃ o.

---

## 2. Tiáº¿t Thiáº¿t Káº¿ ThÃ­ Nghiá»‡m CÆ¡ Há»c (Methodology & Implementations)

### ThÃ­ Nghiá»‡m 1 & 3: Cáº¡m Báº«y Cá»§a Tensor Slices vÃ  Káº¿ Hoáº¡ch .Clone()
Thá»‘ng kÃª CÆ¡ báº£n: Khi TrÃ­ch Xuáº¥t Attention Block $QKV$, ta thÆ°á»ng tÃ¡ch `output` thÃ nh cá»¥m Ä‘á»™c láº­p `[Q, K, V]`.
- **Sai Láº§m á» ThÃ­ Nghiá»‡m 1:** Náº¿u thiáº¿t láº­p: `Q[0, 4, :10] = 0` rá»“i `return torch.cat([Q, K, V], dim=-1)`. TrÃ¬nh biÃªn dá»‹ch sáº½ bÃ¡o lá»—i *VÃ´ hiá»‡u view()* (RuntimeError: modified in-place view). PyTorch báº£o máº­t khÃ´ng cho phÃ©p Sá»­a Ä‘á»•i má»™t VÃ¹ng NhÃ¬n Phá»…u (View) Ä‘ang tham chiáº¿u tá»« Ma tráº­n Cá»‘t lÃµi cá»§a Máº¡ng.
- **Biá»‡n PhÃ¡p Kháº¯c Phá»¥c (ThÃ­ Nghiá»‡m 3):** Pháº£i Táº¡o Báº£n Sao Láº­p TrÃ¬nh (Clone) cho Ma tráº­n. CÃº phÃ¡p: `Q_mod = Q.clone()`. Thao tÃ¡c Zero-out trÃªn `Q_mod` lÃ  hoÃ n toÃ n vÃ´ can vá»›i Bá»™ Ä‘á»‡m Root, vÃ  sau khi Concatenate, hÃ m sáº½ nháº£ ra chuá»—i Ghi-Ä‘Ã¨ an toÃ n. 

### ThÃ­ Nghiá»‡m 2: Trá» Trá»±c Tiáº¿p Theo Má»‘c Dimension (Direct Indexing)
Náº¿u khÃ´ng muá»‘n tÃ¡ch/khÃ¢u nhÆ° trÃªn, ta thao tÃ¡c tháº³ng vÃ o Block Gá»‘c dá»±a trÃªn Ä‘á»™ DÃ i Chiá»u kÃ­ch (Embedding Dimension).
Vá»›i GPT-2 Small, $d\_model = 768$. Ma tráº­n $QKV$ dÃ i $768 \times 3 = 2304$. 
- Äá»ƒ Tá»‹t ngÃ²i $10$ Vector cá»§a lá»›p **Q** cho Token Index sá»‘ $4$: `output[0, 4, 0:10] = 0`
- Äá»ƒ Tá»‹t ngÃ²i $10$ Vector cá»§a lá»›p **K** cho Token Index sá»‘ $4$: `output[0, 4, 768 : 768+10] = 0`
PhÆ°Æ¡ng phÃ¡p nÃ y chá»c ngang vÃ¹ng Core Tensor nÃªn khÃ´ng bá»‹ lá»—i View, nhÆ°ng Ä‘Ã²i há»i pháº£i Hard-code phÃ©p Offset chá»‰ sá»‘ ráº¥t má»‡t má»i.

### ThÃ­ Nghiá»‡m 4 & 5: Biáº¿n Tá»•ng Cá»¥c & Bá»™ Äiá»u Khiá»ƒn CÃ³c (Layer Isolation)
Má»¥c tiÃªu: Äáº·t Hook lÃªn 12 Lá»›p Transformer, Thu hoáº¡ch *ToÃ n bá»™* QKV cá»§a 12 lá»›p, nhÆ°ng **Chá»‰ BÃ³p MÃ©o Duy Nháº¥t Lá»›p thá»© 3**.
- DÃ¹ng `if layer_num == 3: ...` cháº·n cá»•ng Ä‘iá»u tiáº¿t.
- KhÃ´ng gÃ¡n Ã©p giÃ¡ trá»‹ rá»—ng $0$ (Ráº¥t phÃ­ khoa há»c do nhiá»…u báº¥t thÆ°á»ng), mÃ  Khá»Ÿi táº¡o Biáº¿n Dá»¯ liá»‡u ToÃ n Cá»¥c (Global Array `q_to_replace = torch.linspace(-1, 0.7)`). Ta cÃ³ thá»ƒ Ä‘á»•i Biáº¿n Äáº§u VÃ o (Injection Vector) tá»« khÃ´ng gian thÃ­ nghiá»‡m mÃ  khÃ´ng cáº§n rÃ£ Hook ra Ä‘á»‹nh nghÄ©a láº¡i.

### ThÃ­ Nghiá»‡m 6: Báº£ng Äiá»u Phá»‘i Dictionary (The Dictionary Injection Board)
ÄÃ¢y lÃ  Cáº£nh giá»›i Linh hoáº¡t nháº¥t.
- Táº¡o má»™t `replacement_dict = {3: zeros_tensor, 11: specific_vector}`
- Logic HÃ m sáº½ lÃ : `if layer_num in dict.keys(): output = dict[layer_num]`
Vá»›i mÃ´ hÃ¬nh nÃ y, ta cÃ³ thá»ƒ báº­t / táº¯t nhiá»…u tá»«ng pháº§n, Ä‘áº£o Cá»™t tÃ­n hiá»‡u giá»¯a Block $3 \to Block\ 11$ cá»±c ká»³ cÆ¡ Ä‘á»™ng chá»‰ vá»›i 1 thao tÃ¡c GÃ¡n (Assign), táº¡o lá»£i tháº¿ tá»‘c Ä‘á»™ trong cÃ¡c Test Suite Khá»•ng Lá»“.

---

## 3. Lá»i KhuyÃªn (Best Practices)
Kinh nghiá»‡m sÃ¢u sáº¯c vá»›i Causal Manipulation lÃ  "Máº¯c cÃ i In. Ra. In". 
QuÃ¡ trÃ¬nh Forward Pass cháº¡y vá»›i tá»‘c Ä‘á»™ hÃ ng pháº§n nghÃ¬n giÃ¢y vÃ  khÃ´ng cho tÆ°Æ¡ng tÃ¡c Real-time giá»¯a chá»«ng. NÃªn khi má»›i Code Hook:
- HÃ£y BÆ¡m Tá»‹t $0$ (Zero-out check) á»Ÿ má»i lÃºc Ä‘á»ƒ Ä‘o Ä‘áº¿m Äáº§u vÃ o/Äáº§u ra Ä‘Ã£ Khá»›p kÃ­ch thÆ°á»›c hay chÆ°a.
- Lá»£i dá»¥ng HÃ m `print()` Ä‘áº©y Shape vÃ o mÃ n hÃ¬nh Console Ä‘á»ƒ tháº¥y DÃ²ng cháº£y Dá»¯ liá»‡u Ä‘ang cáº¯t á»Ÿ Ä‘Ã¢u.

## TÃ i liá»‡u Tham kháº£o (Citations)
1. 06 Quy trÃ¬nh Thao TÃºng Pytorch Causal Framework - LÆ°á»£c sá»­ tá»« `aero_LLM_02_Activation editing Code implementations.md`. So sÃ¡nh Ä‘á»‘i chá»©ng Lá»—i In-place View Tensor vÃ  giáº£i phÃ¡p Khá»‘i Dictionary.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Dáº«n Nháº­p Vá» Diá»…n Giáº£i CÆ¡ Há»c NhÃ¢n Quáº£ (Causal Mechanistic Interpretability)](aero_llm_01_introduction_to_causal_mech_interp.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_introduction_to_causal_mech_interp.md) |
| ğŸ“Œ **[CÃ¡c Cháº¿ Äá»™ Sá»­a Äá»•i Hoáº¡t HÃ³a CÆ¡ Há»c (Activation Editing Implementations)](aero_llm_02_activation_editing_code_implementations.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_02_activation_editing_code_implementations.md) |
| [Thá»­ thÃ¡ch Láº­p trÃ¬nh: Thay tháº¿ Hoáº¡t hÃ³a Attention, MLP vÃ  Hidden States](aero_llm_03_codechallenge_replacing_attention_mlp_and_hidden_states.md) | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_replacing_attention_mlp_and_hidden_states.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->

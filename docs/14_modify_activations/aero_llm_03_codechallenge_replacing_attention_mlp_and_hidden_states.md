
<!-- Aero-Navigation-Start -->
[ğŸ  Home](../index.md) > [14 modify activations](index.md)

---
### ğŸ§­ Äiá»u hÆ°á»›ng nhanh

- [ğŸ  Cá»•ng tÃ i liá»‡u](../index.md)
- [ğŸ“š Module 01: LLM Course](../01_llm_course/index.md)
- [ğŸ”¢ Module 02: Tokenization](../02_words_to_tokens_to_numbers/index.md)
- [ğŸ—ï¸ Module 04: Build GPT](../04_buildgpt/index.md)
- [ğŸ¯ Module 07: Fine-tuning](../07_fine_tune_pretrained_models/index.md)
- [ğŸ” Module 19: AI Safety](../19_ai_safety/index.md)
- [ğŸ Module 20: Python for AI](../20_python_colab_notebooks/index.md)
---
<!-- Aero-Navigation-End -->
# Thá»­ thÃ¡ch Láº­p trÃ¬nh: Thay tháº¿ Hoáº¡t hÃ³a Attention, MLP vÃ  Hidden States

## TÃ³m táº¯t (Abstract)
Thá»­ thÃ¡ch láº­p trÃ¬nh nÃ y má»Ÿ rá»™ng kháº£ nÄƒng thao tÃºng mÃ´ hÃ¬nh thÃ´ng qua viá»‡c can thiá»‡p sÃ¢u vÃ o cÃ¡c thÃ nh pháº§n ná»™i táº¡i: Attention Heads, MLP Neurons vÃ  Hidden States. NghiÃªn cá»©u thá»±c hiá»‡n bá»‘n bÃ i táº­p thá»±c hÃ nh tá»« viá»‡c triá»‡t tiÃªu (Zero-out) má»™t Attention Head cá»¥ thá»ƒ, bÆ¡m nhiá»…u Gaussian vÃ o cÃ¡c Neuron MLP cÃ³ chá»‰ sá»‘ cháºµn, Ä‘áº¿n viá»‡c thay Ä‘á»•i quy mÃ´ (Scaling) toÃ n bá»™ Hidden States cá»§a má»™t Transformer Block. Káº¿t quáº£ cho tháº¥y sá»± biáº¿n thiÃªn cá»§a phá»• hoáº¡t hÃ³a tÄƒng dáº§n theo Ä‘á»™ sÃ¢u cá»§a mÃ´ hÃ¬nh vÃ  sá»± xuáº¥t hiá»‡n cá»§a cÃ¡c cÆ¡ cháº¿ bÃ¹ trá»« (Compensation) táº¡i lá»›p cuá»‘i cÃ¹ng khi Ä‘á»‘i máº·t vá»›i cÃ¡c can thiá»‡p cá»±c Ä‘oan. BÃ¡o cÃ¡o cÅ©ng lÃ m rÃµ cáº¥u trÃºc dá»¯ liá»‡u Tuple cá»§a Hidden States trong há»‡ sinh thÃ¡i HuggingFace.

---

## 1. Má»Ÿ Äáº§u (Introduction)
Viá»‡c hiá»ƒu rÃµ cÃ¡ch can thiá»‡p vÃ o tá»«ng thÃ nh pháº§n cá»§a mÃ´ hÃ¬nh khÃ´ng chá»‰ giÃºp kiá»ƒm chá»©ng cÃ¡c giáº£ thuyáº¿t nhÃ¢n quáº£ mÃ  cÃ²n hÃ© lá»™ cÃ¡ch thá»©c thÃ´ng tin Ä‘Æ°á»£c truyá»n dáº«n vÃ  biáº¿n Ä‘á»•i qua cÃ¡c táº§ng. Thá»­ thÃ¡ch nÃ y táº­p trung vÃ o ká»¹ nÄƒng láº­p trÃ¬nh Hook nÃ¢ng cao vÃ  kháº£ nÄƒng phÃ¢n tÃ­ch tÃ¡c Ä‘á»™ng háº¡ nguá»“n (Downstream impact). ChÃºng ta sáº½ quan sÃ¡t cÃ¡ch viá»‡c "bÃ³p ngháº¹t" hoáº·c "phÃ³ng Ä‘áº¡i" tÃ­n hiá»‡u táº¡i má»™t táº§ng áº£nh hÆ°á»Ÿng Ä‘áº¿n cÃ¡c táº§ng káº¿ tiáº¿p trong dÃ²ng cháº£y Transformer.

---

## 2. Tiáº¿t Thiáº¿t Láº­p Thá»­ ThÃ¡ch (Methodology & Exercises)

### 2.1. BÃ i táº­p 1: Triá»‡t tiÃªu Attention Head cá»¥ thá»ƒ (Selective Head Ablation)
Má»¥c tiÃªu lÃ  xÃ¡c Ä‘á»‹nh vÃ  Ä‘Æ°a vá» giÃ¡ trá»‹ 0 toÃ n bá»™ ma tráº­n $K$ (Key) cá»§a Attention Head thá»© 3 (Index 2) táº¡i má»™t táº§ng Transformer.
- **TÃ­nh toÃ¡n Index:** Vá»›i $d\_model=768$ vÃ  12 heads, má»—i head cÃ³ $d\_head=64$. Index báº¯t Ä‘áº§u cá»§a Head 3 trong ma tráº­n $K$ rá»i ráº¡c lÃ  $2 \times 64 = 128$. Tuy nhiÃªn, náº¿u thao tÃ¡c trÃªn ma tráº­n $QKV$ gá»™p, ta pháº£i cá»™ng thÃªm $n\_embed$ (Ä‘á»ƒ bá» qua toÃ n bá»™ $Q$).
- **Thá»±c thi:** TÃ¡ch ma tráº­n $QKV \to$ Clone ma tráº­n $K \to$ GÃ¡n 0 cho vÃ¹ng Index tÆ°Æ¡ng á»©ng $\to$ Concatenate láº¡i.

### 2.2. BÃ i táº­p 2: BÆ¡m Nhiá»…u Gaussian vÃ o MLP (MLP Noise Injection)

$$
Thay tháº¿ cÃ¡c Neuron MLP cÃ³ chá»‰ sá»‘ cháºµn (0, 2, 4...) báº±ng nhiá»…u Gaussian cÃ³ trung bÃ¬nh \mu=10.
$$

- **PhÃ¢n tÃ­ch:** ÄÃ¢y lÃ  má»™t can thiá»‡p "phi tá»± nhiÃªn" vÃ¬ phÃ¢n phá»‘i hoáº¡t hÃ³a thá»±c táº¿ cá»§a MLP (sau khi qua GELU) thÆ°á»ng dá»‹ch chuyá»ƒn Ã¢m hoáº·c bá»‹ triá»‡t tiÃªu vá» 0 Ä‘á»ƒ táº¡o tÃ­nh thÆ°a (Sparsity). Viá»‡c bÆ¡m nhiá»…u vá»›i $\mu=10$ táº¡o ra má»™t sá»± lá»‡ch pha phÃ¢n phá»‘i (Out-of-distribution) cá»±c lá»›n, giÃºp quan sÃ¡t rÃµ rá»‡t sá»± há»—n loáº¡n cá»§a tÃ­n hiá»‡u truyá»n Ä‘i.

### 2.3. BÃ i táº­p 3 & 4: Thay Ä‘á»•i quy mÃ´ Hidden States (Hidden State Scaling)
Can thiá»‡p vÃ o Ä‘áº§u ra cuá»‘i cÃ¹ng cá»§a má»™t Transformer Block (trÆ°á»›c khi vÃ o Block káº¿ tiáº¿p).
- **Cáº¥u trÃºc Tuple:** KhÃ¡c vá»›i Attention hay MLP, biáº¿n `output` táº¡i Block Transformer lÃ  má»™t **Tuple**. Pháº§n tá»­ Ä‘áº§u tiÃªn `output[0]` lÃ  Tensor hoáº¡t hÃ³a chÃ­nh, cÃ¡c pháº§n tá»­ sau chá»©a thÃ´ng tin bá»• trá»£ nhÆ° hidden_states hoáº·c attentions (náº¿u Ä‘Æ°á»£c kÃ­ch hoáº¡t).
- **Thá»±c thi:** `modified_hidden = output[0].mul(scaling_factor)`. Sau Ä‘Ã³ pháº£i Ä‘Ã³ng gÃ³i láº¡i vÃ o Tuple trÆ°á»›c khi `return`: `(modified_hidden,) + output[1:]`.
- **Káº¿t quáº£:** Khi giáº£m quy mÃ´ (Scale 0.1) táº¡i Block 9, ta tháº¥y Ä‘á»™ lá»‡ch chuáº©n (variability) sá»¥t giáº£m Ä‘á»™t ngá»™t táº¡i táº§ng Ä‘Ã³ nhÆ°ng báº¯t Ä‘áº§u "há»“i sinh" á»Ÿ cÃ¡c táº§ng sau. Khi phÃ³ng Ä‘áº¡i (Scale 10), tÃ­n hiá»‡u bÃ¹ng ná»• nhÆ°ng lá»›p cuá»‘i cÃ¹ng cá»§a mÃ´ hÃ¬nh cÃ³ xu hÆ°á»›ng nÃ©n láº¡i Ä‘á»ƒ bÃ¹ trá»« (Compensating) trÆ°á»›c khi xuáº¥t ra Embeddings Layer.

---

## 3. KhÃ¡m PhÃ¡ Cáº¥u TrÃºc Tuple Cá»§a HuggingFace
Táº¡i sao Ä‘áº§u ra Transformer láº¡i lÃ  Tuple? 
Thá»±c nghiá»‡m cho tháº¥y khi thiáº¿t láº­p `output_attentions=True`, Tuple nÃ y sáº½ ná»Ÿ rá»™ng tá»« 1 pháº§n tá»­ lÃªn 2 pháº§n tá»­. Pháº§n tá»­ thá»© hai chá»©a ma tráº­n Attention $[Batch, Heads, Seq, Seq]$. Viá»‡c dÃ¹ng Tuple cho phÃ©p mÃ´ hÃ¬nh linh hoáº¡t tráº£ vá» nhiá»u loáº¡i dá»¯ liá»‡u khÃ¡c nhau mÃ  khÃ´ng lÃ m gÃ£y cáº¥u trÃºc Hook Ä‘á»“ng nháº¥t. Tuy nhiÃªn, khuyáº¿n nghá»‹ nghiÃªn cá»©u váº«n lÃ  sá»­ dá»¥ng Hook trá»±c tiáº¿p vÃ o cÃ¡c Sub-layers Ä‘á»ƒ kiá»ƒm soÃ¡t dá»¯ liá»‡u tinh vi hÆ¡n.

---

## 4. Káº¿t Luáº­n
Viá»‡c náº¯m vá»¯ng ká»¹ thuáº­t thao tÃºng Tuple vÃ  Indexing chÃ­nh xÃ¡c trong ma tráº­n QKV lÃ  chÃ¬a khÃ³a Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c thÃ­ nghiá»‡m nhÃ¢n quáº£ phá»©c táº¡p. Thá»­ thÃ¡ch nÃ y kháº³ng Ä‘á»‹nh ráº±ng mÃ´ hÃ¬nh há»c sÃ¢u khÃ´ng pháº£i lÃ  má»™t khá»‘i tÄ©nh, mÃ  lÃ  má»™t thá»±c thá»ƒ Ä‘á»™ng cÃ³ kháº£ nÄƒng pháº£n á»©ng vÃ  bÃ¹ trá»« trÆ°á»›c cÃ¡c tÃ¡c Ä‘á»™ng tá»« bÃªn ngoÃ i, Ä‘áº·c biá»‡t lÃ  á»Ÿ nhá»¯ng táº§ng tiá»‡m cáº­n Ä‘áº§u ra.

---

## TÃ i liá»‡u tham kháº£o (Citations)
1. Thá»­ nghiá»‡m can thiá»‡p Ä‘a thÃ nh pháº§n (Multi-component interference) dá»±a trÃªn `aero_LLM_03_CodeChallenge replacing attention, MLP, and hidden states.md`. Giáº£i thÃ­ch cÆ¡ cháº¿ Ä‘Ã³ng gÃ³i Tuple trong cáº¥u trÃºc `huggingface-transformer`.
<!-- Aero-Footer-Start -->

## ğŸ“„ TÃ i liá»‡u cÃ¹ng chuyÃªn má»¥c
| BÃ i há»c | LiÃªn káº¿t |
| :--- | :--- |
| [Dáº«n Nháº­p Vá» Diá»…n Giáº£i CÆ¡ Há»c NhÃ¢n Quáº£ (Causal Mechanistic Interpretability)](aero_llm_01_introduction_to_causal_mech_interp.md) | [Xem bÃ i viáº¿t â†’](aero_llm_01_introduction_to_causal_mech_interp.md) |
| [CÃ¡c Cháº¿ Äá»™ Sá»­a Äá»•i Hoáº¡t HÃ³a CÆ¡ Há»c (Activation Editing Implementations)](aero_llm_02_activation_editing_code_implementations.md) | [Xem bÃ i viáº¿t â†’](aero_llm_02_activation_editing_code_implementations.md) |
| ğŸ“Œ **[Thá»­ thÃ¡ch Láº­p trÃ¬nh: Thay tháº¿ Hoáº¡t hÃ³a Attention, MLP vÃ  Hidden States](aero_llm_03_codechallenge_replacing_attention_mlp_and_hidden_states.md)** | [Xem bÃ i viáº¿t â†’](aero_llm_03_codechallenge_replacing_attention_mlp_and_hidden_states.md) |

---
## ğŸ¤ LiÃªn há»‡ & ÄÃ³ng gÃ³p
Dá»± Ã¡n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi **Pixibox**. Má»i Ä‘Ã³ng gÃ³p vá» ná»™i dung vÃ  mÃ£ nguá»“n Ä‘á»u Ä‘Æ°á»£c chÃ o Ä‘Ã³n.

> *"Kiáº¿n thá»©c lÃ  Ä‘á»ƒ chia sáº». HÃ£y cÃ¹ng nhau xÃ¢y dá»±ng cá»™ng Ä‘á»“ng AI vá»¯ng máº¡nh!"* ğŸš€

*Cáº­p nháº­t tá»± Ä‘á»™ng bá»Ÿi Aero-Indexer - 2026*
<!-- Aero-Footer-End -->
